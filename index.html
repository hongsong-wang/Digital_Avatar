<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2508.08891.pdf' target='_blank'>https://arxiv.org/pdf/2508.08891.pdf</a></span>   <span><a href='https://github.com/deepreasonings/WholeBodyBenchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyi Wang, Yifan Yang, Jun Pei, Lijie Xia, Jianpo Liu, Xiaobing Yuan, Xinhan Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08891">Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation of Whole-body Talking Avatar Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic, fully animatable whole-body avatars from a single portrait is challenging due to limitations in capturing subtle expressions, body movements, and dynamic backgrounds. Current evaluation datasets and metrics fall short in addressing these complexities. To bridge this gap, we introduce the Whole-Body Benchmark Dataset (WB-DH), an open-source, multi-modal benchmark designed for evaluating whole-body animatable avatar generation. Key features include: (1) detailed multi-modal annotations for fine-grained guidance, (2) a versatile evaluation framework, and (3) public access to the dataset and tools at https://github.com/deepreasonings/WholeBodyBenchmark.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2508.06388.pdf' target='_blank'>https://arxiv.org/pdf/2508.06388.pdf</a></span>   <span><a href='https://github.com/LanlanQiu/ChatAnime' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06388">LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at https://github.com/LanlanQiu/ChatAnime.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2507.20987.pdf' target='_blank'>https://arxiv.org/pdf/2507.20987.pdf</a></span>   <span><a href='https://github.com/deepreasonings/WholeBodyBenchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhan Di, Kristin Qi, Pengqian Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20987">JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2507.13648.pdf' target='_blank'>https://arxiv.org/pdf/2507.13648.pdf</a></span>   <span><a href='https://github.com/seungjun-moon/epsilon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjun Moon, Sangjoon Yu, Gyeong-Moon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13648">EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of neural radiance fields (NeRF) has paved the way to generate animatable human avatars from a monocular video. However, the sole usage of NeRF suffers from a lack of details, which results in the emergence of hybrid representation that utilizes SMPL-based mesh together with NeRF representation. While hybrid-based models show photo-realistic human avatar generation qualities, they suffer from extremely slow inference due to their deformation scheme: to be aligned with the mesh, hybrid-based models use the deformation based on SMPL skinning weights, which needs high computational costs on each sampled point. We observe that since most of the sampled points are located in empty space, they do not affect the generation quality but result in inference latency with deformation. In light of this observation, we propose EPSilon, a hybrid-based 3D avatar generation scheme with novel efficient point sampling strategies that boost both training and inference. In EPSilon, we propose two methods to omit empty points at rendering; empty ray omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that progress through the empty space. Then, EIO narrows down the sampling interval on the ray, which wipes out the region not occupied by either clothes or mesh. The delicate sampling scheme of EPSilon enables not only great computational cost reduction during deformation but also the designation of the important regions to be sampled, which enables a single-stage NeRF structure without hierarchical sampling. Compared to existing methods, EPSilon maintains the generation quality while using only 3.9% of sampled points and achieves around 20 times faster inference, together with 4 times faster training convergence. We provide video results on https://github.com/seungjun-moon/epsilon.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2507.07591.pdf' target='_blank'>https://arxiv.org/pdf/2507.07591.pdf</a></span>   <span><a href='https://github.com/sunkymepro/StableHairV2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuiyuan Sun, Yuxuan Zhang, Jichao Zhang, Jiaming Liu, Wei Wang, Niculae Sebe, Yao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07591">Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion-based methods have shown impressive capabilities in capturing diverse and complex hairstyles, their ability to generate consistent and high-quality multi-view outputs -- crucial for real-world applications such as digital humans and virtual avatars -- remains underexplored. In this paper, we propose Stable-Hair v2, a novel diffusion-based multi-view hair transfer framework. To the best of our knowledge, this is the first work to leverage multi-view diffusion models for robust, high-fidelity, and view-consistent hair transfer across multiple perspectives. We introduce a comprehensive multi-view training data generation pipeline comprising a diffusion-based Bald Converter, a data-augment inpainting model, and a face-finetuned multi-view diffusion model to generate high-quality triplet data, including bald images, reference hairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer model integrates polar-azimuth embeddings for pose conditioning and temporal attention layers to ensure smooth transitions between views. To optimize this model, we design a novel multi-stage training strategy consisting of pose-controllable latent IdentityNet training, hair extractor training, and temporal attention training. Extensive experiments demonstrate that our method accurately transfers detailed and realistic hairstyles to source subjects while achieving seamless and consistent results across views, significantly outperforming existing methods and establishing a new benchmark in multi-view hair transfer. Code is publicly available at https://github.com/sunkymepro/StableHairV2.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2507.02900.pdf' target='_blank'>https://arxiv.org/pdf/2507.02900.pdf</a></span>   <span><a href='https://github.com/VineetKumarRakesh/thg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineet Kumar Rakesh, Soumya Mazumdar, Research Pratim Maity, Sarbajit Pal, Amitabha Das, Tapas Samanta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02900">Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2507.02803.pdf' target='_blank'>https://arxiv.org/pdf/2507.02803.pdf</a></span>   <span><a href='https://github.com/gserifi/HyperGaussians' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gent Serifi, Marcel C. BÃ¼hler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02803">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2507.00792.pdf' target='_blank'>https://arxiv.org/pdf/2507.00792.pdf</a></span>   <span><a href='https://github.com/hvoss-techfak/JAX-IK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendric Voss, Stefan Kopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00792">JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/JAX-IK
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2505.05022.pdf' target='_blank'>https://arxiv.org/pdf/2505.05022.pdf</a></span>   <span><a href='https://github.com/TingtingLiao/soap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Liao, Yujian Zheng, Adilbek Karmanov, Liwen Hu, Leyang Jin, Yuliang Xiu, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05022">SOAP: Style-Omniscient Animatable Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at https://github.com/TingtingLiao/soap.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2504.12909.pdf' target='_blank'>https://arxiv.org/pdf/2504.12909.pdf</a></span>   <span><a href='https://github.com/1231234zhan/mmlphuman' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youyi Zhan, Tianjia Shao, Yin Yang, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12909">Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many works have succeeded in reconstructing Gaussian human avatars from multi-view videos. However, they either struggle to capture pose-dependent appearance details with a single MLP, or rely on a computationally intensive neural network to reconstruct high-fidelity appearance but with rendering performance degraded to non-real-time. We propose a novel Gaussian human avatar representation that can reconstruct high-fidelity pose-dependence appearance with details and meanwhile can be rendered in real time. Our Gaussian avatar is empowered by spatially distributed MLPs which are explicitly located on different positions on human body. The parameters stored in each Gaussian are obtained by interpolating from the outputs of its nearby MLPs based on their distances. To avoid undesired smooth Gaussian property changing during interpolation, for each Gaussian we define a set of Gaussian offset basis, and a linear combination of basis represents the Gaussian property offsets relative to the neutral properties. Then we propose to let the MLPs output a set of coefficients corresponding to the basis. In this way, although Gaussian coefficients are derived from interpolation and change smoothly, the Gaussian offset basis is learned freely without constraints. The smoothly varying coefficients combined with freely learned basis can still produce distinctly different Gaussian property offsets, allowing the ability to learn high-frequency spatial signals. We further use control points to constrain the Gaussians distributed on a surface layer rather than allowing them to be irregularly distributed inside the body, to help the human avatar generalize better when animated under novel poses. Compared to the state-of-the-art method, our method achieves better appearance quality with finer details while the rendering speed is significantly faster under novel views and novel poses.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2503.19207.pdf' target='_blank'>https://arxiv.org/pdf/2503.19207.pdf</a></span>   <span><a href='https://github.com/rongakowang/FRESA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rong Wang, Fabian Prada, Ziyan Wang, Zhongshi Jiang, Chengxiang Yin, Junxuan Li, Shunsuke Saito, Igor Santesteban, Javier Romero, Rohan Joshi, Hongdong Li, Jason Saragih, Yaser Sheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19207">FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2503.18665.pdf' target='_blank'>https://arxiv.org/pdf/2503.18665.pdf</a></span>   <span><a href='https://github.com/antgroup/Similar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18665">Boosting Virtual Agent Learning and Reasoning: A Step-Wise, Multi-Dimensional, and Generalist Reward Model with Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of Generalist Virtual Agents (GVAs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The project is available at https://github.com/antgroup/Similar.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2503.15851.pdf' target='_blank'>https://arxiv.org/pdf/2503.15851.pdf</a></span>   <span><a href='https://github.com/ZhenglinZhou/Zero-1-to-A' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglin Zhou, Fan Ma, Hehe Fan, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15851">Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2503.12886.pdf' target='_blank'>https://arxiv.org/pdf/2503.12886.pdf</a></span>   <span><a href='https://github.com/gapszju/RGBAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linzhou Li, Yumeng Li, Yanlin Weng, Youyi Zheng, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12886">RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for reconstructing photorealistic, animatable head avatars at speeds sufficient for on-the-fly reconstruction. Unlike prior approaches that utilize linear bases from 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps tracked 3DMM parameters into reduced blendshape weights with an MLP, leading to a compact set of blendshape bases. The learned compact base composition effectively captures essential facial details for specific individuals, and does not rely on the fixed base composition weights of 3DMM, leading to enhanced reconstruction quality and higher efficiency. To further expedite the reconstruction process, we develop a novel color initialization estimation method and a batch-parallel Gaussian rasterization process, achieving state-of-the-art quality with training throughput of about 630 images per second. Moreover, we propose a local-global sampling strategy that enables direct on-the-fly reconstruction, immediately reconstructing the model as video streams in real time while achieving quality comparable to offline settings. Our source code is available at https://github.com/gapszju/RGBAvatar.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2503.06154.pdf' target='_blank'>https://arxiv.org/pdf/2503.06154.pdf</a></span>   <span><a href='https://github.com/wang-zidu/SRM-Hair' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zidu Wang, Jiankuo Zhao, Miao Xu, Xiangyu Zhu, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06154">SRM-Hair: Single Image Head Mesh Reconstruction via 3D Morphable Hair</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Morphable Models (3DMMs) have played a pivotal role as a fundamental representation or initialization for 3D avatar animation and reconstruction. However, extending 3DMMs to hair remains challenging due to the difficulty of enforcing vertex-level consistent semantic meaning across hair shapes. This paper introduces a novel method, Semantic-consistent Ray Modeling of Hair (SRM-Hair), for making 3D hair morphable and controlled by coefficients. The key contribution lies in semantic-consistent ray modeling, which extracts ordered hair surface vertices and exhibits notable properties such as additivity for hairstyle fusion, adaptability, flipping, and thickness modification. We collect a dataset of over 250 high-fidelity real hair scans paired with 3D face data to serve as a prior for the 3D morphable hair. Based on this, SRM-Hair can reconstruct a hair mesh combined with a 3D head from a single image. Note that SRM-Hair produces an independent hair mesh, facilitating applications in virtual avatar creation, realistic animation, and high-fidelity hair rendering. Both quantitative and qualitative experiments demonstrate that SRM-Hair achieves state-of-the-art performance in 3D mesh reconstruction. Our project is available at https://github.com/wang-zidu/SRM-Hair
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2502.17796.pdf' target='_blank'>https://arxiv.org/pdf/2502.17796.pdf</a></span>   <span><a href='https://github.com/aigc3d/LAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yisheng He, Xiaodong Gu, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17796">LAM: Large Avatar Model for One-shot Animatable Gaussian Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. Our experimental results demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks. Our code and video are available at https://aigc3d.github.io/projects/LAM/
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2502.02465.pdf' target='_blank'>https://arxiv.org/pdf/2502.02465.pdf</a></span>   <span><a href='https://github.com/weimengting/RigFace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengting Wei, Tuomas Varanka, Yante Li, Xingxun Jiang, Huai-Qian Khor, Guoying Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02465">Towards Consistent and Controllable Image Synthesis for Face Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face editing methods, essential for tasks like virtual avatars, digital human synthesis and identity preservation, have traditionally been built upon GAN-based techniques, while recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in controlling specific attributes and preserving the consistency of other unchanged attributes especially the identity characteristics. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion (SD) models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involves the combinations of target background, identity and face attributes aimed to edit. We strive to sufficiently disentangle the control of these factors to enable consistency of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Attribute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) A high-consistency FaceFusion method that transfers identity features from the Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models. Code is publicly available at https://github.com/weimengting/RigFace.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2412.19149.pdf' target='_blank'>https://arxiv.org/pdf/2412.19149.pdf</a></span>   <span><a href='https://github.com/liguohao96/EGG3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19149">Generating Editable Head Avatars with 3D Gaussian GANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at https://github.com/liguohao96/EGG3D.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2412.13983.pdf' target='_blank'>https://arxiv.org/pdf/2412.13983.pdf</a></span>   <span><a href='https://github.com/ucwxb/GraphAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobao Wei, Peng Chen, Ming Lu, Hui Chen, Feng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13983">GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size. The code will be released at: https://github.com/ucwxb/GraphAvatar
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2412.11599.pdf' target='_blank'>https://arxiv.org/pdf/2412.11599.pdf</a></span>   <span><a href='https://github.com/silence-tang/GaussianActor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Tang, Hongyu Yang, Hanchen Zhang, Jiaxin Chen, Di Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11599">3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic Gaussian Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in neural implicit representations and differentiable rendering have markedly improved the ability to learn animatable 3D avatars from sparse multi-view RGB videos. However, current methods that map observation space to canonical space often face challenges in capturing pose-dependent details and generalizing to novel poses. While diffusion models have demonstrated remarkable zero-shot capabilities in 2D image generation, their potential for creating animatable 3D avatars from 2D inputs remains underexplored. In this work, we introduce 3D$^2$-Actor, a novel approach featuring a pose-conditioned 3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D rectifying steps. The 2D denoiser, guided by pose cues, generates detailed multi-view images that provide the rich feature set necessary for high-fidelity 3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D rectifier renders images with enhanced 3D consistency through a two-stage projection strategy and a novel local coordinate representation. Additionally, we propose an innovative sampling strategy to ensure smooth temporal continuity across frames in video synthesis. Our method effectively addresses the limitations of traditional numerical solutions in handling ill-posed mappings, producing realistic and animatable 3D human avatars. Experimental results demonstrate that 3D$^2$-Actor excels in high-fidelity avatar modeling and robustly generalizes to novel poses. Code is available at: https://github.com/silence-tang/GaussianActor.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2412.04955.pdf' target='_blank'>https://arxiv.org/pdf/2412.04955.pdf</a></span>   <span><a href='https://github.com/ChenVoid/MGA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04955">MixedGaussianAvatar: Realistically and Geometrically Accurate Head Avatar via Mixed 2D-3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: https://github.com/ChenVoid/MGA/.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2411.16758.pdf' target='_blank'>https://arxiv.org/pdf/2411.16758.pdf</a></span>   <span><a href='https://github.com/MyNiuuu/BAGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyao Niu, Yifan Zhan, Qingtian Zhu, Zhuoxiao Li, Wei Wang, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16758">Bundle Adjusted Gaussian Avatars Deblurring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of 3D human avatars from multi-view videos represents a significant yet challenging task in the field. Recent advancements, including 3D Gaussian Splattings (3DGS), have markedly progressed this domain. Nonetheless, existing techniques necessitate the use of high-quality sharp images, which are often impractical to obtain in real-world settings due to variations in human motion speed and intensity. In this study, we attempt to explore deriving sharp intrinsic 3D human Gaussian avatars from blurry video footage in an end-to-end manner. Our approach encompasses a 3D-aware, physics-oriented model of blur formation attributable to human movement, coupled with a 3D human motion model to clarify ambiguities found in motion-induced blurry images. This methodology facilitates the concurrent learning of avatar model parameters and the refinement of sub-frame motion parameters from a coarse initialization. We have established benchmarks for this task through a synthetic dataset derived from existing multi-view captures, alongside a real-captured dataset acquired through a 360-degree synchronous hybrid-exposure camera system. Comprehensive evaluations demonstrate that our model surpasses existing baselines.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2411.09066.pdf' target='_blank'>https://arxiv.org/pdf/2411.09066.pdf</a></span>   <span><a href='https://github.com/microsoft/P.910' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ross Cutler, Babak Naderi, Vishak Gopal, Dharmendar Palle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09066">A multidimensional measurement of photorealistic avatar quality of experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic avatars are human avatars that look, move, and talk like real people. The performance of photorealistic avatars has significantly improved recently based on objective metrics such as PSNR, SSIM, LPIPS, FID, and FVD. However, recent photorealistic avatar publications do not provide subjective tests of the avatars to measure human usability factors. We provide an open source test framework to subjectively measure photorealistic avatar performance in ten dimensions: realism, trust, comfortableness using, comfortableness interacting with, appropriateness for work, creepiness, formality, affinity, resemblance to the person, and emotion accuracy. Using telecommunication scenarios, we show that the correlation of nine of these subjective metrics with PSNR, SSIM, LPIPS, FID, and FVD is weak, and moderate for emotion accuracy. The crowdsourced subjective test framework is highly reproducible and accurate when compared to a panel of experts. We analyze a wide range of avatars from photorealistic to cartoon-like and show that some photorealistic avatars are approaching real video performance based on these dimensions. We also find that for avatars above a certain level of realism, eight of these measured dimensions are strongly correlated. This means that avatars that are not as realistic as real video will have lower trust, comfortableness using, comfortableness interacting with, appropriateness for work, formality, and affinity, and higher creepiness compared to real video. In addition, because there is a strong linear relationship between avatar affinity and realism, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario. We suggest several extensions of this test framework for future work and discuss design implications for telecommunication systems. The test framework is available at https://github.com/microsoft/P.910.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2411.04249.pdf' target='_blank'>https://arxiv.org/pdf/2411.04249.pdf</a></span>   <span><a href='https://github.com/sidsunny/pocoloco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharth Seth, Rishabh Dabral, Diogo Luvizon, Marc Habermann, Ming-Hsuan Yang, Christian Theobalt, Adam Kortylewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04249">PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling a human avatar that can plausibly deform to articulations is an active area of research. We present PocoLoco -- the first template-free, point-based, pose-conditioned generative model for 3D humans in loose clothing. We motivate our work by noting that most methods require a parametric model of the human body to ground pose-dependent deformations. Consequently, they are restricted to modeling clothing that is topologically similar to the naked body and do not extend well to loose clothing. The few methods that attempt to model loose clothing typically require either canonicalization or a UV-parameterization and need to address the challenging problem of explicitly estimating correspondences for the deforming clothes. In this work, we formulate avatar clothing deformation as a conditional point-cloud generation task within the denoising diffusion framework. Crucially, our framework operates directly on unordered point clouds, eliminating the need for a parametric model or a clothing template. This also enables a variety of practical applications, such as point-cloud completion and pose-based editing -- important features for virtual human animation. As current datasets for human avatars in loose clothing are far too small for training diffusion models, we release a dataset of two subjects performing various poses in loose clothing with a total of 75K point clouds. By contributing towards tackling the challenging task of effectively modeling loose clothing and expanding the available data for training these models, we aim to set the stage for further innovation in digital humans. The source code is available at https://github.com/sidsunny/pocoloco .
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2410.08840.pdf' target='_blank'>https://arxiv.org/pdf/2410.08840.pdf</a></span>   <span><a href='https://github.com/XuanHuang0/GuassianHand' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Huang, Hanhui Li, Wanquan Liu, Xiaodan Liang, Yiqiang Yan, Yuhao Cheng, Chengqiang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08840">Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Project Page: \url{https://github.com/XuanHuang0/GuassianHand}.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2410.07971.pdf' target='_blank'>https://arxiv.org/pdf/2410.07971.pdf</a></span>   <span><a href='https://github.com/xg-chu/GAGAvatar,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xg-chu/GAGAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuangeng Chu, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07971">Generalizable and Animatable Gaussian Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Generalizable and Animatable Gaussian head Avatar (GAGAvatar) for one-shot animatable head avatar reconstruction. Existing methods rely on neural radiance fields, leading to heavy rendering consumption and low reenactment speeds. To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass. The key innovation of our work is the proposed dual-lifting method, which produces high-fidelity 3D Gaussians that capture identity and facial details. Additionally, we leverage global image features and the 3D morphable model to construct 3D Gaussians for controlling expressions. After training, our model can reconstruct unseen identities without specific optimizations and perform reenactment rendering at real-time speeds. Experiments show that our method exhibits superior performance compared to previous methods in terms of reconstruction quality and expression accuracy. We believe our method can establish new benchmarks for future research and advance applications of digital avatars. Code and demos are available https://github.com/xg-chu/GAGAvatar.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2409.18057.pdf' target='_blank'>https://arxiv.org/pdf/2409.18057.pdf</a></span>   <span><a href='https://github.com/MingSun-Tse/LightAvatar-TensorFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18057">LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have shown that neural radiance fields (NeRFs) on top of parametric models have reached SOTA quality to build photorealistic head avatars from a monocular video. However, one major limitation of the NeRF-based avatars is the slow rendering speed due to the dense point sampling of NeRF, preventing them from broader utility on resource-constrained devices. We introduce LightAvatar, the first head avatar model based on neural light fields (NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose via a single network forward pass, without using mesh or volume rendering. The proposed approach, while being conceptually appealing, poses a significant challenge towards real-time efficiency and training stability. To resolve them, we introduce dedicated network designs to obtain proper representations for the NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a distillation-based training strategy that uses a pretrained avatar model as teacher to synthesize abundant pseudo data for training. A warping field network is introduced to correct the fitting error in the real data so that the model can learn better. Extensive experiments suggest that our method can achieve new SOTA image quality quantitatively or qualitatively, while being significantly faster than the counterparts, reporting 174.1 FPS (512x512 resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2409.01502.pdf' target='_blank'>https://arxiv.org/pdf/2409.01502.pdf</a></span>   <span><a href='https://github.com/zshyang/amg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangsihao Yang, Mengyi Shan, Mohammad Farazi, Wenhui Zhu, Yanxi Chen, Xuanzhao Dong, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01502">AMG: Avatar Motion Guided Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation task has gained significant attention with the advancement of deep generative models. Generating realistic videos with human movements is challenging in nature, due to the intricacies of human body topology and sensitivity to visual artifacts. The extensively studied 2D media generation methods take advantage of massive human media datasets, but struggle with 3D-aware control; whereas 3D avatar-based approaches, while offering more freedom in control, lack photorealism and cannot be harmonized seamlessly with background scene. We propose AMG, a method that combines the 2D photorealism and 3D controllability by conditioning video diffusion models on controlled rendering of 3D avatars. We additionally introduce a novel data processing pipeline that reconstructs and renders human avatar movements from dynamic camera videos. AMG is the first method that enables multi-person diffusion video generation with precise control over camera positions, human motions, and background style. We also demonstrate through extensive evaluation that it outperforms existing human video generation methods conditioned on pose sequences or driving videos in terms of realism and adaptability.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2408.15777.pdf' target='_blank'>https://arxiv.org/pdf/2408.15777.pdf</a></span>   <span><a href='https://github.com/wangyanckxx/SurveyFER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Wang, Shaoqi Yan, Yang Liu, Wei Song, Jing Liu, Yang Chang, Xinji Mai, Xiping Hu, Wenqiang Zhang, Zhongxue Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15777">A Survey on Facial Expression Recognition of Static and Dynamic Emotions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expression recognition (FER) aims to analyze emotional states from static images and dynamic sequences, which is pivotal in enhancing anthropomorphic communication among humans, robots, and digital avatars by leveraging AI technologies. As the FER field evolves from controlled laboratory environments to more complex in-the-wild scenarios, advanced methods have been rapidly developed and new challenges and apporaches are encounted, which are not well addressed in existing reviews of FER. This paper offers a comprehensive survey of both image-based static FER (SFER) and video-based dynamic FER (DFER) methods, analyzing from model-oriented development to challenge-focused categorization. We begin with a critical comparison of recent reviews, an introduction to common datasets and evaluation criteria, and an in-depth workflow on FER to establish a robust research foundation. We then systematically review representative approaches addressing eight main challenges in SFER (such as expression disturbance, uncertainties, compound emotions, and cross-domain inconsistency) as well as seven main challenges in DFER (such as key frame sampling, expression intensity variations, and cross-modal alignment). Additionally, we analyze recent advancements, benchmark performances, major applications, and ethical considerations. Finally, we propose five promising future directions and development trends to guide ongoing research. The project page for this paper can be found at https://github.com/wangyanckxx/SurveyFER.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2408.01218.pdf' target='_blank'>https://arxiv.org/pdf/2408.01218.pdf</a></span>   <span><a href='https://github.com/wang-zidu/S2TD-Face' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zidu Wang, Xiangyu Zhu, Jiang Yu, Tianshuo Zhang, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01218">S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from a Single Sketch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D textured face reconstruction from sketches applicable in many scenarios such as animation, 3D avatars, artistic design, missing people search, etc., is a highly promising but underdeveloped research topic. On the one hand, the stylistic diversity of sketches leads to existing sketch-to-3D-face methods only being able to handle pose-limited and realistically shaded sketches. On the other hand, texture plays a vital role in representing facial appearance, yet sketches lack this information, necessitating additional texture control in the reconstruction process. This paper proposes a novel method for reconstructing controllable textured and detailed 3D faces from sketches, named S2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework that directly reconstructs detailed geometry from the input sketch. To keep geometry consistent with the delicate strokes of the sketch, we propose a novel sketch-to-geometry loss that ensures the reconstruction accurately fits the input features like dimples and wrinkles. Our training strategies do not rely on hard-to-obtain 3D face scanning data or labor-intensive hand-drawn sketches. Furthermore, S2TD-Face introduces a texture control module utilizing text prompts to select the most suitable textures from a library and seamlessly integrate them into the geometry, resulting in a 3D detailed face with controllable texture. S2TD-Face surpasses existing state-of-the-art methods in extensive quantitative and qualitative experiments. Our project is available at https://github.com/wang-zidu/S2TD-Face .
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2407.10707.pdf' target='_blank'>https://arxiv.org/pdf/2407.10707.pdf</a></span>   <span><a href='https://github.com/1231234zhan/InteractRAGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youyi Zhan, Tianjia Shao, He Wang, Yin Yang, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10707">Interactive Rendering of Relightable and Animatable Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating relightable and animatable avatars from multi-view or monocular videos is a challenging task for digital human creation and virtual reality applications. Previous methods rely on neural radiance fields or ray tracing, resulting in slow training and rendering processes. By utilizing Gaussian Splatting, we propose a simple and efficient method to decouple body materials and lighting from sparse-view or monocular avatar videos, so that the avatar can be rendered simultaneously under novel viewpoints, poses, and lightings at interactive frame rates (6.9 fps). Specifically, we first obtain the canonical body mesh using a signed distance function and assign attributes to each mesh vertex. The Gaussians in the canonical space then interpolate from nearby body mesh vertices to obtain the attributes. We subsequently deform the Gaussians to the posed space using forward skinning, and combine the learnable environment light with the Gaussian attributes for shading computation. To achieve fast shadow modeling, we rasterize the posed body mesh from dense viewpoints to obtain the visibility. Our approach is not only simple but also fast enough to allow interactive rendering of avatar animation under environmental light changes. Experiments demonstrate that, compared to previous works, our method can render higher quality results at a faster speed on both synthetic and real datasets.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2407.04345.pdf' target='_blank'>https://arxiv.org/pdf/2407.04345.pdf</a></span>   <span><a href='https://github.com/jsshin98/CanonicalFusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jisu Shin, Junmyeong Lee, Seongmin Lee, Min-Gyu Park, Ju-Mi Kang, Ju Hong Yoon, Hae-Gon Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04345">CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for reconstructing animatable human avatars from multiple images, termed CanonicalFusion. Our central concept involves integrating individual reconstruction results into the canonical space. To be specific, we first predict Linear Blend Skinning (LBS) weight maps and depth maps using a shared-encoder-dual-decoder network, enabling direct canonicalization of the 3D mesh from the predicted depth maps. Here, instead of predicting high-dimensional skinning weights, we infer compressed skinning weights, i.e., 3-dimensional vector, with the aid of pre-trained MLP networks. We also introduce a forward skinning-based differentiable rendering scheme to merge the reconstructed results from multiple images. This scheme refines the initial mesh by reposing the canonical mesh via the forward skinning and by minimizing photometric and geometric errors between the rendered and the predicted results. Our optimization scheme considers the position and color of vertices as well as the joint angles for each image, thereby mitigating the negative effects of pose errors. We conduct extensive experiments to demonstrate the effectiveness of our method and compare our CanonicalFusion with state-of-the-art methods. Our source codes are available at https://github.com/jsshin98/CanonicalFusion.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2405.14869.pdf' target='_blank'>https://arxiv.org/pdf/2405.14869.pdf</a></span>   <span><a href='https://github.com/YuliangXiu/PuzzleAvatar,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14869">PuzzleAvatar: Assembling 3D Avatars from Personal Albums</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating personalized 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if a user could just upload their personal "OOTD" (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel "Album2Human" task by developing PuzzleAvatar, a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, while bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into (separate) learned tokens and instilling these cues into the VLM. In effect, we exploit the learned tokens as "puzzle pieces" from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total of nearly 1K OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and strong robustness. Our code and data are publicly available for research purpose at https://puzzleavatar.is.tue.mpg.de/
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2403.16510.pdf' target='_blank'>https://arxiv.org/pdf/2403.16510.pdf</a></span>   <span><a href='https://github.com/ICTMCG/Make-Your-Anchor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16510">Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{https://github.com/ICTMCG/Make-Your-Anchor}.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2403.10335.pdf' target='_blank'>https://arxiv.org/pdf/2403.10335.pdf</a></span>   <span><a href='https://github.com/iSEE-Laboratory/NECA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10335">NECA: Neural Customizable Human Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at https://github.com/iSEE-Laboratory/NECA.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2403.05087.pdf' target='_blank'>https://arxiv.org/pdf/2403.05087.pdf</a></span>   <span><a href='https://github.com/initialneil/SplattingAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05087">SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2402.18180.pdf' target='_blank'>https://arxiv.org/pdf/2402.18180.pdf</a></span>   <span><a href='https://github.com/hasakiXie123/Human-Simulacra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuejie Xie, Qiming Feng, Tianqi Zhang, Qingqiu Li, Linyi Yang, Yuejie Zhang, Rui Feng, Liang He, Shang Gao, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18180">Human Simulacra: Benchmarking the Personification of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations. Our code and dataset are available at: https://github.com/hasakiXie123/Human-Simulacra.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2402.06149.pdf' target='_blank'>https://arxiv.org/pdf/2402.06149.pdf</a></span>   <span><a href='https://github.com/ZhenglinZhou/HeadStudio' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06149">HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising results achieved with 2D diffusion priors, current methods struggle to create high-quality and consistent animated avatars efficiently. Previous animatable head models like FLAME have difficulty in accurately representing detailed texture and geometry. Additionally, high-quality 3D static representations face challenges in semantically driving with dynamic priors. In this paper, we introduce \textbf{HeadStudio}, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animatable avatars from text prompts. Firstly, we associate 3D Gaussians with animatable head prior model, facilitating semantic animation on high-quality 3D representations. To ensure consistent animation, we further enhance the optimization from initialization, distillation, and regularization to jointly learn the shape, texture, and animation. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. Moreover, These avatars can be smoothly driven by real-world speech and video. We hope that HeadStudio can enhance digital avatar creation and gain popularity in the community. Code is at: https://github.com/ZhenglinZhou/HeadStudio.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2401.12133.pdf' target='_blank'>https://arxiv.org/pdf/2401.12133.pdf</a></span>   <span><a href='https://github.com/KindOPSTAR/VRMN-bD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>He Zhang, Xinyang Li, Yuanxi Sun, Xinyi Fu, Christine Qiu, John M. Carroll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12133">VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2401.10215.pdf' target='_blank'>https://arxiv.org/pdf/2401.10215.pdf</a></span>   <span><a href='https://github.com/xg-chu/GPAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10215">GPAvatar: Generalizable and Precise Head Avatar from Image(s)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control expressions and postures. Existing methods, categorized into 2D-based warping, mesh-based, and neural rendering approaches, present challenges in maintaining multi-view consistency, incorporating non-facial information, and generalizing to new identities. In this paper, we propose a framework named GPAvatar that reconstructs 3D head avatars from one or several images in a single forward pass. The key idea of this work is to introduce a dynamic point-based expression field driven by a point cloud to precisely and effectively capture expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion module in the tri-planes canonical field to leverage information from multiple input images. The proposed method achieves faithful identity reconstruction, precise expression control, and multi-view consistency, demonstrating promising results for free-viewpoint rendering and novel view synthesis.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2401.04730.pdf' target='_blank'>https://arxiv.org/pdf/2401.04730.pdf</a></span>   <span><a href='https://github.com/FangyunWei/SLRT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronglai Zuo, Fangyun Wei, Zenggui Chen, Brian Mak, Jiaolong Yang, Xin Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04730">A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of this paper is to develop a functional system for translating spoken languages into sign languages, referred to as Spoken2Sign translation. The Spoken2Sign task is orthogonal and complementary to traditional sign language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign translation, we present a simple baseline consisting of three steps: 1) creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2) estimating a 3D sign for each sign video in the dictionary; 3) training a Spoken2Sign model, which is composed of a Text2Gloss translator, a sign connector, and a rendering module, with the aid of the yielded gloss-3D sign dictionary. The translation results are then displayed through a sign avatar. As far as we know, we are the first to present the Spoken2Sign task in an output format of 3D signs. In addition to its capability of Spoken2Sign translation, we also demonstrate that two by-products of our approach-3D keypoint augmentation and multi-view understanding-can assist in keypoint-based sign language understanding. Code and models are available at https://github.com/FangyunWei/SLRT.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2312.02222.pdf' target='_blank'>https://arxiv.org/pdf/2312.02222.pdf</a></span>   <span><a href='https://github.com/XChenZ/invertAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaochen Zhao, Jingxiang Sun, Lizhen Wang, Jinli Suo, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02222">InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While high fidelity and efficiency are central to the creation of digital head avatars, recent methods relying on 2D or 3D generative models often experience limitations such as shape distortion, expression inaccuracy, and identity flickering. Additionally, existing one-shot inversion techniques fail to fully leverage multiple input images for detailed feature extraction. We propose a novel framework, \textbf{Incremental 3D GAN Inversion}, that enhances avatar reconstruction performance using an algorithm designed to increase the fidelity from multiple frames, resulting in improved reconstruction quality proportional to frame count. Our method introduces a unique animatable 3D GAN prior with two crucial modifications for enhanced expression controllability alongside an innovative neural texture encoder that categorizes texture feature spaces based on UV parameterization. Differentiating from traditional techniques, our architecture emphasizes pixel-aligned image-to-image translation, mitigating the need to learn correspondences between observation and canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent networks for temporal data aggregation from multiple frames, boosting geometry and texture detail reconstruction. The proposed paradigm demonstrates state-of-the-art performance on one-shot and few-shot avatar animation tasks. Code will be available at https://github.com/XChenZ/invertAvatar.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2312.01632.pdf' target='_blank'>https://arxiv.org/pdf/2312.01632.pdf</a></span>   <span><a href='https://github.com/chiehwangs/gaussian-head' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Wang, Jiu-Cheng Xie, Xianyan Li, Feng Xu, Chi-Man Pun, Hao Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01632">GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing vivid 3D head avatars for given subjects and realizing a series of animations on them is valuable yet challenging. This paper presents GaussianHead, which models the actional human head with anisotropic 3D Gaussians. In our framework, a motion deformation field and multi-resolution tri-plane are constructed respectively to deal with the head's dynamic geometry and complex texture. Notably, we impose an exclusive derivation scheme on each Gaussian, which generates its multiple doppelgangers through a set of learnable parameters for position transformation. With this design, we can compactly and accurately encode the appearance information of Gaussians, even those fitting the head's particular components with sophisticated structures. In addition, an inherited derivation strategy for newly added Gaussians is adopted to facilitate training acceleration. Extensive experiments show that our method can produce high-fidelity renderings, outperforming state-of-the-art approaches in reconstruction, cross-identity reenactment, and novel view synthesis tasks. Our code is available at: https://github.com/chiehwangs/gaussian-head.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2311.17910.pdf' target='_blank'>https://arxiv.org/pdf/2311.17910.pdf</a></span>   <span><a href='https://github.com/apple/ml-hugs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17910">HUGS: Human Gaussian Splats</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2311.12804.pdf' target='_blank'>https://arxiv.org/pdf/2311.12804.pdf</a></span>   <span><a href='https://github.com/aldelb/non_verbal_facial_animation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alice Delbosc, Magalie Ochs, Nicolas Sabouret, Brian Ravenet, StÃ©phane Ayache
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12804">Towards the generation of synchronized and believable non-verbal facial behaviors of a talking virtual agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new model to generate rhythmically relevant non-verbal facial behaviors for virtual agents while they speak. The model demonstrates perceived performance comparable to behaviors directly extracted from the data and replayed on a virtual agent, in terms of synchronization with speech and believability. Interestingly, we found that training the model with two different sets of data, instead of one, did not necessarily improve its performance. The expressiveness of the people in the dataset and the shooting conditions are key elements. We also show that employing an adversarial model, in which fabricated fake examples are introduced during the training phase, increases the perception of synchronization with speech. A collection of videos demonstrating the results and code can be accessed at: https://github.com/aldelb/non_verbal_facial_animation.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2311.06231.pdf' target='_blank'>https://arxiv.org/pdf/2311.06231.pdf</a></span>   <span><a href='https://github.com/howardzh01/PPMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Howard Zhong, Samarth Mishra, Donghyun Kim, SouYoung Jin, Rameswar Panda, Hilde Kuehne, Leonid Karlinsky, Venkatesh Saligrama, Aude Oliva, Rogerio Feris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06231">Learning Human Action Recognition Representations Without Real Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the transferability of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with humans removed and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. Our approach outperforms previous baselines by up to 5% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github.com/howardzh01/PPMA .
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2310.05215.pdf' target='_blank'>https://arxiv.org/pdf/2310.05215.pdf</a></span>   <span><a href='https://github.com/JLPM22/MotionMatching' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05215">Motion Matching for Character Animation and Virtual Reality Avatars in Unity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time animation of virtual characters has traditionally been accomplished by playing short sequences of animations structured in the form of a graph. These methods are time-consuming to set up and scale poorly with the number of motions required in modern virtual environments. The ever-increasing need for highly-realistic virtual characters in fields such as entertainment, virtual reality, or the metaverse has led to significant advances in the field of data-driven character animation. Techniques like Motion Matching have provided enough versatility to conveniently animate virtual characters using a selection of features from an animation database. Data-driven methods retain the quality of the captured animations, thus delivering smoother and more natural-looking animations. In this work, we researched and developed a Motion Matching technique for the Unity game engine. In this thesis, we present our findings on how to implement an animation system based on Motion Matching. We also introduce a novel method combining body orientation prediction with Motion Matching to animate avatars for consumer-grade virtual reality systems.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2309.13524.pdf' target='_blank'>https://arxiv.org/pdf/2309.13524.pdf</a></span>   <span><a href='https://github.com/River-Zhang/GTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13524">Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D clothed human avatars from single images is a challenging task, especially when encountering complex poses and loose clothing. Current methods exhibit limitations in performance, largely attributable to their dependence on insufficient 2D image features and inconsistent query methods. Owing to this, we present the Global-correlated 3D-decoupling Transformer for clothed Avatar reconstruction (GTA), a novel transformer-based architecture that reconstructs clothed human avatars from monocular images. Our approach leverages transformer architectures by utilizing a Vision Transformer model as an encoder for capturing global-correlated image features. Subsequently, our innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane features, using learnable embeddings as queries for cross-plane generation. To effectively enhance feature fusion with the tri-plane 3D feature and human body prior, we propose a hybrid prior fusion strategy combining spatial and prior-enhanced queries, leveraging the benefits of spatial localization and human body prior knowledge. Comprehensive experiments on CAPE and THuman2.0 datasets illustrate that our method outperforms state-of-the-art approaches in both geometry and texture reconstruction, exhibiting high robustness to challenging poses and loose clothing, and producing higher-resolution textures. Codes will be available at https://github.com/River-Zhang/GTA.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2308.10278.pdf' target='_blank'>https://arxiv.org/pdf/2308.10278.pdf</a></span>   <span><a href='https://github.com/morecry/CharacterChat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Tu, Chuanqi Chen, Jinpeng Li, Yanran Li, Shuo Shang, Dongyan Zhao, Ran Wang, Rui Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10278">CharacterChat: Learning towards Conversational AI with Personalized Social Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In our modern, fast-paced, and interconnected world, the importance of mental well-being has grown into a matter of great urgency. However, traditional methods such as Emotional Support Conversations (ESC) face challenges in effectively addressing a diverse range of individual personalities. In response, we introduce the Social Support Conversation (S2Conv) framework. It comprises a series of support agents and the interpersonal matching mechanism, linking individuals with persona-compatible virtual supporters. Utilizing persona decomposition based on the MBTI (Myers-Briggs Type Indicator), we have created the MBTI-1024 Bank, a group that of virtual characters with distinct profiles. Through improved role-playing prompts with behavior preset and dynamic memory, we facilitate the development of the MBTI-S2Conv dataset, which contains conversations between the characters in the MBTI-1024 Bank. Building upon these foundations, we present CharacterChat, a comprehensive S2Conv system, which includes a conversational model driven by personas and memories, along with an interpersonal matching plugin model that dispatches the optimal supporters from the MBTI-1024 Bank for individuals with specific personas. Empirical results indicate the remarkable efficacy of CharacterChat in providing personalized social support and highlight the substantial advantages derived from interpersonal matching. The source code is available in \url{https://github.com/morecry/CharacterChat}.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2308.08857.pdf' target='_blank'>https://arxiv.org/pdf/2308.08857.pdf</a></span>   <span><a href='https://github.com/psyai-net/D-IF_release' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu, Zhaoxin Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08857">D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple ``value to distribution'' transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can capture more intricate wrinkles, and realistic limbs. Code and models are available for research purposes at https://github.com/psyai-net/D-IF_release.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2307.09153.pdf' target='_blank'>https://arxiv.org/pdf/2307.09153.pdf</a></span>   <span><a href='https://github.com/lsx0101/OPHAvatars' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoxu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09153">OPHAvatars: One-shot Photo-realistic Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method for synthesizing photo-realistic digital avatars from only one portrait as the reference. Given a portrait, our method synthesizes a coarse talking head video using driving keypoints features. And with the coarse video, our method synthesizes a coarse talking head avatar with a deforming neural radiance field. With rendered images of the coarse avatar, our method updates the low-quality images with a blind face restoration model. With updated images, we retrain the avatar for higher quality. After several iterations, our method can synthesize a photo-realistic animatable 3D neural head avatar. The motivation of our method is deformable neural radiance field can eliminate the unnatural distortion caused by the image2video method. Our method outperforms state-of-the-art methods in quantitative and qualitative studies on various subjects.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2307.02609.pdf' target='_blank'>https://arxiv.org/pdf/2307.02609.pdf</a></span>   <span><a href='https://github.com/SSYSteve/MRecGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Xu, Cheng Luo, Weicheng Xie, Linlin Shen, Xiaofeng Liu, Lu Liu, Hatice Gunes, Siyang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02609">MRecGen: Multimodal Appropriate Reaction Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verbal and non-verbal human reaction generation is a challenging task, as different reactions could be appropriate for responding to the same behaviour. This paper proposes the first multiple and multimodal (verbal and nonverbal) appropriate human reaction generation framework that can generate appropriate and realistic human-style reactions (displayed in the form of synchronised text, audio and video streams) in response to an input user behaviour. This novel technique can be applied to various human-computer interaction scenarios by generating appropriate virtual agent/robot behaviours. Our demo is available at \url{https://github.com/SSYSteve/MRecGen}.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2306.02903.pdf' target='_blank'>https://arxiv.org/pdf/2306.02903.pdf</a></span>   <span><a href='https://github.com/lsx0101/Instruct-Video2Avatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoxu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02903">Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method for synthesizing edited photo-realistic digital avatars with text instructions. Given a short monocular RGB video and text instructions, our method uses an image-conditioned diffusion model to edit one head image and uses the video stylization method to accomplish the editing of other head images. Through iterative training and update (three times or more), our method synthesizes edited photo-realistic animatable 3D neural head avatars with a deformable neural radiance field head synthesis method. In quantitative and qualitative studies on various subjects, our method outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2305.19012.pdf' target='_blank'>https://arxiv.org/pdf/2305.19012.pdf</a></span>   <span><a href='https://github.com/icoz69/StyleAvatar3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19012">StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the diversity of the generated avatars. Additionally, we develop a latent diffusion model within the style space of StyleGAN to enable the generation of avatars based on image inputs. Our approach demonstrates superior performance over current state-of-the-art methods in terms of visual quality and diversity of the produced avatars.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2305.13353.pdf' target='_blank'>https://arxiv.org/pdf/2305.13353.pdf</a></span>   <span><a href='https://github.com/RenderMe-360/RenderMe-360' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwei Pan, Long Zhuo, Jingtan Piao, Huiwen Luo, Wei Cheng, Yuxin Wang, Siming Fan, Shengqi Liu, Lei Yang, Bo Dai, Ziwei Liu, Chen Change Loy, Chen Qian, Wayne Wu, Dahua Lin, Kwan-Yee Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13353">RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing high-fidelity head avatars is a central problem for computer vision and graphics. While head avatar synthesis algorithms have advanced rapidly, the best ones still face great obstacles in real-world scenarios. One of the vital causes is inadequate datasets -- 1) current public datasets can only support researchers to explore high-fidelity head avatars in one or two task directions; 2) these datasets usually contain digital head assets with limited data volume, and narrow distribution over different attributes. In this paper, we present RenderMe-360, a comprehensive 4D human head dataset to drive advance in head avatar research. It contains massive data assets, with 243+ million complete head frames, and over 800k video sequences from 500 different identities captured by synchronized multi-view cameras at 30 FPS. It is a large-scale digital library for head avatars with three key attributes: 1) High Fidelity: all subjects are captured by 60 synchronized, high-resolution 2K cameras in 360 degrees. 2) High Diversity: The collected subjects vary from different ages, eras, ethnicities, and cultures, providing abundant materials with distinctive styles in appearance and geometry. Moreover, each subject is asked to perform various motions, such as expressions and head rotations, which further extend the richness of assets. 3) Rich Annotations: we provide annotations with different granularities: cameras' parameters, matting, scan, 2D/3D facial landmarks, FLAME fitting, and text description.
  Based on the dataset, we build a comprehensive benchmark for head avatar research, with 16 state-of-the-art methods performed on five main tasks: novel view synthesis, novel expression synthesis, hair rendering, hair editing, and talking head generation. Our experiments uncover the strengths and weaknesses of current methods. RenderMe-360 opens the door for future exploration in head avatars.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2305.00942.pdf' target='_blank'>https://arxiv.org/pdf/2305.00942.pdf</a></span>   <span><a href='https://github.com/LizhenWangT/StyleAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lizhen Wang, Xiaochen Zhao, Jingxiang Sun, Yuxiang Zhang, Hongwen Zhang, Tao Yu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00942">StyleAvatar: Real-time Photo-realistic Portrait Avatar from a Single Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face reenactment methods attempt to restore and re-animate portrait videos as realistically as possible. Existing methods face a dilemma in quality versus controllability: 2D GAN-based methods achieve higher image quality but suffer in fine-grained control of facial attributes compared with 3D counterparts. In this work, we propose StyleAvatar, a real-time photo-realistic portrait avatar reconstruction method using StyleGAN-based networks, which can generate high-fidelity portrait avatars with faithful expression control. We expand the capabilities of StyleGAN by introducing a compositional representation and a sliding window augmentation method, which enable faster convergence and improve translation generalization. Specifically, we divide the portrait scenes into three parts for adaptive adjustments: facial region, non-facial foreground region, and the background. Besides, our network leverages the best of UNet, StyleGAN and time coding for video learning, which enables high-quality video generation. Furthermore, a sliding window augmentation method together with a pre-training strategy are proposed to improve translation generalization and training performance, respectively. The proposed network can converge within two hours while ensuring high image quality and a forward rendering time of only 20 milliseconds. Furthermore, we propose a real-time live system, which further pushes research into applications. Results and experiments demonstrate the superiority of our method in terms of image quality, full portrait video generation, and real-time re-animation compared to existing facial reenactment methods. Training and inference code for this paper are at https://github.com/LizhenWangT/StyleAvatar.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2304.13006.pdf' target='_blank'>https://arxiv.org/pdf/2304.13006.pdf</a></span>   <span><a href='https://github.com/lizhe00/posevocab' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lizhe00/PoseVocab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13006">PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating pose-driven human avatars is about modeling the mapping from the low-frequency driving pose to high-frequency dynamic human appearances, so an effective pose encoding method that can encode high-fidelity human details is essential to human avatar modeling. To this end, we present PoseVocab, a novel pose encoding method that encourages the network to discover the optimal pose embeddings for learning the dynamic human appearance. Given multi-view RGB videos of a character, PoseVocab constructs key poses and latent embeddings based on the training poses. To achieve pose generalization and temporal consistency, we sample key rotations in $so(3)$ of each joint rather than the global pose vectors, and assign a pose embedding to each sampled key rotation. These joint-structured pose embeddings not only encode the dynamic appearances under different key poses, but also factorize the global pose embedding into joint-structured ones to better learn the appearance variation related to the motion of each joint. To improve the representation ability of the pose embedding while maintaining memory efficiency, we introduce feature lines, a compact yet effective 3D representation, to model more fine-grained details of human appearances. Furthermore, given a query pose and a spatial position, a hierarchical query strategy is introduced to interpolate pose embeddings and acquire the conditional pose feature for dynamic human synthesis. Overall, PoseVocab effectively encodes the dynamic details of human appearance and enables realistic and generalized animation under novel poses. Experiments show that our method outperforms other state-of-the-art baselines both qualitatively and quantitatively in terms of synthesis quality. Code is available at https://github.com/lizhe00/PoseVocab.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2303.09119.pdf' target='_blank'>https://arxiv.org/pdf/2303.09119.pdf</a></span>   <span><a href='https://github.com/Advocate99/DiffGesture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, Lequan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09119">Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual avatars to make co-speech gestures facilitates various applications in human-machine interaction. The existing methods mainly rely on generative adversarial networks (GANs), which typically suffer from notorious mode collapse and unstable training, thus making it difficult to learn accurate audio-gesture joint distributions. In this work, we propose a novel diffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to effectively capture the cross-modal audio-to-gesture associations and preserve temporal coherence for high-fidelity audio-driven co-speech gesture generation. Specifically, we first establish the diffusion-conditional generation process on clips of skeleton sequences and audio to enable the whole framework. Then, a novel Diffusion Audio-Gesture Transformer is devised to better attend to the information from multiple modalities and model the long-term temporal dependency. Moreover, to eliminate temporal inconsistency, we propose an effective Diffusion Gesture Stabilizer with an annealed noise sampling strategy. Benefiting from the architectural advantages of diffusion models, we further incorporate implicit classifier-free guidance to trade off between diversity and gesture quality. Extensive experiments demonstrate that DiffGesture achieves state-of-theart performance, which renders coherent gestures with better mode coverage and stronger audio correlations. Code is available at https://github.com/Advocate99/DiffGesture.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2303.01765.pdf' target='_blank'>https://arxiv.org/pdf/2303.01765.pdf</a></span>   <span><a href='https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Chen Liu, Muyi Sun, Lincheng Li, Changjie Fan, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01765">Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting natural and diverse 3D hand gestures from the upper body dynamics is a practical yet challenging task in virtual avatar creation. Previous works usually overlook the asymmetric motions between two hands and generate two hands in a holistic manner, leading to unnatural results. In this work, we introduce a novel bilateral hand disentanglement based two-stage 3D hand generation method to achieve natural and diverse 3D hand prediction from body dynamics. In the first stage, we intend to generate natural hand gestures by two hand-disentanglement branches. Considering the asymmetric gestures and motions of two hands, we introduce a Spatial-Residual Memory (SRM) module to model spatial interaction between the body and each hand by residual learning. To enhance the coordination of two hand motions wrt. body dynamics holistically, we then present a Temporal-Motion Memory (TMM) module. TMM can effectively model the temporal association between body dynamics and two hand motions. The second stage is built upon the insight that 3D hand predictions should be non-deterministic given the sequential body postures. Thus, we further diversify our 3D hand predictions based on the initial output from the stage one. Concretely, we propose a Prototypical-Memory Sampling Strategy (PSS) to generate the non-deterministic hand gestures by gradient-based Markov Chain Monte Carlo (MCMC) sampling. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on the B2H dataset and our newly collected TED Hands dataset. The dataset and code are available at https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2302.03397.pdf' target='_blank'>https://arxiv.org/pdf/2302.03397.pdf</a></span>   <span><a href='https://github.com/loong8888/AniPixel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Fan, Jing Zhang, Zhi Hou, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03397">AniPixel: Towards Animatable Pixel-Aligned Human Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although human reconstruction typically results in human-specific avatars, recent 3D scene reconstruction techniques utilizing pixel-aligned features show promise in generalizing to new scenes. Applying these techniques to human avatar reconstruction can result in a volumetric avatar with generalizability but limited animatability due to rendering only being possible for static representations. In this paper, we propose AniPixel, a novel animatable and generalizable human avatar reconstruction method that leverages pixel-aligned features for body geometry prediction and RGB color blending. Technically, to align the canonical space with the target space and the observation space, we propose a bidirectional neural skinning field based on skeleton-driven deformation to establish the target-to-canonical and canonical-to-observation correspondences. Then, we disentangle the canonical body geometry into a normalized neutral-sized body and a subject-specific residual for better generalizability. As the geometry and appearance are closely related, we introduce pixel-aligned features to facilitate the body geometry prediction and detailed surface normals to reinforce the RGB color blending. We also devise a pose-dependent and view direction-related shading module to represent the local illumination variance. Experiments show that AniPixel renders comparable novel views while delivering better novel pose animation results than state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2212.08377.pdf' target='_blank'>https://arxiv.org/pdf/2212.08377.pdf</a></span>   <span><a href='https://github.com/zhengyuf/pointavatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08377">PointAvatar: Deformable Point-based Head Avatars from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to create realistic, animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting in the color estimation, thus they are limited in re-rendering the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2309.01426.pdf' target='_blank'>https://arxiv.org/pdf/2309.01426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Deepu Rajan, Shiwen Mao, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01426">A Unified Framework for Guiding Generative AI with Wireless Perception in Resource Constrained Mobile Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the significant advancements in artificial intelligence (AI) technologies and powerful computational capabilities, generative AI (GAI) has become a pivotal digital content generation technique for offering superior digital services. However, directing GAI towards desired outputs still suffer the inherent instability of the AI model. In this paper, we design a novel framework that utilizes wireless perception to guide GAI (WiPe-GAI) for providing digital content generation service, i.e., AI-generated content (AIGC), in resource-constrained mobile edge networks. Specifically, we first propose a new sequential multi-scale perception (SMSP) algorithm to predict user skeleton based on the channel state information (CSI) extracted from wireless signals. This prediction then guides GAI to provide users with AIGC, such as virtual character generation. To ensure the efficient operation of the proposed framework in resource constrained networks, we further design a pricing-based incentive mechanism and introduce a diffusion model based approach to generate an optimal pricing strategy for the service provisioning. The strategy maximizes the user's utility while enhancing the participation of the virtual service provider (VSP) in AIGC provision. The experimental results demonstrate the effectiveness of the designed framework in terms of skeleton prediction and optimal pricing strategy generation comparing with other existing solutions.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2306.14683.pdf' target='_blank'>https://arxiv.org/pdf/2306.14683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlong Chen, Jiawen Kang, Minrui Xu, Zehui Xiong, Dusit Niyato, Chuan Chen, Abbas Jamalipour, Shengli Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14683">Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in AIoT-enabled Vehicular Metaverses with Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars, as promising digital assistants in Vehicular Metaverses, can enable drivers and passengers to immerse in 3D virtual spaces, serving as a practical emerging example of Artificial Intelligence of Things (AIoT) in intelligent vehicular environments. The immersive experience is achieved through seamless human-avatar interaction, e.g., augmented reality navigation, which requires intensive resources that are inefficient and impractical to process on intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide Units (RSUs) or cloud servers for remote execution can effectively reduce resource consumption. However, the high mobility of vehicles, the dynamic workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making avatar migration decisions. To address these challenges, in this paper, we propose a dynamic migration framework for avatar tasks based on real-time trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL). Specifically, we propose a model to predict the future trajectories of intelligent vehicles based on their historical data, indicating the future workloads of RSUs.Based on the expected workloads of RSUs, we formulate the avatar task migration problem as a long-term mixed integer programming problem. To tackle this problem efficiently, the problem is transformed into a Partially Observable Markov Decision Process (POMDP) and solved by multiple DRL agents with hybrid continuous and discrete actions in decentralized. Numerical results demonstrate that our proposed algorithm can effectively reduce the latency of executing avatar tasks by around 25% without prediction and 30% with prediction and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse (AeVeM).
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2411.18197.pdf' target='_blank'>https://arxiv.org/pdf/2411.18197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, Ran Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18197">Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D characters are essential to modern creative industries, but making them animatable often demands extensive manual work in tasks like rigging and skinning. Existing automatic rigging tools face several limitations, including the necessity for manual annotations, rigid skeleton topologies, and limited generalization across diverse shapes and poses. An alternative approach is to generate animatable avatars pre-bound to a rigged template mesh. However, this method often lacks flexibility and is typically limited to realistic human shapes. To address these issues, we present Make-It-Animatable, a novel data-driven method to make any 3D humanoid model ready for character animation in less than one second, regardless of its shapes and poses. Our unified framework generates high-quality blend weights, bones, and pose transformations. By incorporating a particle-based shape autoencoder, our approach supports various 3D representations, including meshes and 3D Gaussian splats. Additionally, we employ a coarse-to-fine representation and a structure-aware modeling strategy to ensure both accuracy and robustness, even for characters with non-standard skeleton structures. We conducted extensive experiments to validate our framework's effectiveness. Compared to existing methods, our approach demonstrates significant improvements in both quality and speed. More demos and code are available at https://jasongzy.github.io/Make-It-Animatable/.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2310.08529.pdf' target='_blank'>https://arxiv.org/pdf/2310.08529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08529">GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2303.12965.pdf' target='_blank'>https://arxiv.org/pdf/2303.12965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoke Huang, Yiji Cheng, Yansong Tang, Xiu Li, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12965">Efficient Meshy Neural Fields for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently digitizing high-fidelity animatable human avatars from videos is a challenging and active research topic. Recent volume rendering-based neural representations open a new way for human digitization with their friendly usability and photo-realistic reconstruction quality. However, they are inefficient for long optimization times and slow inference speed; their implicit nature results in entangled geometry, materials, and dynamics of humans, which are hard to edit afterward. Such drawbacks prevent their direct applicability to downstream applications, especially the prominent rasterization-based graphic ones. We present EMA, a method that Efficiently learns Meshy neural fields to reconstruct animatable human Avatars. It jointly optimizes explicit triangular canonical mesh, spatial-varying material, and motion dynamics, via inverse rendering in an end-to-end fashion. Each above component is derived from separate neural fields, relaxing the requirement of a template, or rigging. The mesh representation is highly compatible with the efficient rasterization-based renderer, thus our method only takes about an hour of training and can render in real-time. Moreover, only minutes of optimization is enough for plausible reconstruction results. The disentanglement of meshes enables direct downstream applications. Extensive experiments illustrate the very competitive performance and significant speed boost against previous methods. We also showcase applications including novel pose synthesis, material editing, and relighting. The project page: https://xk-huang.github.io/ema/.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2406.11208.pdf' target='_blank'>https://arxiv.org/pdf/2406.11208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Su, Xiaofeng Luo, Zhenmou Liu, Jiawen Kang, Min Hao, Zehui Xiong, Zhaohui Yang, Chongwen Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11208">Privacy-preserving Pseudonym Schemes for Personalized 3D Avatars in Mobile Social Metaverses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of mobile social metaverses, a novel paradigm bridging physical and virtual realms, has led to the widespread adoption of avatars as digital representations for Social Metaverse Users (SMUs) within virtual spaces. Equipped with immersive devices, SMUs leverage Edge Servers (ESs) to deploy their avatars and engage with other SMUs in virtual spaces. To enhance immersion, SMUs incline to opt for 3D avatars for social interactions. However, existing 3D avatars are typically generated through scanning the real faces of SMUs, which can raise concerns regarding information privacy and security, such as profile identity leakages. To tackle this, we introduce a new framework for personalized 3D avatar construction, leveraging a two-layer network model that provides SMUs with the option to customize their personal avatars for privacy preservation. Specifically, our approach introduces avatar pseudonyms to jointly safeguard the profile and digital identity privacy of the generated avatars. Then, we design a novel metric named Privacy of Personalized Avatars (PoPA), to evaluate effectiveness of the avatar pseudonyms. To optimize pseudonym resource, we model the pseudonym distribution process as a Stackelberg game and employ Deep Reinforcement Learning (DRL) to learn equilibrium strategies under incomplete information. Simulation results validate the efficacy and feasibility of our proposed schemes for mobile social metaverses.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2405.00954.pdf' target='_blank'>https://arxiv.org/pdf/2405.00954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00954">X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry->Texture->Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: https://xmu-xiaoma666.github.io/Projects/X-Oscar/.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2507.02419.pdf' target='_blank'>https://arxiv.org/pdf/2507.02419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhong, Xiaolin Zhang, Ligang Liu, Yao Zhao, Yunchao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02419">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2508.00748.pdf' target='_blank'>https://arxiv.org/pdf/2508.00748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00748">Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. These avatars allow for more immersive communication, but they also introduce serious security risks. One emerging threat is impersonation: an attacker can steal a user's avatar, preserving his appearance and voice, making it nearly impossible to detect its fraudulent usage by sight or sound alone. In this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. Our main question is whether an individual's facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatar's visual appearance is a facsimile of its owner. To answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and impostor avatar videos. We also propose a lightweight, explainable spatio-temporal Graph Convolutional Network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. Experimental results demonstrate that facial motion cues enable meaningful identity verification with AUC values approaching 80%. The proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2304.06969.pdf' target='_blank'>https://arxiv.org/pdf/2304.06969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Fan, Jing Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06969">UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose rendering, Geometry and Texture Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural radiance field (NeRF) has become a popular 3D representation method for human avatar reconstruction due to its high-quality rendering capabilities, e.g., regarding novel views and poses. However, previous methods for editing the geometry and appearance of the avatar only allow for global editing through body shape parameters and 2D texture maps. In this paper, we propose a new approach named \textbf{U}nified \textbf{V}olumetric \textbf{A}vatar (\textbf{UVA}) that enables local and independent editing of both geometry and texture, while retaining the ability to render novel views and poses. UVA transforms each observation point to a canonical space using a skinning motion field and represents geometry and texture in separate neural fields. Each field is composed of a set of structured latent codes that are attached to anchor nodes on a deformable mesh in canonical space and diffused into the entire space via interpolation, allowing for local editing. To address spatial ambiguity in code interpolation, we use a local signed height indicator. We also replace the view-dependent radiance color with a pose-dependent shading factor to better represent surface illumination in different poses. Experiments on multiple human avatars demonstrate that our UVA achieves competitive results in novel view synthesis and novel pose rendering while enabling local and independent editing of geometry and appearance. The source code will be released.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2503.06499.pdf' target='_blank'>https://arxiv.org/pdf/2503.06499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xukun Zhou, Fengxin Li, Ming Chen, Yan Zhou, Pengfei Wan, Di Zhang, Yeying Jin, Zhaoxin Fan, Hongyan Liu, Jun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06499">ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven human gesture synthesis is a crucial task with broad applications in virtual avatars, human-computer interaction, and creative content generation. Despite notable progress, existing methods often produce gestures that are coarse, lack expressiveness, and fail to fully align with audio semantics. To address these challenges, we propose ExGes, a novel retrieval-enhanced diffusion framework with three key designs: (1) a Motion Base Construction, which builds a gesture library using training dataset; (2) a Motion Retrieval Module, employing constrative learning and momentum distillation for fine-grained reference poses retreiving; and (3) a Precision Control Module, integrating partial masking and stochastic masking to enable flexible and fine-grained control. Experimental evaluations on BEAT2 demonstrate that ExGes reduces FrÃ©chet Gesture Distance by 6.2\% and improves motion diversity by 5.3\% over EMAGE, with user studies revealing a 71.3\% preference for its naturalness and semantic relevance. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2506.08933.pdf' target='_blank'>https://arxiv.org/pdf/2506.08933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08933">What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at https://omni-bench.github.io/.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2411.10943.pdf' target='_blank'>https://arxiv.org/pdf/2411.10943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghe Gao, Wendong Bu, Bingchen Miao, Yang Wu, Yunfei Li, Juncheng Li, Siliang Tang, Qi Wu, Yueting Zhuang, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10943">Generalist Virtual Agents: A Survey on Autonomous Agents Across Digital Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Generalist Virtual Agent (GVA), an autonomous entity engineered to function across diverse digital platforms and environments, assisting users by executing a variety of tasks. This survey delves into the evolution of GVAs, tracing their progress from early intelligent assistants to contemporary implementations that incorporate large-scale models. We explore both the philosophical underpinnings and practical foundations of GVAs, addressing their developmental challenges and the methodologies currently employed in their design and operation. By presenting a detailed taxonomy of GVA environments, tasks, and capabilities, this paper aims to bridge the theoretical and practical aspects of GVAs, concluding those that operate in environments closely mirroring the real world are more likely to demonstrate human-like intelligence. We discuss potential future directions for GVA research, highlighting the necessity for realistic evaluation metrics and the enhancement of long-sequence decision-making capabilities to advance the field toward more systematic or embodied applications. This work not only synthesizes the existing body of literature but also proposes frameworks for future investigations, contributing significantly to the ongoing development of intelligent systems.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2402.16607.pdf' target='_blank'>https://arxiv.org/pdf/2402.16607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16607">GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation lies in addressing the intricate challenges of delivering high-fidelity human body reconstructions and aligning 3D Gaussians with human skin surfaces accurately. The key contributions of this paper are twofold. Firstly, we introduce a pose refinement technique to improve hand and foot pose accuracy by aligning normal maps and silhouettes. Precise pose is crucial for correct shape and appearance reconstruction. Secondly, we address the problems of unbalanced aggregation and initialization bias that previously diminished the quality of 3D Gaussian avatars, through a novel surface-guided re-initialization method that ensures accurate alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive experimental analyses validate the performance qualitatively and quantitatively, demonstrating that it achieves state-of-the-art performance in photo-realistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GVA/.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2412.09892.pdf' target='_blank'>https://arxiv.org/pdf/2412.09892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liu, Ziyang Ma, Qi Chen, Feilong Chen, Shuai Fan, Xie Chen, Kai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09892">VQTalker: Towards Multilingual Talking Avatars through Facial Motion Tokenization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VQTalker, a Vector Quantization-based framework for multilingual talking head generation that addresses the challenges of lip synchronization and natural motion across diverse languages. Our approach is grounded in the phonetic principle that human speech comprises a finite set of distinct sound units (phonemes) and corresponding visual articulations (visemes), which often share commonalities across languages. We introduce a facial motion tokenizer based on Group Residual Finite Scalar Quantization (GRFSQ), which creates a discretized representation of facial features. This method enables comprehensive capture of facial movements while improving generalization to multiple languages, even with limited training data. Building on this quantized representation, we implement a coarse-to-fine motion generation process that progressively refines facial animations. Extensive experiments demonstrate that VQTalker achieves state-of-the-art performance in both video-driven and speech-driven scenarios, particularly in multilingual settings. Notably, our method achieves high-quality results at a resolution of 512*512 pixels while maintaining a lower bitrate of approximately 11 kbps. Our work opens new possibilities for cross-lingual talking face generation. Synthetic results can be viewed at https://x-lance.github.io/VQTalker.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2407.17438.pdf' target='_blank'>https://arxiv.org/pdf/2407.17438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17438">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2508.02106.pdf' target='_blank'>https://arxiv.org/pdf/2508.02106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02106">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2502.06392.pdf' target='_blank'>https://arxiv.org/pdf/2502.06392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Long, Zijun Zhao, Min Ouyang, Qingcheng Zhao, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06392">TANGLED: Generating 3D Hair Strands from Images with Arbitrary Styles and Viewpoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hairstyles are intricate and culturally significant with various geometries, textures, and structures. Existing text or image-guided generation methods fail to handle the richness and complexity of diverse styles. We present TANGLED, a novel approach for 3D hair strand generation that accommodates diverse image inputs across styles, viewpoints, and quantities of input views. TANGLED employs a three-step pipeline. First, our MultiHair Dataset provides 457 diverse hairstyles annotated with 74 attributes, emphasizing complex and culturally significant styles to improve model generalization. Second, we propose a diffusion framework conditioned on multi-view linearts that can capture topological cues (e.g., strand density and parting lines) while filtering out noise. By leveraging a latent diffusion model with cross-attention on lineart features, our method achieves flexible and robust 3D hair generation across diverse input conditions. Third, a parametric post-processing module enforces braid-specific constraints to maintain coherence in complex structures. This framework not only advances hairstyle realism and diversity but also enables culturally inclusive digital avatars and novel applications like sketch-based 3D strand editing for animation and augmented reality.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2505.01746.pdf' target='_blank'>https://arxiv.org/pdf/2505.01746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Yatian Wang, Hengyuan Zhang, Jiahao Pan, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01746">Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter. Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. Considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. Specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a Temporal Interaction Module (TIM). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. The dataset and source code are publicly available at \href{https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2405.16874.pdf' target='_blank'>https://arxiv.org/pdf/2405.16874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Hengyuan Zhang, Yatian Wang, Jiahao Pan, Chen Liu, Peng Li, Xiaowei Chi, Mengfei Li, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16874">CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2311.17532.pdf' target='_blank'>https://arxiv.org/pdf/2311.17532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17532">Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating vivid and emotional 3D co-speech gestures is crucial for virtual avatar animation in human-machine interaction applications. While the existing methods enable generating the gestures to follow a single emotion label, they overlook that long gesture sequence modeling with emotion transition is more practical in real scenes. In addition, the lack of large-scale available datasets with emotional transition speech and corresponding 3D human gestures also limits the addressing of this task. To fulfill this goal, we first incorporate the ChatGPT-4 and an audio inpainting approach to construct the high-fidelity emotion transition human speeches. Considering obtaining the realistic 3D pose annotations corresponding to the dynamically inpainted emotion transition audio is extremely difficult, we propose a novel weakly supervised training strategy to encourage authority gesture transitions. Specifically, to enhance the coordination of transition gestures w.r.t different emotional ones, we model the temporal association representation between two different emotional gesture sequences as style guidance and infuse it into the transition generation. We further devise an emotion mixture mechanism that provides weak supervision based on a learnable mixed emotion label for transition gestures. Last, we present a keyframe sampler to supply effective initial posture cues in long sequences, enabling us to generate diverse gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models constructed by adapting single emotion-conditioned counterparts on our newly defined emotion transition task and datasets. Our code and dataset will be released on the project page: https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2401.08503.pdf' target='_blank'>https://arxiv.org/pdf/2401.08503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08503">Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples and source code are available at https://real3dportrait.github.io .
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2312.04547.pdf' target='_blank'>https://arxiv.org/pdf/2312.04547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04547">Digital Life Project: Autonomous 3D Characters with Social Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: https://digital-life-project.com/
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2507.11949.pdf' target='_blank'>https://arxiv.org/pdf/2507.11949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyang Xu, Zhiyang Dou, Mingyi Shi, Liang Pan, Leo Ho, Jingbo Wang, Yuan Liu, Cheng Lin, Yuexin Ma, Wenping Wang, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11949">MOSPA: Human Motion Generation Driven by Spatial Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2502.01045.pdf' target='_blank'>https://arxiv.org/pdf/2502.01045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01045">WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at https://wyiguanw.github.io/WonderHuman/.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2312.05295.pdf' target='_blank'>https://arxiv.org/pdf/2312.05295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Cheng Lin, Xin Li, Wenping Wang, Rong Xie, Li Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05295">Disentangled Clothed Avatar Generation from Text Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel text-to-avatar generation method that separately generates the human body and the clothes and allows high-quality animation on the generated avatar. While recent advancements in text-to-avatar generation have yielded diverse human avatars from text prompts, these methods typically combine all elements-clothes, hair, and body-into a single 3D representation. Such an entangled approach poses challenges for downstream tasks like editing or animation. To overcome these limitations, we propose a novel disentangled 3D avatar representation named Sequentially Offset-SMPL (SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body and clothes with two separate meshes but associates them with offsets to ensure the physical alignment between the body and the clothes. Then, we design a Score Distillation Sampling (SDS)-based distillation framework to generate the proposed SO-SMPL representation from text prompts. Our approach not only achieves higher texture and geometry quality and better semantic alignment with text prompts, but also significantly improves the visual quality of character animation, virtual try-on, and avatar editing. Project page: https://shanemankiw.github.io/SO-SMPL/.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2211.03703.pdf' target='_blank'>https://arxiv.org/pdf/2211.03703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Latif U. Khan, Ibrar Yaqoob, Khaled Salah, Choong Seon Hong, Dusit Niyato, Zhu Han, Mohsen Guizani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.03703">Machine Learning for Wireless Metaverse: Fundamentals, Use Case, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's wireless systems are posing key challenges in terms of quality of service and quality of physical experience. Metaverse has the potential to reshape, transform, and add innovations to the existing wireless systems. A metaverse is a collective virtual open space that can enable wireless systems using digital twins, digital avatars, and interactive experience technologies. Machine learning (ML) is indispensable for modeling twins, avatars, and deploying interactive experience technologies. In this paper, we present the role of ML in enabling metaverse-based wireless systems. We discuss key fundamental concepts for advancing ML in the metaverse-based wireless systems. Moreover, we present a case study of deep reinforcement learning for metaverse sensing. Finally, we discuss the future directions along with potential solutions.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2207.00413.pdf' target='_blank'>https://arxiv.org/pdf/2207.00413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Latif U. Khan, Zhu Han, Dusit Niyato, Mohsen Guizani, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.00413">Metaverse for Wireless Systems: Vision, Enablers, Architecture, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, significant research efforts have been initiated to enable the next-generation, namely, the sixth-generation (6G) wireless systems. In this article, we present a vision of metaverse towards effectively enabling the development of 6G wireless systems. A metaverse will use virtual representation (e.g., digital twin), digital avatars, and interactive experience technologies (e.g., extended reality) to assist analyses, optimizations, and operations of various wireless applications. Specifically, the metaverse can offer virtual wireless system operations through the digital twin that allows network designers, mobile developers, and telecommunications engineers to monitor, observe, analyze, and simulations their solutions collaboratively and virtually. We first introduce a general architecture for metaverse-based wireless systems. We discuss key driving applications, design trends, and key enablers of metaverse-based wireless systems. Finally, we present several open challenges and their potential solutions.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2312.11587.pdf' target='_blank'>https://arxiv.org/pdf/2312.11587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diogo Luvizon, Vladislav Golyanik, Adam Kortylewski, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11587">Relightable Neural Actor with Intrinsic Decomposition and Pose Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a controllable and relightable digital avatar from multi-view video with fixed illumination is a very challenging problem since humans are highly articulated, creating pose-dependent appearance effects, and skin as well as clothing require space-varying BRDF modeling. Existing works on creating animatible avatars either do not focus on relighting at all, require controlled illumination setups, or try to recover a relightable avatar from very low cost setups, i.e. a single RGB video, at the cost of severely limited result quality, e.g. shadows not even being modeled. To address this, we propose Relightable Neural Actor, a new video-based method for learning a pose-driven neural human model that can be relighted, allows appearance editing, and models pose-dependent effects such as wrinkles and self-shadows. Importantly, for training, our method solely requires a multi-view recording of the human under a known, but static lighting condition. To tackle this challenging problem, we leverage an implicit geometry representation of the actor with a drivable density field that models pose-dependent deformations and derive a dynamic mapping between 3D and UV spaces, where normal, visibility, and materials are effectively encoded. To evaluate our approach in real-world scenarios, we collect a new dataset with four identities recorded under different light conditions, indoors and outdoors, providing the first benchmark of its kind for human relighting, and demonstrating state-of-the-art relighting results for novel human poses.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2310.08583.pdf' target='_blank'>https://arxiv.org/pdf/2310.08583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noshaba Cheema, Rui Xu, Nam Hee Kim, Perttu HÃ¤mÃ¤lÃ¤inen, Vladislav Golyanik, Marc Habermann, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08583">Discovering Fatigued Movements for Virtual Character Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual character animation and movement synthesis have advanced rapidly during recent years, especially through a combination of extensive motion capture datasets and machine learning. A remaining challenge is interactively simulating characters that fatigue when performing extended motions, which is indispensable for the realism of generated animations. However, capturing such movements is problematic, as performing movements like backflips with fatigued variations up to exhaustion raises capture cost and risk of injury. Surprisingly, little research has been done on faithful fatigue modeling. To address this, we propose a deep reinforcement learning-based approach, which -- for the first time in literature -- generates control policies for full-body physically simulated agents aware of cumulative fatigue. For this, we first leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert policy for the skill; Second, we learn a fatigue policy by limiting the generated constant torque bounds based on endurance time to non-linear, state- and time-dependent limits in the joint-actuation space using a Three-Compartment Controller (3CC) model. Our results demonstrate that agents can adapt to different fatigue and rest rates interactively, and discover realistic recovery strategies without the need for any captured data of fatigued movement.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2309.01848.pdf' target='_blank'>https://arxiv.org/pdf/2309.01848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Howe Yuan Zhu, Nguyen Quang Hieu, Dinh Thai Hoang, Diep N. Nguyen, Chin-Teng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01848">A Human-Centric Metaverse Enabled by Brain-Computer Interface: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing interest in the Metaverse has generated momentum for members of academia and industry to innovate toward realizing the Metaverse world. The Metaverse is a unique, continuous, and shared virtual world where humans embody a digital form within an online platform. Through a digital avatar, Metaverse users should have a perceptual presence within the environment and can interact and control the virtual world around them. Thus, a human-centric design is a crucial element of the Metaverse. The human users are not only the central entity but also the source of multi-sensory data that can be used to enrich the Metaverse ecosystem. In this survey, we study the potential applications of Brain-Computer Interface (BCI) technologies that can enhance the experience of Metaverse users. By directly communicating with the human brain, the most complex organ in the human body, BCI technologies hold the potential for the most intuitive human-machine system operating at the speed of thought. BCI technologies can enable various innovative applications for the Metaverse through this neural pathway, such as user cognitive state monitoring, digital avatar control, virtual interactions, and imagined speech communications. This survey first outlines the fundamental background of the Metaverse and BCI technologies. We then discuss the current challenges of the Metaverse that can potentially be addressed by BCI, such as motion sickness when users experience virtual environments or the negative emotional states of users in immersive virtual applications. After that, we propose and discuss a new research direction called Human Digital Twin, in which digital twins can create an intelligent and interactable avatar from the user's brain signals. We also present the challenges and potential solutions in synchronizing and communicating between virtual and physical entities in the Metaverse.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2308.12969.pdf' target='_blank'>https://arxiv.org/pdf/2308.12969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanyue Zhang, Rishabh Dabral, Thomas LeimkÃ¼hler, Vladislav Golyanik, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12969">ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing automatic approaches for 3D virtual character motion synthesis supporting scene interactions do not generalise well to new objects outside training distributions, even when trained on extensive motion capture datasets with diverse objects and annotated interactions. This paper addresses this limitation and shows that robustness and generalisation to novel scene objects in 3D object-aware character synthesis can be achieved by training a motion model with as few as one reference object. We leverage an implicit feature representation trained on object-only datasets, which encodes an SE(3)-equivariant descriptor field around the object. Given an unseen object and a reference pose-object pair, we optimise for the object-aware pose that is closest in the feature space to the reference pose. Finally, we use l-NSM, i.e., our motion generation model that is trained to seamlessly transition from locomotion to object interaction with the proposed bidirectional pose blending scheme. Through comprehensive numerical comparisons to state-of-the-art methods and in a user study, we demonstrate substantial improvements in 3D virtual character motion and interaction quality and robustness to scenarios with unseen objects. Our project page is available at https://vcai.mpi-inf.mpg.de/projects/ROAM/.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2308.07903.pdf' target='_blank'>https://arxiv.org/pdf/2308.07903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07903">Relightable and Animatable Neural Avatar from Sparse-View Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse distances based on a parametric human model and compute fine distances by exploiting the local deformation invariance of SDF. Based on the HDQ algorithm, we leverage sphere tracing to efficiently estimate the surface intersection and light visibility. This allows us to develop the first system to recover animatable and relightable neural avatars from sparse view (or monocular) inputs. Experiments demonstrate that our approach is able to produce superior results compared to state-of-the-art methods. Our code will be released for reproducibility.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2307.00842.pdf' target='_blank'>https://arxiv.org/pdf/2307.00842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyingcheng Liao, Vladislav Golyanik, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00842">VINECS: Video-based Neural Character Skinning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rigging and skinning clothed human avatars is a challenging task and traditionally requires a lot of manual work and expertise. Recent methods addressing it either generalize across different characters or focus on capturing the dynamics of a single character observed under different pose configurations. However, the former methods typically predict solely static skinning weights, which perform poorly for highly articulated poses, and the latter ones either require dense 3D character scans in different poses or cannot generate an explicit mesh with vertex correspondence over time. To address these challenges, we propose a fully automated approach for creating a fully rigged character with pose-dependent skinning weights, which can be solely learned from multi-view video. Therefore, we first acquire a rigged template, which is then statically skinned. Next, a coordinate-based MLP learns a skinning weights field parameterized over the position in a canonical pose space and the respective pose. Moreover, we introduce our pose- and view-dependent appearance field allowing us to differentiably render and supervise the posed mesh using multi-view imagery. We show that our approach outperforms state-of-the-art while not relying on dense 4D scans.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2306.00547.pdf' target='_blank'>https://arxiv.org/pdf/2306.00547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00547">AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing and editing full head performances enables the creation of virtual characters with various applications such as extended reality and media production. The past few years witnessed a steep rise in the photorealism of human head avatars. Such avatars can be controlled through different input data modalities, including RGB, audio, depth, IMUs and others. While these data modalities provide effective means of control, they mostly focus on editing the head movements such as the facial expressions, head pose and/or camera viewpoint. In this paper, we propose AvatarStudio, a text-based method for editing the appearance of a dynamic full head avatar. Our approach builds on existing work to capture dynamic performances of human heads using neural radiance field (NeRF) and edits this representation with a text-to-image diffusion model. Specifically, we introduce an optimization strategy for incorporating multiple keyframes representing different camera viewpoints and time stamps of a video performance into a single diffusion model. Using this personalized diffusion model, we edit the dynamic NeRF by introducing view-and-time-aware Score Distillation Sampling (VT-SDS) following a model-based guidance approach. Our method edits the full head in a canonical space, and then propagates these edits to remaining time steps via a pretrained deformation network. We evaluate our method visually and numerically via a user study, and results show that our method outperforms existing approaches. Our experiments validate the design choices of our method and highlight that our edits are genuine, personalized, as well as 3D- and time-consistent.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2212.08811.pdf' target='_blank'>https://arxiv.org/pdf/2212.08811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Quang Hieu, Dinh Thai Hoang, Diep N. Nguyen, Eryk Dutkiewicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08811">Toward BCI-enabled Metaverse: A Joint Learning and Resource Allocation Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Toward user-driven Metaverse applications with fast wireless connectivity and tremendous computing demand through future 6G infrastructures, we propose a Brain-Computer Interface (BCI) enabled framework that paves the way for the creation of intelligent human-like avatars. Our approach takes a first step toward the Metaverse systems in which the digital avatars are envisioned to be more intelligent by collecting and analyzing brain signals through cellular networks. In our proposed system, Metaverse users experience Metaverse applications while sending their brain signals via uplink wireless channels in order to create intelligent human-like avatars at the base station. As such, the digital avatars can not only give useful recommendations for the users but also enable the system to create user-driven applications. Our proposed framework involves a mixed decision-making and classification problem in which the base station has to allocate its computing and radio resources to the users and classify the brain signals of users in an efficient manner. To this end, we propose a hybrid training algorithm that utilizes recent advances in deep reinforcement learning to address the problem. Specifically, our hybrid training algorithm contains three deep neural networks cooperating with each other to enable better realization of the mixed decision-making and classification problem. Simulation results show that our proposed framework can jointly address resource allocation for the system and classify brain signals of the users with highly accurate predictions.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2212.07555.pdf' target='_blank'>https://arxiv.org/pdf/2212.07555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.07555">IMos: Intent-Driven Full-Body Motion Synthesis for Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we make virtual characters in a scene interact with their surrounding objects through simple instructions? Is it possible to synthesize such motion plausibly with a diverse set of objects and instructions? Inspired by these questions, we present the first framework to synthesize the full-body motion of virtual human characters performing specified actions with 3D objects placed within their reach. Our system takes textual instructions specifying the objects and the associated intentions of the virtual characters as input and outputs diverse sequences of full-body motions. This contrasts existing works, where full-body action synthesis methods generally do not consider object interactions, and human-object interaction methods focus mainly on synthesizing hand or finger movements for grasping objects. We accomplish our objective by designing an intent-driven fullbody motion generator, which uses a pair of decoupled conditional variational auto-regressors to learn the motion of the body parts in an autoregressive manner. We also optimize the 6-DoF pose of the objects such that they plausibly fit within the hands of the synthesized characters. We compare our proposed method with the existing methods of motion synthesis and establish a new and stronger state-of-the-art for the task of intent-driven motion synthesis.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2212.04495.pdf' target='_blank'>https://arxiv.org/pdf/2212.04495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04495">MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional methods for human motion synthesis are either deterministic or struggle with the trade-off between motion diversity and motion quality. In response to these limitations, we introduce MoFusion, i.e., a new denoising-diffusion-based framework for high-quality conditional human motion synthesis that can generate long, temporally plausible, and semantically accurate motions based on a range of conditioning contexts (such as music and text). We also present ways to introduce well-known kinematic losses for motion plausibility within the motion diffusion framework through our scheduled weighting strategy. The learned latent space can be used for several interactive motion editing applications -- like inbetweening, seed conditioning, and text-based editing -- thus, providing crucial abilities for virtual character animation and robotics. Through comprehensive quantitative evaluations and a perceptual user study, we demonstrate the effectiveness of MoFusion compared to the state of the art on established benchmarks in the literature. We urge the reader to watch our supplementary video and visit https://vcai.mpi-inf.mpg.de/projects/MoFusion.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2506.04606.pdf' target='_blank'>https://arxiv.org/pdf/2506.04606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Huang-Menders, Xinhang Liu, Andy Xu, Yuyao Zhang, Chi-Keung Tang, Yu-Wing Tai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04606">SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SmartAvatar is a vision-language-agent-driven framework for generating fully rigged, animation-ready 3D human avatars from a single photo or textual prompt. While diffusion-based methods have made progress in general 3D object generation, they continue to struggle with precise control over human identity, body shape, and animation readiness. In contrast, SmartAvatar leverages the commonsense reasoning capabilities of large vision-language models (VLMs) in combination with off-the-shelf parametric human generators to deliver high-quality, customizable avatars. A key innovation is an autonomous verification loop, where the agent renders draft avatars, evaluates facial similarity, anatomical plausibility, and prompt alignment, and iteratively adjusts generation parameters for convergence. This interactive, AI-guided refinement process promotes fine-grained control over both facial and body features, enabling users to iteratively refine their avatars via natural-language conversations. Unlike diffusion models that rely on static pre-trained datasets and offer limited flexibility, SmartAvatar brings users into the modeling loop and ensures continuous improvement through an LLM-driven procedural generation and verification system. The generated avatars are fully rigged and support pose manipulation with consistent identity and appearance, making them suitable for downstream animation and interactive applications. Quantitative benchmarks and user studies demonstrate that SmartAvatar outperforms recent text- and image-driven avatar generation systems in terms of reconstructed mesh quality, identity fidelity, attribute accuracy, and animation readiness, making it a versatile tool for realistic, customizable avatar creation on consumer-grade hardware.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2503.08165.pdf' target='_blank'>https://arxiv.org/pdf/2503.08165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08165">Multimodal Generation of Animatable 3D Human Models with AvatarForge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AvatarForge, a framework for generating animatable 3D human avatars from text or image inputs using AI-driven procedural generation. While diffusion-based methods have made strides in general 3D object generation, they struggle with high-quality, customizable human avatars due to the complexity and diversity of human body shapes, poses, exacerbated by the scarcity of high-quality data. Additionally, animating these avatars remains a significant challenge for existing methods. AvatarForge overcomes these limitations by combining LLM-based commonsense reasoning with off-the-shelf 3D human generators, enabling fine-grained control over body and facial details. Unlike diffusion models which often rely on pre-trained datasets lacking precise control over individual human features, AvatarForge offers a more flexible approach, bringing humans into the iterative design and modeling loop, with its auto-verification system allowing for continuous refinement of the generated avatars, and thus promoting high accuracy and customization. Our evaluations show that AvatarForge outperforms state-of-the-art methods in both text- and image-to-avatar generation, making it a versatile tool for artistic creation and animation.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2312.15059.pdf' target='_blank'>https://arxiv.org/pdf/2312.15059.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15059">Deformable 3D Gaussian Splatting for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2301.02700.pdf' target='_blank'>https://arxiv.org/pdf/2301.02700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, Sergey Tulyakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02700">3DAvatarGAN: Bridging Domains for Personalized Editable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We then distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling -- as a byproduct -- personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions -- for the first time -- allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2509.18924.pdf' target='_blank'>https://arxiv.org/pdf/2509.18924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Teotia, Helge Rhodin, Mohit Mendiratta, Hyeongwoo Kim, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18924">Audio-Driven Universal Gaussian Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the first method for audio-driven universal photorealistic avatar synthesis, combining a person-agnostic speech model with our novel Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity multi-view videos. In particular, our UHAP is supervised with neutral scan data, enabling it to capture the identity-specific details at high fidelity. In contrast to previous approaches, which predominantly map audio features to geometric deformations only while ignoring audio-dependent appearance variations, our universal speech model directly maps raw audio inputs into the UHAP latent expression space. This expression space inherently encodes, both, geometric and appearance variations. For efficient personalization to new subjects, we employ a monocular encoder, which enables lightweight regression of dynamic expression variations across video frames. By accounting for these expression-dependent changes, it enables the subsequent model fine-tuning stage to focus exclusively on capturing the subject's global appearance and geometry. Decoding these audio-driven expression codes via UHAP generates highly realistic avatars with precise lip synchronization and nuanced expressive details, such as eyebrow movement, gaze shifts, and realistic mouth interior appearance as well as motion. Extensive evaluations demonstrate that our method is not only the first generalizable audio-driven avatar model that can account for detailed appearance modeling and rendering, but it also outperforms competing (geometry-only) methods across metrics measuring lip-sync accuracy, quantitative image quality, and perceptual realism.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2506.01802.pdf' target='_blank'>https://arxiv.org/pdf/2506.01802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Zhu, Guoxing Sun, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01802">UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning an animatable and clothed human avatar model with vivid dynamics and photorealistic appearance from multi-view videos is an important foundational research problem in computer graphics and vision. Fueled by recent advances in implicit representations, the quality of the animatable avatars has achieved an unprecedented level by attaching the implicit representation to drivable human template meshes. However, they usually fail to preserve the highest level of detail, particularly apparent when the virtual camera is zoomed in and when rendering at 4K resolution and higher. We argue that this limitation stems from inaccurate surface tracking, specifically, depth misalignment and surface drift between character geometry and the ground truth surface, which forces the detailed appearance model to compensate for geometric errors. To address this, we propose a latent deformation model and supervising the 3D deformation of the animatable character using guidance from foundational 2D video point trackers, which offer improved robustness to shading and surface variations, and are less prone to local minima than differentiable rendering. To mitigate the drift over time and lack of 3D awareness of 2D point trackers, we introduce a cascaded training strategy that generates consistent 3D point tracks by anchoring point tracks to the rendered avatar, which ultimately supervises our avatar at the vertex and texel level. To validate the effectiveness of our approach, we introduce a novel dataset comprising five multi-view video sequences, each over 10 minutes in duration, captured using 40 calibrated 6K-resolution cameras, featuring subjects dressed in clothing with challenging texture patterns and wrinkle deformations. Our approach demonstrates significantly improved performance in rendering quality and geometric accuracy over the prior state of the art.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2505.15385.pdf' target='_blank'>https://arxiv.org/pdf/2505.15385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendrik Junkawitsch, Guoxing Sun, Heming Zhu, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15385">EVA: Expressive Virtual Avatars from Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an expressive template geometry layer and a 3D Gaussian appearance layer. First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2410.01835.pdf' target='_blank'>https://arxiv.org/pdf/2410.01835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchun Chen, Jian Wang, Yinda Zhang, Rohit Pandey, Thabo Beeler, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01835">EgoAvatar: Egocentric View-Driven and Photorealistic Full-body Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive VR telepresence ideally means being able to interact and communicate with digital avatars that are indistinguishable from and precisely reflect the behaviour of their real counterparts. The core technical challenge is two fold: Creating a digital double that faithfully reflects the real human and tracking the real human solely from egocentric sensing devices that are lightweight and have a low energy consumption, e.g. a single RGB camera. Up to date, no unified solution to this problem exists as recent works solely focus on egocentric motion capture, only model the head, or build avatars from multi-view captures. In this work, we, for the first time in literature, propose a person-specific egocentric telepresence approach, which jointly models the photoreal digital avatar while also driving it from a single egocentric video. We first present a character model that is animatible, i.e. can be solely driven by skeletal motion, while being capable of modeling geometry and appearance. Then, we introduce a personalized egocentric motion capture component, which recovers full-body motion from an egocentric video. Finally, we apply the recovered pose to our character model and perform a test-time mesh refinement such that the geometry faithfully projects onto the egocentric view. To validate our design choices, we propose a new and challenging benchmark, which provides paired egocentric and dense multi-view videos of real humans performing various motions. Our experiments demonstrate a clear step towards egocentric and photoreal telepresence as our method outperforms baselines as well as competing methods. For more details, code, and data, we refer to our project page.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2409.11951.pdf' target='_blank'>https://arxiv.org/pdf/2409.11951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Teotia, Hyeongwoo Kim, Pablo Garrido, Marc Habermann, Mohamed Elgharib, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11951">GaussianHeads: End-to-End Learning of Drivable Gaussian Head Avatars from Coarse-to-fine Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time rendering of human head avatars is a cornerstone of many computer graphics applications, such as augmented reality, video games, and films, to name a few. Recent approaches address this challenge with computationally efficient geometry primitives in a carefully calibrated multi-view setup. Albeit producing photorealistic head renderings, it often fails to represent complex motion changes such as the mouth interior and strongly varying head poses. We propose a new method to generate highly dynamic and deformable human head avatars from multi-view imagery in real-time. At the core of our method is a hierarchical representation of head models that allows to capture the complex dynamics of facial expressions and head movements. First, with rich facial features extracted from raw input frames, we learn to deform the coarse facial geometry of the template mesh. We then initialize 3D Gaussians on the deformed surface and refine their positions in a fine step. We train this coarse-to-fine facial avatar model along with the head pose as a learnable parameter in an end-to-end framework. This enables not only controllable facial animation via video inputs, but also high-fidelity novel view synthesis of challenging facial expressions, such as tongue deformations and fine-grained teeth structure under large motion changes. Moreover, it encourages the learned head avatar to generalize towards new facial expressions and head poses at inference time. We demonstrate the performance of our method with comparisons against the related methods on different datasets, spanning challenging facial expression sequences across multiple identities. We also show the potential application of our approach by demonstrating a cross-identity facial performance transfer application.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2408.15995.pdf' target='_blank'>https://arxiv.org/pdf/2408.15995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basavaraj Sunagad, Heming Zhu, Mohit Mendiratta, Adam Kortylewski, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15995">TEDRA: Text-based Editing of Dynamic and Photoreal Actors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past years, significant progress has been made in creating photorealistic and drivable 3D avatars solely from videos of real humans. However, a core remaining challenge is the fine-grained and user-friendly editing of clothing styles by means of textual descriptions. To this end, we present TEDRA, the first method allowing text-based edits of an avatar, which maintains the avatar's high fidelity, space-time coherency, as well as dynamics, and enables skeletal pose and view control. We begin by training a model to create a controllable and high-fidelity digital replica of the real actor. Next, we personalize a pretrained generative diffusion model by fine-tuning it on various frames of the real character captured from different camera angles, ensuring the digital representation faithfully captures the dynamics and movements of the real person. This two-stage process lays the foundation for our approach to dynamic human avatar editing. Utilizing this personalized diffusion model, we modify the dynamic avatar based on a provided text prompt using our Personalized Normal Aligned Score Distillation Sampling (PNA-SDS) within a model-based guidance framework. Additionally, we propose a time step annealing strategy to ensure high-quality edits. Our results demonstrate a clear improvement over prior work in functionality and visual quality.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2312.05941.pdf' target='_blank'>https://arxiv.org/pdf/2312.05941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05941">ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2310.11449.pdf' target='_blank'>https://arxiv.org/pdf/2310.11449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11449">DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance. This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime. Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time. The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2210.12003.pdf' target='_blank'>https://arxiv.org/pdf/2210.12003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-Moll, Michael Zollhoefer, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12003">HDHumans: A Hybrid Approach for High-fidelity Digital Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photo-real digital human avatars are of enormous importance in graphics, as they enable immersive communication over the globe, improve gaming and entertainment experiences, and can be particularly beneficial for AR and VR settings. However, current avatar generation approaches either fall short in high-fidelity novel view synthesis, generalization to novel motions, reproduction of loose clothing, or they cannot render characters at the high resolution offered by modern displays. To this end, we propose HDHumans, which is the first method for HD human character synthesis that jointly produces an accurate and temporally coherent 3D deforming surface and highly photo-realistic images of arbitrary novel views and of motions not seen at training time. At the technical core, our method tightly integrates a classical deforming character template with neural radiance fields (NeRF). Our method is carefully designed to achieve a synergy between classical surface deformation and NeRF. First, the template guides the NeRF, which allows synthesizing novel views of a highly dynamic and articulated character and even enables the synthesis of novel motions. Second, we also leverage the dense pointclouds resulting from NeRF to further improve the deforming surface via 3D-to-3D supervision. We outperform the state of the art quantitatively and qualitatively in terms of synthesis quality and resolution, as well as the quality of 3D surface reconstruction.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2306.03576.pdf' target='_blank'>https://arxiv.org/pdf/2306.03576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Sun, Dingkang Yang, Dongliang Kou, Yang Jiang, Weihua Shan, Zhe Yan, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03576">Human 3D Avatar Modeling with Implicit Neural Representation: A Brief Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A human 3D avatar is one of the important elements in the metaverse, and the modeling effect directly affects people's visual experience. However, the human body has a complex topology and diverse details, so it is often expensive, time-consuming, and laborious to build a satisfactory model. Recent studies have proposed a novel method, implicit neural representation, which is a continuous representation method and can describe objects with arbitrary topology at arbitrary resolution. Researchers have applied implicit neural representation to human 3D avatar modeling and obtained more excellent results than traditional methods. This paper comprehensively reviews the application of implicit neural representation in human body modeling. First, we introduce three implicit representations of occupancy field, SDF, and NeRF, and make a classification of the literature investigated in this paper. Then the application of implicit modeling methods in the body, hand, and head are compared and analyzed respectively. Finally, we point out the shortcomings of current work and provide available suggestions for researchers.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2407.12371.pdf' target='_blank'>https://arxiv.org/pdf/2407.12371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xintao Lv, Liang Xu, Yichao Yan, Xin Jin, Congsheng Xu, Shuwen Wu, Yifan Liu, Lincheng Li, Mengxiao Bi, Wenjun Zeng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12371">HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human-object interactions (HOIs) is critical with the tremendous advances of digital avatars. Existing datasets are typically limited to humans interacting with a single object while neglecting the ubiquitous manipulation of multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of full-body human interacting with multiple objects, containing 3.3K 4D HOI sequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual descriptions and temporal segments, benchmarking two novel tasks of HOI synthesis conditioned on either the whole text prompt or the segmented text prompts as fine-grained timeline control. To address these novel tasks, we propose a dual-branch conditional diffusion model with a mutual interaction module for HOI synthesis. Besides, an auto-regressive generation pipeline is also designed to obtain smooth transitions between HOI segments. Experimental results demonstrate the generalization ability to unseen object geometries and temporal compositions.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2304.03903.pdf' target='_blank'>https://arxiv.org/pdf/2304.03903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi, Xudong Liu, Guo-Jun Qi, Yong Zhang, Xuan Wang, Xiangyu Zhu, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03903">High-Fidelity Clothed Avatar Reconstruction from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence o f the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2204.11184.pdf' target='_blank'>https://arxiv.org/pdf/2204.11184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Zhu, Tingting Liao, Jiangjing Lyu, Xiang Yan, Yunfeng Wang, Kan Guo, Qiong Cao, Stan Z. Li, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.11184">MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider a novel problem of reconstructing a 3D human avatar from multiple unconstrained frames, independent of assumptions on camera calibration, capture space, and constrained actions. The problem should be addressed by a framework that takes multiple unconstrained images as inputs, and generates a shape-with-skinning avatar in the canonical space, finished in one feed-forward pass. To this end, we present 3D Avatar Reconstruction in the wild (ARwild), which first reconstructs the implicit skinning fields in a multi-level manner, by which the image features from multiple images are aligned and integrated to estimate a pixel-aligned implicit function that represents the clothed shape. To enable the training and testing of the new framework, we contribute a large-scale dataset, MVP-Human (Multi-View and multi-Pose 3D Human), which contains 400 subjects, each of which has 15 scans in different poses and 8-view images for each pose, providing 6,000 3D scans and 48,000 images in total. Overall, benefits from the specific network architecture and the diverse data, the trained model enables 3D avatar reconstruction from unconstrained frames and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2509.00403.pdf' target='_blank'>https://arxiv.org/pdf/2509.00403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushuo Chen, Ruizhi Shao, Youxin Pang, Hongwen Zhang, Xinyi Wu, Rihui Wu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00403">DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2411.15205.pdf' target='_blank'>https://arxiv.org/pdf/2411.15205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Zhuang, Di Kang, Linchao Bao, Liang Lin, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15205">DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven avatar generation has gained significant attention owing to its convenience. However, existing methods typically model the human body with all garments as a single 3D model, limiting its usability, such as clothing replacement, and reducing user control over the generation process. To overcome the limitations above, we propose DAGSM, a novel pipeline that generates disentangled human bodies and garments from the given text prompts. Specifically, we model each part (e.g., body, upper/lower clothes) of the clothed human as one GS-enhanced mesh (GSM), which is a traditional mesh attached with 2D Gaussians to better handle complicated textures (e.g., woolen, translucent clothes) and produce realistic cloth animations. During the generation, we first create the unclothed body, followed by a sequence of individual cloth generation based on the body, where we introduce a semantic-based algorithm to achieve better human-cloth and garment-garment separation. To improve texture quality, we propose a view-consistent texture refinement module, including a cross-view attention mechanism for texture style consistency and an incident-angle-weighted denoising (IAW-DE) strategy to update the appearance. Extensive experiments have demonstrated that DAGSM generates high-quality disentangled avatars, supports clothing replacement and realistic animation, and outperforms the baselines in visual quality.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2405.07319.pdf' target='_blank'>https://arxiv.org/pdf/2405.07319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07319">LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2312.03029.pdf' target='_blank'>https://arxiv.org/pdf/2312.03029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanfeng Liao, Yuelang Xu, Zhe Li, Qijing Li, Boyao Zhou, Ruifeng Bai, Di Xu, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03029">HHAvatar: Gaussian Head Avatar with Dynamic Hairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity 3D head avatars has always been a research hotspot, but it remains a great challenge under lightweight sparse view setups. In this paper, we propose HHAvatar represented by controllable 3D Gaussians for high-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians to represent the appearance of the head, and then jointly optimize neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. To address the problem of dynamic hair modeling, we introduce a hybrid head model into our avatar representation based Gaussian Head Avatar and a training method that considers timing information and an occlusion perception module to model the non-rigid motion of hair. Experiments show that our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions and driving hairs reasonably with the motion of the head
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2309.17128.pdf' target='_blank'>https://arxiv.org/pdf/2309.17128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen Zhang, Jinli Suo, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17128">HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of modeling an animatable 3D human head avatar under light-weight setups is of significant importance but has not been well solved. Existing 3D representations either perform well in the realism of portrait images synthesis or the accuracy of expression control, but not both. To address the problem, we introduce a novel hybrid explicit-implicit 3D representation, Facial Model Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF and the prior information from the parametric template. At the core of our representation, a synthetic-renderings-based condition method is proposed to fuse the prior information from the parametric model into the implicit field without constraining its topological flexibility. Besides, based on the hybrid representation, we properly overcome the inconsistent shape issue presented in existing methods and improve the animation stability. Moreover, by adopting an overall GAN-based architecture using an image-to-image translation network, we achieve high-resolution, realistic and view-consistent synthesis of dynamic head appearance. Experiments demonstrate that our method can achieve state-of-the-art performance for 3D head avatar animation compared with previous methods.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2308.05925.pdf' target='_blank'>https://arxiv.org/pdf/2308.05925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqi Su, Liangxiao Hu, Siyou Lin, Hongwen Zhang, Shengping Zhang, Justus Thies, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05925">CaPhy: Capturing Physical Properties for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and physical properties of the clothing from real observations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing. To this end, we combine unsupervised training with physics-based losses and 3D-supervised training using scanned data to reconstruct a dynamic model of clothing that is physically realistic and conforms to the human scans. We also optimize the physical parameters of the underlying physical model from the scans by introducing gradient constraints of the physics-based losses. In contrast to previous work on 3D avatar reconstruction, our method is able to generalize to novel poses with realistic dynamic cloth deformations. Experiments on several subjects demonstrate that our method can estimate the physical properties of the garments, resulting in superior quantitative and qualitative results compared with previous methods.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2308.04914.pdf' target='_blank'>https://arxiv.org/pdf/2308.04914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xumin Huang, Yuan Wu, Jiawen Kang, Jiangtian Nie, Weifeng Zhong, Dong In Kim, Shengli Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04914">Service Reservation and Pricing for Green Metaverses: A Stackelberg Game Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse enables users to communicate, collaborate and socialize with each other through their digital avatars. Due to the spatio-temporal characteristics, co-located users are served well by performing their software components in a collaborative manner such that a Metaverse service provider (MSP) eliminates redundant data transmission and processing, ultimately reducing the total energy consumption. The energyefficient service provision is crucial for enabling the green and sustainable Metaverse. In this article, we take an augmented reality (AR) application as an example to achieve this goal. Moreover, we study an economic issue on how the users reserve offloading services from the MSP and how the MSP determines an optimal charging price since each user is rational to decide whether to accept the offloading service by taking into account the monetary cost. A single-leader multi-follower Stackelberg game is formulated between the MSP and users while each user optimizes an offloading probability to minimize the weighted sum of time, energy consumption and monetary cost. Numerical results show that our scheme achieves energy savings and satisfies individual rationality simultaneously compared with the conventional schemes. Finally, we identify and discuss open directions on how several emerging technologies are combined with the sustainable green Metaverse.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2305.01190.pdf' target='_blank'>https://arxiv.org/pdf/2305.01190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen Zhao, Han Huang, Guojun Qi, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01190">LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing approaches to animatable NeRF-based head avatars are either built upon face templates or use the expression coefficients of templates as the driving signal. Despite the promising progress, their performances are heavily bound by the expression power and the tracking accuracy of the templates. In this work, we present LatentAvatar, an expressive neural head avatar driven by latent expression codes. Such latent expression codes are learned in an end-to-end and self-supervised manner without templates, enabling our method to get rid of expression and tracking issues. To achieve this, we leverage a latent head NeRF to learn the person-specific latent expression codes from a monocular portrait video, and further design a Y-shaped network to learn the shared latent expression codes of different subjects for cross-identity reenactment. By optimizing the photometric reconstruction objectives in NeRF, the latent expression codes are learned to be 3D-aware while faithfully capturing the high-frequency detailed expressions. Moreover, by learning a mapping between the latent expression code learned in shared and person-specific settings, LatentAvatar is able to perform expressive reenactment between different subjects. Experimental results show that our LatentAvatar is able to capture challenging expressions and the subtle movement of teeth and even eyeballs, which outperforms previous state-of-the-art solutions in both quantitative and qualitative comparisons. Project page: https://www.liuyebin.com/latentavatar.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2304.03167.pdf' target='_blank'>https://arxiv.org/pdf/2304.03167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwen Zhang, Siyou Lin, Ruizhi Shao, Yuxiang Zhang, Zerong Zheng, Han Huang, Yandong Guo, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03167">CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating animatable avatars from static scans requires the modeling of clothing deformations in different poses. Existing learning-based methods typically add pose-dependent deformations upon a minimally-clothed mesh template or a learned implicit template, which have limitations in capturing details or hinder end-to-end learning. In this paper, we revisit point-based solutions and propose to decompose explicit garment-related templates and then add pose-dependent wrinkles to them. In this way, the clothing deformations are disentangled such that the pose-dependent wrinkles can be better learned and applied to unseen poses. Additionally, to tackle the seam artifact issues in recent state-of-the-art point-based methods, we propose to learn point features on a body surface, which establishes a continuous and compact feature space to capture the fine-grained and pose-dependent clothing geometry. To facilitate the research in this field, we also introduce a high-quality scan dataset of humans in real-world clothing. Our approach is validated on two existing datasets and our newly introduced dataset, showing better clothing deformation results in unseen poses. The project page with code and dataset can be found at https://www.liuyebin.com/closet.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2211.13206.pdf' target='_blank'>https://arxiv.org/pdf/2211.13206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13206">AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural Voxels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With NeRF widely used for facial reenactment, recent methods can recover photo-realistic 3D head avatar from just a monocular video. Unfortunately, the training process of the NeRF-based methods is quite time-consuming, as MLP used in the NeRF-based methods is inefficient and requires too many iterations to converge. To overcome this problem, we propose AvatarMAV, a fast 3D head avatar reconstruction method using Motion-Aware Neural Voxels. AvatarMAV is the first to model both the canonical appearance and the decoupled expression motion by neural voxels for head avatar. In particular, the motion-aware neural voxels is generated from the weighted concatenation of multiple 4D tensors. The 4D tensors semantically correspond one-to-one with 3DMM expression basis and share the same weights as 3DMM expression coefficients. Benefiting from our novel representation, the proposed AvatarMAV can recover photo-realistic head avatars in just 5 minutes (implemented with pure PyTorch), which is significantly faster than the state-of-the-art facial reenactment methods. Project page: https://www.liuyebin.com/avatarmav.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2211.11208.pdf' target='_blank'>https://arxiv.org/pdf/2211.11208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11208">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D-aware generative adversarial networks (GANs) synthesize high-fidelity and multi-view-consistent facial images using only collections of single-view 2D imagery. Towards fine-grained control over facial attributes, recent efforts incorporate 3D Morphable Face Model (3DMM) to describe deformation in generative radiance fields either explicitly or implicitly. Explicit methods provide fine-grained expression control but cannot handle topological changes caused by hair and accessories, while implicit ones can model varied topologies but have limited generalization caused by the unconstrained deformation fields. We propose a novel 3D GAN framework for unsupervised learning of generative, high-quality and 3D-consistent facial avatars from unstructured 2D images. To achieve both deformation accuracy and topological flexibility, we propose a 3D representation called Generative Texture-Rasterized Tri-planes. The proposed representation learns Generative Neural Textures on top of parametric mesh templates and then projects them into three orthogonal-viewed feature planes through rasterization, forming a tri-plane feature representation for volume rendering. In this way, we combine both fine-grained expression control of mesh-guided explicit deformation and the flexibility of implicit volumetric representation. We further propose specific modules for modeling mouth interior which is not taken into account by 3DMM. Our method demonstrates state-of-the-art 3D-aware synthesis quality and animation ability through extensive experiments. Furthermore, serving as 3D prior, our animatable 3D representation boosts multiple applications including one-shot facial avatars and 3D-aware stylization.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2312.08889.pdf' target='_blank'>https://arxiv.org/pdf/2312.08889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyou Xu, Zongxin Yang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08889">SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: https://yoxu515.github.io/SEEAvatar/.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2309.04946.pdf' target='_blank'>https://arxiv.org/pdf/2309.04946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04946">Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking-head synthesis is a popular research topic for virtual human-related applications. However, the inflexibility and inefficiency of existing methods, which necessitate expensive end-to-end training to transfer emotions from guidance videos to talking-head predictions, are significant limitations. In this work, we propose the Emotional Adaptation for Audio-driven Talking-head (EAT) method, which transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adaptations. Our approach utilizes a pretrained emotion-agnostic talking-head transformer and introduces three lightweight adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and Emotional Adaptation Module) from different perspectives to enable precise and realistic emotion controls. Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including LRW and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable generalization ability, even in scenarios where emotional training videos are scarce or nonexistent. Project website: https://yuangan.github.io/eat/
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2307.06526.pdf' target='_blank'>https://arxiv.org/pdf/2307.06526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Huang, Zongxin Yang, Liangting Li, Yi Yang, Jia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06526">AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale pre-trained vision-language models allow for the zero-shot text-based generation of 3D avatars. The previous state-of-the-art method utilized CLIP to supervise neural implicit models that reconstructed a human body mesh. However, this approach has two limitations. Firstly, the lack of avatar-specific models can cause facial distortion and unrealistic clothing in the generated avatars. Secondly, CLIP only provides optimization direction for the overall appearance, resulting in less impressive results. To address these limitations, we propose AvatarFusion, the first framework to use a latent diffusion model to provide pixel-level guidance for generating human-realistic avatars while simultaneously segmenting clothing from the avatar's body. AvatarFusion includes the first clothing-decoupled neural implicit avatar model that employs a novel Dual Volume Rendering strategy to render the decoupled skin and clothing sub-models in one space. We also introduce a novel optimization method, called Pixel-Semantics Difference-Sampling (PS-DS), which semantically separates the generation of body and clothes, and generates a variety of clothing styles. Moreover, we establish the first benchmark for zero-shot text-to-avatar generation. Our experimental results demonstrate that our framework outperforms previous approaches, with significant improvements observed in all metrics. Additionally, since our model is clothing-decoupled, we can exchange the clothes of avatars. Code are available on our project page https://hansenhuang0823.github.io/AvatarFusion.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2504.14967.pdf' target='_blank'>https://arxiv.org/pdf/2504.14967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Wang, Xuan Wang, Ran Yi, Yanbo Fan, Jichen Hu, Jingcheng Zhu, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14967">3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to construct high-quality 3D head avatars. In this line of research, existing methods either fail to capture the dynamic textures or incur significant overhead in terms of runtime speed or storage space. To this end, we propose a novel method that addresses all the aforementioned demands. In specific, we introduce an expressive and compact representation that encodes texture-related attributes of the 3D Gaussians in the tensorial format. We store appearance of neutral expression in static tri-planes, and represents dynamic texture details for different expressions using lightweight 1D feature lines, which are then decoded into opacity offset relative to the neutral face. We further propose adaptive truncated opacity penalty and class-balanced sampling to improve generalization across different expressions. Experiments show this design enables accurate face dynamic details capturing while maintains real-time rendering and significantly reduces storage costs, thus broadening the applicability to more scenarios.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2404.02152.pdf' target='_blank'>https://arxiv.org/pdf/2404.02152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02152">GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2306.03038.pdf' target='_blank'>https://arxiv.org/pdf/2306.03038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03038">HeadSculpt: Crafting 3D Head Avatars with Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text-guided 3D generative methods have made remarkable advancements in producing high-quality textures and geometry, capitalizing on the proliferation of large vision-language and image diffusion models. However, existing methods still struggle to create high-fidelity 3D head avatars in two aspects: (1) They rely mostly on a pre-trained text-to-image diffusion model whilst missing the necessary 3D awareness and head priors. This makes them prone to inconsistency and geometric distortions in the generated avatars. (2) They fall short in fine-grained editing. This is primarily due to the inherited limitations from the pre-trained 2D image diffusion models, which become more pronounced when it comes to 3D head avatars. In this work, we address these challenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculpt for crafting (i.e., generating and editing) 3D head avatars from textual prompts. Specifically, we first equip the diffusion model with 3D awareness by leveraging landmark-based control and a learned textual embedding representing the back view appearance of heads, enabling 3D-consistent head avatar generations. We further propose a novel identity-aware editing score distillation strategy to optimize a textured mesh with a high-resolution differentiable rendering technique. This enables identity preservation while following the editing instruction. We showcase HeadSculpt's superior fidelity and editing capabilities through comprehensive experiments and comparisons with existing methods.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2405.08527.pdf' target='_blank'>https://arxiv.org/pdf/2405.08527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arian Beckmann, Tilman Stephani, Felix Klotzsche, Yonghao Chen, Simon M. Hofmann, Arno Villringer, Michael Gaebler, Vadim Nikulin, Sebastian Bosse, Peter Eisert, Anna Hilsmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08527">EEG-Features for Generalized Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the advent of Deepfakes in digital media, the development of robust and reliable detection mechanism is urgently called for. In this study, we explore a novel approach to Deepfake detection by utilizing electroencephalography (EEG) measured from the neural processing of a human participant who viewed and categorized Deepfake stimuli from the FaceForensics++ datset. These measurements serve as input features to a binary support vector classifier, trained to discriminate between real and manipulated facial images. We examine whether EEG data can inform Deepfake detection and also if it can provide a generalized representation capable of identifying Deepfakes beyond the training domain. Our preliminary results indicate that human neural processing signals can be successfully integrated into Deepfake detection frameworks and hint at the potential for a generalized neural representation of artifacts in computer generated faces. Moreover, our study provides next steps towards the understanding of how digital realism is embedded in the human cognitive system, possibly enabling the development of more realistic digital avatars in the future.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2403.04380.pdf' target='_blank'>https://arxiv.org/pdf/2403.04380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wolfgang Paier, Paul Hinzer, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04380">Video-Driven Animation of Neural Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new approach for video-driven animation of high-quality neural 3D head models, addressing the challenge of person-independent animation from video input. Typically, high-quality generative models are learned for specific individuals from multi-view video footage, resulting in person-specific latent representations that drive the generation process. In order to achieve person-independent animation from video input, we introduce an LSTM-based animation network capable of translating person-independent expression features into personalized animation parameters of person-specific 3D head models. Our approach combines the advantages of personalized head models (high quality and realism) with the convenience of video-driven animation employing multi-person facial performance capture. We demonstrate the effectiveness of our approach on synthesized animations with high quality based on different source videos as well as an ablation study.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2401.00711.pdf' target='_blank'>https://arxiv.org/pdf/2401.00711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00711">Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human models directly from text helps reduce the cost and time of character modeling. However, achieving multi-attribute controllable and realistic 3D human avatar generation is still challenging due to feature coupling and the scarcity of realistic 3D human avatar datasets. To address these issues, we propose Text2Avatar, which can generate realistic-style 3D avatars based on the coupled text prompts. Text2Avatar leverages a discrete codebook as an intermediate feature to establish a connection between text and avatars, enabling the disentanglement of features. Furthermore, to alleviate the scarcity of realistic style 3D human avatar data, we utilize a pre-trained unconditional 3D human avatar generation model to obtain a large amount of 3D avatar pseudo data, which allows Text2Avatar to achieve realistic style generation. Experimental results demonstrate that our method can generate realistic 3D avatars from coupled textual data, which is challenging for other existing methods in this field.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2310.03615.pdf' target='_blank'>https://arxiv.org/pdf/2310.03615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wieland Morgenstern, Milena T. Bagdasarian, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03615">Animatable Virtual Humans: Learning pose-dependent human representations in UV space for interactive performance synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel representation of virtual humans for highly realistic real-time animation and rendering in 3D applications. We learn pose dependent appearance and geometry from highly accurate dynamic mesh sequences obtained from state-of-the-art multiview-video reconstruction. Learning pose-dependent appearance and geometry from mesh sequences poses significant challenges, as it requires the network to learn the intricate shape and articulated motion of a human body. However, statistical body models like SMPL provide valuable a-priori knowledge which we leverage in order to constrain the dimension of the search space enabling more efficient and targeted learning and define pose-dependency. Instead of directly learning absolute pose-dependent geometry, we learn the difference between the observed geometry and the fitted SMPL model. This allows us to encode both pose-dependent appearance and geometry in the consistent UV space of the SMPL model. This approach not only ensures a high level of realism but also facilitates streamlined processing and rendering of virtual humans in real-time scenarios.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2307.03441.pdf' target='_blank'>https://arxiv.org/pdf/2307.03441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, Baoyuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03441">NOFA: NeRF-based One-shot Facial Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D facial avatar reconstruction has been a significant research topic in computer graphics and computer vision, where photo-realistic rendering and flexible controls over poses and expressions are necessary for many related applications. Recently, its performance has been greatly improved with the development of neural radiance fields (NeRF). However, most existing NeRF-based facial avatars focus on subject-specific reconstruction and reenactment, requiring multi-shot images containing different views of the specific subject for training, and the learned model cannot generalize to new identities, limiting its further applications. In this work, we propose a one-shot 3D facial avatar reconstruction framework that only requires a single source image to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking generalization ability and missing multi-view information, we leverage the generative prior of 3D GAN and develop an efficient encoder-decoder network to reconstruct the canonical neural volume of the source image, and further propose a compensation network to complement facial details. To enable fine-grained control over facial dynamics, we propose a deformation field to warp the canonical volume into driven expressions. Through extensive experimental comparisons, we achieve superior synthesis results compared to several state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2306.03504.pdf' target='_blank'>https://arxiv.org/pdf/2306.03504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03504">Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We are interested in a novel task, namely low-resource text-to-talking avatar. Given only a few-minute-long talking person video with the audio track as the training data and arbitrary texts as the driving input, we aim to synthesize high-quality talking portrait videos corresponding to the input text. This task has broad application prospects in the digital human industry but has not been technically achieved yet due to two challenges: (1) It is challenging to mimic the timbre from out-of-domain audio for a traditional multi-speaker Text-to-Speech system. (2) It is hard to render high-fidelity and lip-synchronized talking avatars with limited training data. In this paper, we introduce Adaptive Text-to-Talking Avatar (Ada-TTA), which (1) designs a generic zero-shot multi-speaker TTS model that well disentangles the text content, timbre, and prosody; and (2) embraces recent advances in neural rendering to achieve realistic audio-driven talking face video generation. With these designs, our method overcomes the aforementioned two challenges and achieves to generate identity-preserving speech and realistic talking person video. Experiments demonstrate that our method could synthesize realistic, identity-preserving, and audio-visual synchronized talking avatar videos.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2211.15064.pdf' target='_blank'>https://arxiv.org/pdf/2211.15064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunpeng Bai, Yanbo Fan, Xuan Wang, Yong Zhang, Jingxiang Sun, Chun Yuan, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15064">High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity facial avatar reconstruction from a monocular video is a significant research problem in computer graphics and computer vision. Recently, Neural Radiance Field (NeRF) has shown impressive novel view rendering results and has been considered for facial avatar reconstruction. However, the complex facial dynamics and missing 3D information in monocular videos raise significant challenges for faithful facial reconstruction. In this work, we propose a new method for NeRF-based facial avatar reconstruction that utilizes 3D-aware generative prior. Different from existing works that depend on a conditional deformation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN. We propose an efficient method to construct the personalized generative prior based on a small set of facial images of a given individual. After learning, it allows for photo-realistic rendering with novel views and the face reenactment can be realized by performing navigation in the latent space. Our proposed method is applicable for different driven signals, including RGB images, 3DMM coefficients, and audios. Compared with existing works, we obtain superior novel view synthesis results and faithfully face reenactment performance.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2502.12080.pdf' target='_blank'>https://arxiv.org/pdf/2502.12080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12080">HumanGif: Single-View Human Diffusion with Generative Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous 3D human creation methods have made significant progress in synthesizing view-consistent and temporally aligned results from sparse-view images or monocular videos. However, it remains challenging to produce perpetually realistic, view-consistent, and temporally coherent human avatars from a single image, as limited information is available in the single-view input setting. Motivated by the success of 2D character animation, we propose HumanGif, a single-view human diffusion model with generative prior. Specifically, we formulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models to complement the missing information. To ensure fine-grained and consistent novel view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn spatially aligned features from the input image, implicitly capturing the relative camera and human pose transformation. Furthermore, we introduce an image-level loss during optimization to bridge the gap between latent and image spaces in diffusion models. Extensive experiments on RenderPeople, DNA-Rendering, THuman 2.1, and TikTok datasets demonstrate that HumanGif achieves the best perceptual performance, with better generalizability for novel view and pose synthesis.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2502.10124.pdf' target='_blank'>https://arxiv.org/pdf/2502.10124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Li, Yishu Ji, Ruijia Chen, Tianqi Liu, Yuntao Wang, Yuanchun Shi, Yukang Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10124">Modeling the Impact of Visual Stimuli on Redirection Noticeability with Gaze Behavior in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While users could embody virtual avatars that mirror their physical movements in Virtual Reality, these avatars' motions can be redirected to enable novel interactions. Excessive redirection, however, could break the user's sense of embodiment due to perceptual conflicts between vision and proprioception. While prior work focused on avatar-related factors influencing the noticeability of redirection, we investigate how the visual stimuli in the surrounding virtual environment affect user behavior and, in turn, the noticeability of redirection. Given the wide variety of different types of visual stimuli and their tendency to elicit varying individual reactions, we propose to use users' gaze behavior as an indicator of their response to the stimuli and model the noticeability of redirection. We conducted two user studies to collect users' gaze behavior and noticeability, investigating the relationship between them and identifying the most effective gaze behavior features for predicting noticeability. Based on the data, we developed a regression model that takes users' gaze behavior as input and outputs the noticeability of redirection. We then conducted an evaluation study to test our model on unseen visual stimuli, achieving an accuracy of 0.012 MSE. We further implemented an adaptive redirection technique and conducted a proof-of-concept study to evaluate its effectiveness with complex visual stimuli in two applications. The results indicated that participants experienced less physical demanding and a stronger sense of body ownership when using our adaptive technique, demonstrating the potential of our model to support real-world use cases.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2407.03204.pdf' target='_blank'>https://arxiv.org/pdf/2407.03204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, Zhangyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03204">Expressive Gaussian Human Avatars from Monocular RGB Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuanced expressiveness, particularly through fine-grained hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations. In this work, we focus on investigating the expressiveness of human avatars when learned from monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details. To this end, we introduce EVA, a drivable human model that meticulously sculpts fine details based on 3D Gaussians and SMPL-X, an expressive parametric human model. Focused on enhancing expressiveness, our work makes three key contributions. First, we highlight the critical importance of aligning the SMPL-X model with RGB frames for effective avatar learning. Recognizing the limitations of current SMPL-X prediction methods for in-the-wild videos, we introduce a plug-and-play module that significantly ameliorates misalignment issues. Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts. Last but not least, we develop a feedback mechanism that predicts per-pixel confidence to better guide the learning of 3D Gaussians. Extensive experiments on two benchmarks demonstrate the superiority of our framework both quantitatively and qualitatively, especially on the fine-grained hand and facial details. See the project website at \url{https://evahuman.github.io}
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2311.05126.pdf' target='_blank'>https://arxiv.org/pdf/2311.05126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianqi Liu, Joshua Rafael Sanchez, Yuntao Wang, Xin Yi, Yuanchun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05126">Exploring and Analyzing the Effect of Avatar's Realism on Anxiety of English as Second Language (ESL) Speakers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of virtual avatars provides innovative opportunities for remote conferencing, education, and more. Our study investigates how the realism of avatars, used by native English speakers, impacts the anxiety levels of English as a Second Language (ESL) speakers during interactions. ESL participants engaged in conversations with native English speakers represented through cartoonish avatars, realistic-like avatars, or actual video streams. We measured both the ESL speakers' self-reported anxiety and their physiological indicators of anxiety. Our findings show that interactions with native speakers using cartoonish avatars or direct video lead to reduced anxiety levels among ESL participants. However, interactions with avatars that closely resemble humans heightened these anxieties. These insights are critically important for the design and application of virtual avatars, especially in addressing cross-cultural communication barriers and enhancing user experience.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2509.02466.pdf' target='_blank'>https://arxiv.org/pdf/2509.02466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwen Wang, Yiyu Zhuang, Jiawei Zhang, Li Wang, Yifei Zeng, Xun Cao, Xinxin Zuo, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02466">TeRA: Rethinking Text-guided Realistic 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative models. Our approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human representation. Experiments have proven our approach's superiority over previous text-to-avatar generative models in subjective and objective evaluation.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2504.08581.pdf' target='_blank'>https://arxiv.org/pdf/2504.08581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08581">FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2501.13335.pdf' target='_blank'>https://arxiv.org/pdf/2501.13335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianrui Luo, Juewen Peng, Zhongang Cai, Lei Yang, Fan Yang, Zhiguo Cao, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13335">Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for modeling high-fidelity, animatable 3D human avatars from motion-blurred monocular video inputs. Motion blur is prevalent in real-world dynamic video capture, especially due to human movements in 3D human avatar modeling. Existing methods either (1) assume sharp image inputs, failing to address the detail loss introduced by motion blur, or (2) mainly consider blur by camera movements, neglecting the human motion blur which is more common in animatable avatars. Our proposed approach integrates a human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By explicitly modeling human motion trajectories during exposure time, we jointly optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality human avatars. We employ a pose-dependent fusion mechanism to distinguish moving body regions, optimizing both blurred and sharp areas effectively. Extensive experiments on synthetic and real-world datasets demonstrate that our method significantly outperforms existing methods in rendering quality and quantitative metrics, producing sharp avatar reconstructions and enabling real-time rendering under challenging motion blur conditions.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2501.04631.pdf' target='_blank'>https://arxiv.org/pdf/2501.04631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitian Zhang, Yichao Yan, Sijing Wu, Manwen Liao, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04631">Disentangled Clothed Avatar Generation with Layered Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. Previous methods have achieved success in generating diverse digital avatars, however, generating avatars with disentangled components (\eg, body, hair, and clothes) has long been a challenge. In this paper, we propose LayerAvatar, the first feed-forward diffusion-based method for generating component-disentangled clothed avatars. To achieve this, we first propose a layered UV feature plane representation, where components are distributed in different layers of the Gaussian-based UV feature plane with corresponding semantic labels. This representation supports high-resolution and real-time rendering, as well as expressive animation including controllable gestures and facial expressions. Based on the well-designed representation, we train a single-stage diffusion model and introduce constrain terms to address the severe occlusion problem of the innermost human body layer. Extensive experiments demonstrate the impressive performances of our method in generating disentangled clothed avatars, and we further explore its applications in component transfer. The project page is available at: https://olivia23333.github.io/LayerAvatar/
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2411.15604.pdf' target='_blank'>https://arxiv.org/pdf/2411.15604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Zhang, Zijian Wu, Zhiyang Liang, Yicheng Gong, Dongfang Hu, Yao Yao, Xun Cao, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15604">FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing high-fidelity, animatable 3D head avatars from effortlessly captured monocular videos is a pivotal yet formidable challenge. Although significant progress has been made in rendering performance and manipulation capabilities, notable challenges remain, including incomplete reconstruction and inefficient Gaussian representation. To address these challenges, we introduce FATE, a novel method for reconstructing an editable full-head avatar from a single monocular video. FATE integrates a sampling-based densification strategy to ensure optimal positional distribution of points, improving rendering efficiency. A neural baking technique is introduced to convert discrete Gaussian representations into continuous attribute maps, facilitating intuitive appearance editing. Furthermore, we propose a universal completion framework to recover non-frontal appearance, culminating in a 360$^\circ$-renderable 3D head avatar. FATE outperforms previous approaches in both qualitative and quantitative evaluations, achieving state-of-the-art performance. To the best of our knowledge, FATE is the first animatable and 360$^\circ$ full-head monocular reconstruction method for a 3D head avatar.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2410.01226.pdf' target='_blank'>https://arxiv.org/pdf/2410.01226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyu Zhuang, Yuxiao He, Jiawei Zhang, Yanwen Wang, Jiahe Zhu, Yao Yao, Siyu Zhu, Xun Cao, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01226">Towards Native Generative Model for 3D Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating 3D head avatars is a significant yet challenging task for many applicated scenarios. Previous studies have set out to learn 3D human head generative models using massive 2D image data. Although these models are highly generalizable for human appearance, their result models are not 360$^\circ$-renderable, and the predicted 3D geometry is unreliable. Therefore, such results cannot be used in VR, game modeling, and other scenarios that require 360$^\circ$-renderable 3D head models. An intuitive idea is that 3D head models with limited amount but high 3D accuracy are more reliable training data for a high-quality 3D generative model. In this vein, we delve into how to learn a native generative model for 360$^\circ$ full head from a limited 3D head dataset. Specifically, three major problems are studied: 1) how to effectively utilize various representations for generating the 360$^\circ$-renderable human head; 2) how to disentangle the appearance, shape, and motion of human faces to generate a 3D head model that can be edited by appearance and driven by motion; 3) and how to extend the generalization capability of the generative model to support downstream tasks. Comprehensive experiments are conducted to verify the effectiveness of the proposed model. We hope the proposed models and artist-designed dataset can inspire future research on learning native generative 3D head models from limited 3D datasets.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2405.19203.pdf' target='_blank'>https://arxiv.org/pdf/2405.19203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19203">$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing. Our project page is https://olivia23333.github.io/E3Gen.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2312.02209.pdf' target='_blank'>https://arxiv.org/pdf/2312.02209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yang, Tianyi Chen, Xiaosheng He, Zhongang Cai, Lei Yang, Si Wu, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02209">AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Editable 3D-aware generation, which supports user-interacted editing, has witnessed rapid development recently. However, existing editable 3D GANs either fail to achieve high-accuracy local editing or suffer from huge computational costs. We propose AttriHuman-3D, an editable 3D human generation model, which address the aforementioned problems with attribute decomposition and indexing. The core idea of the proposed model is to generate all attributes (e.g. human body, hair, clothes and so on) in an overall attribute space with six feature planes, which are then decomposed and manipulated with different attribute indexes. To precisely extract features of different attributes from the generated feature planes, we propose a novel attribute indexing method as well as an orthogonal projection regularization to enhance the disentanglement. We also introduce a hyper-latent training strategy and an attribute-specific sampling strategy to avoid style entanglement and misleading punishment from the discriminator. Our method allows users to interactively edit selected attributes in the generated 3D human avatars while keeping others fixed. Both qualitative and quantitative experiments demonstrate that our model provides a strong disentanglement between different attributes, allows fine-grained image editing and generates high-quality 3D human avatars.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2306.09864.pdf' target='_blank'>https://arxiv.org/pdf/2306.09864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09864">AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AvatarBooth, a novel method for generating high-quality 3D avatars using text prompts or specific images. Unlike previous approaches that can only synthesize avatars based on simple text descriptions, our method enables the creation of personalized avatars from casually captured face or body images, while still supporting text-based model generation and editing. Our key contribution is the precise avatar generation control by using dual fine-tuned diffusion models separately for the human face and body. This enables us to capture intricate details of facial appearance, clothing, and accessories, resulting in highly realistic avatar generations. Furthermore, we introduce pose-consistent constraint to the optimization process to enhance the multi-view consistency of synthesized head images from the diffusion model and thus eliminate interference from uncontrolled human poses. In addition, we present a multi-resolution rendering strategy that facilitates coarse-to-fine supervision of 3D avatar generation, thereby enhancing the performance of the proposed system. The resulting avatar model can be further edited using additional text descriptions and driven by motion sequences. Experiments show that AvatarBooth outperforms previous text-to-3D methods in terms of rendering and geometric quality from either text prompts or specific images. Please check our project website at https://zeng-yifei.github.io/avatarbooth_page/.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2203.07931.pdf' target='_blank'>https://arxiv.org/pdf/2203.07931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.07931">DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversation is an essential component of virtual avatar activities in the metaverse. With the development of natural language processing, textual and vocal conversation generation has achieved a significant breakthrough. However, face-to-face conversations account for the vast majority of daily conversations, while most existing methods focused on single-person talking head generation. In this work, we take a step further and consider generating realistic face-to-face conversation videos. Conversation generation is more challenging than single-person talking head generation, since it not only requires generating photo-realistic individual talking heads but also demands the listener to respond to the speaker. In this paper, we propose a novel unified framework based on neural radiance field (NeRF) to address this task. Specifically, we model both the speaker and listener with a NeRF framework, with different conditions to control individual expressions. The speaker is driven by the audio signal, while the response of the listener depends on both visual and acoustic information. In this way, face-to-face conversation videos are generated between human avatars, with all the interlocutors modeled within the same network. Moreover, to facilitate future research on this task, we collect a new human conversation dataset containing 34 clips of videos. Quantitative and qualitative experiments evaluate our method in different aspects, e.g., image quality, pose sequence trend, and naturalness of the rendering videos. Experimental results demonstrate that the avatars in the resulting videos are able to perform a realistic conversation, and maintain individual styles. All the code, data, and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2507.13052.pdf' target='_blank'>https://arxiv.org/pdf/2507.13052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13052">Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2404.01053.pdf' target='_blank'>https://arxiv.org/pdf/2404.01053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01053">HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2402.04101.pdf' target='_blank'>https://arxiv.org/pdf/2402.04101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04101">VRMM: A Volumetric Relightable Morphable Head Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables high-quality 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2309.04247.pdf' target='_blank'>https://arxiv.org/pdf/2309.04247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Yang, Mingwu Zheng, Wanquan Feng, Haibin Huang, Yu-Kun Lai, Pengfei Wan, Zhongyuan Wang, Chongyang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04247">Towards Practical Capture of High-Fidelity Relightable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel framework, Tracking-free Relightable Avatar (TRAvatar), for capturing and reconstructing high-fidelity 3D avatars. Compared to previous methods, TRAvatar works in a more practical and efficient setting. Specifically, TRAvatar is trained with dynamic image sequences captured in a Light Stage under varying lighting conditions, enabling realistic relighting and real-time animation for avatars in diverse scenes. Additionally, TRAvatar allows for tracking-free avatar capture and obviates the need for accurate surface tracking under varying illumination conditions. Our contributions are two-fold: First, we propose a novel network architecture that explicitly builds on and ensures the satisfaction of the linear nature of lighting. Trained on simple group light captures, TRAvatar can predict the appearance in real-time with a single forward pass, achieving high-quality relighting effects under illuminations of arbitrary environment maps. Second, we jointly optimize the facial geometry and relightable appearance from scratch based on image sequences, where the tracking is implicitly learned. This tracking-free approach brings robustness for establishing temporal correspondences between frames under different lighting conditions. Extensive qualitative and quantitative experiments demonstrate that our framework achieves superior performance for photorealistic avatar animation and relighting.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2308.09705.pdf' target='_blank'>https://arxiv.org/pdf/2308.09705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09705">Guide3D: Create 3D Avatars from Text and Image Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text-to-image generation has exhibited remarkable advancements, with the ability to produce visually impressive results. In contrast, text-to-3D generation has not yet reached a comparable level of quality. Existing methods primarily rely on text-guided score distillation sampling (SDS), and they encounter difficulties in transferring 2D attributes of the generated images to 3D content. In this work, we aim to develop an effective 3D generative model capable of synthesizing high-resolution textured meshes by leveraging both textual and image information. To this end, we introduce Guide3D, a zero-shot text-and-image-guided generative model for 3D avatar generation based on diffusion models. Our model involves (1) generating sparse-view images of a text-consistent character using diffusion models, and (2) jointly optimizing multi-resolution differentiable marching tetrahedral grids with pixel-aligned image features. We further propose a similarity-aware feature fusion strategy for efficiently integrating features from different views. Moreover, we introduce two novel training objectives as an alternative to calculating SDS, significantly enhancing the optimization process. We thoroughly evaluate the performance and components of our framework, which outperforms the current state-of-the-art in producing topologically and structurally correct geometry and high-resolution textures. Guide3D enables the direct transfer of 2D-generated images to the 3D space. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2307.05000.pdf' target='_blank'>https://arxiv.org/pdf/2307.05000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Wang, Di Kang, Yan-Pei Cao, Linchao Bao, Ying Shan, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05000">Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rendering photorealistic and dynamically moving human heads is crucial for ensuring a pleasant and immersive experience in AR/VR and video conferencing applications. However, existing methods often struggle to model challenging facial regions (e.g., mouth interior, eyes, hair/beard), resulting in unrealistic and blurry results. In this paper, we propose {\fullname} ({\name}), a method that adopts the neural point representation as well as the neural volume rendering process and discards the predefined connectivity and hard correspondence imposed by mesh-based approaches. Specifically, the neural points are strategically constrained around the surface of the target expression via a high-resolution UV displacement map, achieving increased modeling capacity and more accurate control. We introduce three technical innovations to improve the rendering and training efficiency: a patch-wise depth-guided (shading point) sampling strategy, a lightweight radiance decoding process, and a Grid-Error-Patch (GEP) ray sampling strategy during training. By design, our {\name} is better equipped to handle topologically changing regions and thin structures while also ensuring accurate expression control when animating avatars. Experiments conducted on three subjects from the Multiface dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods, especially in handling challenging facial regions.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2304.00916.pdf' target='_blank'>https://arxiv.org/pdf/2304.00916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00916">DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body's shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face ''Janus'' problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing methods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2506.13766.pdf' target='_blank'>https://arxiv.org/pdf/2506.13766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingteng Qiu, Peihao Li, Qi Zuo, Xiaodong Gu, Yuan Dong, Weihao Yuan, Siyu Zhu, Xiaoguang Han, Guanying Chen, Zilong Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13766">PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations. Code and models will be released to the public.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2412.11586.pdf' target='_blank'>https://arxiv.org/pdf/2412.11586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaokun Sun, Zeyu Cai, Ying Tai, Jian Yang, Zhenyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11586">StrandHead: Text to Hair-Disentangled 3D Head Avatars Using Human-Centric Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the data limitation or entangled representation. We propose StrandHead, a novel text-driven method capable of generating 3D hair strands and disentangled head avatars with strand-level attributes. Instead of using large-scale hair-text paired data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative models pre-trained on human mesh data. To this end, we propose a meshing approach guided by strand geometry to guarantee the gradient flow from the distillation objective to the neural strand representation. The optimization is then regularized by statistically significant haircut features, leading to stable updating of strands against unreasonable drifting. These employed 2D/3D human-centric priors contribute to text-aligned and realistic 3D strand generation. Extensive experiments show that StrandHead achieves the state-of-the-art performance on text to strand generation and disentangled 3D head avatar modeling. The generated 3D hair can be applied on avatars for strand-level editing, as well as implemented in the graphics engine for physical simulation or other applications. Project page: https://xiaokunsun.github.io/StrandHead.github.io/.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2412.02684.pdf' target='_blank'>https://arxiv.org/pdf/2412.02684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02684">AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2408.09126.pdf' target='_blank'>https://arxiv.org/pdf/2408.09126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaokun Sun, Zhenyu Zhang, Ying Tai, Hao Tang, Zili Yi, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09126">Barbie: Text to Barbie-Style 3D Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To integrate digital humans into everyday life, there is a strong demand for generating high-quality, fine-grained disentangled 3D avatars that support expressive animation and simulation capabilities, ideally from low-cost textual inputs. Although text-driven 3D avatar generation has made significant progress by leveraging 2D generative priors, existing methods still struggle to fulfill all these requirements simultaneously. To address this challenge, we propose Barbie, a novel text-driven framework for generating animatable 3D avatars with separable shoes, accessories, and simulation-ready garments, truly capturing the iconic ``Barbie doll'' aesthetic. The core of our framework lies in an expressive 3D representation combined with appropriate modeling constraints. Unlike previous methods, we innovatively employ G-Shell to uniformly model both watertight components (e.g., bodies, shoes, and accessories) and non-watertight garments compatible with simulation. Furthermore, we introduce a well-designed initialization and a hole regularization loss to ensure clean open surface modeling. These disentangled 3D representations are then optimized by specialized expert diffusion models tailored to each domain, ensuring high-fidelity outputs. To mitigate geometric artifacts and texture conflicts when combining different expert models, we further propose several effective geometric losses and strategies. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation. Our framework further enables diverse applications, including apparel combination, editing, expressive animation, and physical simulation. Our project page is: https://xiaokunsun.github.io/Barbie.github.io
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2409.17145.pdf' target='_blank'>https://arxiv.org/pdf/2409.17145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17145">DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2405.15758.pdf' target='_blank'>https://arxiv.org/pdf/2405.15758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15758">InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is https://wangyuchi369.github.io/InstructAvatar/.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2404.19110.pdf' target='_blank'>https://arxiv.org/pdf/2404.19110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19110">EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:
  Enhance the model's capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.
  Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.
  We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2402.17364.pdf' target='_blank'>https://arxiv.org/pdf/2402.17364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17364">Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2311.15230.pdf' target='_blank'>https://arxiv.org/pdf/2311.15230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, Sheng Zhao, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15230">GAIA: Zero-shot Talking Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot talking avatar generation aims at synthesizing natural talking videos from speech and a single portrait image. Previous methods have relied on domain-specific heuristics such as warping-based motion representation and 3D Morphable Models, which limit the naturalness and diversity of the generated avatars. In this work, we introduce GAIA (Generative AI for Avatar), which eliminates the domain priors in talking avatar generation. In light of the observation that the speech only drives the motion of the avatar while the appearance of the avatar and the background typically remain the same throughout the entire video, we divide our approach into two stages: 1) disentangling each frame into motion and appearance representations; 2) generating motion sequences conditioned on the speech and reference portrait image. We collect a large-scale high-quality talking avatar dataset and train the model on it with different scales (up to 2B parameters). Experimental results verify the superiority, scalability, and flexibility of GAIA as 1) the resulting model beats previous baseline models in terms of naturalness, diversity, lip-sync quality, and visual quality; 2) the framework is scalable since larger models yield better results; 3) it is general and enables different applications like controllable talking avatar generation and text-instructed avatar generation.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2308.15016.pdf' target='_blank'>https://arxiv.org/pdf/2308.15016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longbin Ji, Pengfei Wei, Yi Ren, Jinglin Liu, Chen Zhang, Xiang Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15016">C2G2: Controllable Co-speech Gesture Generation with Latent Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gesture generation is crucial for automatic digital avatar animation. However, existing methods suffer from issues such as unstable training and temporal inconsistency, particularly in generating high-fidelity and comprehensive gestures. Additionally, these methods lack effective control over speaker identity and temporal editing of the generated gestures. Focusing on capturing temporal latent information and applying practical controlling, we propose a Controllable Co-speech Gesture Generation framework, named C2G2. Specifically, we propose a two-stage temporal dependency enhancement strategy motivated by latent diffusion models. We further introduce two key features to C2G2, namely a speaker-specific decoder to generate speaker-related real-length skeletons and a repainting strategy for flexible gesture generation/editing. Extensive experiments on benchmark gesture datasets verify the effectiveness of our proposed C2G2 compared with several state-of-the-art baselines. The link of the project demo page can be found at https://c2g2-gesture.github.io/c2_gesture
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2305.12529.pdf' target='_blank'>https://arxiv.org/pdf/2305.12529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12529">DreamWaltz: Make a Scene with Complex 3D Animatable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable 3D avatar representation from abundant image priors of diffusion model conditioned on various poses, which could animate complex non-rigged avatars given arbitrary poses without retraining. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The proposed framework further enables the creation of complex scenes with diverse compositions, including avatar-avatar, avatar-object and avatar-scene interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and animation results.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2405.05691.pdf' target='_blank'>https://arxiv.org/pdf/2405.05691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Huang, Hui Yang, Chuanchen Luo, Yuxi Wang, Shibiao Xu, Zhaoxiang Zhang, Man Zhang, Junran Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05691">StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thanks to the powerful generative capacity of diffusion models, recent years have witnessed rapid progress in human motion generation. Existing diffusion-based methods employ disparate network architectures and training strategies. The effect of the design of each component is still unclear. In addition, the iterative denoising process consumes considerable computational overhead, which is prohibitive for real-time scenarios such as virtual characters and humanoid robots. For this reason, we first conduct a comprehensive investigation into network architectures, training strategies, and inference processs. Based on the profound analysis, we tailor each component for efficient high-quality human motion generation. Despite the promising performance, the tailored model still suffers from foot skating which is an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we identify foot-ground contact and correct foot motions along the denoising process. By organically combining these well-designed components together, we present StableMoFusion, a robust and efficient framework for human motion generation. Extensive experimental results show that our StableMoFusion performs favorably against current state-of-the-art methods. Project page: https://h-y1heng.github.io/StableMoFusion-page/
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2405.11286.pdf' target='_blank'>https://arxiv.org/pdf/2405.11286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11286">Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant interest in creating 3D avatars and motions, driven by their diverse applications in areas like film-making, video games, AR/VR, and human-robot interaction. However, current efforts primarily concentrate on either generating the 3D avatar mesh alone or producing motion sequences, with integrating these two aspects proving to be a persistent challenge. Additionally, while avatar and motion generation predominantly target humans, extending these techniques to animals remains a significant challenge due to inadequate training data and methods. To bridge these gaps, our paper presents three key contributions. Firstly, we proposed a novel agent-based approach named Motion Avatar, which allows for the automatic generation of high-quality customizable human and animal avatars with motions through text queries. The method significantly advanced the progress in dynamic 3D character generation. Secondly, we introduced a LLM planner that coordinates both motion and avatar generation, which transforms a discriminative planning into a customizable Q&A fashion. Lastly, we presented an animal motion dataset named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65 animal categories and its building pipeline ZooGen, which serves as a valuable resource for the community. See project website https://steve-zeyu-zhang.github.io/MotionAvatar/
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2404.04421.pdf' target='_blank'>https://arxiv.org/pdf/2404.04421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zheng, Qingqing Zhao, Guandao Yang, Wang Yifan, Donglai Xiang, Florian Dubost, Dmitry Lagun, Thabo Beeler, Federico Tombari, Leonidas Guibas, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04421">PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling and rendering photorealistic avatars is of crucial importance in many applications. Existing methods that build a 3D avatar from visual observations, however, struggle to reconstruct clothed humans. We introduce PhysAvatar, a novel framework that combines inverse rendering with inverse physics to automatically estimate the shape and appearance of a human from multi-view video data along with the physical parameters of the fabric of their clothes. For this purpose, we adopt a mesh-aligned 4D Gaussian technique for spatio-temporal mesh tracking as well as a physically based inverse renderer to estimate the intrinsic material properties. PhysAvatar integrates a physics simulator to estimate the physical parameters of the garments using gradient-based optimization in a principled manner. These novel capabilities enable PhysAvatar to create high-quality novel-view renderings of avatars dressed in loose-fitting clothes under motions and lighting conditions not seen in the training data. This marks a significant advancement towards modeling photorealistic digital humans using physically based inverse rendering with physics in the loop. Our project website is at: https://qingqing-zhao.github.io/PhysAvatar
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2403.11589.pdf' target='_blank'>https://arxiv.org/pdf/2403.11589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11589">UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the paper is accepted.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2311.17917.pdf' target='_blank'>https://arxiv.org/pdf/2311.17917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfeng Zhang, Xuanmeng Zhang, Huichao Zhang, Jun Hao Liew, Chenxu Zhang, Yi Yang, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17917">AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of creating high-fidelity and animatable 3D avatars from only textual descriptions. Existing text-to-avatar methods are either limited to static avatars which cannot be animated or struggle to generate animatable avatars with promising quality and precise pose control. To address these limitations, we propose AvatarStudio, a coarse-to-fine generative model that generates explicit textured 3D meshes for animatable human avatars. Specifically, AvatarStudio begins with a low-resolution NeRF-based representation for coarse generation, followed by incorporating SMPL-guided articulation into the explicit mesh representation to support avatar animation and high resolution rendering. To ensure view consistency and pose controllability of the resulting avatars, we introduce a 2D diffusion model conditioned on DensePose for Score Distillation Sampling supervision. By effectively leveraging the synergy between the articulated mesh representation and the DensePose-conditional diffusion model, AvatarStudio can create high-quality avatars from text that are ready for animation, significantly outperforming previous methods. Moreover, it is competent for many applications, e.g., multimodal avatar animations and style-guided avatar creation. For more results, please refer to our project page: http://jeff95.me/projects/avatarstudio.html
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2311.13574.pdf' target='_blank'>https://arxiv.org/pdf/2311.13574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Jiashi Feng, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13574">XAGen: 3D Expressive Human Avatars Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D-aware GAN models have enabled the generation of realistic and controllable human body images. However, existing methods focus on the control of major body joints, neglecting the manipulation of expressive attributes, such as facial expressions, jaw poses, hand poses, and so on. In this work, we present XAGen, the first 3D generative model for human avatars capable of expressive control over body, face, and hands. To enhance the fidelity of small-scale regions like face and hands, we devise a multi-scale and multi-part 3D representation that models fine details. Based on this representation, we propose a multi-part rendering technique that disentangles the synthesis of body, face, and hands to ease model training and enhance geometric quality. Furthermore, we design multi-part discriminators that evaluate the quality of the generated avatars with respect to their appearance and fine-grained control capabilities. Experiments show that XAGen surpasses state-of-the-art methods in terms of realism, diversity, and expressive control abilities. Code and data will be made available at https://showlab.github.io/xagen.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2308.14748.pdf' target='_blank'>https://arxiv.org/pdf/2308.14748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng, Jun Hao Liew
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14748">MagicAvatar: Multimodal Avatar Generation and Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents MagicAvatar, a framework for multimodal video generation and animation of human avatars. Unlike most existing methods that generate avatar-centric videos directly from multimodal inputs (e.g., text prompts), MagicAvatar explicitly disentangles avatar video generation into two stages: (1) multimodal-to-motion and (2) motion-to-video generation. The first stage translates the multimodal inputs into motion/ control signals (e.g., human pose, depth, DensePose); while the second stage generates avatar-centric video guided by these motion signals. Additionally, MagicAvatar supports avatar animation by simply providing a few images of the target person. This capability enables the animation of the provided human identity according to the specific motion derived from the first stage. We demonstrate the flexibility of MagicAvatar through various applications, including text-guided and video-guided avatar generation, as well as multimodal avatar animation.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2305.03043.pdf' target='_blank'>https://arxiv.org/pdf/2305.03043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03043">Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing demand for the accessible creation of high-quality 3D avatars that are animatable and customizable. Although 3D morphable models provide intuitive control for editing and animation, and robustness for single-view face reconstruction, they cannot easily capture geometric and appearance details. Methods based on neural implicit representations, such as signed distance functions (SDF) or neural radiance fields, approach photo-realism, but are difficult to animate and do not generalize well to unseen data. To tackle this problem, we propose a novel method for constructing implicit 3D morphable face models that are both generalizable and intuitive for editing. Trained from a collection of high-quality 3D scans, our face model is parameterized by geometry, expression, and texture latent codes with a learned SDF and explicit UV texture parameterization. Once trained, we can reconstruct an avatar from a single in-the-wild image by leveraging the learned prior to project the image into the latent space of our model. Our implicit morphable face models can be used to render an avatar from novel views, animate facial expressions by modifying expression codes, and edit textures by directly painting on the learned UV-texture maps. We demonstrate quantitatively and qualitatively that our method improves upon photo-realism, geometry, and expression accuracy compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2504.03536.pdf' target='_blank'>https://arxiv.org/pdf/2504.03536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03536">HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce \textbf{HumanDreamer-X}, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, \textbf{HumanFixer} is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2408.13995.pdf' target='_blank'>https://arxiv.org/pdf/2408.13995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Geng Foo, Yixuan He, Ajmal Saeed Mian, Hossein Rahmani, Jun Liu, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13995">Avatar Concept Slider: Controllable Editing of Concepts in 3D Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based editing of 3D human avatars to precisely match user requirements is challenging due to the inherent ambiguity and limited expressiveness of natural language. To overcome this, we propose the Avatar Concept Slider (ACS), a 3D avatar editing method that allows precise editing of semantic concepts in human avatars towards a specified intermediate point between two extremes of concepts, akin to moving a knob along a slider track. To achieve this, our ACS has three designs: Firstly, a Concept Sliding Loss based on linear discriminant analysis to pinpoint the concept-specific axes for precise editing. Secondly, an Attribute Preserving Loss based on principal component analysis for improved preservation of avatar identity during editing. We further propose a 3D Gaussian Splatting primitive selection mechanism based on concept-sensitivity, which updates only the primitives that are the most sensitive to our target concept, to improve efficiency. Results demonstrate that our ACS enables controllable 3D avatar editing, without compromising the avatar quality or its identifying attributes.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2405.12663.pdf' target='_blank'>https://arxiv.org/pdf/2405.12663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Gong, Shenyu Ji, Lin Geng Foo, Kang Chen, Hossein Rahmani, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12663">LAGA: Layered 3D Avatar Generation and Customization via Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating and customizing a 3D clothed avatar from textual descriptions is a critical and challenging task. Traditional methods often treat the human body and clothing as inseparable, limiting users' ability to freely mix and match garments. In response to this limitation, we present LAyered Gaussian Avatar (LAGA), a carefully designed framework enabling the creation of high-fidelity decomposable avatars with diverse garments. By decoupling garments from avatar, our framework empowers users to conviniently edit avatars at the garment level. Our approach begins by modeling the avatar using a set of Gaussian points organized in a layered structure, where each layer corresponds to a specific garment or the human body itself. To generate high-quality garments for each layer, we introduce a coarse-to-fine strategy for diverse garment generation and a novel dual-SDS loss function to maintain coherence between the generated garments and avatar components, including the human body and other garments. Moreover, we introduce three regularization losses to guide the movement of Gaussians for garment transfer, allowing garments to be freely transferred to various avatars. Extensive experimentation demonstrates that our approach surpasses existing methods in the generation of 3D clothed humans.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2402.00827.pdf' target='_blank'>https://arxiv.org/pdf/2402.00827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, Yunlong Tang, Huaijin Tu, Jiebo Luo, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00827">GaussianStyle: Gaussian Head Avatar via StyleGAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods like Neural Radiation Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant strides in facial attribute control such as facial animation and components editing, yet they struggle with fine-grained representation and scalability in dynamic head modeling. To address these limitations, we propose GaussianStyle, a novel framework that integrates the volumetric strengths of 3DGS with the powerful implicit representation of StyleGAN. The GaussianStyle preserves structural information, such as expressions and poses, using Gaussian points, while projecting the implicit volumetric representation into StyleGAN to capture high-frequency details and mitigate the over-smoothing commonly observed in neural texture rendering. Experimental outcomes indicate that our method achieves state-of-the-art performance in reenactment, novel view synthesis, and animation.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2309.16964.pdf' target='_blank'>https://arxiv.org/pdf/2309.16964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunjiao Zhou, Jianfei Yang, He Huang, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16964">AdaPose: Towards Cross-Site Device-Free Human Pose Estimation with Commodity WiFi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>WiFi-based pose estimation is a technology with great potential for the development of smart homes and metaverse avatar generation. However, current WiFi-based pose estimation methods are predominantly evaluated under controlled laboratory conditions with sophisticated vision models to acquire accurately labeled data. Furthermore, WiFi CSI is highly sensitive to environmental variables, and direct application of a pre-trained model to a new environment may yield suboptimal results due to domain shift. In this paper, we proposes a domain adaptation algorithm, AdaPose, designed specifically for weakly-supervised WiFi-based pose estimation. The proposed method aims to identify consistent human poses that are highly resistant to environmental dynamics. To achieve this goal, we introduce a Mapping Consistency Loss that aligns the domain discrepancy of source and target domains based on inner consistency between input and output at the mapping level. We conduct extensive experiments on domain adaptation in two different scenes using our self-collected pose estimation dataset containing WiFi CSI frames. The results demonstrate the effectiveness and robustness of AdaPose in eliminating domain shift, thereby facilitating the widespread application of WiFi-based pose estimation in smart cities.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2308.14177.pdf' target='_blank'>https://arxiv.org/pdf/2308.14177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Geng Foo, Hossein Rahmani, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14177">AI-Generated Content (AIGC) for Various Data Modalities: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated content (AIGC) methods aim to produce text, images, videos, 3D assets, and other media using AI algorithms. Due to its wide range of applications and the potential of recent works, AIGC developments -- especially in Machine Learning (ML) and Deep Learning (DL) -- have been attracting significant attention, and this survey focuses on comprehensively reviewing such advancements in ML/DL. AIGC methods have been developed for various data modalities, such as image, video, text, 3D shape, 3D scene, 3D human avatar, 3D motion, and audio -- each presenting unique characteristics and challenges. Furthermore, there have been significant developments in cross-modality AIGC methods, where generative methods receive conditioning input in one modality and produce outputs in another. Examples include going from various modalities to image, video, 3D, and audio. This paper provides a comprehensive review of AIGC methods across different data modalities, including both single-modality and cross-modality methods, highlighting the various challenges, representative works, and recent technical directions in each setting. We also survey the representative datasets throughout the modalities, and present comparative results for various modalities. Moreover, we discuss the typical applications of AIGC methods in various domains, challenges, and future research directions.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2305.06131.pdf' target='_blank'>https://arxiv.org/pdf/2305.06131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Li, Chaoning Zhang, Joseph Cho, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06131">Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI has made significant progress in recent years, with text-guided content generation being the most practical as it facilitates interaction between human instructions and AI-generated content (AIGC). Thanks to advancements in text-to-image and 3D modeling technologies, like neural radiance field (NeRF), text-to-3D has emerged as a nascent yet highly active research field. Our work conducts a comprehensive survey on this topic and follows up on subsequent research progress in the overall field, aiming to help readers interested in this direction quickly catch up with its rapid development. First, we introduce 3D data representations, including both Structured and non-Structured data. Building on this pre-requisite, we introduce various core technologies to achieve satisfactory text-to-3D results. Additionally, we present mainstream baselines and research directions in recent text-to-3D technology, including fidelity, efficiency, consistency, controllability, diversity, and applicability. Furthermore, we summarize the usage of text-to-3D technology in various applications, including avatar generation, texture generation, scene generation and 3D editing. Finally, we discuss the agenda for the future development of text-to-3D.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2412.03889.pdf' target='_blank'>https://arxiv.org/pdf/2412.03889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michelle Guo, Mia Tang, Hannah Cha, Ruohan Zhang, C. Karen Liu, Jiajun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03889">CRAFT: Designing Creative and Functional 3D Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For designing a wide range of everyday objects, the design process should be aware of both the human body and the underlying semantics of the design specification. However, these two objectives present significant challenges to the current AI-based designing tools. In this work, we present a method to synthesize body-aware 3D objects from a base mesh given an input body geometry and either text or image as guidance. The generated objects can be simulated on virtual characters, or fabricated for real-world use. We propose to use a mesh deformation procedure that optimizes for both semantic alignment as well as contact and penetration losses. Using our method, users can generate both virtual or real-world objects from text, image, or sketch, without the need for manual artist intervention. We present both qualitative and quantitative results on various object categories, demonstrating the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2404.19398.pdf' target='_blank'>https://arxiv.org/pdf/2404.19398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjie Ma, Yanlin Weng, Tianjia Shao, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19398">3D Gaussian Blendshapes for Head Avatar Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars. Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models. Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance. The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients. High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting. Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2409.13180.pdf' target='_blank'>https://arxiv.org/pdf/2409.13180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Qiu, Wei Zhang, Chen Liu, Rudong An, Lincheng Li, Yu Ding, Changjie Fan, Zhipeng Hu, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13180">FreeAvatar: Robust 3D Facial Animation Transfer by Learning an Expression Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-driven 3D facial animation transfer aims to drive avatars to reproduce the expressions of actors. Existing methods have achieved remarkable results by constraining both geometric and perceptual consistency. However, geometric constraints (like those designed on facial landmarks) are insufficient to capture subtle emotions, while expression features trained on classification tasks lack fine granularity for complex emotions. To address this, we propose \textbf{FreeAvatar}, a robust facial animation transfer method that relies solely on our learned expression representation. Specifically, FreeAvatar consists of two main components: the expression foundation model and the facial animation transfer model. In the first component, we initially construct a facial feature space through a face reconstruction task and then optimize the expression feature space by exploring the similarities among different expressions. Benefiting from training on the amounts of unlabeled facial images and re-collected expression comparison dataset, our model adapts freely and effectively to any in-the-wild input facial images. In the facial animation transfer component, we propose a novel Expression-driven Multi-avatar Animator, which first maps expressive semantics to the facial control parameters of 3D avatars and then imposes perceptual constraints between the input and output images to maintain expression consistency. To make the entire process differentiable, we employ a trained neural renderer to translate rig parameters into corresponding images. Furthermore, unlike previous methods that require separate decoders for each avatar, we propose a dynamic identity injection module that allows for the joint training of multiple avatars within a single network.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2311.17053.pdf' target='_blank'>https://arxiv.org/pdf/2311.17053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsun-Hsuan Wang, Juntian Zheng, Pingchuan Ma, Yilun Du, Byungchul Kim, Andrew Spielberg, Joshua Tenenbaum, Chuang Gan, Daniela Rus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17053">DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy. Co-optimization of artificial creatures' morphology and control in silico shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure. In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks. DiffuseBot bridges the gap between virtually generated content and physical utility by (i) augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and (ii) introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation. We showcase a range of simulated and fabricated robots along with their capabilities. Check our website at https://diffusebot.github.io/
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2305.18891.pdf' target='_blank'>https://arxiv.org/pdf/2305.18891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Chen Liu, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18891">EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating vivid and diverse 3D co-speech gestures is crucial for various applications in animating virtual avatars. While most existing methods can generate gestures from audio directly, they usually overlook that emotion is one of the key factors of authentic co-speech gesture generation. In this work, we propose EmotionGesture, a novel framework for synthesizing vivid and diverse emotional co-speech 3D gestures from audio. Considering emotion is often entangled with the rhythmic beat in speech audio, we first develop an Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features as well as model their correlation via a transcript-based visual-rhythm alignment. Then, we propose an initial pose based Spatial-Temporal Prompter (STP) to generate future gestures from the given initial poses. STP effectively models the spatial-temporal correlations between the initial poses and the future gestures, thus producing the spatial-temporal coherent pose prompt. Once we obtain pose prompts, emotion, and audio beat features, we will generate 3D co-speech gestures through a transformer architecture. However, considering the poses of existing datasets often contain jittering effects, this would lead to generating unstable gestures. To address this issue, we propose an effective objective function, dubbed Motion-Smooth Loss. Specifically, we model motion offset to compensate for jittering ground-truth by forcing gestures to be smooth. Last, we present an emotion-conditioned VAE to sample emotion features, enabling us to generate diverse emotional results. Extensive experiments demonstrate that our framework outperforms the state-of-the-art, achieving vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be released at the project page: https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2212.02469.pdf' target='_blank'>https://arxiv.org/pdf/2212.02469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, Deng Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02469">One-shot Implicit Animatable Avatars with Model-based Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pretrained models. Both priors are used to jointly guide the optimization for creating plausible content in the invisible areas. Taking advantage of the CLIP models, ELICIT can use text descriptions to generate text-conditioned unseen regions. In order to further improve visual details, we propose a segmentation-based sampling strategy that locally refines different parts of the avatar. Comprehensive evaluations on multiple popular benchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT has outperformed strong baseline methods of avatar creation when only a single image is available. The code is public for research purposes at https://huangyangyi.github.io/ELICIT/.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2505.21437.pdf' target='_blank'>https://arxiv.org/pdf/2505.21437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaijin Pi, Zhi Cen, Zhiyang Dou, Taku Komura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21437">CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is a critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose a novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt a unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2411.18675.pdf' target='_blank'>https://arxiv.org/pdf/2411.18675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivangi Aneja, Artem Sevastopolsky, Tobias Kirschstein, Justus Thies, Angela Dai, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18675">GaussianSpeech: Audio-Driven Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GaussianSpeech, a novel approach that synthesizes high-fidelity animation sequences of photo-realistic, personalized 3D human head avatars from spoken audio. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple speech signal with 3D Gaussian splatting to create realistic, temporally coherent motion sequences. We propose a compact and efficient 3DGS-based avatar representation that generates expression-dependent color and leverages wrinkle- and perceptually-based losses to synthesize facial details, including wrinkles that occur with different expressions. To enable sequence modeling of 3D Gaussian splats with audio, we devise an audio-conditioned transformer model capable of extracting lip and expression features directly from audio input. Due to the absence of high-quality datasets of talking humans in correspondence with audio, we captured a new large-scale multi-view dataset of audio-visual sequences of talking humans with native English accents and diverse facial geometry. GaussianSpeech consistently achieves state-of-the-art performance with visually natural motion at real time rendering rates, while encompassing diverse facial expressions and styles.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2411.12981.pdf' target='_blank'>https://arxiv.org/pdf/2411.12981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobao Wei, Peng Chen, Guangyu Li, Ming Lu, Hui Chen, Feng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12981">GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, the first high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. Leveraging the unstructured nature of 3DGS, we develop a novel representation of the eye for rigid eye rotation based on the target gaze direction. To enable synthesis generalization across various subjects, we integrate an expression-guided module to inject subject-specific information into the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. The code is available at: https://ucwxb.github.io/GazeGaussian.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2308.04830.pdf' target='_blank'>https://arxiv.org/pdf/2308.04830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyang Chen, Zhiyong Wu, Runnan Li, Weihong Bao, Jun Ling, Xu Tan, Sheng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04830">VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current talking face generation methods mainly focus on speech-lip synchronization. However, insufficient investigation on the facial talking style leads to a lifeless and monotonous avatar. Most previous works fail to imitate expressive styles from arbitrary video prompts and ensure the authenticity of the generated video. This paper proposes an unsupervised variational style transfer model (VAST) to vivify the neutral photo-realistic avatars. Our model consists of three key components: a style encoder that extracts facial style representations from the given video prompts; a hybrid facial expression decoder to model accurate speech-related movements; a variational style enhancer that enhances the style space to be highly expressive and meaningful. With our essential designs on facial style learning, our model is able to flexibly capture the expressive facial style from arbitrary video prompts and transfer it onto a personalized image renderer in a zero-shot manner. Experimental results demonstrate the proposed approach contributes to a more vivid talking avatar with higher authenticity and richer expressiveness.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2508.18337.pdf' target='_blank'>https://arxiv.org/pdf/2508.18337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18337">EAI-Avatar: Emotion-Aware Interactive Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have advanced rapidly, enabling impressive talking head generation that brings AI to life. However, most existing methods focus solely on one-way portrait animation. Even the few that support bidirectional conversational interactions lack precise emotion-adaptive capabilities, significantly limiting their practical applicability. In this paper, we propose EAI-Avatar, a novel emotion-aware talking head generation framework for dyadic interactions. Leveraging the dialogue generation capability of large language models (LLMs, e.g., GPT-4), our method produces temporally consistent virtual avatars with rich emotional variations that seamlessly transition between speaking and listening states. Specifically, we design a Transformer-based head mask generator that learns temporally consistent motion features in a latent mask space, capable of generating arbitrary-length, temporally consistent mask sequences to constrain head motions. Furthermore, we introduce an interactive talking tree structure to represent dialogue state transitions, where each tree node contains information such as child/parent/sibling nodes and the current character's emotional state. By performing reverse-level traversal, we extract rich historical emotional cues from the current node to guide expression synthesis. Extensive experiments demonstrate the superior performance and effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2506.16852.pdf' target='_blank'>https://arxiv.org/pdf/2506.16852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaonan Ji, Jinwei Qi, Peng Zhang, Bang Zhang, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16852">Controllable and Expressive One-Shot Video Head Swapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel diffusion-based multi-condition controllable framework for video head swapping, which seamlessly transplant a human head from a static image into a dynamic video, while preserving the original body and background of target video, and further allowing to tweak head expressions and movements during swapping as needed. Existing face-swapping methods mainly focus on localized facial replacement neglecting holistic head morphology, while head-swapping approaches struggling with hairstyle diversity and complex backgrounds, and none of these methods allow users to modify the transplanted head expressions after swapping. To tackle these challenges, our method incorporates several innovative strategies through a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We propose a shape-agnostic mask strategy to explicitly disentangle foreground head identity features from background/body contexts, combining hair enhancement strategy to achieve robust holistic head identity preservation across diverse hair types and complex backgrounds. 2) Expression-aware landmark retargeting and editing: We propose a disentangled 3DMM-driven retargeting module that decouples identity, expression, and head poses, minimizing the impact of original expressions in input images and supporting expression editing. While a scale-aware retargeting strategy is further employed to minimize cross-identity expression distortion for higher transfer precision. Experimental results demonstrate that our method excels in seamless background integration while preserving the identity of the source portrait, as well as showcasing superior expression transfer capabilities applicable to both real and virtual characters.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2502.10841.pdf' target='_blank'>https://arxiv.org/pdf/2502.10841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, Xiang Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10841">SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SkyReels-A1, a simple yet effective framework built upon video diffusion Transformer to facilitate portrait image animation. Existing methodologies still encounter issues, including identity distortion, background instability, and unrealistic facial dynamics, particularly in head-only animation scenarios. Besides, extending to accommodate diverse body proportions usually leads to visual inconsistencies or unnatural articulations. To address these challenges, SkyReels-A1 capitalizes on the strong generative capabilities of video DiT, enhancing facial motion transfer precision, identity retention, and temporal coherence. The system incorporates an expression-aware conditioning module that enables seamless video synthesis driven by expression-guided landmark inputs. Integrating the facial image-text alignment module strengthens the fusion of facial attributes with motion trajectories, reinforcing identity preservation. Additionally, SkyReels-A1 incorporates a multi-stage training paradigm to incrementally refine the correlation between expressions and motion while ensuring stable identity reproduction. Extensive empirical evaluations highlight the model's ability to produce visually coherent and compositionally diverse results, making it highly applicable to domains such as virtual avatars, remote communication, and digital media generation.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2411.15436.pdf' target='_blank'>https://arxiv.org/pdf/2411.15436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15436">ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head Avatar with Temporal Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have shown impressive potential on talking head generation. While plausible appearance and talking effect are achieved, these methods still suffer from temporal, 3D or expression inconsistency due to the error accumulation and inherent limitation of single-image generation ability. In this paper, we propose ConsistentAvatar, a novel framework for fully consistent and high-fidelity talking avatar generation. Instead of directly employing multi-modal conditions to the diffusion process, our method learns to first model the temporal representation for stability between adjacent frames. Specifically, we propose a Temporally-Sensitive Detail (TSD) map containing high-frequency feature and contours that vary significantly along the time axis. Using a temporal consistent diffusion module, we learn to align TSD of the initial result to that of the video frame ground truth. The final avatar is generated by a fully consistent diffusion module, conditioned on the aligned TSD, rough head normal, and emotion prompt embedding. We find that the aligned TSD, which represents the temporal patterns, constrains the diffusion process to generate temporally stable talking head. Further, its reliable guidance complements the inaccuracy of other conditions, suppressing the accumulated error while improving the consistency on various aspects. Extensive experiments demonstrate that ConsistentAvatar outperforms the state-of-the-art methods on the generated appearance, 3D, expression and temporal consistency. Project page: https://njust-yang.github.io/ConsistentAvatar.github.io/
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2410.18975.pdf' target='_blank'>https://arxiv.org/pdf/2410.18975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Li, Yuanzhen Li, Neal Wadhwa, Yael Pritch, David E. Jacobs, Michael Rubinstein, Mohit Bansal, Nataniel Ruiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18975">Unbounded: A Generative Infinite Game of Character Life Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2408.02110.pdf' target='_blank'>https://arxiv.org/pdf/2408.02110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feichi Lu, Zijian Dong, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02110">AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite progress in human motion capture, existing multi-view methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2404.18630.pdf' target='_blank'>https://arxiv.org/pdf/2404.18630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18630">4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2311.05599.pdf' target='_blank'>https://arxiv.org/pdf/2311.05599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05599">SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines. Project page: https://eth-ait.github.io/synthetic-handovers/
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2305.00121.pdf' target='_blank'>https://arxiv.org/pdf/2305.00121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsuan-I Ho, Lixin Xue, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00121">Learning Locally Editable Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of our work lies a representation that combines the modeling power of neural fields with the ease of use and inherent 3D consistency of skinned meshes. To this end, we construct a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model, thus exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that admits fitting to unseen scans and sampling of realistic avatars with varied appearances and geometries. Furthermore, our representation allows local editing by swapping local features between 3D assets. To verify our method for avatar creation and editing, we contribute a new high-quality dataset, dubbed CustomHumans, for training and evaluation. Our experiments quantitatively and qualitatively show that our method generates diverse detailed avatars and achieves better model fitting performance compared to state-of-the-art methods. Our code and dataset are available at https://custom-humans.github.io/.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2303.04805.pdf' target='_blank'>https://arxiv.org/pdf/2303.04805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Julien Valentin, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04805">X-Avatar: Expressive Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-Avatar, a novel avatar model that captures the full expressiveness of digital humans to bring about life-like experiences in telepresence, AR/VR and beyond. Our method models bodies, hands, facial expressions and appearance in a holistic fashion and can be learned from either full 3D scans or RGB-D data. To achieve this, we propose a part-aware learned forward skinning module that can be driven by the parameter space of SMPL-X, allowing for expressive animation of X-Avatars. To efficiently learn the neural shape and deformation fields, we propose novel part-aware sampling and initialization strategies. This leads to higher fidelity results, especially for smaller body parts while maintaining efficient training despite increased number of articulated bones. To capture the appearance of the avatar with high-frequency details, we extend the geometry and deformation fields with a texture network that is conditioned on pose, facial expression, geometry and the normals of the deformed surface. We show experimentally that our method outperforms strong baselines in both data domains both quantitatively and qualitatively on the animation task. To facilitate future research on expressive avatars we contribute a new dataset, called X-Humans, containing 233 sequences of high-quality textured scans from 20 participants, totalling 35,500 data frames.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2302.11566.pdf' target='_blank'>https://arxiv.org/pdf/2302.11566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11566">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human geometry reconstructions. We evaluate our methods on publicly available datasets and show improvements over prior art.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2507.17029.pdf' target='_blank'>https://arxiv.org/pdf/2507.17029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17029">StreamME: Simplify 3D Gaussian Avatar within Live Stream</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2504.20629.pdf' target='_blank'>https://arxiv.org/pdf/2504.20629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongsoo Choi, Ji-Hoon Kim, Kim Sung-Bin, Tae-Hyun Oh, Joon Son Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20629">AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT .
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2504.02542.pdf' target='_blank'>https://arxiv.org/pdf/2504.02542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fa-Ting Hong, Zunnan Xu, Zixiang Zhou, Jun Zhou, Xiu Li, Qin Lin, Qinglin Lu, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02542">Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce \textbf{ACTalker}, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict. The project website can be found at https://harlanhong.github.io/publications/actalker/index.html.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2410.15536.pdf' target='_blank'>https://arxiv.org/pdf/2410.15536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Zook, Fan-Yun Sun, Josef Spjut, Valts Blukis, Stan Birchfield, Jonathan Tremblay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15536">GRS: Generating Robotic Simulation Tasks from Real-World Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GRS (Generating Robotic Simulation tasks), a system addressing real-to-sim for robotic simulations. GRS creates digital twin simulations from single RGB-D observations with solvable tasks for virtual agent training. Using vision-language models (VLMs), our pipeline operates in three stages: 1) scene comprehension with SAM2 for segmentation and object description, 2) matching objects with simulation-ready assets, and 3) generating appropriate tasks. We ensure simulation-task alignment through generated test suites and introduce a router that iteratively refines both simulation and test code. Experiments demonstrate our system's effectiveness in object correspondence and task environment generation through our novel router mechanism.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2410.07160.pdf' target='_blank'>https://arxiv.org/pdf/2410.07160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song, Lele Chen, Celong Liu, Pinxin Liu, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07160">TextToon: Real-Time Text Toonify Head Avatar from Single Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2408.13674.pdf' target='_blank'>https://arxiv.org/pdf/2408.13674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keqiang Sun, Amin Jourabloo, Riddhish Bhalodia, Moustafa Meshry, Yu Rong, Zhengyu Yang, Thu Nguyen-Phuoc, Christian Haene, Jiu Xu, Sam Johnson, Hongsheng Li, Sofien Bouaziz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13674">GenCA: A Text-conditioned Generative Model for Realistic and Drivable Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photo-realistic and controllable 3D avatars are crucial for various applications such as virtual and mixed reality (VR/MR), telepresence, gaming, and film production. Traditional methods for avatar creation often involve time-consuming scanning and reconstruction processes for each avatar, which limits their scalability. Furthermore, these methods do not offer the flexibility to sample new identities or modify existing ones. On the other hand, by learning a strong prior from data, generative models provide a promising alternative to traditional reconstruction methods, easing the time constraints for both data capture and processing. Additionally, generative methods enable downstream applications beyond reconstruction, such as editing and stylization. Nonetheless, the research on generative 3D avatars is still in its infancy, and therefore current methods still have limitations such as creating static avatars, lacking photo-realism, having incomplete facial details, or having limited drivability. To address this, we propose a text-conditioned generative model that can generate photo-realistic facial avatars of diverse identities, with more complete details like hair, eyes and mouth interior, and which can be driven through a powerful non-parametric latent expression space. Specifically, we integrate the generative and editing capabilities of latent diffusion models with a strong prior model for avatar expression driving.
  Our model can generate and control high-fidelity avatars, even those out-of-distribution. We also highlight its potential for downstream applications, including avatar editing and single-shot avatar reconstruction.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2401.09386.pdf' target='_blank'>https://arxiv.org/pdf/2401.09386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song, Pinxin Liu, Lele Chen, Guojun Yin, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09386">Tri$^{2}$-plane: Thinking Head Avatar via Feature Pyramid</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed considerable achievements in facial avatar reconstruction with neural volume rendering. Despite notable advancements, the reconstruction of complex and dynamic head movements from monocular videos still suffers from capturing and restoring fine-grained details. In this work, we propose a novel approach, named Tri$^2$-plane, for monocular photo-realistic volumetric head avatar reconstructions. Distinct from the existing works that rely on a single tri-plane deformation field for dynamic facial modeling, the proposed Tri$^2$-plane leverages the principle of feature pyramids and three top-to-down lateral connections tri-planes for details improvement. It samples and renders facial details at multiple scales, transitioning from the entire face to specific local regions and then to even more refined sub-regions. Moreover, we incorporate a camera-based geometry-aware sliding window method as an augmentation in training, which improves the robustness beyond the canonical space, with a particular improvement in cross-identity generation capabilities. Experimental outcomes indicate that the Tri$^2$-plane not only surpasses existing methodologies but also achieves superior performance across quantitative and qualitative assessments. The project website is: \url{https://songluchuan.github.io/Tri2Plane.github.io/}.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2312.13578.pdf' target='_blank'>https://arxiv.org/pdf/2312.13578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13578">DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of emotional talking faces from a single portrait image remains a significant challenge. The simultaneous achievement of expressive emotional talking and accurate lip-sync is particularly difficult, as expressiveness is often compromised for the accuracy of lip-sync. As widely adopted by many prior works, the LSTM network often fails to capture the subtleties and variations of emotional expressions. To address these challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven framework, tailored for generating diverse expressions and accurate lip-sync concurrently. In the first stage, we propose EmoDiff, a novel diffusion module that generates diverse highly dynamic emotional expressions and head poses in accordance with the audio and the referenced emotion style. Given the strong correlation between lip motion and audio, we then refine the dynamics with enhanced lip-sync accuracy using audio features and emotion style. To this end, we deploy a video-to-video rendering module to transfer the expressions and lip motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of expressiveness, lip-sync accuracy and perceptual quality.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2312.02963.pdf' target='_blank'>https://arxiv.org/pdf/2312.02963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02963">MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset. Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data. To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation. Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2312.02702.pdf' target='_blank'>https://arxiv.org/pdf/2312.02702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Baltatzis, Rolandos Alexandros Potamias, Evangelos Ververas, Guanxiong Sun, Jiankang Deng, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02702">Neural Sign Actors: A diffusion model for 3D sign language production from text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign Languages (SL) serve as the primary mode of communication for the Deaf and Hard of Hearing communities. Deep learning methods for SL recognition and translation have achieved promising results. However, Sign Language Production (SLP) poses a challenge as the generated motions must be realistic and have precise semantic meaning. Most SLP methods rely on 2D data, which hinders their realism. In this work, a diffusion-based SLP model is trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an unconstrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neural network defined on the SMPL-X body skeleton. Through quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous methods of SLP. This work makes an important step towards realistic neural sign avatars, bridging the communication gap between Deaf and hearing communities.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2310.02714.pdf' target='_blank'>https://arxiv.org/pdf/2310.02714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanmeng Zhang, Jianfeng Zhang, Rohan Chacko, Hongyi Xu, Guoxian Song, Yi Yang, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02714">GETAvatar: Generative Textured Meshes for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of 3D-aware full-body human generation, aiming at creating animatable human avatars with high-quality textures and geometries. Generally, two challenges remain in this field: i) existing methods struggle to generate geometries with rich realistic details such as the wrinkles of garments; ii) they typically utilize volumetric radiance fields and neural renderers in the synthesis process, making high-resolution rendering non-trivial. To overcome these problems, we propose GETAvatar, a Generative model that directly generates Explicit Textured 3D meshes for animatable human Avatar, with photo-realistic appearance and fine geometric details. Specifically, we first design an articulated 3D human representation with explicit surface modeling, and enrich the generated humans with realistic surface details by learning from the 2D normal maps of 3D scan data. Second, with the explicit mesh representation, we can use a rasterization-based renderer to perform surface rendering, allowing us to achieve high-resolution image generation efficiently. Extensive experiments demonstrate that GETAvatar achieves state-of-the-art performance on 3D-aware human generation both in appearance and geometry quality. Notably, GETAvatar can generate images at 512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving upon previous methods by 2x. Our code and models will be available.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2305.09641.pdf' target='_blank'>https://arxiv.org/pdf/2305.09641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09641">FitMe: Deep Photorealistic 3D Morphable Model Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable human avatars from single or multiple images. The model consists of a multi-modal style-based generator, that captures facial appearance in terms of diffuse and specular reflectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an optimization pipeline, while also achieving photorealistic facial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by exploiting the expressivity of the style-based latent representation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on single "in-the-wild" facial images, while it produces impressive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent implicit avatar reconstructions, FitMe requires only one minute and produces relightable mesh and texture-based avatars, that can be used by end-user applications.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2508.14357.pdf' target='_blank'>https://arxiv.org/pdf/2508.14357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rihao Chang, He Jiao, Weizhi Nie, Honglin Guo, Keliang Xie, Zhenhua Wu, Lina Zhao, Yunpeng Bai, Yongtao Ma, Lanjun Wang, Yuting Su, Xi Gao, Weijie Wang, Nicu Sebe, Bruno Lepri, Bingwei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14357">Organ-Agents: Virtual Human Physiology Simulator via LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have enabled new possibilities in simulating complex physiological systems. We introduce Organ-Agents, a multi-agent framework that simulates human physiology via LLM-driven agents. Each Simulator models a specific system (e.g., cardiovascular, renal, immune). Training consists of supervised fine-tuning on system-specific time-series data, followed by reinforcement-guided coordination using dynamic reference selection and error correction. We curated data from 7,134 sepsis patients and 7,895 controls, generating high-resolution trajectories across 9 systems and 125 variables. Organ-Agents achieved high simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and robustness across SOFA-based severity strata. External validation on 22,689 ICU patients from two hospitals showed moderate degradation under distribution shifts with stable simulation. Organ-Agents faithfully reproduces critical multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with coherent timing and phase progression. Evaluation by 15 critical care physicians confirmed realism and physiological plausibility (mean Likert ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations under alternative sepsis treatment strategies, generating trajectories and APACHE II scores aligned with matched real-world patients. In downstream early warning tasks, classifiers trained on synthetic data showed minimal AUROC drops (<0.04), indicating preserved decision-relevant patterns. These results position Organ-Agents as a credible, interpretable, and generalizable digital twin for precision diagnosis, treatment simulation, and hypothesis testing in critical care.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2506.06645.pdf' target='_blank'>https://arxiv.org/pdf/2506.06645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Peng, Jingxiang Sun, Yushuo Chen, Zhaoqi Su, Zhuo Su, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06645">Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2503.22249.pdf' target='_blank'>https://arxiv.org/pdf/2503.22249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22249">FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2503.12751.pdf' target='_blank'>https://arxiv.org/pdf/2503.12751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhan, Wangze Xu, Qingtian Zhu, Muyao Niu, Mingze Ma, Yifei Liu, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12751">R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing Photorealistic Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present R3-Avatar, incorporating a temporal codebook, to overcome the inability of human avatars to be both animatable and of high-fidelity rendering quality. Existing video-based reconstruction of 3D human avatars either focuses solely on rendering, lacking animation support, or learns a pose-appearance mapping for animating, which degrades under limited training poses or complex clothing. In this paper, we adopt a "record-retrieve-reconstruct" strategy that ensures high-quality rendering from novel views while mitigating degradation in novel poses. Specifically, disambiguating timestamps record temporal appearance variations in a codebook, ensuring high-fidelity novel-view rendering, while novel poses retrieve corresponding timestamps by matching the most similar training poses for augmented appearance. Our R3-Avatar outperforms cutting-edge video-based human avatar reconstruction, particularly in overcoming visual quality degradation in extreme scenarios with limited training human poses and complex clothing.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2502.11563.pdf' target='_blank'>https://arxiv.org/pdf/2502.11563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Wang, Caoyuan Ma, Jian Zhao, Hanrui Xu, Dongfang Sun, Haoyang Chen, Lin Xiong, Zheng Wang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11563">Leader and Follower: Interactive Motion Generation under Trajectory Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of game and film production, generating interactive motion from texts has garnered significant attention due to its potential to revolutionize content creation processes. In many practical applications, there is a need to impose strict constraints on the motion range or trajectory of virtual characters. However, existing methods that rely solely on textual input face substantial challenges in accurately capturing the user's intent, particularly in specifying the desired trajectory. As a result, the generated motions often lack plausibility and accuracy. Moreover, existing trajectory - based methods for customized motion generation rely on retraining for single - actor scenarios, which limits flexibility and adaptability to different datasets, as well as interactivity in two-actor motions. To generate interactive motion following specified trajectories, this paper decouples complex motion into a Leader - Follower dynamic, inspired by role allocation in partner dancing. Based on this framework, this paper explores the motion range refinement process in interactive motion generation and proposes a training-free approach, integrating a Pace Controller and a Kinematic Synchronization Adapter. The framework enhances the ability of existing models to generate motion that adheres to trajectory by controlling the leader's movement and correcting the follower's motion to align with the leader. Experimental results show that the proposed approach, by better leveraging trajectory information, outperforms existing methods in both realism and accuracy.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2411.16768.pdf' target='_blank'>https://arxiv.org/pdf/2411.16768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangze Xu, Yifan Zhan, Zhihang Zhong, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16768">Sequential Gaussian Avatars with Hierarchical Motion Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of neural rendering has significantly advanced the rendering quality of 3D human avatars, with the recently popular 3DGS technique enabling real-time performance. However, SMPL-driven 3DGS human avatars still struggle to capture fine appearance details due to the complex mapping from pose to appearance during fitting. In this paper, we propose SeqAvatar, which excavates the explicit 3DGS representation to better model human avatars based on a hierarchical motion context. Specifically, we utilize a coarse-to-fine motion conditions that incorporate both the overall human skeleton and fine-grained vertex motions for non-rigid deformation. To enhance the robustness of the proposed motion conditions, we adopt a spatio-temporal multi-scale sampling strategy to hierarchically integrate more motion clues to model human avatars. Extensive experiments demonstrate that our method significantly outperforms 3DGS-based approaches and renders human avatars orders of magnitude faster than the latest NeRF-based models that incorporate temporal context, all while delivering performance that is at least comparable or even superior. Project page: https://zezeaaa.github.io/projects/SeqAvatar/
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2410.08082.pdf' target='_blank'>https://arxiv.org/pdf/2410.08082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhan, Qingtian Zhu, Muyao Niu, Mingze Ma, Jiancheng Zhao, Zhihang Zhong, Xiao Sun, Yu Qiao, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08082">ToMiE: Towards Explicit Exoskeleton for the Reconstruction of Complicated 3D Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we highlight a critical yet often overlooked factor in most 3D human tasks, namely modeling complicated 3D human with with hand-held objects or loose-fitting clothing. It is known that the parameterized formulation of SMPL is able to fit human skin; while hand-held objects and loose-fitting clothing, are difficult to get modeled within the unified framework, since their movements are usually decoupled with the human body. To enhance the capability of SMPL skeleton in response to this situation, we propose a growth strategy that enables the joint tree of the skeleton to expand adaptively. Specifically, our method, called ToMiE, consists of parent joints localization and external joints optimization. For parent joints localization, we employ a gradient-based approach guided by both LBS blending weights and motion kernels. Once the external joints are obtained, we proceed to optimize their transformations in SE(3) across different frames, enabling rendering and explicit animation. ToMiE manages to outperform other methods across various cases with hand-held objects and loose-fitting clothing, not only in rendering quality but also by offering free animation of grown joints, thereby enhancing the expressive ability of SMPL skeleton for a broader range of applications.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2408.09665.pdf' target='_blank'>https://arxiv.org/pdf/2408.09665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Chen Yang, Hao Wang, Xingyue Zhao, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09665">Topology-aware Human Avatars with Semantically-guided Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photo-realistic and topology-aware animatable human avatars from monocular videos remains challenging in computer vision and graphics. Recently, methods using 3D Gaussians to represent the human body have emerged, offering faster optimization and real-time rendering. However, due to ignoring the crucial role of human body semantic information which represents the explicit topological and intrinsic structure within human body, they fail to achieve fine-detail reconstruction of human avatars. To address this issue, we propose SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid deformation, and non-rigid cloth dynamics deformation to create photo-realistic human avatars. We then design a Semantic Human-Body Annotator (SHA) which utilizes SMPL's semantic prior for efficient body part semantic labeling. The generated labels are used to guide the optimization of semantic attributes of Gaussian. To capture the explicit topological structure of the human body, we employ a 3D network that integrates both topological and geometric associations for human avatar deformation. We further implement three key strategies to enhance the semantic accuracy of 3D Gaussians and rendering quality: semantic projection with 2D regularization, semantic-guided density regularization and semantic-aware regularization with neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves state-of-the-art geometry and appearance reconstruction performance.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2408.09663.pdf' target='_blank'>https://arxiv.org/pdf/2408.09663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09663">3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing approaches for human avatar generation--both NeRF-based and 3D Gaussian Splatting (3DGS) based--struggle with maintaining 3D consistency and exhibit degraded detail reconstruction, particularly when training with sparse inputs. To address this challenge, we propose CHASE, a novel framework that achieves dense-input-level performance using only sparse inputs through two key innovations: cross-pose intrinsic 3D consistency supervision and 3D geometry contrastive learning. Building upon prior skeleton-driven approaches that combine rigid deformation with non-rigid cloth dynamics, we first establish baseline avatars with fundamental 3D consistency. To enhance 3D consistency under sparse inputs, we introduce a Dynamic Avatar Adjustment (DAA) module, which refines deformed Gaussians by leveraging similar poses from the training set. By minimizing the rendering discrepancy between adjusted Gaussians and reference poses, DAA provides additional supervision for avatar reconstruction. We further maintain global 3D consistency through a novel geometry-aware contrastive learning strategy. While designed for sparse inputs, CHASE surpasses state-of-the-art methods across both full and sparse settings on ZJU-MoCap and H36M datasets, demonstrating that our enhanced 3D consistency leads to superior rendering quality.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2407.06938.pdf' target='_blank'>https://arxiv.org/pdf/2407.06938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Yiji Cheng, Chunyu Wang, Ting Zhang, Jiaolong Yang, Yansong Tang, Feng Zhao, Dong Chen, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06938">RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RodinHD, which can generate high-fidelity 3D avatars from a portrait image. Existing methods fail to capture intricate details such as hairstyles which we tackle in this paper. We first identify an overlooked problem of catastrophic forgetting that arises when fitting triplanes sequentially on many avatars, caused by the MLP decoder sharing scheme. To overcome this issue, we raise a novel data scheduling strategy and a weight consolidation regularization term, which improves the decoder's capability of rendering sharper details. Additionally, we optimize the guiding effect of the portrait image by computing a finer-grained hierarchical representation that captures rich 2D texture cues, and injecting them to the 3D diffusion model at multiple layers via cross-attention. When trained on 46K avatars with a noise schedule optimized for triplanes, the resulting model can generate 3D avatars with notably better details than previous methods and can generalize to in-the-wild portrait input.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2404.14463.pdf' target='_blank'>https://arxiv.org/pdf/2404.14463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Burdisso, Ernesto Reyes-RamÃ­rez, EsaÃº Villatoro-Tello, Fernando SÃ¡nchez-Vega, Pastor LÃ³pez-Monroy, Petr Motlicek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14463">DAIC-WOZ: On the Validity of Using the Therapist's prompts in Automatic Depression Detection from Clinical Interviews</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic depression detection from conversational data has gained significant interest in recent years. The DAIC-WOZ dataset, interviews conducted by a human-controlled virtual agent, has been widely used for this task. Recent studies have reported enhanced performance when incorporating interviewer's prompts into the model. In this work, we hypothesize that this improvement might be mainly due to a bias present in these prompts, rather than the proposed architectures and methods. Through ablation experiments and qualitative analysis, we discover that models using interviewer's prompts learn to focus on a specific region of the interviews, where questions about past experiences with mental health issues are asked, and use them as discriminative shortcuts to detect depressed participants. In contrast, models using participant responses gather evidence from across the entire interview. Finally, to highlight the magnitude of this bias, we achieve a 0.90 F1 score by intentionally exploiting it, the highest result reported to date on this dataset using only textual information. Our findings underline the need for caution when incorporating interviewers' prompts into models, as they may inadvertently learn to exploit targeted prompts, rather than learning to characterize the language and behavior that are genuinely indicative of the patient's mental health condition.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2403.19655.pdf' target='_blank'>https://arxiv.org/pdf/2403.19655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19655">GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling. Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods. We derive GaussianCube by first using a novel densification-constrained Gaussian fitting algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians, and then rearranging these Gaussians into a predefined voxel grid via Optimal Transport. Since GaussianCube is a structured grid representation, it allows us to use standard 3D U-Net as our backbone in diffusion modeling without elaborate designs. More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude. The compactness of GaussianCube greatly eases the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling. Project page: https://gaussiancube.github.io/.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2311.12817.pdf' target='_blank'>https://arxiv.org/pdf/2311.12817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binzhe Li, Bolin Chen, Zhao Wang, Shiqi Wang, Yan Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12817">Semantic Face Compression for Metaverse: A Compact 3D Descriptor Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this letter, we envision a new metaverse communication paradigm for virtual avatar faces, and develop the semantic face compression with compact 3D facial descriptors. The fundamental principle is that the communication of virtual avatar faces primarily emphasizes the conveyance of semantic information. In light of this, the proposed scheme offers the advantages of being highly flexible, efficient and semantically meaningful. The semantic face compression, which allows the communication of the descriptors for artificial intelligence based understanding, could facilitate numerous applications without the involvement of humans in metaverse. The promise of the proposed paradigm is also demonstrated by performance comparisons with the state-of-the-art video coding standard, Versatile Video Coding. A significant improvement in terms of rate-accuracy performance has been achieved. The proposed scheme is expected to enable numerous applications, such as digital human communication based on machine analysis, and to form the cornerstone of interaction and communication in the metaverse.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2309.07125.pdf' target='_blank'>https://arxiv.org/pdf/2309.07125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07125">Text-Guided Generation and Editing of Compositional 3D Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to create a realistic 3D facial avatar with hair and accessories using only a text description. While this challenge has attracted significant recent interest, existing methods either lack realism, produce unrealistic shapes, or do not support editing, such as modifications to the hairstyle. We argue that existing methods are limited because they employ a monolithic modeling approach, using a single representation for the head, face, hair, and accessories. Our observation is that the hair and face, for example, have very different structural qualities that benefit from different representations. Building on this insight, we generate avatars with a compositional model, in which the head, face, and upper body are represented with traditional 3D meshes, and the hair, clothing, and accessories with neural radiance fields (NeRF). The model-based mesh representation provides a strong geometric prior for the face region, improving realism while enabling editing of the person's appearance. By using NeRFs to represent the remaining components, our method is able to model and synthesize parts with complex geometry and appearance, such as curly hair and fluffy scarves. Our novel system synthesizes these high-quality compositional avatars from text descriptions. The experimental results demonstrate that our method, Text-guided generation and Editing of Compositional Avatars (TECA), produces avatars that are more realistic than those of recent methods while being editable because of their compositional nature. For example, our TECA enables the seamless transfer of compositional features like hairstyles, scarves, and other accessories between avatars. This capability supports applications such as virtual try-on.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2309.06441.pdf' target='_blank'>https://arxiv.org/pdf/2309.06441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06441">Learning Disentangled Avatars with Hybrid 3D Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tremendous efforts have been made to learn animatable and photorealistic human avatars. Towards this end, both explicit and implicit 3D representations are heavily studied for a holistic modeling and capture of the whole human (e.g., body, clothing, face and hair), but neither representation is an optimal choice in terms of representation efficacy since different parts of the human avatar have different modeling desiderata. For example, meshes are generally not suitable for modeling clothing and hair. Motivated by this, we present Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit 3D representations. DELTA takes a monocular RGB video as input, and produces a human avatar with separate body and clothing/hair layers. Specifically, we demonstrate two important applications for DELTA. For the first one, we consider the disentanglement of the human body and clothing and in the second, we disentangle the face and hair. To do so, DELTA represents the body or face with an explicit mesh-based parametric 3D model and the clothing or hair with an implicit neural radiance field. To make this possible, we design an end-to-end differentiable renderer that integrates meshes into volumetric rendering, enabling DELTA to learn directly from monocular videos without any 3D supervision. Finally, we show that how these two applications can be easily combined to model full-body avatars, such that the hair, face, body and clothing can be fully disentangled yet jointly rendered. Such a disentanglement enables hair and clothing transfer to arbitrary body shapes. We empirically validate the effectiveness of DELTA's disentanglement by demonstrating its promising performance on disentangled reconstruction, virtual clothing try-on and hairstyle transfer. To facilitate future research, we also release an open-sourced pipeline for the study of hybrid human avatar modeling.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2303.13071.pdf' target='_blank'>https://arxiv.org/pdf/2303.13071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13071">PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in $360^\circ$ with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2509.02278.pdf' target='_blank'>https://arxiv.org/pdf/2509.02278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikai Huang, Yihan Zhou, Xuemiao Xu, Cheng Xu, Xiaofen Xing, Jing Qin, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02278">Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Singing-driven 3D head animation is a challenging yet promising task with applications in virtual avatars, entertainment, and education. Unlike speech, singing involves richer emotional nuance, dynamic prosody, and lyric-based semantics, requiring the synthesis of fine-grained, temporally coherent facial motion. Existing speech-driven approaches often produce oversimplified, emotionally flat, and semantically inconsistent results, which are insufficient for singing animation. To address this, we propose Think2Sing, a diffusion-based framework that leverages pretrained large language models to generate semantically coherent and temporally consistent 3D head animations, conditioned on both lyrics and acoustics. A key innovation is the introduction of motion subtitles, an auxiliary semantic representation derived through a novel Singing Chain-of-Thought reasoning process combined with acoustic-guided retrieval. These subtitles contain precise timestamps and region-specific motion descriptions, serving as interpretable motion priors. We frame the task as a motion intensity prediction problem, enabling finer control over facial regions and improving the modeling of expressive motion. To support this, we create a multimodal singing dataset with synchronized video, acoustic descriptors, and motion subtitles, enabling diverse and expressive motion learning. Extensive experiments show that Think2Sing outperforms state-of-the-art methods in realism, expressiveness, and emotional fidelity, while also offering flexible, user-controllable animation editing.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2505.03351.pdf' target='_blank'>https://arxiv.org/pdf/2505.03351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Yang Li, Minghan Qin, Yu Li, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03351">GUAVA: Generalizable Upper Body 3D Gaussian Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing a high-quality, animatable 3D human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3D human avatar reconstruction typically requires multi-view or monocular videos and training on individual IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's expressiveness, these methods often focus on body motion but struggle with facial expressions. To address these challenges, we first introduce an expressive human model (EHM) to enhance facial expression capabilities and develop an accurate tracking method. Based on this template model, we propose GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction. We leverage inverse texture mapping and projection sampling techniques to infer Ubody (upper-body) Gaussians from a single image. The rendered images are refined through a neural refiner. Experimental results demonstrate that GUAVA significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2503.11978.pdf' target='_blank'>https://arxiv.org/pdf/2503.11978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric M. Chen, Di Liu, Sizhuo Ma, Michael Vasilkovsky, Bing Zhou, Qiang Gao, Wenzhou Wang, Jiahao Luo, Dimitris N. Metaxas, Vincent Sitzmann, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11978">Snapmoji: Instant Generation of Animatable Dual-Stylized Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing popularity of personalized avatar systems, such as Snapchat Bitmojis and Apple Memojis, highlights the growing demand for digital self-representation. Despite their widespread use, existing avatar platforms face significant limitations, including restricted expressivity due to predefined assets, tedious customization processes, or inefficient rendering requirements. Addressing these shortcomings, we introduce Snapmoji, an avatar generation system that instantly creates animatable, dual-stylized avatars from a selfie. We propose Gaussian Domain Adaptation (GDA), which is pre-trained on large-scale Gaussian models using 3D data from sources such as Objaverse and fine-tuned with 2D style transfer tasks, endowing it with a rich 3D prior. This enables Snapmoji to transform a selfie into a primary stylized avatar, like the Bitmoji style, and apply a secondary style, such as Plastic Toy or Alien, all while preserving the user's identity and the primary style's integrity. Our system is capable of producing 3D Gaussian avatars that support dynamic animation, including accurate facial expression transfer. Designed for efficiency, Snapmoji achieves selfie-to-avatar conversion in just 0.9 seconds and supports real-time interactions on mobile devices at 30 to 40 frames per second. Extensive testing confirms that Snapmoji outperforms existing methods in versatility and speed, making it a convenient tool for automatic avatar creation in various styles.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2503.08224.pdf' target='_blank'>https://arxiv.org/pdf/2503.08224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Kangjie Chen, Minghan Qin, Yu Li, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08224">HRAvatar: High-Quality and Relightable Gaussian Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing animatable and high-quality 3D head avatars from monocular videos, especially with realistic relighting, is a valuable task. However, the limited information from single-view input, combined with the complex head poses and facial movements, makes this challenging. Previous methods achieve real-time performance by combining 3D Gaussian Splatting with a parametric head model, but the resulting head quality suffers from inaccurate face tracking and limited expressiveness of the deformation model. These methods also fail to produce realistic effects under novel lighting conditions. To address these issues, we propose HRAvatar, a 3DGS-based method that reconstructs high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors through end-to-end optimization and better captures individual facial deformations using learnable blendshapes and learnable linear blend skinning. Additionally, it decomposes head appearance into several physical properties and incorporates physically-based shading to account for environmental lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs superior-quality heads but also achieves realistic visual effects under varying lighting conditions.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2502.19739.pdf' target='_blank'>https://arxiv.org/pdf/2502.19739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Liu, Teng Deng, Giljoo Nam, Yu Rong, Stanislav Pidhorskyi, Junxuan Li, Jason Saragih, Dimitris N. Metaxas, Chen Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19739">LUCAS: Layered Universal Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic 3D head avatar reconstruction faces critical challenges in modeling dynamic face-hair interactions and achieving cross-identity generalization, particularly during expressions and head movements. We present LUCAS, a novel Universal Prior Model (UPM) for codec avatar modeling that disentangles face and hair through a layered representation. Unlike previous UPMs that treat hair as an integral part of the head, our approach separates the modeling of the hairless head and hair into distinct branches. LUCAS is the first to introduce a mesh-based UPM, facilitating real-time rendering on devices. Our layered representation also improves the anchor geometry for precise and visually appealing Gaussian renderings. Experimental results indicate that LUCAS outperforms existing single-mesh and Gaussian-based avatar models in both quantitative and qualitative assessments, including evaluations on held-out subjects in zero-shot driving scenarios. LUCAS demonstrates superior dynamic performance in managing head pose changes, expression transfer, and hairstyle variations, thereby advancing the state-of-the-art in 3D head avatar reconstruction.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2411.19942.pdf' target='_blank'>https://arxiv.org/pdf/2411.19942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Ye, Xiaoxuan Ma, Hai Ci, Wentao Zhu, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19942">FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving realistic animated human avatars requires accurate modeling of pose-dependent clothing deformations. Existing learning-based methods heavily rely on the Linear Blend Skinning (LBS) of minimally-clothed human models like SMPL to model deformation. However, they struggle to handle loose clothing, such as long dresses, where the canonicalization process becomes ill-defined when the clothing is far from the body, leading to disjointed and fragmented results. To overcome this limitation, we propose FreeCloth, a novel hybrid framework to model challenging clothed humans. Our core idea is to use dedicated strategies to model different regions, depending on whether they are close to or distant from the body. Specifically, we segment the human body into three categories: unclothed, deformed, and generated. We simply replicate unclothed regions that require no deformation. For deformed regions close to the body, we leverage LBS to handle the deformation. As for the generated regions, which correspond to loose clothing areas, we introduce a novel free-form, part-aware generator to model them, as they are less affected by movements. This free-form generation paradigm brings enhanced flexibility and expressiveness to our hybrid framework, enabling it to capture the intricate geometric details of challenging loose clothing, such as skirts and dresses. Experimental results on the benchmark dataset featuring loose clothing demonstrate that FreeCloth achieves state-of-the-art performance with superior visual fidelity and realism, particularly in the most challenging cases.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2407.15212.pdf' target='_blank'>https://arxiv.org/pdf/2407.15212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Zhao, Chenming Wu, Binbin Huang, Yihao Zhi, Chen Zhao, Jingdong Wang, Shenghua Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15212">Surfel-based Gaussian Inverse Rendering for Fast and Relightable Dynamic Human Reconstruction from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate reconstruction of a relightable, dynamic clothed human avatar from a monocular video is crucial for the entertainment industry. This paper introduces the Surfel-based Gaussian Inverse Avatar (SGIA) method, which introduces efficient training and rendering for relightable dynamic human reconstruction. SGIA advances previous Gaussian Avatar methods by comprehensively modeling Physically-Based Rendering (PBR) properties for clothed human avatars, allowing for the manipulation of avatars into novel poses under diverse lighting conditions. Specifically, our approach integrates pre-integration and image-based lighting for fast light calculations that surpass the performance of existing implicit-based techniques. To address challenges related to material lighting disentanglement and accurate geometry reconstruction, we propose an innovative occlusion approximation strategy and a progressive training approach. Extensive experiments demonstrate that SGIA not only achieves highly accurate physical properties but also significantly enhances the realistic relighting of dynamic human avatars, providing a substantial speed advantage. We exhibit more results in our project page: https://GS-IA.github.io.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2405.19712.pdf' target='_blank'>https://arxiv.org/pdf/2405.19712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Sanvito, Andrea Ramazzina, Stefanie Walz, Mario Bijelic, Felix Heide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19712">HINT: Learning Complete Human Neural Representations from Limited Viewpoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No augmented application is possible without animated humanoid avatars. At the same time, generating human replicas from real-world monocular hand-held or robotic sensor setups is challenging due to the limited availability of views. Previous work showed the feasibility of virtual avatars but required the presence of 360 degree views of the targeted subject. To address this issue, we propose HINT, a NeRF-based algorithm able to learn a detailed and complete human model from limited viewing angles. We achieve this by introducing a symmetry prior, regularization constraints, and training cues from large human datasets. In particular, we introduce a sagittal plane symmetry prior to the appearance of the human, directly supervise the density function of the human model using explicit 3D body modeling, and leverage a co-learned human digitization network as additional supervision for the unseen angles. As a result, our method can reconstruct complete humans even from a few viewing angles, increasing performance by more than 15% PSNR compared to previous state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2404.19026.pdf' target='_blank'>https://arxiv.org/pdf/2404.19026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19026">MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2403.17008.pdf' target='_blank'>https://arxiv.org/pdf/2403.17008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilong Zhang, Lianghua Huang, Xi Chen, Yifei Zhang, Zhi-Fan Wu, Yutong Feng, Wei Wang, Yujun Shen, Yu Liu, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17008">FlashFace: Human Image Personalization with High-fidelity Identity Preservation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents FlashFace, a practical tool with which users can easily personalize their own photos on the fly by providing one or a few reference face images and a text prompt. Our approach is distinguishable from existing human photo customization methods by higher-fidelity identity preservation and better instruction following, benefiting from two subtle designs. First, we encode the face identity into a series of feature maps instead of one image token as in prior arts, allowing the model to retain more details of the reference faces (e.g., scars, tattoos, and face shape ). Second, we introduce a disentangled integration strategy to balance the text and image guidance during the text-to-image generation process, alleviating the conflict between the reference faces and the text prompts (e.g., personalizing an adult into a "child" or an "elder"). Extensive experimental results demonstrate the effectiveness of our method on various applications, including human image personalization, face swapping under language prompts, making virtual characters into real people, etc. Project Page: https://jshilong.github.io/flashface-page.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2403.16210.pdf' target='_blank'>https://arxiv.org/pdf/2403.16210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16210">Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar cloth re-targeting. Our project page is available at: https://wolfball.github.io/frankenstein/.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2311.06443.pdf' target='_blank'>https://arxiv.org/pdf/2311.06443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Ma, Tong Zhang, Shanlin Sun, Xiangyi Yan, Kun Han, Xiaohui Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06443">CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing personalized animatable head avatars has significant implications in the fields of AR/VR. Existing methods for achieving explicit face control of 3D Morphable Models (3DMM) typically rely on multi-view images or videos of a single subject, making the reconstruction process complex. Additionally, the traditional rendering pipeline is time-consuming, limiting real-time animation possibilities. In this paper, we introduce CVTHead, a novel approach that generates controllable neural head avatars from a single reference image using point-based neural rendering. CVTHead considers the sparse vertices of mesh as the point set and employs the proposed Vertex-feature Transformer to learn local feature descriptors for each vertex. This enables the modeling of long-range dependencies among all the vertices. Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves comparable performance to state-of-the-art graphics-based methods. Moreover, it enables efficient rendering of novel human heads with various expressions, head poses, and camera views. These attributes can be explicitly controlled using the coefficients of 3DMMs, facilitating versatile and realistic animation in real-time scenarios.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2305.02195.pdf' target='_blank'>https://arxiv.org/pdf/2305.02195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02195">CALM: Conditional Adversarial Latent Models for Directable Virtual Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Conditional Adversarial Latent Models (CALM), an approach for generating diverse and directable behaviors for user-controlled interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human motion, and enables direct control over character movements. The approach jointly learns a control policy and a motion encoder that reconstructs key characteristics of a given motion without merely replicating it. The results show that CALM learns a semantic motion representation, enabling control over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces, akin to those found in video games.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2305.01309.pdf' target='_blank'>https://arxiv.org/pdf/2305.01309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinju Wu, Pingping Zhang, Meng Wang, Peilin Chen, Shiqi Wang, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01309">Geometric Prior Based Deep Human Point Cloud Geometry Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of digital avatars has raised an exponential increase in the demand for human point clouds with realistic and intricate details. The compression of such data becomes challenging with overwhelming data amounts comprising millions of points. Herein, we leverage the human geometric prior in geometry redundancy removal of point clouds, greatly promoting the compression performance. More specifically, the prior provides topological constraints as geometry initialization, allowing adaptive adjustments with a compact parameter set that could be represented with only a few bits. Therefore, we can envisage high-resolution human point clouds as a combination of geometric priors and structural deviations. The priors could first be derived with an aligned point cloud, and subsequently the difference of features is compressed into a compact latent code. The proposed framework can operate in a play-and-plug fashion with existing learning based point cloud compression methods. Extensive experimental results show that our approach significantly improves the compression performance without deteriorating the quality, demonstrating its promise in a variety of applications.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2304.10482.pdf' target='_blank'>https://arxiv.org/pdf/2304.10482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria-Paola Forte, Peter Kulits, Chun-Hao Huang, Vasileios Choutas, Dimitrios Tzionas, Katherine J. Kuchenbecker, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10482">Reconstructing Signing Avatars From Video Using Linguistic Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify's 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at $\href{http://sgnify.is.tue.mpg.de}{\text{sgnify.is.tue.mpg.de}}$.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2210.13289.pdf' target='_blank'>https://arxiv.org/pdf/2210.13289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adnan Qayyum, Muhammad Atif Butt, Hassan Ali, Muhammad Usman, Osama Halabi, Ala Al-Fuqaha, Qammer H. Abbasi, Muhammad Ali Imran, Junaid Qadir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13289">Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR) for Metaverses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalised experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies like augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization, etc.), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users' privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2112.11454.pdf' target='_blank'>https://arxiv.org/pdf/2112.11454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omid Taheri, Vasileios Choutas, Michael J. Black, Dimitrios Tzionas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.11454">GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied, but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this, the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. GOAL takes a step towards synthesizing realistic full-body object grasping.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2509.11342.pdf' target='_blank'>https://arxiv.org/pdf/2509.11342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyun Han, Siyeon Bak, So-Hui Kim, Kangsoo Kim, Sun-Jeong Kim, Isaac Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11342">What if Virtual Agents Had Scents? Users' Judgments of Virtual Agent Personality and Appeals in Encounters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating multi-sensory cues into Virtual Reality (VR) can significantly enhance user experiences, mirroring the multi-sensory interactions we encounter in the real-world. Olfaction plays a crucial role in shaping impressions when engaging with others. This study examines how non-verbal cues from virtual agents-specifically olfactory cues, emotional expressions, and gender-influence user perceptions during encounters with virtual agents. Our findings indicate that in unscented, woodsy, and floral scent conditions, participants primarily relied on visually observable cues to form their impressions of virtual agents. Positive emotional expressions, conveyed through facial expressions and gestures, contributed to more favorable impressions, with this effect being stronger for the female agent than the male agent. However, in the unpleasant scent condition, participants consistently formed negative impressions, which overpowered the influence of emotional expressions and gender, suggesting that aversive olfactory stimuli can detrimentally impact user perceptions. Our results emphasize the importance of carefully selecting olfactory stimuli when designing immersive and engaging VR interactions. Finally, we present our findings and outline future research directions for effectively integrating olfactory cues into virtual agents.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2505.07736.pdf' target='_blank'>https://arxiv.org/pdf/2505.07736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eason Chen, Xinyi Tang, Aprille Xi, Chenyu Lin, Conrad Borchers, Shivang Gupta, Jionghao Lin, Kenneth R Koedinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07736">VTutor for High-Impact Tutoring at Scale: Managing Engagement and Real-Time Multi-Screen Monitoring with P2P Connections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid tutoring, where a human tutor supports multiple students in learning with educational technology, is an increasingly common application to deliver high-impact tutoring at scale. However, past hybrid tutoring applications are limited in guiding tutor attention to students that require support. Specifically, existing conferencing tools, commonly used in hybrid tutoring, do not allow tutors to monitor multiple students' screens while directly communicating and attending to multiple students simultaneously. To address this issue, this paper introduces VTutor, a web-based platform leveraging peer-to-peer screen sharing and virtual avatars to deliver real-time, context-aware tutoring feedback at scale. By integrating a multi-student monitoring dashboard with AI-powered avatar prompts, VTutor empowers a single educator or tutor to rapidly detect off-task or struggling students and intervene proactively, thus enhancing the benefits of one-on-one interactions in classroom contexts with several students. Drawing on insight from the learning sciences and past research on animated pedagogical agents, we demonstrate how stylized avatars can potentially sustain student engagement while accommodating varying infrastructure constraints. Finally, we address open questions on refining large-scale, AI-driven tutoring solutions for improved learner outcomes, and how VTutor could help interpret real-time learner interactions to support remote tutors at scale. The VTutor platform can be accessed at https://ls2025.vtutor.ai. The system demo video is at https://ls2025.vtutor.ai/video.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2410.05131.pdf' target='_blank'>https://arxiv.org/pdf/2410.05131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ashrafi, Francesco Vona, Carina Ringsdorf, Christian Hertel, Luca Toni, Sarina Kailer, Alice Bartels, Tanja Kojic, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05131">Enhancing Job Interview Preparation Through Immersive Experiences Using Photorealistic, AI-powered Metahuman Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study will investigate the user experience while interacting with highly photorealistic virtual job interviewer avatars in Virtual Reality (VR), Augmented Reality (AR), and on a 2D screen. Having a precise speech recognition mechanism, our virtual character performs a mock-up software engineering job interview to adequately immerse the user in a life-like scenario. To evaluate the efficiency of our system, we measure factors such as the provoked level of anxiety, social presence, self-esteem, and intrinsic motivation. This research is a work in progress with a prospective within-subject user study including approximately 40 participants. All users will engage with three job interview conditions (VR, AR, and desktop) and provide their feedback. Additionally, users' bio-physical responses will be collected using a biosensor to measure the level of anxiety during the job interview.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2408.04068.pdf' target='_blank'>https://arxiv.org/pdf/2408.04068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timothy Rupprecht, Sung-En Chang, Yushu Wu, Lei Lu, Enfu Nan, Chih-hsiang Li, Caiyue Lai, Zhimin Li, Zhijun Hu, Yumei He, David Kaeli, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04068">Digital Avatars: Framework Development and Their Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel prompting strategy for artificial intelligence driven digital avatars. To better quantify how our prompting strategy affects anthropomorphic features like humor, authenticity, and favorability we present Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a large language model (LLM) candidate over competitors answering the same or similar prompts. To visualize the responses of our LLM, and the effectiveness of our prompting strategy we propose an end-to-end framework for creating high-fidelity artificial intelligence (AI) driven digital avatars. This pipeline effectively captures an individual's essence for interaction and our streaming algorithm delivers a high-quality digital avatar with real-time audio-video streaming from server to mobile device. Both our visualization tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have state-of-the-art humor, authenticity, and favorability outperforming all competitors and baselines. In the case of our Donald Trump and Joe Biden avatars, their authenticity and favorability are rated higher than even their real-world equivalents.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2406.04253.pdf' target='_blank'>https://arxiv.org/pdf/2406.04253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihe Wang, Yukang Cao, Kai Han, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04253">A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2404.15383.pdf' target='_blank'>https://arxiv.org/pdf/2404.15383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Nikos Athanasiou, Omid Taheri, Xi Wang, Otmar Hilliges, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15383">WANDR: Intention-guided Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching. To address this, we introduce WANDR, a data-driven model that takes an avatar's initial pose and a goal's 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement. Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations. Our models and code are available for research purposes at wandr.is.tue.mpg.de.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2404.01700.pdf' target='_blank'>https://arxiv.org/pdf/2404.01700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang YU, Jiayuan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01700">MotionChain: Conversational Motion Controllers via Multimodal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2403.17477.pdf' target='_blank'>https://arxiv.org/pdf/2403.17477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai BÃ¢ce, Zhiming Hu, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17477">DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360Â° Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360Â° images based on a conditional score-based denoising diffusion model. Generating human gaze on 360Â° images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360Â° images as condition and uses two transformers to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360Â° image benchmarks for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks. We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2403.09879.pdf' target='_blank'>https://arxiv.org/pdf/2403.09879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanja KojiÄ, Maurizio Vergari, Marco Podratz, Sebastian MÃ¶ller, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09879">The Influence of Extended Reality and Virtual Characters' Embodiment Levels on User Experience in Well-Being Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Millions of people have seen their daily habits transform, reducing physical activity and leading to mental health issues. This study explores how virtual characters impact motivation for well-being. Three prototypes with cartoon, robotic, and human-like avatars were tested by 22 participants. Results show that animated virtual avatars, especially with extended reality, boost motivation, enhance comprehension of activities, and heighten presence. Multiple output modalities, like audio and text, with character animations, improve the user experience. Notably, the cartoon-like character evoked positive responses. This research highlights virtual characters' potential to engage individuals in daily well-being activities.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2403.09544.pdf' target='_blank'>https://arxiv.org/pdf/2403.09544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ashrafi, Vanessa Neuhaus, Francesco Vona, Nicolina Laura Peperkorn, Youssef Shiban, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09544">Effect of external characteristics of a virtual human being during the use of a computer-assisted therapy tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identification within media, whether with real or fictional characters, significantly impacts users, shaping their behavior and enriching their social and emotional experiences. Immersive media, like video games, utilize virtual entities such as agents, avatars, or NPCs to connect users with virtual worlds, fostering a heightened sense of immersion and identification. However, challenges arise in visually representing these entities, with design decisions crucial for enhancing user interaction. Recent research highlights the potential of user-defined design, or customization, which goes beyond mere visual resemblance to the user. Understanding how identification with virtual avatars influences user experiences, especially in psychological interventions, is pivotal. In a study exploring this, 22 participants created virtual agents either similar or dissimilar to themselves, which then addressed their dysfunctional thoughts. Results indicate that similarity between users and virtual agents not only boosts identification but also positively impacts emotions and motivation, enhancing interest and enjoyment. This study sheds light on the significance of customization and identification, particularly in computer-assisted therapy tools, underscoring the importance of visual design for optimizing user experiences.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2402.13724.pdf' target='_blank'>https://arxiv.org/pdf/2402.13724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Bai, Peng Chen, Xiaolan Peng, Lu Liu, Hui Chen, Mike Zheng Shou, Feng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13724">Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2312.12090.pdf' target='_blank'>https://arxiv.org/pdf/2312.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Yan, Zhiming Hu, Syn Schmitt, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12090">GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is important for many virtual and augmented reality (VR/AR) applications such as collision avoidance and realistic avatar generation. Existing methods have synthesised body motion only from observed past motion, despite the fact that human eye gaze is known to correlate strongly with body movements and is readily available in recent VR/AR headsets. We present GazeMoDiff - a novel gaze-guided denoising diffusion model to generate stochastic human motions. Our method first uses a gaze encoder and a motion encoder to extract the gaze and motion features respectively, then employs a graph attention network to fuse these features, and finally injects the gaze-motion features into a noise prediction network via a cross-attention mechanism to progressively generate multiple reasonable human motions in the future. Extensive experiments on the MoGaze and GIMO datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin in terms of multi-modal final displacement error (17.3% on MoGaze and 13.3% on GIMO). We further conducted a human study (N=21) and validated that the motions generated by our method were perceived as both more precise and more realistic than those of prior methods. Taken together, these results reveal the significant information content available in eye gaze for stochastic human motion prediction as well as the effectiveness of our method in exploiting this information.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2310.17519.pdf' target='_blank'>https://arxiv.org/pdf/2310.17519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrisha Bharadwaj, Yufeng Zheng, Otmar Hilliges, Michael J. Black, Victoria Fernandez-Abrevaya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17519">FLARE: Fast Learning of Animatable and Relightable Mesh Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2308.13953.pdf' target='_blank'>https://arxiv.org/pdf/2308.13953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyun Han, Donghoon Kim, Kangsoo Kim, Isaac Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13953">Investigating Psychological Ownership in a Shared AR Space: Effects of Human and Object Reality and Object Controllability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented reality (AR) provides users with a unique social space where virtual objects are natural parts of the real world. The users can interact with 3D virtual objects and virtual humans projected onto the physical environment. This work examines perceived ownership based on the reality of objects and partners, as well as object controllability in a shared AR setting. Our formal user study with 28 participants shows a sense of possession, control, separation, and partner presence affect their perceived ownership of a shared object. Finally, we discuss the findings and present a conclusion.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2306.08768.pdf' target='_blank'>https://arxiv.org/pdf/2306.08768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano, Umar Iqbal, Jan Kautz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08768">Generalizable One-shot Neural Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method that reconstructs and animates a 3D head avatar from a single-view portrait image. Existing methods either involve time-consuming optimization for a specific person with multiple images, or they struggle to synthesize intricate appearance details beyond the facial region. To address these limitations, we propose a framework that not only generalizes to unseen identities based on a single-view image without requiring person-specific optimization, but also captures characteristic details within and beyond the face area (e.g. hairstyle, accessories, etc.). At the core of our method are three branches that produce three tri-planes representing the coarse 3D geometry, detailed appearance of a source image, as well as the expression of a target image. By applying volumetric rendering to the combination of the three tri-planes followed by a super-resolution module, our method yields a high fidelity image of the desired identity, expression and pose. Once trained, our model enables efficient 3D head avatar reconstruction and animation via a single forward pass through a network. Experiments show that the proposed approach generalizes well to unseen validation datasets, surpassing SOTA baseline methods by a large margin on head avatar reconstruction and animation.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2305.02312.pdf' target='_blank'>https://arxiv.org/pdf/2305.02312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Dong, Xu Chen, Jinlong Yang, Michael J. Black, Otmar Hilliges, Andreas Geiger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02312">AG3D: Learning to Generate 3D Avatars from 2D Image Collections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses. In this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps. We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2509.16922.pdf' target='_blank'>https://arxiv.org/pdf/2509.16922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16922">PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2509.04145.pdf' target='_blank'>https://arxiv.org/pdf/2509.04145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongliang Cao, Guoxing Sun, Marc Habermann, Florian Bernard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04145">Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2508.10576.pdf' target='_blank'>https://arxiv.org/pdf/2508.10576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10576">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2505.24877.pdf' target='_blank'>https://arxiv.org/pdf/2505.24877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24877">AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2504.09671.pdf' target='_blank'>https://arxiv.org/pdf/2504.09671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Manu, Astitva Srivastava, Amit Raj, Varun Jampani, Avinash Sharma, P. J. Narayanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09671">LightHeadEd: Relightable & Editable Head Avatars from a Smartphone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic, animatable, and relightable 3D head avatars traditionally requires expensive Lightstage with multiple calibrated cameras, making it inaccessible for widespread adoption. To bridge this gap, we present a novel, cost-effective approach for creating high-quality relightable head avatars using only a smartphone equipped with polaroid filters. Our approach involves simultaneously capturing cross-polarized and parallel-polarized video streams in a dark room with a single point-light source, separating the skin's diffuse and specular components during dynamic facial performances. We introduce a hybrid representation that embeds 2D Gaussians in the UV space of a parametric head model, facilitating efficient real-time rendering while preserving high-fidelity geometric details. Our learning-based neural analysis-by-synthesis pipeline decouples pose and expression-dependent geometrical offsets from appearance, decomposing the surface into albedo, normal, and specular UV texture maps, along with the environment maps. We collect a unique dataset of various subjects performing diverse facial expressions and head movements.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2504.05046.pdf' target='_blank'>https://arxiv.org/pdf/2504.05046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghao Ren, Yi Lu, Jiayi Huang, Jiayi Zhao, He Zhang, Tao Yu, Qiu Shen, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05046">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human Motion Capture (MoCap) methods mostly focus on the visual similarity while neglecting the physical plausibility. As a result, downstream tasks such as driving virtual human in 3D scene or humanoid robots in real world suffer from issues such as timing drift and jitter, spatial problems like sliding and penetration, and poor global trajectory accuracy. In this paper, we revisit human MoCap from the perspective of interaction between human body and physical world by exploring the role of pressure. Firstly, we construct a large-scale human Motion capture dataset with Pressure, RGB and Optical sensors (named MotionPRO), which comprises 70 volunteers performing 400 types of motion, encompassing a total of 12.4M pose frames. Secondly, we examine both the necessity and effectiveness of the pressure signal through two challenging tasks: (1) pose and trajectory estimation based solely on pressure: We propose a network that incorporates a small kernel decoder and a long-short-term attention module, and proof that pressure could provide accurate global trajectory and plausible lower body pose. (2) pose and trajectory estimation by fusing pressure and RGB: We impose constraints on orthographic similarity along the camera axis and whole-body contact along the vertical axis to enhance the cross-attention strategy to fuse pressure and RGB feature maps. Experiments demonstrate that fusing pressure with RGB features not only significantly improves performance in terms of objective metrics, but also plausibly drives virtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that incorporating physical perception enables humanoid robots to perform more precise and stable actions, which is highly beneficial for the development of embodied artificial intelligence. Project page is available at: https://nju-cite-mocaphumanoid.github.io/MotionPRO/
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2503.12242.pdf' target='_blank'>https://arxiv.org/pdf/2503.12242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Jiang, Zhehao Shen, Chengcheng Guo, Yu Hong, Zhuo Su, Yingliang Zhang, Marc Habermann, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12242">RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric volumetric videos offer immersive free-viewpoint experiences, yet existing methods focus either on replaying general dynamic scenes or animating human avatars, limiting their ability to re-perform general dynamic scenes. In this paper, we present RePerformer, a novel Gaussian-based representation that unifies playback and re-performance for high-fidelity human-centric volumetric videos. Specifically, we hierarchically disentangle the dynamic scenes into motion Gaussians and appearance Gaussians which are associated in the canonical space. We further employ a Morton-based parameterization to efficiently encode the appearance Gaussians into 2D position and attribute maps. For enhanced generalization, we adopt 2D CNNs to map position maps to attribute maps, which can be assembled into appearance Gaussians for high-fidelity rendering of the dynamic scenes. For re-performance, we develop a semantic-aware alignment module and apply deformation transfer on motion Gaussians, enabling photo-real rendering under novel motions. Extensive experiments validate the robustness and effectiveness of RePerformer, setting a new benchmark for playback-then-reperformance paradigm in human-centric volumetric videos.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2503.12052.pdf' target='_blank'>https://arxiv.org/pdf/2503.12052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyao Sun, Yu-Hui Wen, Matthieu Lin, Ho-Jui Fang, Sheng Ye, Tian Lv, Yong-Jin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12052">Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating detailed 3D human avatars with garments typically requires specialized expertise and labor-intensive processes. Although recent advances in generative AI have enabled text-to-3D human/clothing generation, current methods fall short in offering accessible, integrated pipelines for producing ready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated text-to-avatar system that generates high-fidelity, customizable 3D humans with simulation-ready garments. Our system includes a three-stage pipeline. We first employ a large language model to interpret textual descriptions into parameterized body shapes and semantically matched garment templates. Next, we develop topology-preserving deformation with novel geometric losses to adapt garments precisely to body geometries. Furthermore, an enhanced texture diffusion module with a symmetric local attention mechanism ensures both view consistency and photorealistic details. Quantitative and qualitative evaluations demonstrate that Tailor outperforms existing SoTA methods in terms of fidelity, usability, and diversity. Code will be available for academic use.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2412.16915.pdf' target='_blank'>https://arxiv.org/pdf/2412.16915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16915">FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage http://fadavatar.github.io.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2412.15609.pdf' target='_blank'>https://arxiv.org/pdf/2412.15609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jotaro Sakamiya, I-Chao Shen, Jinsong Zhang, Mustafa Doga Dogan, Takeo Igarashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15609">AvatarPerfect: User-Assisted 3D Gaussian Splatting Avatar Refinement with Automatic Pose Suggestion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality 3D avatars using 3D Gaussian Splatting (3DGS) from a monocular video benefits virtual reality and telecommunication applications. However, existing automatic methods exhibit artifacts under novel poses due to limited information in the input video. We propose AvatarPerfect, a novel system that allows users to iteratively refine 3DGS avatars by manually editing the rendered avatar images. In each iteration, our system suggests a new body and camera pose to help users identify and correct artifacts. The edited images are then used to update the current avatar, and our system suggests the next body and camera pose for further refinement. To investigate the effectiveness of AvatarPerfect, we conducted a user study comparing our method to an existing 3DGS editor SuperSplat, which allows direct manipulation of Gaussians without automatic pose suggestions. The results indicate that our system enables users to obtain higher quality refined 3DGS avatars than the existing 3DGS editor.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2412.10523.pdf' target='_blank'>https://arxiv.org/pdf/2412.10523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changan Chen, Juze Zhang, Shrinidhi K. Lakshmikanth, Yusu Fang, Ruizhi Shao, Gordon Wetzstein, Li Fei-Fei, Ehsan Adeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10523">The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human communication is inherently multimodal, involving a combination of verbal and non-verbal cues such as speech, facial expressions, and body gestures. Modeling these behaviors is essential for understanding human interaction and for creating virtual characters that can communicate naturally in applications like games, films, and virtual reality. However, existing motion generation models are typically limited to specific input modalities -- either speech, text, or motion data -- and cannot fully leverage the diversity of available data. In this paper, we propose a novel framework that unifies verbal and non-verbal language using multimodal language models for human motion understanding and generation. This model is flexible in taking text, speech, and motion or any combination of them as input. Coupled with our novel pre-training strategy, our model not only achieves state-of-the-art performance on co-speech gesture generation but also requires much less data for training. Our model also unlocks an array of novel tasks such as editable gesture generation and emotion prediction from motion. We believe unifying the verbal and non-verbal language of human motion is essential for real-world applications, and language models offer a powerful approach to achieving this goal. Project page: languageofmotion.github.io.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2412.09545.pdf' target='_blank'>https://arxiv.org/pdf/2412.09545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09545">SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2411.08228.pdf' target='_blank'>https://arxiv.org/pdf/2411.08228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atieh Taheri, Purav Bhardwaj, Arthur Caetano, Alice Zhong, Misha Sra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08228">Virtual Buddy: Redefining Conversational AI Interactions for Individuals with Hand Motor Disabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence have transformed the paradigm of human-computer interaction, with the development of conversational AI systems playing a pivotal role. These systems employ technologies such as natural language processing and machine learning to simulate intelligent and human-like conversations. Driven by the personal experience of an individual with a neuromuscular disease who faces challenges with leaving home and contends with limited hand-motor control when operating digital systems, including conversational AI platforms, we propose a method aimed at enriching their interaction with conversational AI. Our prototype allows the creation of multiple agent personas based on hobbies and interests, to support topic-based conversations. In contrast with existing systems, such as Replika, that offer a 1:1 relation with a virtual agent, our design enables one-to-many relationships, easing the process of interaction for this individual by reducing the need for constant data input. We can imagine our prototype potentially helping others who are in a similar situation with reduced typing/input ability.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2410.11682.pdf' target='_blank'>https://arxiv.org/pdf/2410.11682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeseong Lee, Taewoong Kang, Marcel C. BÃ¼hler, Min-Jung Kim, Sungwon Hwang, Junha Hyung, Hyojin Jang, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11682">SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2407.10865.pdf' target='_blank'>https://arxiv.org/pdf/2407.10865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexey Kotcov, Maria Dronova, Vladislav Cheremnykh, Sausar Karaf, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10865">AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future Communication Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving landscape of digital content creation, the demand for fast, convenient, and autonomous methods of crafting detailed 3D reconstructions of humans has grown significantly. Addressing this pressing need, our AirNeRF system presents an innovative pathway to the creation of a realistic 3D human avatar. Our approach leverages Neural Radiance Fields (NeRF) with an automated drone-based video capturing method. The acquired data provides a swift and precise way to create high-quality human body reconstructions following several stages of our system. The rigged mesh derived from our system proves to be an excellent foundation for free-view synthesis of dynamic humans, particularly well-suited for the immersive experiences within gaming and virtual reality.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2407.05712.pdf' target='_blank'>https://arxiv.org/pdf/2407.05712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang, Yongming Zhu, Jiaqi Yang, Tianyun Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05712">MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, we introduce a mixed representation of explicit and implicit keypoints for precise motion modeling and precomputed visual features for enhanced foreground and background synthesis. With these two key designs and using simple U-Nets as backbones, our method achieves state-of-the-art performance with less than one-tenth the computational demand. It has been validated to reach speeds of over 100 FPS on mobile devices and support both video and audio-driven inputs.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2404.01543.pdf' target='_blank'>https://arxiv.org/pdf/2404.01543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01543">Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2402.11909.pdf' target='_blank'>https://arxiv.org/pdf/2402.11909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11909">One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2401.04728.pdf' target='_blank'>https://arxiv.org/pdf/2401.04728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04728">Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multi-view-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks. The code for our project is publicly available.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2401.03108.pdf' target='_blank'>https://arxiv.org/pdf/2401.03108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanthika Naik, Kunwar Singh, Astitva Srivastava, Dhawal Sirikonda, Amit Raj, Varun Jampani, Avinash Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03108">Dress-Me-Up: A Dataset & Method for Self-Supervised 3D Garment Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel self-supervised framework for retargeting non-parameterized 3D garments onto 3D human avatars of arbitrary shapes and poses, enabling 3D virtual try-on (VTON). Existing self-supervised 3D retargeting methods only support parametric and canonical garments, which can only be draped over parametric body, e.g. SMPL. To facilitate the non-parametric garments and body, we propose a novel method that introduces Isomap Embedding based correspondences matching between the garment and the human body to get a coarse alignment between the two meshes. We perform neural refinement of the coarse alignment in a self-supervised setting. Further, we leverage a Laplacian detail integration method for preserving the inherent details of the input garment. For evaluating our 3D non-parametric garment retargeting framework, we propose a dataset of 255 real-world garments with realistic noise and topological deformations. The dataset contains $44$ unique garments worn by 15 different subjects in 5 distinctive poses, captured using a multi-view RGBD capture setup. We show superior retargeting quality on non-parametric garments and human avatars over existing state-of-the-art methods, acting as the first-ever baseline on the proposed dataset for non-parametric 3D garment retargeting.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2312.11461.pdf' target='_blank'>https://arxiv.org/pdf/2312.11461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11461">GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2312.04558.pdf' target='_blank'>https://arxiv.org/pdf/2312.04558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04558">MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2312.03763.pdf' target='_blank'>https://arxiv.org/pdf/2312.03763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushi Lan, Feitong Tan, Di Qiu, Qiangeng Xu, Kyle Genova, Zeng Huang, Sean Fanello, Rohit Pandey, Thomas Funkhouser, Chen Change Loy, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03763">Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for generating photorealistic 3D human head and subsequently manipulating and reposing them with remarkable flexibility. The proposed approach leverages an implicit function representation of 3D human heads, employing 3D Gaussians anchored on a parametric face model. To enhance representational capabilities and encode spatial information, we embed a lightweight tri-plane payload within each Gaussian rather than directly storing color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space via a 3DMM, enabling effective utilization of the diffusion model for 3D head avatar generation. Our method facilitates the creation of diverse and realistic 3D human heads with fine-grained editing over facial features and expressions. Extensive experiments demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2311.16482.pdf' target='_blank'>https://arxiv.org/pdf/2311.16482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Xiang Huang, Minghan Qin, Qinwei Lin, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16482">Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural radiance fields are capable of reconstructing high-quality drivable human avatars but are expensive to train and render and not suitable for multi-human scenes with complex shadows. To reduce consumption, we propose Animatable 3D Gaussian, which learns human avatars from input images and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of skinned 3D Gaussians and a corresponding skeleton in canonical space and deforming 3D Gaussians to posed space according to the input poses. We introduce a multi-head hash encoder for pose-dependent shape and appearance and a time-dependent ambient occlusion module to achieve high-quality reconstructions in scenes containing complex motions and dynamic shadows. On both novel view synthesis and novel pose synthesis tasks, our method achieves higher reconstruction quality than InstantAvatar with less training time (1/60), less GPU memory (1/4), and faster rendering speed (7x). Our method can be easily extended to multi-human scenes and achieve comparable novel view synthesis results on a scene with ten people in only 25 seconds of training.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2311.16096.pdf' target='_blank'>https://arxiv.org/pdf/2311.16096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Yipengjing Sun, Zerong Zheng, Lizhen Wang, Shengping Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16096">Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front & back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. To tackle the realistic relighting of animatable avatars, we introduce physically-based rendering into the avatar representation for decomposing avatar materials and environment illumination. Overall, our method can create lifelike avatars with dynamic, realistic, generalized and relightable appearances. Experiments show that our method outperforms other state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2310.06275.pdf' target='_blank'>https://arxiv.org/pdf/2310.06275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghan Qin, Yifan Liu, Yuelang Xu, Xiaochen Zhao, Yebin Liu, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06275">High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One crucial aspect of 3D head avatar reconstruction lies in the details of facial expressions. Although recent NeRF-based photo-realistic 3D head avatar methods achieve high-quality avatar rendering, they still encounter challenges retaining intricate facial expression details because they overlook the potential of specific expression variations at different spatial positions when conditioning the radiance field. Motivated by this observation, we introduce a novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained by a simple MLP-based generation network, encompassing both spatial positional features and global expression information. Benefiting from rich and diverse information of the SVE at different positions, the proposed SVE-conditioned neural radiance field can deal with intricate facial expressions and achieve realistic rendering and geometry details of high-fidelity 3D head avatars. Additionally, to further elevate the geometric and rendering quality, we introduce a new coarse-to-fine training strategy, including a geometry initialization strategy at the coarse stage and an adaptive importance sampling strategy at the fine stage. Extensive experiments indicate that our method outperforms other state-of-the-art (SOTA) methods in rendering and geometry quality on mobile phone-collected and public datasets.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2309.03550.pdf' target='_blank'>https://arxiv.org/pdf/2309.03550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungwon Hwang, Junha Hyung, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03550">Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models such as ControlNet have enabled geometrically controllable, high-fidelity text-to-image generation. However, none of them addresses the question of adding such controllability to text-to-3D generation. In response, we propose Text2Control3D, a controllable text-to-3D avatar generation method whose facial expression is controllable given a monocular video casually captured with hand-held camera. Our main strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF) optimized with a set of controlled viewpoint-aware images that we generate from ControlNet, whose condition input is the depth map extracted from the input video. When generating the viewpoint-aware images, we utilize cross-reference attention to inject well-controlled, referential facial expression and appearance via cross attention. We also conduct low-pass filtering of Gaussian latent of the diffusion model in order to ameliorate the viewpoint-agnostic texture problem we observed from our empirical analysis, where the viewpoint-aware images contain identical textures on identical pixel positions that are incomprehensible in 3D. Finally, to train NeRF with the images that are viewpoint-aware yet are not strictly consistent in geometry, our approach considers per-image geometric variation as a view of deformation from a shared 3D canonical space. Consequently, we construct the 3D avatar in a canonical space of deformable NeRF by learning a set of per-image deformation via deformation field table. We demonstrate the empirical results and discuss the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2307.04859.pdf' target='_blank'>https://arxiv.org/pdf/2307.04859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander W. Bergman, Wang Yifan, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04859">Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar that is consistent with the text description and can be readily articulated using the deformation model of the 3DMM. We show that our diffusion-based articulated head avatars outperform state-of-the-art approaches for this task. The latter are typically based on CLIP, which is known to provide limited diversity of generation and accuracy for 3D object generation.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2306.12570.pdf' target='_blank'>https://arxiv.org/pdf/2306.12570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12570">Local 3D Editing via 3D Distillation of CLIP Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D content manipulation is an important computer vision task with many real-world applications (e.g., product design, cartoon generation, and 3D Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic 3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of NeRF still remains a challenging problem since the visual quality tends to degrade after manipulation and suboptimal control handles such as 2D semantic maps are used for manipulations. While text-guided manipulations have shown potential in 3D editing, such approaches often lack locality. To overcome these problems, we propose Local Editing NeRF (LENeRF), which only requires text inputs for fine-grained and localized manipulation. Specifically, we present three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field Network, and the Deformation Network, which are jointly used for local manipulations of 3D features by estimating a 3D attention field. The 3D attention field is learned in an unsupervised way, by distilling the zero-shot mask generation capability of CLIP to the 3D space with multi-view guidance. We conduct diverse experiments and thorough evaluations both quantitatively and qualitatively.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2304.03950.pdf' target='_blank'>https://arxiv.org/pdf/2304.03950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijing Wu, Yichao Yan, Yunhao Li, Yuhao Cheng, Wenhan Zhu, Ke Gao, Xiaobo Li, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03950">GANHead: Towards Generative Animatable Neural Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To bring digital avatars into people's lives, it is highly demanded to efficiently generate complete, realistic, and animatable head avatars. This task is challenging, and it is difficult for existing methods to satisfy all the requirements at once. To achieve these goals, we propose GANHead (Generative Animatable Neural Head Avatar), a novel generative head model that takes advantages of both the fine-grained control over the explicit expression parameters and the realistic rendering results of implicit representations. Specifically, GANHead represents coarse geometry, fine-gained details and texture via three networks in canonical space to obtain the ability to generate complete and realistic head avatars. To achieve flexible animation, we define the deformation filed by standard linear blend skinning (LBS), with the learned continuous pose and expression bases and LBS weights. This allows the avatars to be directly animated by FLAME parameters and generalize well to unseen poses and expressions. Compared to state-of-the-art (SOTA) methods, GANHead achieves superior performance on head avatar generation and raw scan fitting.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2304.02626.pdf' target='_blank'>https://arxiv.org/pdf/2304.02626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergey Prokudin, Qianli Ma, Maxime Raafat, Julien Valentin, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02626">Dynamic Point Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed significant progress in the field of neural surface reconstruction. While the extensive focus was put on volumetric and implicit approaches, a number of works have shown that explicit graphics primitives such as point clouds can significantly reduce computational complexity, without sacrificing the reconstructed surface quality. However, less emphasis has been put on modeling dynamic surfaces with point primitives. In this work, we present a dynamic point field model that combines the representational benefits of explicit point-based graphics with implicit deformation networks to allow efficient modeling of non-rigid 3D surfaces. Using explicit surface primitives also allows us to easily incorporate well-established constraints such as-isometric-as-possible regularisation. While learning this deformation model is prone to local optima when trained in a fully unsupervised manner, we propose to additionally leverage semantic information such as keypoint dynamics to guide the deformation learning. We demonstrate our model with an example application of creating an expressive animatable human avatar from a collection of 3D scans. Here, previous methods mostly rely on variants of the linear blend skinning paradigm, which fundamentally limits the expressivity of such models when dealing with complex cloth appearances such as long skirts. We show the advantages of our dynamic point field framework in terms of its representational power, learning efficiency, and robustness to out-of-distribution novel poses.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2304.01436.pdf' target='_blank'>https://arxiv.org/pdf/2304.01436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar, Danhang Tang, Di Qiu, Abhimitra Meka, Ruofei Du, Mingsong Dou, Sergio Orts-Escolano, Rohit Pandey, Ping Tan, Thabo Beeler, Sean Fanello, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01436">Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to learn a high-quality implicit 3D head avatar from a monocular RGB video captured in the wild. The learnt avatar is driven by a parametric face model to achieve user-controlled facial expressions and head poses. Our hybrid pipeline combines the geometry prior and dynamic tracking of a 3DMM with a neural radiance field to achieve fine-grained control and photorealism. To reduce over-smoothing and improve out-of-model expressions synthesis, we propose to predict local features anchored on the 3DMM geometry. These learnt features are driven by 3DMM deformation and interpolated in 3D space to yield the volumetric radiance at a designated query point. We further show that using a Convolutional Neural Network in the UV space is critical in incorporating spatial context and producing representative local features. Extensive experiments show that we are able to reconstruct high-quality avatars, with more accurate expression-dependent details, good generalization to out-of-training expressions, and quantitatively superior renderings compared to other state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2509.19259.pdf' target='_blank'>https://arxiv.org/pdf/2509.19259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh, Omid Taheri, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19259">Moving by Looking: Towards Vision-Driven Avatar Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The way we perceive the world fundamentally shapes how we move, whether it is how we navigate in a room or how we interact with other humans. Current human motion generation methods, neglect this interdependency and use task-specific ``perception'' that differs radically from that of humans. We argue that the generation of human-like avatar behavior requires human-like perception. Consequently, in this work we present CLOPS, the first human avatar that solely uses egocentric vision to perceive its surroundings and navigate. Using vision as the primary driver of motion however, gives rise to a significant challenge for training avatars: existing datasets have either isolated human motion, without the context of a scene, or lack scale. We overcome this challenge by decoupling the learning of low-level motion skills from learning of high-level control that maps visual input to motion. First, we train a motion prior model on a large motion capture dataset. Then, a policy is trained using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior. Our experiments empirically demonstrate that egocentric vision can give rise to human-like motion characteristics in our avatars. For example, the avatars walk such that they avoid obstacles present in their visual field. These findings suggest that equipping avatars with human-like sensors, particularly egocentric vision, holds promise for training avatars that behave like humans.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2507.10542.pdf' target='_blank'>https://arxiv.org/pdf/2507.10542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias NieÃner, Derek Bradley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10542">ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2506.22355.pdf' target='_blank'>https://arxiv.org/pdf/2506.22355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Louis-Philippe Morency, ThÃ©o Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Paden Tomasello, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22355">Embodied AI Agents: Modeling the World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2506.05397.pdf' target='_blank'>https://arxiv.org/pdf/2506.05397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jerrin Bright, Zhibo Wang, Yuhao Chen, Sirisha Rambhatla, John Zelek, David Clausi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05397">Gen4D: Synthesizing Humans and Scenes in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lack of input data for in-the-wild activities often results in low performance across various computer vision tasks. This challenge is particularly pronounced in uncommon human-centric domains like sports, where real-world data collection is complex and impractical. While synthetic datasets offer a promising alternative, existing approaches typically suffer from limited diversity in human appearance, motion, and scene composition due to their reliance on rigid asset libraries and hand-crafted rendering pipelines. To address this, we introduce Gen4D, a fully automated pipeline for generating diverse and photorealistic 4D human animations. Gen4D integrates expert-driven motion encoding, prompt-guided avatar generation using diffusion-based Gaussian splatting, and human-aware background synthesis to produce highly varied and lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale synthetic dataset spanning three sports: baseball, icehockey, and soccer. Together, Gen4D and SportPAL provide a scalable foundation for constructing synthetic datasets tailored to in-the-wild human-centric vision tasks, with no need for manual 3D modeling or scene design.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2503.13229.pdf' target='_blank'>https://arxiv.org/pdf/2503.13229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkang Cheng, Shaoli Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13229">HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual characters with holistic co-speech gestures is a challenging but critical task. Previous systems have primarily focused on the weak correlation between audio and gestures, leading to physically unnatural outcomes that degrade the user experience. To address this problem, we introduce HoleGest, a novel neural network framework based on decoupled diffusion and motion priors for the automatic generation of high-quality, expressive co-speech gestures. Our system leverages large-scale human motion datasets to learn a robust prior with low audio dependency and high motion reliance, enabling stable global motion and detailed finger movements. To improve the generation efficiency of diffusion-based models, we integrate implicit joint constraints with explicit geometric and conditional constraints, capturing complex motion distributions between large strides. This integration significantly enhances generation speed while maintaining high-quality motion. Furthermore, we design a shared embedding space for gesture-transcription text alignment, enabling the generation of semantically correct gesture actions. Extensive experiments and user feedback demonstrate the effectiveness and potential applications of our model, with our method achieving a level of realism close to the ground truth, providing an immersive user experience. Our code, model, and demo are are available at https://cyk990422.github.io/HoloGest.github.io/.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2502.02372.pdf' target='_blank'>https://arxiv.org/pdf/2502.02372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02372">MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of a virtual digital avatar is a crucial research topic in the field of computer vision. Many existing works utilize Neural Radiance Fields (NeRF) to address this issue and have achieved impressive results. However, previous works assume the images of the training person are available and fixed while the appearances and poses of a subject could constantly change and increase in real-world scenarios. How to update the human avatar but also maintain the ability to render the old appearance of the person is a practical challenge. One trivial solution is to combine the existing virtual avatar models based on NeRF with continual learning methods. However, there are some critical issues in this approach: learning new appearances and poses can cause the model to forget past information, which in turn leads to a degradation in the rendering quality of past appearances, especially color bleeding issues, and incorrect human body poses. In this work, we propose a maintainable avatar (MaintaAvatar) based on neural radiance fields by continual learning, which resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose Distillation Module. Overall, our model requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting, thus achieving a maintainable virtual avatar. The experimental results validate the effectiveness of our MaintaAvatar model.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2501.05379.pdf' target='_blank'>https://arxiv.org/pdf/2501.05379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05379">Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail. Please visit https://arc2avatar.github.io for more resources.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2412.08684.pdf' target='_blank'>https://arxiv.org/pdf/2412.08684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengze Wang, Xueting Li, Chao Liu, Matthew Chan, Michael Stengel, Henry Fuchs, Shalini De Mello, Koki Nagano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08684">Coherent3D: Coherent 3D Portrait Video Reconstruction via Triplane Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in single-image 3D portrait reconstruction have enabled telepresence systems to stream 3D portrait videos from a single camera in real-time, democratizing telepresence. However, per-frame 3D reconstruction exhibits temporal inconsistency and forgets the user's appearance. On the other hand, self-reenactment methods can render coherent 3D portraits by driving a 3D avatar built from a single reference image, but fail to faithfully preserve the user's per-frame appearance (e.g., instantaneous facial expression and lighting). As a result, none of these two frameworks is an ideal solution for democratized 3D telepresence. In this work, we address this dilemma and propose a novel solution that maintains both coherent identity and dynamic per-frame appearance to enable the best possible realism. To this end, we propose a new fusion-based method that takes the best of both worlds by fusing a canonical 3D prior from a reference view with dynamic appearance from per-frame input views, producing temporally stable 3D videos with faithful reconstruction of the user's per-frame appearance. Trained only using synthetic data produced by an expression-conditioned 3D GAN, our encoder-based method achieves both state-of-the-art 3D reconstruction and temporal consistency on in-studio and in-the-wild datasets. https://research.nvidia.com/labs/amri/projects/coherent3d
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2412.06698.pdf' target='_blank'>https://arxiv.org/pdf/2412.06698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06698">Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic 3D objects and clothed avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot guarantee the generated multi-view images are 3D consistent. In this paper, we propose Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy. We leverage a pre-trained 2D diffusion model and a 3D diffusion model via our elegantly designed process that synchronizes two diffusion models at both training and sampling time. The synergy between the 2D and 3D diffusion models brings two major advantages: 1) 2D helps 3D in generalization: the pretrained 2D model has strong generalization ability to unseen images, providing strong shape priors for the 3D diffusion model; 2) 3D helps 2D in multi-view consistency: the 3D diffusion model enhances the 3D consistency of 2D multi-view sampling process, resulting in more accurate multi-view generation. We validate our idea through extensive experiments in image-based objects and clothed avatar generation tasks. Results show that our method generates realistic 3D objects and avatars with high-fidelity geometry and texture. Extensive ablations also validate our design choices and demonstrate the strong generalization ability to diverse clothing and compositional shapes. Our code and pretrained models will be publicly released on https://yuxuan-xue.com/gen-3diffusion.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2407.08414.pdf' target='_blank'>https://arxiv.org/pdf/2407.08414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushuo Chen, Zerong Zheng, Zhe Li, Chao Xu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08414">MeshAvatar: Learning High-quality Triangular Human Avatars from Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel pipeline for learning high-quality triangular human avatars from multi-view videos. Recent methods for avatar learning are typically based on neural radiance fields (NeRF), which is not compatible with traditional graphics pipeline and poses great challenges for operations like editing or synthesizing under different environments. To overcome these limitations, our method represents the avatar with an explicit triangular mesh extracted from an implicit SDF field, complemented by an implicit material field conditioned on given poses. Leveraging this triangular avatar representation, we incorporate physics-based rendering to accurately decompose geometry and texture. To enhance both the geometric and appearance details, we further employ a 2D UNet as the network backbone and introduce pseudo normal ground-truth as additional supervision. Experiments show that our method can learn triangular avatars with high-quality geometry reconstruction and plausible material decomposition, inherently supporting editing, manipulation or relighting operations.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2407.02165.pdf' target='_blank'>https://arxiv.org/pdf/2407.02165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Huang, Shoukang Hu, Guangcong Wang, Tianqi Liu, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02165">WildAvatar: Learning In-the-wild 3D Avatars from the Web</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing research on avatar creation is typically limited to laboratory datasets, which require high costs against scalability and exhibit insufficient representation of the real world. On the other hand, the web abounds with off-the-shelf real-world human videos, but these videos vary in quality and require accurate annotations for avatar creation. To this end, we propose an automatic annotating pipeline with filtering protocols to curate these humans from the web. Our pipeline surpasses state-of-the-art methods on the EMDB benchmark, and the filtering protocols boost verification metrics on web videos. We then curate WildAvatar, a web-scale in-the-wild human avatar creation dataset extracted from YouTube, with $10000+$ different human subjects and scenes. WildAvatar is at least $10\times$ richer than previous datasets for 3D human avatar creation and closer to the real world. To explore its potential, we demonstrate the quality and generalizability of avatar creation methods on WildAvatar. We will publicly release our code, data source links and annotations to push forward 3D human avatar creation and other related fields for real-world applications.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2405.15939.pdf' target='_blank'>https://arxiv.org/pdf/2405.15939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Ting Shen, Hyungtae Lee, Heesung Kwon, Shuvra S. Bhattacharyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15939">Diversifying Human Pose in Synthetic Data for Aerial-view Human Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic data generation has emerged as a promising solution to the data scarcity issue in aerial-view human detection. However, creating datasets that accurately reflect varying real-world human appearances, particularly diverse poses, remains challenging and labor-intensive. To address this, we propose SynPoseDiv, a novel framework that diversifies human poses within existing synthetic datasets. SynPoseDiv tackles two key challenges: generating realistic, diverse 3D human poses using a diffusion-based pose generator, and producing images of virtual characters in novel poses through a source-to-target image translator. The framework incrementally transitions characters into new poses using optimized pose sequences identified via Dijkstra's algorithm. Experiments demonstrate that SynPoseDiv significantly improves detection accuracy across multiple aerial-view human detection benchmarks, especially in low-shot scenarios, and remains effective regardless of the training approach or dataset size.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2405.12069.pdf' target='_blank'>https://arxiv.org/pdf/2405.12069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Oztireli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12069">Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2404.02686.pdf' target='_blank'>https://arxiv.org/pdf/2404.02686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiali Zheng, Rolandos Alexandros Potamias, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02686">Design2Cloth: 3D Cloth Generation from 2D Masks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a significant shift in the field of digital avatar research, towards modeling, animating and reconstructing clothed human representations, as a key step towards creating realistic avatars. However, current 3D cloth generation methods are garment specific or trained completely on synthetic data, hence lacking fine details and realism. In this work, we make a step towards automatic realistic garment design and propose Design2Cloth, a high fidelity 3D generative model trained on a real world dataset from more than 2000 subject scans. To provide vital contribution to the fashion industry, we developed a user-friendly adversarial model capable of generating diverse and detailed clothes simply by drawing a 2D cloth mask. Under a series of both qualitative and quantitative experiments, we showcase that Design2Cloth outperforms current state-of-the-art cloth generative models by a large margin. In addition to the generative properties of our network, we showcase that the proposed method can be used to achieve high quality reconstructions from single in-the-wild images and 3D scans. Dataset, code and pre-trained model will become publicly available.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2403.19773.pdf' target='_blank'>https://arxiv.org/pdf/2403.19773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rolandos Alexandros Potamias, Michail Tarasiou, Stylianos Ploumpis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19773">ShapeFusion: A 3D diffusion model for localized shape editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of 3D computer vision, parametric models have emerged as a ground-breaking methodology for the creation of realistic and expressive 3D avatars. Traditionally, they rely on Principal Component Analysis (PCA), given its ability to decompose data to an orthonormal space that maximally captures shape variations. However, due to the orthogonality constraints and the global nature of PCA's decomposition, these models struggle to perform localized and disentangled editing of 3D shapes, which severely affects their use in applications requiring fine control such as face sculpting. In this paper, we leverage diffusion models to enable diverse and fully localized edits on 3D meshes, while completely preserving the un-edited regions. We propose an effective diffusion masking training strategy that, by design, facilitates localized manipulation of any shape region, without being limited to predefined regions or to sparse sets of predefined control vertices. Following our framework, a user can explicitly set their manipulation region of choice and define an arbitrary set of vertices as handles to edit a 3D mesh. Compared to the current state-of-the-art our method leads to more interpretable shape manipulations than methods relying on latent code state, greater localization and generation diversity while offering faster inference than optimization based approaches. Project page: https://rolpotamias.github.io/Shapefusion/
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2403.17213.pdf' target='_blank'>https://arxiv.org/pdf/2403.17213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17213">AnimateMe: 4D Facial Expressions via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of photorealistic 3D avatar reconstruction and generation has garnered significant attention in recent years; however, animating such avatars remains challenging. Recent advances in diffusion models have notably enhanced the capabilities of generative models in 2D animation. In this work, we directly utilize these models within the 3D domain to achieve controllable and high-fidelity 4D facial animation. By integrating the strengths of diffusion processes and geometric deep learning, we employ Graph Neural Networks (GNNs) as denoising diffusion models in a novel approach, formulating the diffusion process directly on the mesh space and enabling the generation of 3D facial expressions. This facilitates the generation of facial deformations through a mesh-diffusion-based model. Additionally, to ensure temporal coherence in our animations, we propose a consistent noise sampling method. Under a series of both quantitative and qualitative experiments, we showcase that the proposed method outperforms prior work in 4D expression synthesis by generating high-fidelity extreme expressions. Furthermore, we applied our method to textured 4D facial expression generation, implementing a straightforward extension that involves training on a large-scale textured 4D facial expression database.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2402.05803.pdf' target='_blank'>https://arxiv.org/pdf/2402.05803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wamiq Reyaz Para, Abdelrahman Eldesokey, Zhenyu Li, Pradyumna Reddy, Jiankang Deng, Peter Wonka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05803">AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an approach for 3D head avatar generation and editing with multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and a Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatars given a single or no condition. However, it is challenging to generate samples that adhere to multiple conditions of different modalities. On the other hand, LDMs excel at learning complex conditional distributions. To this end, we propose to exploit the conditioning capabilities of LDMs to enable multi-modal control over the latent space of a pre-trained 3D GAN. Our method can generate and edit 3D head avatars given a mixture of control signals such as RGB input, segmentation masks, and global attributes. This provides better control over the generation and editing of synthetic avatars both globally and locally. Experiments show that our proposed approach outperforms a solely GAN-based approach both qualitatively and quantitatively on generation and editing tasks. To the best of our knowledge, our approach is the first to introduce multi-modal conditioning to 3D avatar generation and editing. \\href{avatarmmc-sig24.github.io}{Project Page}
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2401.03476.pdf' target='_blank'>https://arxiv.org/pdf/2401.03476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicheng Yang, Zunnan Xu, Haiwei Xue, Yongkang Cheng, Shaoli Huang, Mingming Gong, Zhiyong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03476">Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current talking avatars mostly generate co-speech gestures based on audio and text of the utterance, without considering the non-speaking motion of the speaker. Furthermore, previous works on co-speech gesture generation have designed network structures based on individual gesture datasets, which results in limited data volume, compromised generalizability, and restricted speaker movements. To tackle these issues, we introduce FreeTalker, which, to the best of our knowledge, is the first framework for the generation of both spontaneous (e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium) speaker motions. Specifically, we train a diffusion-based model for speaker motion generation that employs unified representations of both speech-driven gestures and text-driven motions, utilizing heterogeneous data sourced from various motion datasets. During inference, we utilize classifier-free guidance to highly control the style in the clips. Additionally, to create smooth transitions between clips, we utilize DoubleTake, a method that leverages a generative prior and ensures seamless motion blending. Extensive experiments show that our method generates natural and controllable speaker movements. Our code, model, and demo are are available at \url{https://youngseng.github.io/FreeTalker/}.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2312.09228.pdf' target='_blank'>https://arxiv.org/pdf/2312.09228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09228">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2312.03420.pdf' target='_blank'>https://arxiv.org/pdf/2312.03420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingyan Xu, Prashanth Chandran, Sebastian Weiss, Markus Gross, Gaspard Zoss, Derek Bradley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03420">Artist-Friendly Relightable and Animatable Neural Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An increasingly common approach for creating photo-realistic digital avatars is through the use of volumetric neural fields. The original neural radiance field (NeRF) allowed for impressive novel view synthesis of static heads when trained on a set of multi-view images, and follow up methods showed that these neural representations can be extended to dynamic avatars. Recently, new variants also surpassed the usual drawback of baked-in illumination in neural representations, showing that static neural avatars can be relit in any environment. In this work we simultaneously tackle both the motion and illumination problem, proposing a new method for relightable and animatable neural heads. Our method builds on a proven dynamic avatar approach based on a mixture of volumetric primitives, combined with a recently-proposed lightweight hardware setup for relightable neural fields, and includes a novel architecture that allows relighting dynamic neural avatars performing unseen expressions in any environment, even with nearfield illumination and viewpoints.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2312.02134.pdf' target='_blank'>https://arxiv.org/pdf/2312.02134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02134">GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2310.03952.pdf' target='_blank'>https://arxiv.org/pdf/2310.03952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiali Zheng, Youngkyoon Jang, Athanasios Papaioannou, Christos Kampouris, Rolandos Alexandros Potamias, Foivos Paraperas Papantoniou, Efstathios Galanakis, Ales Leonardis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03952">ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the Imperial Light-Stage Head (ILSH) dataset, a novel light-stage-captured human head dataset designed to support view synthesis academic challenges for human heads. The ILSH dataset is intended to facilitate diverse approaches, such as scene-specific or generic neural rendering, multiple-view geometry, 3D vision, and computer graphics, to further advance the development of photo-realistic human avatars. This paper details the setup of a light-stage specifically designed to capture high-resolution (4K) human head images and describes the process of addressing challenges (preprocessing, ethical issues) in collecting high-quality data. In addition to the data collection, we address the split of the dataset into train, validation, and test sets. Our goal is to design and support a fair view synthesis challenge task for this novel dataset, such that a similar level of performance can be maintained and expected when using the test set, as when using the validation set. The ILSH dataset consists of 52 subjects captured using 24 cameras with all 82 lighting sources turned on, resulting in a total of 1,248 close-up head images, border masks, and camera pose pairs.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2308.14847.pdf' target='_blank'>https://arxiv.org/pdf/2308.14847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xue, Bharat Lal Bhatnagar, Riccardo Marin, Nikolaos Sarafianos, Yuanlu Xu, Gerard Pons-Moll, Tony Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14847">NSF: Neural Surface Fields for Human Modeling from Monocular Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obtaining personalized 3D animatable avatars from a monocular camera has several real world applications in gaming, virtual try-on, animation, and VR/XR, etc. However, it is very challenging to model dynamic and fine-grained clothing deformations from such sparse data. Existing methods for modeling 3D humans from depth data have limitations in terms of computational efficiency, mesh coherency, and flexibility in resolution and topology. For instance, reconstructing shapes using implicit functions and extracting explicit meshes per frame is computationally expensive and cannot ensure coherent meshes across frames. Moreover, predicting per-vertex deformations on a pre-designed human template with a discrete surface lacks flexibility in resolution and topology. To overcome these limitations, we propose a novel method Neural Surface Fields for modeling 3D clothed humans from monocular depth. NSF defines a neural field solely on the base surface which models a continuous and flexible displacement field. NSF can be adapted to the base surface with different resolution and topology without retraining at inference time. Compared to existing approaches, our method eliminates the expensive per-frame surface extraction while maintaining mesh coherency, and is capable of reconstructing meshes with arbitrary resolution without retraining. To foster research in this direction, we release our code in project page at: https://yuxuan-xue.com/nsf.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2305.03713.pdf' target='_blank'>https://arxiv.org/pdf/2305.03713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekta Prashnani, Koki Nagano, Shalini De Mello, David Luebke, Orazio Gallo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03713">Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern avatar generators allow anyone to synthesize photorealistic real-time talking avatars, ushering in a new era of avatar-based human communication, such as with immersive AR/VR interactions or videoconferencing with limited bandwidths. Their safe adoption, however, requires a mechanism to verify if the rendered avatar is trustworthy: does it use the appearance of an individual without their consent? We term this task avatar fingerprinting. To tackle it, we first introduce a large-scale dataset of real and synthetic videos of people interacting on a video call, where the synthetic videos are generated using the facial appearance of one person and the expressions of another. We verify the identity driving the expressions in a synthetic video, by learning motion signatures that are independent of the facial appearance shown. Our solution, the first in this space, achieves an average AUC of 0.85. Critical to its practical use, it also generalizes to new generators never seen in training (average AUC of 0.83). The proposed dataset and other resources can be found at: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2509.09595.pdf' target='_blank'>https://arxiv.org/pdf/2509.09595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikang Ding, Jiwen Liu, Wenyuan Zhang, Zekun Wang, Wentao Hu, Liyuan Cui, Mingming Lao, Yingchao Shao, Hui Liu, Xiaohan Li, Ming Chen, Xiaoqiang Liu, Yu-Shen Liu, Pengfei Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09595">Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2508.16401.pdf' target='_blank'>https://arxiv.org/pdf/2508.16401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NVIDIA, :, Chaeyeon Chung, Ilya Fedorov, Michael Huang, Aleksey Karmanov, Dmitry Korobchenko, Roger Ribera, Yeongho Seol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16401">Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven facial animation presents an effective solution for animating digital avatars. In this paper, we detail the technical aspects of NVIDIA Audio2Face-3D, including data acquisition, network architecture, retargeting methodology, evaluation metrics, and use cases. Audio2Face-3D system enables real-time interaction between human users and interactive avatars, facilitating facial animation authoring for game characters. To assist digital avatar creators and game developers in generating realistic facial animations, we have open-sourced Audio2Face-3D networks, SDK, training framework, and example dataset.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2508.09597.pdf' target='_blank'>https://arxiv.org/pdf/2508.09597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heyi Sun, Cong Wang, Tian-Xing Xu, Jingwei Huang, Di Kang, Chunchao Guo, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09597">SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity and editable head avatars is a pivotal challenge in computer vision and graphics, boosting many AR/VR applications. While recent advancements have achieved photorealistic renderings and plausible animation, head editing, especially real-time appearance editing, remains challenging due to the implicit representation and entangled modeling of the geometry and global appearance. To address this, we propose Surface-Volumetric Gaussian Head Avatar (SVG-Head), a novel hybrid representation that explicitly models the geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled texture images to capture the global appearance. Technically, it contains two types of Gaussians, in which surface Gaussians explicitly model the appearance of head avatars using learnable texture images, facilitating real-time texture editing, while volumetric Gaussians enhance the reconstruction quality of non-Lambertian regions (e.g., lips and hair). To model the correspondence between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping method, which leverages UV coordinates given by the FLAME mesh to obtain sharp texture images and real-time rendering speed. A hierarchical optimization strategy is further designed to pursue the optimal performance in both reconstruction quality and editing flexibility. Experiments on the NeRSemble dataset show that SVG-Head not only generates high-fidelity rendering results, but also is the first method to obtain explicit texture images for Gaussian head avatars and support real-time appearance editing.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2507.19481.pdf' target='_blank'>https://arxiv.org/pdf/2507.19481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byungjun Kim, Shunsuke Saito, Giljoo Nam, Tomas Simon, Jason Saragih, Hanbyul Joo, Junxuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19481">HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2507.15979.pdf' target='_blank'>https://arxiv.org/pdf/2507.15979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel C. BÃ¼hler, Ye Yuan, Xueting Li, Yangyi Huang, Koki Nagano, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15979">Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2507.05191.pdf' target='_blank'>https://arxiv.org/pdf/2507.05191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Doug Roble, Tuur Stuyck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05191">Neuralocks: Real-Time Dynamic Neural Hair Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time hair simulation is a vital component in creating believable virtual avatars, as it provides a sense of immersion and authenticity. The dynamic behavior of hair, such as bouncing or swaying in response to character movements like jumping or walking, plays a significant role in enhancing the overall realism and engagement of virtual experiences. Current methods for simulating hair have been constrained by two primary approaches: highly optimized physics-based systems and neural methods. However, state-of-the-art neural techniques have been limited to quasi-static solutions, failing to capture the dynamic behavior of hair. This paper introduces a novel neural method that breaks through these limitations, achieving efficient and stable dynamic hair simulation while outperforming existing approaches. We propose a fully self-supervised method which can be trained without any manual intervention or artist generated training data allowing the method to be integrated with hair reconstruction methods to enable automatic end-to-end methods for avatar reconstruction. Our approach harnesses the power of compact, memory-efficient neural networks to simulate hair at the strand level, allowing for the simulation of diverse hairstyles without excessive computational resources or memory requirements. We validate the effectiveness of our method through a variety of hairstyle examples, showcasing its potential for real-world applications.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2504.13378.pdf' target='_blank'>https://arxiv.org/pdf/2504.13378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxiao Tu, Shuchang Ye, Hoijoon Jung, Jinman Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13378">SMPL-GPTexture: Dual-View 3D Human Texture Estimation using Text-to-Image Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-quality, photorealistic textures for 3D human avatars remains a fundamental yet challenging task in computer vision and multimedia field. However, real paired front and back images of human subjects are rarely available with privacy, ethical and cost of acquisition, which restricts scalability of the data. Additionally, learning priors from image inputs using deep generative models, such as GANs or diffusion models, to infer unseen regions such as the human back often leads to artifacts, structural inconsistencies, or loss of fine-grained detail. To address these issues, we present SMPL-GPTexture (skinned multi-person linear model - general purpose Texture), a novel pipeline that takes natural language prompts as input and leverages a state-of-the-art text-to-image generation model to produce paired high-resolution front and back images of a human subject as the starting point for texture estimation. Using the generated paired dual-view images, we first employ a human mesh recovery model to obtain a robust 2D-to-3D SMPL alignment between image pixels and the 3D model's UV coordinates for each views. Second, we use an inverted rasterization technique that explicitly projects the observed colour from the input images into the UV space, thereby producing accurate, complete texture maps. Finally, we apply a diffusion-based inpainting module to fill in the missing regions, and the fusion mechanism then combines these results into a unified full texture map. Extensive experiments shows that our SMPL-GPTexture can generate high resolution texture aligned with user's prompts.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2504.05265.pdf' target='_blank'>https://arxiv.org/pdf/2504.05265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>German Barquero, Nadine Bertsch, Manojkumar Marramreddy, Carlos ChacÃ³n, Filippo Arcadu, Ferran Rigual, Nicky Sijia He, Cristina Palmero, Sergio Escalera, Yuting Ye, Robin Kips
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05265">From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In extended reality (XR), generating full-body motion of the users is important to understand their actions, drive their virtual avatars for social interaction, and convey a realistic sense of presence. While prior works focused on spatially sparse and always-on input signals from motion controllers, many XR applications opt for vision-based hand tracking for reduced user friction and better immersion. Compared to controllers, hand tracking signals are less accurate and can even be missing for an extended period of time. To handle such unreliable inputs, we present Rolling Prediction Model (RPM), an online and real-time approach that generates smooth full-body motion from temporally and spatially sparse input signals. Our model generates 1) accurate motion that matches the inputs (i.e., tracking mode) and 2) plausible motion when inputs are missing (i.e., synthesis mode). More importantly, RPM generates seamless transitions from tracking to synthesis, and vice versa. To demonstrate the practical importance of handling noisy and missing inputs, we present GORP, the first dataset of realistic sparse inputs from a commercial virtual reality (VR) headset with paired high quality body motion ground truth. GORP provides >14 hours of VR gameplay data from 28 people using motion controllers (spatially sparse) and hand tracking (spatially and temporally sparse). We benchmark RPM against the state of the art on both synthetic data and GORP to highlight how we can bridge the gap for real-world applications with a realistic dataset and by handling unreliable input signals. Our code, pretrained models, and GORP dataset are available in the project webpage.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2503.00495.pdf' target='_blank'>https://arxiv.org/pdf/2503.00495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanchen Li, Jianyu Wang, Yuhao Cheng, Yikun Zeng, Xingyu Ren, Wenhan Zhu, Weiming Zhao, Yichao Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00495">Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant progress has been made for speech-driven 3D face animation, but most works focus on learning the motion of mesh/geometry, ignoring the impact of dynamic texture. In this work, we reveal that dynamic texture plays a key role in rendering high-fidelity talking avatars, and introduce a high-resolution 4D dataset \textbf{TexTalk4D}, consisting of 100 minutes of audio-synced scan-level meshes with detailed 8K dynamic textures from 100 subjects. Based on the dataset, we explore the inherent correlation between motion and texture, and propose a diffusion-based framework \textbf{TexTalker} to simultaneously generate facial motions and dynamic textures from speech. Furthermore, we propose a novel pivot-based style injection strategy to capture the complicity of different texture and motion styles, which allows disentangled control. TexTalker, as the first method to generate audio-synced facial motion with dynamic texture, not only outperforms the prior arts in synthesising facial motions, but also produces realistic textures that are consistent with the underlying facial movements. Project page: https://xuanchenli.github.io/TexTalk/.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2502.20323.pdf' target='_blank'>https://arxiv.org/pdf/2502.20323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuangeng Chu, Nabarun Goswami, Ziteng Cui, Hanqin Wang, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20323">ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D facial animation aims to generate realistic lip movements and facial expressions for 3D head models from arbitrary audio clips. Although existing diffusion-based methods are capable of producing natural motions, their slow generation speed limits their application potential. In this paper, we introduce a novel autoregressive model that achieves real-time generation of highly synchronized lip movements and realistic head poses and eye blinks by learning a mapping from speech to a multi-scale motion codebook. Furthermore, our model can adapt to unseen speaking styles, enabling the creation of 3D talking avatars with unique personal styles beyond the identities seen during training. Extensive evaluations and user studies demonstrate that our method outperforms existing approaches in lip synchronization accuracy and perceived quality.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2501.16557.pdf' target='_blank'>https://arxiv.org/pdf/2501.16557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Shi, Rahul Jain, Seungguen Chi, Hyungjun Doh, Hyunggun Chi, Alexander J. Quinn, Karthik Ramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16557">CARING-AI: Towards Authoring Context-aware Augmented Reality INstruction through Generative Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context-aware AR instruction enables adaptive and in-situ learning experiences. However, hardware limitations and expertise requirements constrain the creation of such instructions. With recent developments in Generative Artificial Intelligence (Gen-AI), current research tries to tackle these constraints by deploying AI-generated content (AIGC) in AR applications. However, our preliminary study with six AR practitioners revealed that the current AIGC lacks contextual information to adapt to varying application scenarios and is therefore limited in authoring. To utilize the strong generative power of GenAI to ease the authoring of AR instruction while capturing the context, we developed CARING-AI, an AR system to author context-aware humanoid-avatar-based instructions with GenAI. By navigating in the environment, users naturally provide contextual information to generate humanoid-avatar animation as AR instructions that blend in the context spatially and temporally. We showcased three application scenarios of CARING-AI: Asynchronous Instructions, Remote Instructions, and Ad Hoc Instructions based on a design space of AIGC in AR Instructions. With two user studies (N=12), we assessed the system usability of CARING-AI and demonstrated the easiness and effectiveness of authoring with Gen-AI.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2412.15171.pdf' target='_blank'>https://arxiv.org/pdf/2412.15171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Forrest Iandola, Stanislav Pidhorskyi, Igor Santesteban, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15171">SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian-based human avatars have achieved an unprecedented level of visual fidelity. However, existing approaches based on high-capacity neural networks typically require a desktop GPU to achieve real-time performance for a single avatar, and it remains non-trivial to animate and render such avatars on mobile devices including a standalone VR headset due to substantially limited memory and computational bandwidth. In this paper, we present SqueezeMe, a simple and highly effective framework to convert high-fidelity 3D Gaussian full-body avatars into a lightweight representation that supports both animation and rendering with mobile-grade compute. Our key observation is that the decoding of pose-dependent Gaussian attributes from a neural network creates non-negligible memory and computational overhead. Inspired by blendshapes and linear pose correctives widely used in Computer Graphics, we address this by distilling the pose correctives learned with neural networks into linear layers. Moreover, we further reduce the parameters by sharing the correctives among nearby Gaussians. Combining them with a custom splatting pipeline based on Vulkan, we achieve, for the first time, simultaneous animation and rendering of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset. Demo videos are available at https://forresti.github.io/squeezeme.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2412.13265.pdf' target='_blank'>https://arxiv.org/pdf/2412.13265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Ma, Ju Zhang, Lev Tankelevitch, Payod Panda, Torang Asadi, Charlie Hewitt, Lohit Petikam, James Clemoes, Marco Gillies, Xueni Pan, Sean Rintel, Marta Wilczkowiak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13265">Nods of Agreement: Webcam-Driven Avatars Improve Meeting Outcomes and Avatar Satisfaction Over Audio-Driven or Static Avatars in All-Avatar Work Videoconferencing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars are edging into mainstream videoconferencing, but evaluation of how avatar animation modalities contribute to work meeting outcomes has been limited. We report a within-group videoconferencing experiment in which 68 employees of a global technology company, in 16 groups, used the same stylized avatars in three modalities (static picture, audio-animation, and webcam-animation) to complete collaborative decision-making tasks. Quantitatively, for meeting outcomes, webcam-animated avatars improved meeting effectiveness over the picture modality and were also reported to be more comfortable and inclusive than both other modalities. In terms of avatar satisfaction, there was a similar preference for webcam animation as compared to both other modalities. Our qualitative analysis shows participants expressing a preference for the holistic motion of webcam animation, and that meaningful movement outweighs realism for meeting outcomes, as evidenced through a systematic overview of ten thematic factors. We discuss implications for research and commercial deployment and conclude that webcam-animated avatars are a plausible alternative to video in work meetings.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2412.10487.pdf' target='_blank'>https://arxiv.org/pdf/2412.10487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonello Ceravola, Frank Joublin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10487">HyperGraphOS: A Modern Meta-Operating System for the Scientific and Engineering Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents HyperGraphOS, a significant innovation in the domain of operating systems, specifically designed to address the needs of scientific and engineering domains. This platform aims to combine model-based engineering, graph modeling, data containers, and documents, along with tools for handling computational elements. HyperGraphOS functions as an Operating System offering to users an infinite workspace for creating and managing complex models represented as graphs with customizable semantics. By leveraging a web-based architecture, it requires only a modern web browser for access, allowing organization of knowledge, documents, and content into models represented in a network of workspaces. Elements of the workspace are defined in terms of domain-specific languages (DSLs). These DSLs are pivotal for navigating workspaces, generating code, triggering AI components, and organizing information and processes. The models' dual nature as both visual drawings and data structures allows dynamic modifications and inspections both interactively as well as programaticaly. We evaluated HyperGraphOS's efficiency and applicability across a large set of diverse domains, including the design and development of a virtual Avatar dialog system, a robotic task planner based on large language models (LLMs), a new meta-model for feature-based code development and many others. Our findings show that HyperGraphOS offers substantial benefits in the interaction with a computer as information system, as platoform for experiments and data analysis, as streamlined engineering processes, demonstrating enhanced flexibility in managing data, computation and documents, showing an innovative approaches to persistent desktop environments.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2412.04923.pdf' target='_blank'>https://arxiv.org/pdf/2412.04923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonello Ceravola, Frank Joublin, Ahmed R. Sadik, Bram Bolder, Juha-Pekka Tolvanen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04923">HyperGraphOS: A Meta Operating System for Science and Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents HyperGraphOS, an innovative Operating System designed for the scientific and engineering domains. It combines model based engineering, graph modeling, data containers, and computational tools, offering users a dynamic workspace for creating and managing complex models represented as customizable graphs. Using a web based architecture, HyperGraphOS requires only a modern browser to organize knowledge, documents, and content into interconnected models. Domain Specific Languages drive workspace navigation, code generation, AI integration, and process organization.The platform models function as both visual drawings and data structures, enabling dynamic modifications and inspection, both interactively and programmatically. HyperGraphOS was evaluated across various domains, including virtual avatars, robotic task planning using Large Language Models, and meta modeling for feature based code development. Results show significant improvements in flexibility, data management, computation, and document handling.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2410.24223.pdf' target='_blank'>https://arxiv.org/pdf/2410.24223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24223">URAvatar: Universal Relightable Gaussian Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new approach to creating photorealistic and relightable head avatars from a phone scan with unknown illumination. The reconstructed avatars can be animated and relit in real time with the global illumination of diverse environments. Unlike existing approaches that estimate parametric reflectance parameters via inverse rendering, our approach directly models learnable radiance transfer that incorporates global light transport in an efficient manner for real-time rendering. However, learning such a complex light transport that can generalize across identities is non-trivial. A phone scan in a single environment lacks sufficient information to infer how the head would appear in general environments. To address this, we build a universal relightable avatar model represented by 3D Gaussians. We train on hundreds of high-quality multi-view human scans with controllable point lights. High-resolution geometric guidance further enhances the reconstruction accuracy and generalization. Once trained, we finetune the pretrained model on a phone scan using inverse rendering to obtain a personalized relightable avatar. Our experiments establish the efficacy of our design, outperforming existing approaches while retaining real-time rendering capability.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2410.00441.pdf' target='_blank'>https://arxiv.org/pdf/2410.00441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyang Luo, Jenanan Vairavamurthy, Xiaoman Zhang, Abhinav Kumar, Ramon R. Ter-Oganesyan, Stuart T. Schroff, Dan Shilo, Rydhwana Hossain, Mike Moritz, Pranav Rajpurkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00441">ReXplain: Translating Radiology into Patient-Friendly Video Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiology reports, designed for efficient communication between medical experts, often remain incomprehensible to patients. This inaccessibility could potentially lead to anxiety, decreased engagement in treatment decisions, and poorer health outcomes, undermining patient-centered care. We present ReXplain (Radiology eXplanation), an innovative AI-driven system that translates radiology findings into patient-friendly video reports. ReXplain uniquely integrates a large language model for medical text simplification and text-anatomy association, an image segmentation model for anatomical region identification, and an avatar generation tool for engaging interface visualization. ReXplain enables producing comprehensive explanations with plain language, highlighted imagery, and 3D organ renderings in the form of video reports. To evaluate the utility of ReXplain-generated explanations, we conducted two rounds of user feedback collection from six board-certified radiologists. The results of this proof-of-concept study indicate that ReXplain could accurately deliver radiological information and effectively simulate one-on-one consultation, shedding light on enhancing patient-centered radiology with potential clinical usage. This work demonstrates a new paradigm in AI-assisted medical communication, potentially improving patient engagement and satisfaction in radiology care, and opens new avenues for research in multimodal medical communication.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2406.15177.pdf' target='_blank'>https://arxiv.org/pdf/2406.15177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu, Erik Cambria
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15177">EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2406.12035.pdf' target='_blank'>https://arxiv.org/pdf/2406.12035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rhythm Arora, Pooja Prajod, Matteo Lavit Nicora, Daniele Panzeri, Giovanni Tauro, Rocco Vertechy, Matteo Malosio, Elisabeth AndrÃ©, Patrick Gebhard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12035">Socially Interactive Agents for Robotic Neurorehabilitation Training: Conceptualization and Proof-of-concept Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Individuals with diverse motor abilities often benefit from intensive and specialized rehabilitation therapies aimed at enhancing their functional recovery. Nevertheless, the challenge lies in the restricted availability of neurorehabilitation professionals, hindering the effective delivery of the necessary level of care. Robotic devices hold great potential in reducing the dependence on medical personnel during therapy but, at the same time, they generally lack the crucial human interaction and motivation that traditional in-person sessions provide. To bridge this gap, we introduce an AI-based system aimed at delivering personalized, out-of-hospital assistance during neurorehabilitation training. This system includes a rehabilitation training device, affective signal classification models, training exercises, and a socially interactive agent as the user interface. With the assistance of a professional, the envisioned system is designed to be tailored to accommodate the unique rehabilitation requirements of an individual patient. Conceptually, after a preliminary setup and instruction phase, the patient is equipped to continue their rehabilitation regimen autonomously in the comfort of their home, facilitated by a socially interactive agent functioning as a virtual coaching assistant. Our approach involves the integration of an interactive socially-aware virtual agent into a neurorehabilitation robotic framework, with the primary objective of recreating the social aspects inherent to in-person rehabilitation sessions. We also conducted a feasibility study to test the framework with healthy patients. The results of our preliminary investigation indicate that participants demonstrated a propensity to adapt to the system. Notably, the presence of the interactive agent during the proposed exercises did not act as a source of distraction; instead, it positively impacted users' engagement.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2406.09839.pdf' target='_blank'>https://arxiv.org/pdf/2406.09839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Yeza Baihaqi, Angel GarcÃ­a Contreras, Seiya Kawano, Koichiro Yoshino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09839">Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapport is known as a conversational aspect focusing on relationship building, which influences outcomes in collaborative tasks. This study aims to establish human-agent rapport through small talk by using a rapport-building strategy. We implemented this strategy for the virtual agents based on dialogue strategies by prompting a large language model (LLM). In particular, we utilized two dialogue strategies-predefined sequence and free-form-to guide the dialogue generation framework. We conducted analyses based on human evaluations, examining correlations between total turn, utterance characters, rapport score, and user experience variables: naturalness, satisfaction, interest, engagement, and usability. We investigated correlations between rapport score and naturalness, satisfaction, engagement, and conversation flow. Our experimental results also indicated that using free-form to prompt the rapport-building strategy performed the best in subjective scores.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2406.06297.pdf' target='_blank'>https://arxiv.org/pdf/2406.06297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Grotta, Marco Coraggio, Antonio Spallone, Francesco De Lellis, Mario di Bernardo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06297">Learning-based cognitive architecture for enhancing coordination in human groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As interactions with autonomous agents-ranging from robots in physical settings to avatars in virtual and augmented realities-become more prevalent, developing advanced cognitive architectures is critical for enhancing the dynamics of human-avatar groups. This paper presents a reinforcement-learning-based cognitive architecture, trained via a sim-to-real approach, designed to improve synchronization in periodic motor tasks, crucial for applications in group rehabilitation and sports training. Extensive numerical validation consistently demonstrates improvements in synchronization. Theoretical derivations and numerical investigations are complemented by preliminary experiments with real participants, showing that our avatars can integrate seamlessly into human groups, often being indistinguishable from humans.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2405.19331.pdf' target='_blank'>https://arxiv.org/pdf/2405.19331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Giebenhain, Tobias Kirschstein, Martin RÃ¼nz, Lourdes Agapito, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19331">NPGA: Neural Parametric Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. For increased representational capacity of our avatars, we propose per-Gaussian latent features that condition each primitives dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2403.11453.pdf' target='_blank'>https://arxiv.org/pdf/2403.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongrui Cai, Yuting Xiao, Xuan Wang, Jiafei Li, Yudong Guo, Yanbo Fan, Shenghua Gao, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11453">Hybrid Explicit Representation for Ultra-Realistic Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel approach to creating ultra-realistic head avatars and rendering them in real-time (>30fps at $2048 \times 1334$ resolution). First, we propose a hybrid explicit representation that combines the advantages of two primitive-based efficient rendering techniques. UV-mapped 3D mesh is utilized to capture sharp and rich textures on smooth surfaces, while 3D Gaussian Splatting is employed to represent complex geometric structures. In the pipeline of modeling an avatar, after tracking parametric models based on captured multi-view RGB videos, our goal is to simultaneously optimize the texture and opacity map of mesh, as well as a set of 3D Gaussian splats localized and rigged onto the mesh facets. Specifically, we perform $Î±$-blending on the color and opacity values based on the merged and re-ordered z-buffer from the rasterization results of mesh and 3DGS. This process involves the mesh and 3DGS adaptively fitting the captured visual information to outline a high-fidelity digital avatar. To avoid artifacts caused by Gaussian splats crossing the mesh facets, we design a stable hybrid depth sorting strategy. Experiments illustrate that our modeled results exceed those of state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2401.15348.pdf' target='_blank'>https://arxiv.org/pdf/2401.15348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beijia Chen, Yuefan Shen, Qing Shuai, Xiaowei Zhou, Kun Zhou, Youyi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15348">AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent communities have seen significant progress in building photo-realistic animatable avatars from sparse multi-view videos. However, current workflows struggle to render realistic garment dynamics for loose-fitting characters as they predominantly rely on naked body models for human modeling while leaving the garment part un-modeled. This is mainly due to that the deformations yielded by loose garments are highly non-rigid, and capturing such deformations often requires dense views as supervision. In this paper, we introduce AniDress, a novel method for generating animatable human avatars in loose clothes using very sparse multi-view videos (4-8 in our setting). To allow the capturing and appearance learning of loose garments in such a situation, we employ a virtual bone-based garment rigging model obtained from physics-based simulation data. Such a model allows us to capture and render complex garment dynamics through a set of low-dimensional bone transformations. Technically, we develop a novel method for estimating temporal coherent garment dynamics from a sparse multi-view video. To build a realistic rendering for unseen garment status using coarse estimations, a pose-driven deformable neural radiance field conditioned on both body and garment motions is introduced, providing explicit control of both parts. At test time, the new garment poses can be captured from unseen situations, derived from a physics-based or neural network-based simulator to drive unseen garment dynamics. To evaluate our approach, we create a multi-view dataset that captures loose-dressed performers with diverse motions. Experiments show that our method is able to render natural garment dynamics that deviate highly from the body and generalize well to both unseen views and poses, surpassing the performance of existing methods. The code and data will be publicly available.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2312.03704.pdf' target='_blank'>https://arxiv.org/pdf/2312.03704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03704">Relightable Gaussian Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2312.02069.pdf' target='_blank'>https://arxiv.org/pdf/2312.02069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02069">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2311.18635.pdf' target='_blank'>https://arxiv.org/pdf/2311.18635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Kirschstein, Simon Giebenhain, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18635">DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DiffusionAvatars synthesizes a high-fidelity 3D head avatar of a person, offering intuitive control over both pose and expression. We propose a diffusion-based neural renderer that leverages generic 2D priors to produce compelling images of faces. For coarse guidance of the expression and head pose, we render a neural parametric head model (NPHM) from the target viewpoint, which acts as a proxy geometry of the person. Additionally, to enhance the modeling of intricate facial expressions, we condition DiffusionAvatars directly on the expression codes obtained from NPHM via cross-attention. Finally, to synthesize consistent surface details across different viewpoints and expressions, we rig learnable spatial features to the head's surface via TriPlane lookup in NPHM's canonical space. We train DiffusionAvatars on RGB videos and corresponding fitted NPHM meshes of a person and test the obtained avatars in both self-reenactment and animation scenarios. Our experiments demonstrate that DiffusionAvatars generates temporally consistent and visually appealing videos for novel poses and expressions of a person, outperforming existing approaches.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2311.13655.pdf' target='_blank'>https://arxiv.org/pdf/2311.13655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Berna Kabadayi, Wojciech Zielonka, Bharat Lal Bhatnagar, Gerard Pons-Moll, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13655">GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital humans and, especially, 3D facial avatars have raised a lot of attention in the past years, as they are the backbone of several applications like immersive telepresence in AR or VR. Despite the progress, facial avatars reconstructed from commodity hardware are incomplete and miss out on parts of the side and back of the head, severely limiting the usability of the avatar. This limitation in prior work stems from their requirement of face tracking, which fails for profile and back views. To address this issue, we propose to learn person-specific animatable avatars from images without assuming to have access to precise facial expression tracking. At the core of our method, we leverage a 3D-aware generative model that is trained to reproduce the distribution of facial expressions from the training data. To train this appearance model, we only assume to have a collection of 2D images with the corresponding camera parameters. For controlling the model, we learn a mapping from 3DMM facial expression parameters to the latent space of the generative model. This mapping can be learned by sampling the latent space of the appearance model and reconstructing the facial parameters from a normalized frontal view, where facial expression estimation performs well. With this scheme, we decouple 3D appearance reconstruction and animation control to achieve high fidelity in image synthesis. In a series of experiments, we compare our proposed technique to state-of-the-art monocular methods and show superior quality while not requiring expression tracking of the training data.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2311.12194.pdf' target='_blank'>https://arxiv.org/pdf/2311.12194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Li, Hsiao-yu Chen, Egor Larionov, Nikolaos Sarafianos, Wojciech Matusik, Tuur Stuyck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12194">DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The realism of digital avatars is crucial in enabling telepresence applications with self-expression and customization. While physical simulations can produce realistic motions for clothed humans, they require high-quality garment assets with associated physical parameters for cloth simulations. However, manually creating these assets and calibrating their parameters is labor-intensive and requires specialized expertise. Current methods focus on reconstructing geometry, but don't generate complete assets for physics-based applications. To address this gap, we propose \papername,~a novel approach that performs body and garment co-optimization using differentiable simulation. By integrating physical simulation into the optimization loop and accounting for the complex nonlinear behavior of cloth and its intricate interaction with the body, our framework recovers body and garment geometry and extracts important material parameters in a physically plausible way. Our experiments demonstrate that our approach generates realistic clothing and body shape suitable for downstream applications. We provide additional insights and results on our webpage: https://people.csail.mit.edu/liyifei/publication/diffavatar/
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2303.14471.pdf' target='_blank'>https://arxiv.org/pdf/2303.14471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Teotia, Mallikarjun B R, Xingang Pan, Hyeongwoo Kim, Pablo Garrido, Mohamed Elgharib, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14471">HQ3DAvatar: High Quality Controllable 3D Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view volumetric rendering techniques have recently shown great potential in modeling and synthesizing high-quality head avatars. A common approach to capture full head dynamic performances is to track the underlying geometry using a mesh-based template or 3D cube-based graphics primitives. While these model-based approaches achieve promising results, they often fail to learn complex geometric details such as the mouth interior, hair, and topological changes over time. This paper presents a novel approach to building highly photorealistic digital head avatars. Our method learns a canonical space via an implicit function parameterized by a neural network. It leverages multiresolution hash encoding in the learned feature space, allowing for high-quality, faster training and high-resolution rendering. At test time, our method is driven by a monocular RGB video. Here, an image encoder extracts face-specific features that also condition the learnable canonical space. This encourages deformation-dependent texture variations during training. We also propose a novel optical flow based loss that ensures correspondences in the learned canonical space, thus encouraging artifact-free and temporally consistent renderings. We show results on challenging facial expressions and show free-viewpoint renderings at interactive real-time rates for medium image resolutions. Our method outperforms all existing approaches, both visually and numerically. We will release our multiple-identity dataset to encourage further research. Our Project page is available at: https://vcai.mpi-inf.mpg.de/projects/HQ3DAvatar/
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2108.05681.pdf' target='_blank'>https://arxiv.org/pdf/2108.05681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyowoon Seo, Jihong Park, Mehdi Bennis, MÃ©rouane Debbah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.05681">Semantics-Native Communication with Contextual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spurred by a huge interest in the post-Shannon communication, it has recently been shown that leveraging semantics can significantly improve the communication effectiveness across many tasks. In this article, inspired by human communication, we propose a novel stochastic model of System 1 semantics-native communication (SNC) for generic tasks, where a speaker has an intention of referring to an entity, extracts the semantics, and communicates its symbolic representation to a target listener. To further reach its full potential, we additionally infuse contextual reasoning into SNC such that the speaker locally and iteratively self-communicates with a virtual agent built on the physical listener's unique way of coding its semantics, i.e., communication context. The resultant System 2 SNC allows the speaker to extract the most effective semantics for its listener. Leveraging the proposed stochastic model, we show that the reliability of System 2 SNC increases with the number of meaningful concepts, and derive the expected semantic representation (SR) bit length which quantifies the extracted effective semantics. It is also shown that System 2 SNC significantly reduces the SR length without compromising communication reliability.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2509.14739.pdf' target='_blank'>https://arxiv.org/pdf/2509.14739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Fan, Bingyu Hu, Xingguang Li, Yuxiang Yang, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14739">FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2509.12816.pdf' target='_blank'>https://arxiv.org/pdf/2509.12816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Axel Wiebe Werner, Jonas Beskow, Anna Deichler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12816">Gesture Evaluation in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are central to human communication, enriching interactions through non-verbal expression. Virtual avatars increasingly use AI-generated gestures to enhance life-likeness, yet evaluations have largely been confined to 2D. Virtual Reality (VR) provides an immersive alternative that may affect how gestures are perceived. This paper presents a comparative evaluation of computer-generated gestures in VR and 2D, examining three models from the 2023 GENEA Challenge. Results show that gestures viewed in VR were rated slightly higher on average, with the strongest effect observed for motion-capture "true movement." While model rankings remained consistent across settings, VR influenced participants' overall perception and offered unique benefits over traditional 2D evaluation.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2508.19688.pdf' target='_blank'>https://arxiv.org/pdf/2508.19688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gangjian Zhang, Jian Shu, Nanjie Yao, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19688">SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular texture 3D human reconstruction aims to create a complete 3D digital avatar from just a single front-view human RGB image. However, the geometric ambiguity inherent in a single 2D image and the scarcity of 3D human training data are the main obstacles limiting progress in this field. To address these issues, current methods employ prior geometric estimation networks to derive various human geometric forms, such as the SMPL model and normal maps. However, they struggle to integrate these modalities effectively, leading to view inconsistencies, such as facial distortions. To this end, we propose a two-process 3D human reconstruction framework, SAT, which seamlessly learns various prior geometries in a unified manner and reconstructs high-quality textured 3D avatars as the final output. To further facilitate geometry learning, we introduce a Supervisor Feature Regularization module. By employing a multi-view network with the same structure to provide intermediate features as training supervision, these varied geometric priors can be better fused. To tackle data scarcity and further improve reconstruction quality, we also propose an Online Animation Augmentation module. By building a one-feed-forward animation network, we augment a massive number of samples from the original 3D human data online for model training. Extensive experiments on two benchmarks show the superiority of our approach compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2505.22141.pdf' target='_blank'>https://arxiv.org/pdf/2505.22141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanwen Feng, Zhiyuan Ma, Yunan Li, Jiahao Yang, Junwei Jing, Qiguang Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22141">FaceEditTalker: Controllable Talking Head Generation with Facial Attribute Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is indispensable for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, flexible adjustment of visual attributes, such as hairstyle, accessories, and subtle facial features, is essential for aligning with user preferences, reflecting diverse brand identities and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method achieves comparable or superior performance to representative baseline methods in lip-sync accuracy, video quality, and attribute controllability. Project page: https://peterfanfan.github.io/FaceEditTalker/
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2505.18869.pdf' target='_blank'>https://arxiv.org/pdf/2505.18869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankan Dash, Jingyi Gu, Guiling Wang, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18869">Eye-See-You: Reverse Pass-Through VR and Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) headsets, while integral to the evolving digital ecosystem, present a critical challenge: the occlusion of users' eyes and portions of their faces, which hinders visual communication and may contribute to social isolation. To address this, we introduce RevAvatar, an innovative framework that leverages AI methodologies to enable reverse pass-through technology, fundamentally transforming VR headset design and interaction paradigms. RevAvatar integrates state-of-the-art generative models and multimodal AI techniques to reconstruct high-fidelity 2D facial images and generate accurate 3D head avatars from partially observed eye and lower-face regions. This framework represents a significant advancement in AI4Tech by enabling seamless interaction between virtual and physical environments, fostering immersive experiences such as VR meetings and social engagements. Additionally, we present VR-Face, a novel dataset comprising 200,000 samples designed to emulate diverse VR-specific conditions, including occlusions, lighting variations, and distortions. By addressing fundamental limitations in current VR systems, RevAvatar exemplifies the transformative synergy between AI and next-generation technologies, offering a robust platform for enhancing human connection and interaction in virtual environments.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2504.18215.pdf' target='_blank'>https://arxiv.org/pdf/2504.18215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nanjie Yao, Gangjian Zhang, Wenhao Shen, Jian Shu, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18215">Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : https://e2e3dgsrecon.github.io/e2e3dgsrecon/.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2504.15179.pdf' target='_blank'>https://arxiv.org/pdf/2504.15179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Yin, Mallikarjun B R, Chun-Han Yao, RafaÅ Mantiuk, Varun Jampani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15179">FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for generating high-quality, animatable 4D avatar from a single image. While recent advances have shown promising results in 4D avatar creation, existing methods either require extensive multiview data or struggle with shape accuracy and identity consistency. To address these limitations, we propose a comprehensive system that leverages shape, image, and video priors to create full-view, animatable avatars. Our approach first obtains initial coarse shape through 3D-GAN inversion. Then, it enhances multiview textures using depth-guided warping signals for cross-view consistency with the help of the image diffusion model. To handle expression animation, we incorporate a video prior with synchronized driving signals across viewpoints. We further introduce a Consistent-Inconsistent training to effectively handle data inconsistencies during 4D reconstruction. Experimental results demonstrate that our method achieves superior quality compared to the prior art, while maintaining consistency across different viewpoints and expressions.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2503.02452.pdf' target='_blank'>https://arxiv.org/pdf/2503.02452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qipeng Yan, Mingyang Sun, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02452">2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem in computer vision and graphics. Over the past few years, the Neural Radiance Field (NeRF) has made significant progress in rendering quality but behaves poorly in run-time performance due to the low efficiency of volumetric rendering. Recently, methods based on 3D Gaussian Splatting (3DGS) have shown great potential in fast training and real-time rendering. However, they still suffer from artifacts caused by inaccurate geometry. To address these problems, we propose 2DGS-Avatar, a novel approach based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars with high-fidelity and fast training performance. Given monocular RGB videos as input, our method generates an avatar that can be driven by poses and rendered in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the advantages of fast training and rendering while also capturing detailed, dynamic, and photo-realistic appearances. We conduct abundant experiments on popular datasets such as AvatarRex and THuman4.0, demonstrating impressive performance in both qualitative and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2503.01610.pdf' target='_blank'>https://arxiv.org/pdf/2503.01610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Junxuan Li, Yash Kant, Yaser Sheikh, Shunsuke Saito, Chen Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01610">Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Vid2Avatar-Pro, a method to create photorealistic and animatable 3D human avatars from monocular in-the-wild videos. Building a high-quality avatar that supports animation with diverse poses from a monocular video is challenging because the observation of pose diversity and view points is inherently limited. The lack of pose variations typically leads to poor generalization to novel poses, and avatars can easily overfit to limited input view points, producing artifacts and distortions from other views. In this work, we address these limitations by leveraging a universal prior model (UPM) learned from a large corpus of multi-view clothed human performance capture data. We build our representation on top of expressive 3D Gaussians with canonical front and back maps shared across identities. Once the UPM is learned to accurately reproduce the large-scale multi-view human images, we fine-tune the model with an in-the-wild video via inverse rendering to obtain a personalized photorealistic human avatar that can be faithfully animated to novel human motions and rendered from novel views. The experiments show that our approach based on the learned universal prior sets a new state-of-the-art in monocular avatar reconstruction by substantially outperforming existing approaches relying only on heuristic regularization or a shape prior of minimally clothed bodies (e.g., SMPL) on publicly available datasets.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2502.20858.pdf' target='_blank'>https://arxiv.org/pdf/2502.20858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaochuan Liu, Xin Cheng, Yuchong Sun, Xiaoxue Wu, Ruihua Song, Hao Sun, Denghao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20858">EyEar: Learning Audio Synchronized Human Gaze Trajectory Based on Physics-Informed Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitating how humans move their gaze in a visual scene is a vital research problem for both visual understanding and psychology, kindling crucial applications such as building alive virtual characters. Previous studies aim to predict gaze trajectories when humans are free-viewing an image, searching for required targets, or looking for clues to answer questions in an image. While these tasks focus on visual-centric scenarios, humans move their gaze also along with audio signal inputs in more common scenarios. To fill this gap, we introduce a new task that predicts human gaze trajectories in a visual scene with synchronized audio inputs and provide a new dataset containing 20k gaze points from 8 subjects. To effectively integrate audio information and simulate the dynamic process of human gaze motion, we propose a novel learning framework called EyEar (Eye moving while Ear listening) based on physics-informed dynamics, which considers three key factors to predict gazes: eye inherent motion tendency, vision salient attraction, and audio semantic attraction. We also propose a probability density score to overcome the high individual variability of gaze trajectories, thereby improving the stabilization of optimization and the reliability of the evaluation. Experimental results show that EyEar outperforms all the baselines in the context of all evaluation metrics, thanks to the proposed components in the learning model.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2502.20220.pdf' target='_blank'>https://arxiv.org/pdf/2502.20220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Kirschstein, Javier Romero, Artem Sevastopolsky, Matthias Nießner, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20220">Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditionally, creating photo-realistic 3D head avatars requires a studio-level multi-view capture setup and expensive optimization during test-time, limiting the use of digital human doubles to the VFX industry or offline renderings. To address this shortcoming, we present Avat3r, which regresses a high-quality and animatable 3D head avatar from just a few input images, vastly reducing compute requirements during inference. More specifically, we make Large Reconstruction Models animatable and learn a powerful prior over 3D human heads from a large multi-view video dataset. For better 3D head reconstructions, we employ position maps from DUSt3R and generalized feature maps from the human foundation model Sapiens. To animate the 3D head, our key discovery is that simple cross-attention to an expression code is already sufficient. Finally, we increase robustness by feeding input images with different expressions to our model during training, enabling the reconstruction of 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture with accidental movement, or frames from a monocular video. We compare Avat3r with current state-of-the-art methods for few-input and single-input scenarios, and find that our method has a competitive advantage in both tasks. Finally, we demonstrate the wide applicability of our proposed model, creating 3D head avatars from images of different sources, smartphone captures, single images, and even out-of-domain inputs like antique busts. Project website: https://tobias-kirschstein.github.io/avat3r/
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2502.06957.pdf' target='_blank'>https://arxiv.org/pdf/2502.06957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, Fernando De la Torre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06957">GAS: Generative Avatar Synthesis from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a unified and generalizable framework for synthesizing view-consistent and temporally coherent avatars from a single image, addressing the challenging task of single-image avatar generation. Existing diffusion-based methods often condition on sparse human templates (e.g., depth or normal maps), which leads to multi-view and temporal inconsistencies due to the mismatch between these signals and the true appearance of the subject. Our approach bridges this gap by combining the reconstruction power of regression-based 3D human reconstruction with the generative capabilities of a diffusion model. In a first step, an initial 3D reconstructed human through a generalized NeRF provides comprehensive conditioning, ensuring high-quality synthesis faithful to the reference appearance and structure. Subsequently, the derived geometry and appearance from the generalized NeRF serve as input to a video-based diffusion model. This strategic integration is pivotal for enforcing both multi-view and temporal consistency throughout the avatar's generation. Empirical results underscore the superior generalization ability of our proposed method, demonstrating its effectiveness across diverse in-domain and out-of-domain in-the-wild datasets.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2502.01046.pdf' target='_blank'>https://arxiv.org/pdf/2502.01046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Ye, Boyuan Cao, Hongming Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01046">Emotional Face-to-Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How much can we infer about an emotional voice solely from an expressive face? This intriguing question holds great potential for applications such as virtual character dubbing and aiding individuals with expressive language disorders. Existing face-to-speech methods offer great promise in capturing identity characteristics but struggle to generate diverse vocal styles with emotional expression. In this paper, we explore a new task, termed emotional face-to-speech, aiming to synthesize emotional speech directly from expressive facial cues. To that end, we introduce DEmoFace, a novel generative framework that leverages a discrete diffusion transformer (DiT) with curriculum learning, built upon a multi-level neural audio codec. Specifically, we propose multimodal DiT blocks to dynamically align text and speech while tailoring vocal styles based on facial emotion and identity. To enhance training efficiency and generation quality, we further introduce a coarse-to-fine curriculum learning algorithm for multi-level token processing. In addition, we develop an enhanced predictor-free guidance to handle diverse conditioning scenarios, enabling multi-conditional generation and disentangling complex attributes effectively. Extensive experimental results demonstrate that DEmoFace generates more natural and consistent speech compared to baselines, even surpassing speech-driven methods. Demos are shown at https://demoface-ai.github.io/.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2501.09978.pdf' target='_blank'>https://arxiv.org/pdf/2501.09978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyue Liu, Kunming Luo, Heng Li, Qi Zhang, Yuan Liu, Li Yi, Ping Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09978">GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GaussianAvatar-Editor, an innovative framework for text-driven editing of animatable Gaussian head avatars that can be fully controlled in expression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing animatable 4D Gaussian avatars presents challenges related to motion occlusion and spatial-temporal inconsistency. To address these issues, we propose the Weighted Alpha Blending Equation (WABE). This function enhances the blending weight of visible Gaussians while suppressing the influence on non-visible Gaussians, effectively handling motion occlusion during editing. Furthermore, to improve editing quality and ensure 4D consistency, we incorporate conditional adversarial learning into the editing process. This strategy helps to refine the edited results and maintain consistency throughout the animation. By integrating these methods, our GaussianAvatar-Editor achieves photorealistic and consistent results in animatable 4D Gaussian editing. We conduct comprehensive experiments across various subjects to validate the effectiveness of our proposed techniques, which demonstrates the superiority of our approach over existing methods. More results and code are available at: [Project Link](https://xiangyueliu.github.io/GaussianAvatar-Editor/).
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2501.01808.pdf' target='_blank'>https://arxiv.org/pdf/2501.01808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaize Liu, Wenzhang Sun, Donglin Di, Shibo Sun, Jiahui Yang, Changqing Zou, Hujun Bao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01808">MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of talking avatars has achieved significant advancements in precise audio synchronization. However, crafting lifelike talking head videos requires capturing a broad spectrum of emotions and subtle facial expressions. Current methods face fundamental challenges: a) the absence of frameworks for modeling single basic emotional expressions, which restricts the generation of complex emotions such as compound emotions; b) the lack of comprehensive datasets rich in human emotional expressions, which limits the potential of models. To address these challenges, we propose the following innovations: 1) the Mixture of Emotion Experts (MoEE) model, which decouples six fundamental emotions to enable the precise synthesis of both singular and compound emotional states; 2) the DH-FaceEmoVid-150 dataset, specifically curated to include six prevalent human emotional expressions as well as four types of compound emotions, thereby expanding the training potential of emotion-driven models. Furthermore, to enhance the flexibility of emotion control, we propose an emotion-to-latents module that leverages multimodal inputs, aligning diverse control signals-such as audio, text, and labels-to ensure more varied control inputs as well as the ability to control emotions using audio alone. Through extensive quantitative and qualitative evaluations, we demonstrate that the MoEE framework, in conjunction with the DH-FaceEmoVid-150 dataset, excels in generating complex emotional expressions and nuanced facial details, setting a new benchmark in the field. These datasets will be publicly released.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2410.00253.pdf' target='_blank'>https://arxiv.org/pdf/2410.00253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Deichler, Jim O'Regan, Jonas Beskow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00253">MM-Conv: A Multi-modal Conversational Dataset for Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel dataset captured using a VR headset to record conversations between participants within a physics simulator (AI2-THOR). Our primary objective is to extend the field of co-speech gesture generation by incorporating rich contextual information within referential settings. Participants engaged in various conversational scenarios, all based on referential communication tasks. The dataset provides a rich set of multimodal recordings such as motion capture, speech, gaze, and scene graphs. This comprehensive dataset aims to enhance the understanding and development of gesture generation models in 3D scenes by providing diverse and contextually rich data.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2406.07516.pdf' target='_blank'>https://arxiv.org/pdf/2406.07516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Kolotouros, Thiemo Alldieck, Enric Corona, Eduard Gabriel Bazavan, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07516">Instant 3D Human Avatar Generation using Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape. The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network. We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs. We fine-tune latent diffusion networks with additional image conditioning for image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses. Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting. In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals. Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup wrt the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls. AvatarPopUp enables applications that require the controlled 3D generation of human avatars at scale. The project website can be found at https://www.nikoskolot.com/avatarpopup/.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2406.00637.pdf' target='_blank'>https://arxiv.org/pdf/2406.00637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00637">Representing Animatable Avatar via Factorized Neural Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results can be factorized into a pose-independent component and a corresponding pose-dependent equivalent to facilitate frame consistency. Pose adaptive textures can be further improved by restricting frequency bands of these two components. In detail, pose-independent outputs are expected to be low-frequency, while highfrequency information is linked to pose-dependent factors. We achieve a coherent preservation of both coarse body contours across the entire input video and finegrained texture features that are time variant with a dual-branch network with distinct frequency components. The first branch takes coordinates in canonical space as input, while the second branch additionally considers features outputted by the first branch and pose information of each frame. Our network integrates the information predicted by both branches and utilizes volume rendering to generate photo-realistic 3D human images. Through experiments, we demonstrate that our network surpasses the neural radiance fields (NeRF) based state-of-the-art methods in preserving high-frequency details and ensuring consistent body contours.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2404.00485.pdf' target='_blank'>https://arxiv.org/pdf/2404.00485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akash Sengupta, Thiemo Alldieck, Nikos Kolotouros, Enric Corona, Andrei Zanfir, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00485">DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. Despite the ill-posed nature of this problem, most methods are deterministic and output a single solution, often resulting in a lack of geometric detail and blurriness in unseen or uncertain regions. In contrast, DiffHuman predicts a probability distribution over 3D reconstructions conditioned on an input 2D image, which allows us to sample multiple detailed 3D avatars that are consistent with the image. DiffHuman is implemented as a conditional diffusion model that denoises pixel-aligned 2D observations of an underlying 3D shape representation. During inference, we may sample 3D avatars by iteratively denoising 2D renders of the predicted 3D representation. Furthermore, we introduce a generator neural network that approximates rendering with considerably reduced runtime (55x speed up), resulting in a novel dual-branch diffusion framework. Our experiments show that DiffHuman can produce diverse and detailed reconstructions for the parts of the person that are unseen or uncertain in the input image, while remaining competitive with the state-of-the-art when reconstructing visible surfaces.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2403.08764.pdf' target='_blank'>https://arxiv.org/pdf/2403.08764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08764">VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2402.17292.pdf' target='_blank'>https://arxiv.org/pdf/2402.17292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17292">DivAvatar: Diverse 3D Avatar Generation with a Single Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Avatar generation has recently made significant strides due to advancements in diffusion models. However, most existing work remains constrained by limited diversity, producing avatars with subtle differences in appearance for a given text prompt. We design DivAvatar, a novel framework that generates diverse avatars, empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text prompt. Different from most existing work that exploits scene-specific 3D representations such as NeRF, DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss, the former producing appearances of high textual fidelity by separate fine-tuning of specific body parts and the latter improving geometry quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2401.13832.pdf' target='_blank'>https://arxiv.org/pdf/2401.13832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elizaveta Kuznetsova, Mykola Makhortykh, Maryna Sydorova, Aleksandra Urman, Ilaria Vitulano, Martha Stolze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13832">Algorithmically Curated Lies: How Search Engines Handle Misinformation about US Biolabs in Ukraine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing volume of online content prompts the need for adopting algorithmic systems of information curation. These systems range from web search engines to recommender systems and are integral for helping users stay informed about important societal developments. However, unlike journalistic editing the algorithmic information curation systems (AICSs) are known to be subject to different forms of malperformance which make them vulnerable to possible manipulation. The risk of manipulation is particularly prominent in the case when AICSs have to deal with information about false claims that underpin propaganda campaigns of authoritarian regimes. Using as a case study of the Russian disinformation campaign concerning the US biolabs in Ukraine, we investigate how one of the most commonly used forms of AICSs - i.e. web search engines - curate misinformation-related content. For this aim, we conduct virtual agent-based algorithm audits of Google, Bing, and Yandex search outputs in June 2022. Our findings highlight the troubling performance of search engines. Even though some search engines, like Google, were less likely to return misinformation results, across all languages and locations, the three search engines still mentioned or promoted a considerable share of false content (33% on Google; 44% on Bing, and 70% on Yandex). We also find significant disparities in misinformation exposure based on the language of search, with all search engines presenting a higher number of false stories in Russian. Location matters as well with users from Germany being more likely to be exposed to search results promoting false information. These observations stress the possibility of AICSs being vulnerable to manipulation, in particular in the case of the unfolding propaganda campaigns, and underline the importance of monitoring performance of these systems to prevent it.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2401.01173.pdf' target='_blank'>https://arxiv.org/pdf/2401.01173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01173">En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present En3D, an enhanced generative scheme for sculpting high-quality 3D human avatars. Unlike previous works that rely on scarce 3D datasets or limited 2D collections with imbalanced viewing angles and imprecise pose priors, our approach aims to develop a zero-shot 3D generative scheme capable of producing visually realistic, geometrically accurate and content-wise diverse 3D humans without relying on pre-existing 3D or 2D assets. To address this challenge, we introduce a meticulously crafted workflow that implements accurate physical modeling to learn the enhanced 3D generative model from synthetic 2D data. During inference, we integrate optimization modules to bridge the gap between realistic appearances and coarse 3D shapes. Specifically, En3D comprises three modules: a 3D generator that accurately models generalizable 3D humans with realistic appearance from synthesized balanced, diverse, and structured human images; a geometry sculptor that enhances shape quality using multi-view normal constraints for intricate human anatomy; and a texturing module that disentangles explicit texture maps with fidelity and editability, leveraging semantical UV partitioning and a differentiable rasterizer. Experimental results show that our approach significantly outperforms prior works in terms of image quality, geometry accuracy and content diversity. We also showcase the applicability of our generated avatars for animation and editing, as well as the scalability of our approach for content-style free adaptation.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2312.16842.pdf' target='_blank'>https://arxiv.org/pdf/2312.16842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hansol Lee, Junuk Cha, Yunhoe Ku, Jae Shin Yoon, Seungryul Baek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16842">Dynamic Appearance Modeling of Clothed 3D Human Avatars using a Single Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The appearance of a human in clothing is driven not only by the pose but also by its temporal context, i.e., motion. However, such context has been largely neglected by existing monocular human modeling methods whose neural networks often struggle to learn a video of a person with large dynamics due to the motion ambiguity, i.e., there exist numerous geometric configurations of clothes that are dependent on the context of motion even for the same pose. In this paper, we introduce a method for high-quality modeling of clothed 3D human avatars using a video of a person with dynamic movements. The main challenge comes from the lack of 3D ground truth data of geometry and its temporal correspondences. We address this challenge by introducing a novel compositional human modeling framework that takes advantage of both explicit and implicit human modeling. For explicit modeling, a neural network learns to generate point-wise shape residuals and appearance features of a 3D body model by comparing its 2D rendering results and the original images. This explicit model allows for the reconstruction of discriminative 3D motion features from UV space by encoding their temporal correspondences. For implicit modeling, an implicit network combines the appearance and 3D motion features to decode high-fidelity clothed 3D human avatars with motion-dependent geometry and texture. The experiments show that our method can generate a large variation of secondary motion in a physically plausible way.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2312.16837.pdf' target='_blank'>https://arxiv.org/pdf/2312.16837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, Xuansong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16837">DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaptation by Combining 3D GANs and Diffusion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-guided domain adaptation and generation of 3D-aware portraits find many applications in various fields. However, due to the lack of training data and the challenges in handling the high variety of geometry and appearance, the existing methods for these tasks suffer from issues like inflexibility, instability, and low fidelity. In this paper, we propose a novel framework DiffusionGAN3D, which boosts text-guided 3D domain adaptation and generation by combining 3D GANs and diffusion priors. Specifically, we integrate the pre-trained 3D generative models (e.g., EG3D) and text-to-image diffusion models. The former provides a strong foundation for stable and high-quality avatar generation from text. And the diffusion models in turn offer powerful priors and guide the 3D generator finetuning with informative direction to achieve flexible and efficient text-guided domain adaptation. To enhance the diversity in domain adaptation and the generation capability in text-to-avatar, we introduce the relative distance loss and case-specific learnable triplane respectively. Besides, we design a progressive texture refinement module to improve the texture quality for both tasks above. Extensive experiments demonstrate that the proposed framework achieves excellent results in both domain adaptation and text-to-avatar tasks, outperforming existing methods in terms of generation quality and efficiency. The project homepage is at https://younglbw.github.io/DiffusionGAN3D-homepage/.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2312.04465.pdf' target='_blank'>https://arxiv.org/pdf/2312.04465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stathis Galanakis, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04465">FitDiff: Robust monocular 3D facial shape and reflectance estimation using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable progress in 3D face reconstruction has resulted in high-detail and photorealistic facial representations. Recently, Diffusion Models have revolutionized the capabilities of generative methods by surpassing the performance of GANs. In this work, we present FitDiff, a diffusion-based 3D facial avatar generative model. Leveraging diffusion principles, our model accurately generates relightable facial avatars, utilizing an identity embedding extracted from an "in-the-wild" 2D facial image. The introduced multi-modal diffusion model is the first to concurrently output facial reflectance maps (diffuse and specular albedo and normals) and shapes, showcasing great generalization capabilities. It is solely trained on an annotated subset of a public facial dataset, paired with 3D reconstructions. We revisit the typical 3D facial fitting approach by guiding a reverse diffusion process using perceptual and face recognition losses. Being the first 3D LDM conditioned on face recognition embeddings, FitDiff reconstructs relightable human avatars, that can be used as-is in common rendering engines, starting only from an unconstrained facial image, and achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2311.08581.pdf' target='_blank'>https://arxiv.org/pdf/2311.08581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael ZollhÃ¶fer, Justus Thies, Javier Romero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08581">Drivable 3D Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Drivable 3D Gaussian Avatars (D3GA), a multi-layered 3D controllable model for human bodies that utilizes 3D Gaussian primitives embedded into tetrahedral cages. The advantage of using cages compared to commonly employed linear blend skinning (LBS) is that primitives like 3D Gaussians are naturally re-oriented and their kernels are stretched via the deformation gradients of the encapsulating tetrahedron. Additional offsets are modeled for the tetrahedron vertices, effectively decoupling the low-dimensional driving poses from the extensive set of primitives to be rendered. This separation is achieved through the localized influence of each tetrahedron on 3D Gaussians, resulting in improved optimization. Using the cage-based deformation model, we introduce a compositional pipeline that decomposes an avatar into layers, such as garments, hands, or faces, improving the modeling of phenomena like garment sliding. These parts can be conditioned on different driving signals, such as keypoints for facial expressions or joint-angle vectors for garments and the body. Our experiments on two multi-view datasets with varied body shapes, clothes, and motions show higher-quality results. They surpass PSNR and SSIM metrics of other SOTA methods using the same data while offering greater flexibility and compactness.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2308.09716.pdf' target='_blank'>https://arxiv.org/pdf/2308.09716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumik Mukhopadhyay, Saksham Suri, Ravi Teja Gadde, Abhinav Shrivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09716">Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of lip synchronization (lip-sync) seeks to match the lips of human faces with different audio. It has various applications in the film industry as well as for creating virtual avatars and for video conferencing. This is a challenging problem as one needs to simultaneously introduce detailed, realistic lip movements while preserving the identity, pose, emotions, and image quality. Many of the previous methods trying to solve this problem suffer from image quality degradation due to a lack of complete contextual information. In this paper, we present Diff2Lip, an audio-conditioned diffusion-based model which is able to do lip synchronization in-the-wild while preserving these qualities. We train our model on Voxceleb2, a video dataset containing in-the-wild talking face videos. Extensive studies show that our method outperforms popular methods like Wav2Lip and PC-AVS in FrÃ©chet inception distance (FID) metric and Mean Opinion Scores (MOS) of the users. We show results on both reconstruction (same audio-video inputs) as well as cross (different audio-video inputs) settings on Voxceleb2 and LRW datasets. Video results and code can be accessed from our project page ( https://soumik-kanad.github.io/diff2lip ).
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2306.09329.pdf' target='_blank'>https://arxiv.org/pdf/2306.09329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09329">DreamHuman: Animatable 3D Avatars from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamHuman, a method to generate realistic animatable 3D human avatar models solely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than animated 3D human models, and anthropometric consistency for complex structures like people remains a challenge. DreamHuman connects large text-to-image synthesis models, neural radiance fields, and statistical human body models in a novel modeling and optimization framework. This makes it possible to generate dynamic 3D human avatars with high-quality textures and learned, instance-specific, surface deformations. We demonstrate that our method is capable to generate a wide variety of animatable, realistic 3D human models from text. Our 3D models have diverse appearance, clothing, skin tones and body shapes, and significantly outperform both generic text-to-3D approaches and previous text-based 3D avatar generators in visual fidelity. For more results and animations please check our website at https://dream-human.github.io.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2304.00247.pdf' target='_blank'>https://arxiv.org/pdf/2304.00247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takahiro Tsumura, Seiji Yamada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00247">Improving of Robotic Virtual Agent's errors that are accepted by reaction and human's preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One way to improve the relationship between humans and anthropomorphic agents is to have humans empathize with the agents. In this study, we focused on a task between an agent and a human in which the agent makes a mistake. To investigate significant factors for designing a robotic agent that can promote humans empathy, we experimentally examined the hypothesis that agent reaction and human's preference affect human empathy and acceptance of the agent's mistakes. The experiment consisted of a four-condition, three-factor mixed design with agent reaction, selected agent's body color for human's preference, and pre- and post-task as factors. The results showed that agent reaction and human's preference did not affect empathy toward the agent but did allow the agent to make mistakes. It was also shown that empathy for the agent decreased when the agent made a mistake on the task. The results of this study provide a way to control impressions of the robotic virtual agent's behaviors, which are increasingly used in society.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2509.01469.pdf' target='_blank'>https://arxiv.org/pdf/2509.01469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vanessa Sklyarova, Egor Zakharov, Malte Prinzler, Giorgio Becherini, Michael J. Black, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01469">Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach for 3D hair reconstruction from single photographs based on a global hair prior combined with local optimization. Capturing strand-based hair geometry from single photographs is challenging due to the variety and geometric complexity of hairstyles and the lack of ground truth training data. Classical reconstruction methods like multi-view stereo only reconstruct the visible hair strands, missing the inner structure of hairstyles and hampering realistic hair simulation. To address this, existing methods leverage hairstyle priors trained on synthetic data. Such data, however, is limited in both quantity and quality since it requires manual work from skilled artists to model the 3D hairstyles and create near-photorealistic renderings. To address this, we propose a novel approach that uses both, real and synthetic data to learn an effective hairstyle prior. Specifically, we train a transformer-based prior model on synthetic data to obtain knowledge of the internal hairstyle geometry and introduce real data in the learning process to model the outer structure. This training scheme is able to model the visible hair strands depicted in an input image, while preserving the general 3D structure of hairstyles. We exploit this prior to create a Gaussian-splatting-based reconstruction method that creates hairstyles from one or more images. Qualitative and quantitative comparisons with existing reconstruction pipelines demonstrate the effectiveness and superior performance of our method for capturing detailed hair orientation, overall silhouette, and backside consistency. For additional results and code, please refer to https://im2haircut.is.tue.mpg.de.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2508.17342.pdf' target='_blank'>https://arxiv.org/pdf/2508.17342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengyuan Zhang, Zhe Li, Xingqun Qi, Mengze Li, Muyi Sun, Man Zhang, Sirui Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17342">DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating coherent and diverse human dances from music signals has gained tremendous progress in animating virtual avatars. While existing methods support direct dance synthesis, they fail to recognize that enabling users to edit dance movements is far more practical in real-world choreography scenarios. Moreover, the lack of high-quality dance datasets incorporating iterative editing also limits addressing this challenge. To achieve this goal, we first construct DanceRemix, a large-scale multi-turn editable dance dataset comprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In addition, we propose a novel framework for iterative and editable dance generation coherently aligned with given music signals, namely DanceEditor. Considering the dance motion should be both musical rhythmic and enable iterative editing by user descriptions, our framework is built upon a prediction-then-editing paradigm unifying multi-modal conditions. At the initial prediction stage, our framework improves the authority of generated results by directly modeling dance movements from tailored, aligned music. Moreover, at the subsequent iterative editing stages, we incorporate text descriptions as conditioning information to draw the editable results through a specifically designed Cross-modality Editing Module (CEM). Specifically, CEM adaptively integrates the initial prediction with music and text prompts as temporal motion cues to guide the synthesized sequences. Thereby, the results display music harmonics while preserving fine-grained semantic alignment with text descriptions. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected DanceRemix dataset. Code is available at https://lzvsdy.github.io/DanceEditor/.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2508.09461.pdf' target='_blank'>https://arxiv.org/pdf/2508.09461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09461">Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different forms of customized 2D avatars are widely used in gaming applications, virtual communication, education, and content creation. However, existing approaches often fail to capture fine-grained facial expressions and struggle to preserve identity across different expressions. We propose GEN-AFFECT, a novel framework for personalized avatar generation that generates expressive and identity-consistent avatars with a diverse set of facial expressions. Our framework proposes conditioning a multimodal diffusion transformer on an extracted identity-expression representation. This enables identity preservation and representation of a wide range of facial expressions. GEN-AFFECT additionally employs consistent attention at inference for information sharing across the set of generated expressions, enabling the generation process to maintain identity consistency over the array of generated fine-grained expressions. GEN-AFFECT demonstrates superior performance compared to previous state-of-the-art methods on the basis of the accuracy of the generated expressions, the preservation of the identity and the consistency of the target identity across an array of fine-grained facial expressions.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2508.08467.pdf' target='_blank'>https://arxiv.org/pdf/2508.08467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Zhang, Shuyao Zhou, Amna Liaqat, Tinney Mak, Brian Berengard, Emily Qian, AndrÃ©s Monroy-HernÃ¡ndez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08467">Empowering Children to Create AI-Enabled Augmented Reality Experiences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their potential to enhance children's learning experiences, AI-enabled AR technologies are predominantly used in ways that position children as consumers rather than creators. We introduce Capybara, an AR-based and AI-powered visual programming environment that empowers children to create, customize, and program 3D characters overlaid onto the physical world. Capybara enables children to create virtual characters and accessories using text-to-3D generative AI models, and to animate these characters through auto-rigging and body tracking. In addition, our system employs vision-based AI models to recognize physical objects, allowing children to program interactive behaviors between virtual characters and their physical surroundings. We demonstrate the expressiveness of Capybara through a set of novel AR experiences. We conducted user studies with 20 children in the United States and Argentina. Our findings suggest that Capybara can empower children to harness AI in authoring personalized and engaging AR experiences that seamlessly bridge the virtual and physical worlds.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2508.01218.pdf' target='_blank'>https://arxiv.org/pdf/2508.01218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujian Liu, Linlang Cao, Chuang Chen, Fanyu Geng, Dongxu Shen, Peng Cao, Shidang Xu, Xiaoli Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01218">MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing 3D head avatar reconstruction methods adopt a two-stage process, relying on tracked FLAME meshes derived from facial landmarks, followed by Gaussian-based rendering. However, misalignment between the estimated mesh and target images often leads to suboptimal rendering quality and loss of fine visual details. In this paper, we present MoGaFace, a novel 3D head avatar modeling framework that continuously refines facial geometry and texture attributes throughout the Gaussian rendering process. To address the misalignment between estimated FLAME meshes and target images, we introduce the Momentum-Guided Consistent Geometry module, which incorporates a momentum-updated expression bank and an expression-aware correction mechanism to ensure temporal and multi-view consistency. Additionally, we propose Latent Texture Attention, which encodes compact multi-view features into head-aware representations, enabling geometry-aware texture refinement via integration into Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity head avatar reconstruction and significantly improves novel-view synthesis quality, even under inaccurate mesh initialization and unconstrained real-world settings.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2507.11138.pdf' target='_blank'>https://arxiv.org/pdf/2507.11138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adriano Castro, Simon Hanisch, Matin Fallahi, Thorsten Strufe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11138">FacialMotionID: Identifying Users of Mixed Reality Headsets using Abstract Facial Motion Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial motion capture in mixed reality headsets enables real-time avatar animation, allowing users to convey non-verbal cues during virtual interactions. However, as facial motion data constitutes a behavioral biometric, its use raises novel privacy concerns. With mixed reality systems becoming more immersive and widespread, understanding whether face motion data can lead to user identification or inference of sensitive attributes is increasingly important.
  To address this, we conducted a study with 116 participants using three types of headsets across three sessions, collecting facial, eye, and head motion data during verbal and non-verbal tasks. The data used is not raw video, but rather, abstract representations that are used to animate digital avatars. Our analysis shows that individuals can be re-identified from this data with up to 98% balanced accuracy, are even identifiable across device types, and that emotional states can be inferred with up to 86% accuracy. These results underscore the potential privacy risks inherent in face motion tracking in mixed reality environments.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2506.01935.pdf' target='_blank'>https://arxiv.org/pdf/2506.01935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Tanmay Reddy Chakkera, Aggelina Chatziagapi, Md Moniruzzaman, Chen-Ping Yu, Yi-Hsuan Tsai, Dimitris Samaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01935">Low-Rank Head Avatar Personalization with Registers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel method for low-rank personalization of a generic model for head avatar generation. Prior work proposes generic models that achieve high-quality face animation by leveraging large-scale datasets of multiple identities. However, such generic models usually fail to synthesize unique identity-specific details, since they learn a general domain prior. To adapt to specific subjects, we find that it is still challenging to capture high-frequency facial details via popular solutions like low-rank adaptation (LoRA). This motivates us to propose a specific architecture, a Register Module, that enhances the performance of LoRA, while requiring only a small number of parameters to adapt to an unseen identity. Our module is applied to intermediate features of a pre-trained model, storing and re-purposing information in a learnable 3D feature space. To demonstrate the efficacy of our personalization method, we collect a dataset of talking videos of individuals with distinctive facial details, such as wrinkles and tattoos. Our approach faithfully captures unseen faces, outperforming existing methods quantitatively and qualitatively. We will release the code, models, and dataset to the public.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2505.21531.pdf' target='_blank'>https://arxiv.org/pdf/2505.21531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21531">How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the human motion knowledge of Large Language Models (LLMs) through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations. Using 20 representative motion instructions that cover fundamental movements and balance body part usage, we conduct comprehensive evaluations, including human and automatic scoring of both high-level movement plans and generated animations, as well as automatic comparison with oracle positions in low-level planning. Our findings show that LLMs are strong at interpreting high-level body movements but struggle with precise body part positioning. While decomposing motion queries into atomic components improves planning, LLMs face challenges in multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximations for general spatial descriptions, but fall short in handling precise spatial specifications. Notably, LLMs demonstrate promise in conceptualizing creative motions and distinguishing culturally specific motion patterns.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2504.21718.pdf' target='_blank'>https://arxiv.org/pdf/2504.21718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiying Li, Xingqun Qi, Bingkun Yang, Chen Weile, Zezhao Tian, Muyi Sun, Qifeng Liu, Man Zhang, Zhenan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21718">VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating responsive listener head dynamics with nuanced emotions and expressive reactions is crucial for practical dialogue modeling in various virtual avatar animations. Previous studies mainly focus on the direct short-term production of listener behavior. They overlook the fine-grained control over motion variations and emotional intensity, especially in long-sequence modeling. Moreover, the lack of long-term and large-scale paired speaker-listener corpora including head dynamics and fine-grained multi-modality annotations (e.g., text-based expression descriptions, emotional intensity) also limits the application of dialogue modeling.Therefore, we first newly collect a large-scale multi-turn dataset of 3D dyadic conversation containing more than 1.4M valid frames for multi-modal responsive interaction, dubbed ListenerX. Additionally, we propose VividListener, a novel framework enabling fine-grained, expressive and controllable listener dynamics modeling. This framework leverages multi-modal conditions as guiding principles for fostering coherent interactions between speakers and listeners.Specifically, we design the Responsive Interaction Module (RIM) to adaptively represent the multi-modal interactive embeddings. RIM ensures the listener dynamics achieve fine-grained semantic coordination with textual descriptions and adjustments, while preserving expressive reaction with speaker behavior. Meanwhile, we design the Emotional Intensity Tags (EIT) for emotion intensity editing with multi-modal information integration, applying to both text descriptions and listener motion amplitude.Extensive experiments conducted on our newly collected ListenerX dataset demonstrate that VividListener achieves state-of-the-art performance, realizing expressive and controllable listener dynamics.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2504.10486.pdf' target='_blank'>https://arxiv.org/pdf/2504.10486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeren Jiang, Shaofei Wang, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10486">DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable or even better relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2504.07945.pdf' target='_blank'>https://arxiv.org/pdf/2504.07945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07945">GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2504.06031.pdf' target='_blank'>https://arxiv.org/pdf/2504.06031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Leichert, Monique Koke, Britta Wrede, Birte Richter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06031">Virtual Agent Tutors in Sheltered Workshops: A Feasibility Study on Attention Training for Individuals with Intellectual Disabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we evaluate the feasibility of socially assistive virtual agent-based cognitive training for people with intellectual disabilities (ID) in a sheltered workshop. The Robo- Camp system, originally developed for children with Attention Deficit Hyperactivity Disorder (ADHD), is adapted based on the results of a pilot study in which we identified barriers and collected feedback from workshop staff. In a subsequent study, we investigate the aspects of usability, technical reliability, attention training capabilities and novelty effect in the feasibility of integrating the RoboCamp system.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2502.13133.pdf' target='_blank'>https://arxiv.org/pdf/2502.13133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aggelina Chatziagapi, Louis-Philippe Morency, Hongyu Gong, Michael Zollhoefer, Dimitris Samaras, Alexander Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13133">AV-Flow: Transforming Text to Audio-Visual Human-like Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AV-Flow, an audio-visual generative model that animates photo-realistic 4D talking avatars given only text input. In contrast to prior work that assumes an existing speech signal, we synthesize speech and vision jointly. We demonstrate human-like speech synthesis, synchronized lip motion, lively facial expressions and head pose; all generated from just text characters. The core premise of our approach lies in the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized speech intonation and facial dynamics (e.g., eyebrow motion). Our model is trained with flow matching, leading to expressive results and fast inference. In case of dyadic conversations, AV-Flow produces an always-on avatar, that actively listens and reacts to the audio-visual input of a user. Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars. Project page: https://aggelinacha.github.io/AV-Flow/
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2502.10088.pdf' target='_blank'>https://arxiv.org/pdf/2502.10088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Song, Felix Pabst, Ulrich Eck, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10088">Enhancing Patient Acceptance of Robotic Ultrasound through Conversational Virtual Agent and Immersive Visualizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic ultrasound systems can enhance medical diagnostics, but patient acceptance is a challenge. We propose a system combining an AI-powered conversational virtual agent with three mixed reality visualizations to improve trust and comfort. The virtual agent, powered by a large language model, engages in natural conversations and guides the ultrasound robot, enhancing interaction reliability. The visualizations include augmented reality, augmented virtuality, and fully immersive virtual reality, each designed to create patient-friendly experiences. A user study demonstrated significant improvements in trust and acceptance, offering valuable insights for designing mixed reality and virtual agents in autonomous medical procedures.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2412.01106.pdf' target='_blank'>https://arxiv.org/pdf/2412.01106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Xiang, Yudong Guo, Leipeng Hu, Boyang Guo, Yancheng Yuan, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01106">One Shot, One Talk: Whole-body Talking Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2410.21280.pdf' target='_blank'>https://arxiv.org/pdf/2410.21280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alicia Vidler, Toby Walsh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21280">TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel hybrid approach that augments Agent-Based Models (ABMs) with behaviors generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMs trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in financial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges, such as coordinating turn-taking between realistic LLM-based agents, and design challenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2409.14103.pdf' target='_blank'>https://arxiv.org/pdf/2409.14103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanghao Chen, Zeyu Wang, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14103">ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed tremendous progress in the 3D reconstruction of dynamic humans from a monocular video with the advent of neural rendering techniques. This task has a wide range of applications, including the creation of virtual characters for virtual reality (VR) environments. However, it is still challenging to reconstruct clear humans when the monocular video is affected by motion blur, particularly caused by rapid human motion (e.g., running, dancing), as often occurs in the wild. This leads to distinct inconsistency of shape and appearance for the rendered 3D humans, especially in the blurry regions with rapid motion, e.g., hands and legs. In this paper, we propose ExFMan, the first neural rendering framework that unveils the possibility of rendering high-quality humans in rapid motion with a hybrid frame-based RGB and bio-inspired event camera. The ``out-of-the-box'' insight is to leverage the high temporal information of event data in a complementary manner and adaptively reweight the effect of losses for both RGB frames and events in the local regions, according to the velocity of the rendered human. This significantly mitigates the inconsistency associated with motion blur in the RGB frames. Specifically, we first formulate a velocity field of the 3D body in the canonical space and render it to image space to identify the body parts with motion blur. We then propose two novel losses, i.e., velocity-aware photometric loss and velocity-relative event loss, to optimize the neural human for both modalities under the guidance of the estimated velocity. In addition, we incorporate novel pose regularization and alpha losses to facilitate continuous pose and clear boundary. Extensive experiments on synthetic and real-world datasets demonstrate that ExFMan can reconstruct sharper and higher quality humans.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2409.08738.pdf' target='_blank'>https://arxiv.org/pdf/2409.08738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Gao, Haochun Huai, Sena Yildiz-Degirmenci, Maria Bannert, Enkelejda Kasneci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08738">DataliVR: Transformation of Data Literacy Education through Virtual Reality with ChatGPT-Powered Enhancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data literacy is essential in today's data-driven world, emphasizing individuals' abilities to effectively manage data and extract meaningful insights. However, traditional classroom-based educational approaches often struggle to fully address the multifaceted nature of data literacy. As education undergoes digital transformation, innovative technologies such as Virtual Reality (VR) offer promising avenues for immersive and engaging learning experiences. This paper introduces DataliVR, a pioneering VR application aimed at enhancing the data literacy skills of university students within a contextual and gamified virtual learning environment. By integrating Large Language Models (LLMs) like ChatGPT as a conversational artificial intelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides personalized learning assistance, enriching user learning experiences. Our study employed an experimental approach, with chatbot availability as the independent variable, analyzing learning experiences and outcomes as dependent variables with a sample of thirty participants. Our approach underscores the effectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering data literacy skills. Moreover, our study examines the impact of the ChatGPT-based AI chatbot on users' learning, revealing significant effects on both learning experiences and outcomes. Our study presents a robust tool for fostering data literacy skills, contributing significantly to the digital advancement of data literacy education through cutting-edge VR and AI technologies. Moreover, our research provides valuable insights and implications for future research endeavors aiming to integrate LLMs (e.g., ChatGPT) into educational VR platforms.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2407.13038.pdf' target='_blank'>https://arxiv.org/pdf/2407.13038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaojie Bai, Te-Li Wang, Chenghui Li, Akshay Venkatesh, Tomas Simon, Chen Cao, Gabriel Schwartz, Ryan Wrench, Jason Saragih, Yaser Sheikh, Shih-En Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13038">Universal Facial Encoding of Codec Avatars from VR Headsets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Faithful real-time facial animation is essential for avatar-mediated telepresence in Virtual Reality (VR). To emulate authentic communication, avatar animation needs to be efficient and accurate: able to capture both extreme and subtle expressions within a few milliseconds to sustain the rhythm of natural conversations. The oblique and incomplete views of the face, variability in the donning of headsets, and illumination variation due to the environment are some of the unique challenges in generalization to unseen faces. In this paper, we present a method that can animate a photorealistic avatar in realtime from head-mounted cameras (HMCs) on a consumer VR headset. We present a self-supervised learning approach, based on a cross-view reconstruction objective, that enables generalization to unseen users. We present a lightweight expression calibration mechanism that increases accuracy with minimal additional cost to run-time efficiency. We present an improved parameterization for precise ground-truth generation that provides robustness to environmental variation. The resulting system produces accurate facial animation for unseen users wearing VR headsets in realtime. We compare our approach to prior face-encoding methods demonstrating significant improvements in both quantitative metrics and qualitative results.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2407.07284.pdf' target='_blank'>https://arxiv.org/pdf/2407.07284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07284">MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that learns a single neural representation for multiple identities, using only monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human avatars require per-identity optimization. However, learning a multi-identity representation presents advantages in robustly animating humans under arbitrary poses. We propose to construct a high-order tensor that combines all the learnable 3DGS parameters for all the training identities. By assuming a low-rank structure and factorizing the tensor, we model the complex rigid and non-rigid deformations of multiple subjects in a unified network, significantly reducing the total number of parameters. Our proposed approach leverages information from all the training identities and enables robust animation under challenging unseen poses, outperforming existing approaches. It can also be extended to learn unseen identities.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2407.05324.pdf' target='_blank'>https://arxiv.org/pdf/2407.05324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Peng, Yunfan Tao, Haoyu Zhan, Yudong Guo, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05324">PICA: Physics-Integrated Clothed Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PICA, a novel representation for high-fidelity animatable clothed human avatars with physics-accurate dynamics, even for loose clothing. Previous neural rendering-based representations of animatable clothed humans typically employ a single model to represent both the clothing and the underlying body. While efficient, these approaches often fail to accurately represent complex garment dynamics, leading to incorrect deformations and noticeable rendering artifacts, especially for sliding or loose garments. Furthermore, previous works represent garment dynamics as pose-dependent deformations and facilitate novel pose animations in a data-driven manner. This often results in outcomes that do not faithfully represent the mechanics of motion and are prone to generating artifacts in out-of-distribution poses. To address these issues, we adopt two individual 3D Gaussian Splatting (3DGS) models with different deformation characteristics, modeling the human body and clothing separately. This distinction allows for better handling of their respective motion characteristics. With this representation, we integrate a graph neural network (GNN)-based clothed body physics simulation module to ensure an accurate representation of clothing dynamics. Our method, through its carefully designed features, achieves high-fidelity rendering of clothed human bodies in complex and novel driving poses, significantly outperforming previous methods under the same settings.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2406.04629.pdf' target='_blank'>https://arxiv.org/pdf/2406.04629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenghao Chai, Chen Tang, Yongkang Wong, Mohan Kankanhalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04629">STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network Motion Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of 4D avatars (i.e., animated 3D avatars) from text description typically uses text-to-image (T2I) diffusion models to synthesize 3D avatars in the canonical space and subsequently applies animation with target motions. However, such an optimization-by-animation paradigm has several drawbacks. (1) For pose-agnostic optimization, the rendered images in canonical pose for naive Score Distillation Sampling (SDS) exhibit domain gap and cannot preserve view-consistency using only T2I priors, and (2) For post hoc animation, simply applying the source motions to target 3D avatars yields translation artifacts and misalignment. To address these issues, we propose Skeleton-aware Text-based 4D Avatar generation with in-network motion Retargeting (STAR). STAR considers the geometry and skeleton differences between the template mesh and target avatar, and corrects the mismatched source motion by resorting to the pretrained motion retargeting techniques. With the informatively retargeted and occlusion-aware skeleton, we embrace the skeleton-conditioned T2I and text-to-video (T2V) priors, and propose a hybrid SDS module to coherently provide multi-view and frame-consistent supervision signals. Hence, STAR can progressively optimize the geometry, texture, and motion in an end-to-end manner. The quantitative and qualitative experiments demonstrate our proposed STAR can synthesize high-quality 4D avatars with vivid animations that align well with the text description. Additional ablation studies shows the contributions of each component in STAR. The source code and demos are available at: \href{https://star-avatar.github.io}{https://star-avatar.github.io}.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2403.15227.pdf' target='_blank'>https://arxiv.org/pdf/2403.15227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15227">LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D face stylization have made significant strides in few to zero-shot settings. However, the degree of stylization achieved by existing methods is often not sufficient for practical applications because they are mostly based on statistical 3D Morphable Models (3DMM) with limited variations. To this end, we propose a method that can produce a highly stylized 3D face model with desired topology. Our methods train a surface deformation network with 3DMM and translate its domain to the target style using a paired exemplar. The network achieves stylization of the 3D face mesh by mimicking the style of the target using a differentiable renderer and directional CLIP losses. Additionally, during the inference process, we utilize a Mesh Agnostic Encoder (MAGE) that takes deformation target, a mesh of diverse topologies as input to the stylization process and encodes its shape into our latent space. The resulting stylized face model can be animated by commonly used 3DMM blend shapes. A set of quantitative and qualitative evaluations demonstrate that our method can produce highly stylized face meshes according to a given style and output them in a desired topology. We also demonstrate example applications of our method including image-based stylized avatar generation, linear interpolation of geometric styles, and facial animation of stylized avatars.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2403.10805.pdf' target='_blank'>https://arxiv.org/pdf/2403.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, Shunman Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10805">Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture. The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy features, represented as a unified latent feature, are fed into the AdaLN transformer. The AdaLN transformer introduces a conditional mechanism that applies a uniform function across all tokens, thereby effectively modeling the correlation between the fuzzy features and the gesture sequence. This module ensures a high level of gesture-speech synchronization while preserving naturalness. Finally, we employ the diffusion model to train and infer various gestures. Extensive subjective and objective evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's superior performance to the current state-of-the-art approaches. Persona-Gestor improves the system's usability and generalization capabilities, setting a new benchmark in speech-driven gesture synthesis and broadening the horizon for virtual human technology. Supplementary videos and code can be accessed at https://zf223669.github.io/Diffmotion-v2-website/
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2401.12900.pdf' target='_blank'>https://arxiv.org/pdf/2401.12900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12900">PSAvatar: A Point-based Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress, achieving real-time high-fidelity head avatar animation is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency. Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a resolution of 512 $\times$ 512 ).
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2312.05210.pdf' target='_blank'>https://arxiv.org/pdf/2312.05210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Wang, BoÅ¾idar AntiÄ, Andreas Geiger, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05210">IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present IntrinsicAvatar, a novel approach to recovering the intrinsic properties of clothed human avatars including geometry, albedo, material, and environment lighting from only monocular videos. Recent advancements in human-based neural rendering have enabled high-quality geometry and appearance reconstruction of clothed humans from just monocular videos. However, these methods bake intrinsic properties such as albedo, material, and environment lighting into a single entangled neural representation. On the other hand, only a handful of works tackle the problem of estimating geometry and disentangled appearance properties of clothed humans from monocular videos. They usually achieve limited quality and disentanglement due to approximations of secondary shading effects via learned MLPs. In this work, we propose to model secondary shading effects explicitly via Monte-Carlo ray tracing. We model the rendering process of clothed humans as a volumetric scattering process, and combine ray tracing with body articulation. Our approach can recover high-quality geometry, albedo, material, and lighting properties of clothed humans from a single monocular video, without requiring supervised pre-training using ground truth materials. Furthermore, since we explicitly model the volumetric scattering process and ray tracing, our model naturally generalizes to novel poses, enabling animation of the reconstructed avatar in novel lighting conditions.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2312.02214.pdf' target='_blank'>https://arxiv.org/pdf/2312.02214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02214">FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D Gaussian field embedded in the surface of a parametric face model and learn extra spatial offset to model non-surface regions and subtle facial details. While full use of geometric priors can capture high-frequency facial details and preserve exaggerated expressions, proper initialization can help reduce the number of Gaussians, thus enabling super-fast rendering speed. Extensive experimental results demonstrate that FlashAvatar outperforms existing works regarding visual quality and personalized details and is almost an order of magnitude faster in rendering speed. Project page: https://ustc3dv.github.io/FlashAvatar/
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2308.12642.pdf' target='_blank'>https://arxiv.org/pdf/2308.12642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Ngo, Daniel Phelps, Derrick Lai, Thanyared Wong, Lucas Mathias, Anish Shivamurthy, Mustafa Ajmal, Minghao Liu, James Davis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12642">Tag-Based Annotation for Avatar Face Creation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, digital avatars can be created manually using human images as reference. Systems such as Bitmoji are excellent producers of detailed avatar designs, with hundreds of choices for customization. A supervised learning model could be trained to generate avatars automatically, but the hundreds of possible options create difficulty in securing non-noisy data to train a model. As a solution, we train a model to produce avatars from human images using tag-based annotations. This method provides better annotator agreement, leading to less noisy data and higher quality model predictions. Our contribution is an application of tag-based annotation to train a model for avatar face creation. We design tags for 3 different facial facial features offered by Bitmoji, and train a model using tag-based annotation to predict the nose.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2308.10899.pdf' target='_blank'>https://arxiv.org/pdf/2308.10899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10899">TADA! Text to Animatable Digital Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent alignment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2306.07011.pdf' target='_blank'>https://arxiv.org/pdf/2306.07011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Wu, Pan Hui, Pengyuan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07011">Deepfake in the Metaverse: An Outlook Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We envision deepfake technologies, which synthesize realistic fake images and videos, will play an important role in the future metaverse. While enhancing users' immersion and experience with synthesized virtual characters and scenes, deepfake can cause serious consequences if used for fraud, impersonation, and dissemination of fake information. In this paper, we introduce the principles, applications, and risks of deepfake technology, and propose some countermeasures to help users and developers in the metaverse deal with the challenges brought by deepfake technologies. Further, we provide an outlook on the future development of deepfake in the metaverse.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2305.19245.pdf' target='_blank'>https://arxiv.org/pdf/2305.19245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thu Nguyen-Phuoc, Gabriel Schwartz, Yuting Ye, Stephen Lombardi, Lei Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19245">AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method that can quickly adapt dynamic 3D avatars to arbitrary text descriptions of novel styles. Among existing approaches for avatar stylization, direct optimization methods can produce excellent results for arbitrary styles but they are unpleasantly slow. Furthermore, they require redoing the optimization process from scratch for every new input. Fast approximation methods using feed-forward networks trained on a large dataset of style images can generate results for new inputs quickly, but tend not to generalize well to novel styles and fall short in quality. We therefore investigate a new approach, AlteredAvatar, that combines those two approaches using the meta-learning framework. In the inner loop, the model learns to optimize to match a single target style well; while in the outer loop, the model learns to stylize efficiently across many styles. After training, AlteredAvatar learns an initialization that can quickly adapt within a small number of update steps to a novel style, which can be given using texts, a reference image, or a combination of both. We show that AlteredAvatar can achieve a good balance between speed, flexibility and quality, while maintaining consistency across a wide range of novel views and facial expressions.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2305.12411.pdf' target='_blank'>https://arxiv.org/pdf/2305.12411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12411">Synthesizing Diverse Human Motions in 3D Indoor Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for populating 3D indoor scenes with virtual humans that can navigate in the environment and interact with objects in a realistic manner. Existing approaches rely on training sequences that contain captured human motions and the 3D scenes they interact with. However, such interaction data are costly, difficult to capture, and can hardly cover all plausible human-scene interactions in complex environments. To address these challenges, we propose a reinforcement learning-based approach that enables virtual humans to navigate in 3D scenes and interact with objects realistically and autonomously, driven by learned motion control policies. The motion control policies employ latent motion action spaces, which correspond to realistic motion primitives and are learned from large-scale motion capture data using a powerful generative motion model. For navigation in a 3D environment, we propose a scene-aware policy with novel state and reward designs for collision avoidance. Combined with navigation mesh-based path-finding algorithms to generate intermediate waypoints, our approach enables the synthesis of diverse human motions navigating in 3D indoor scenes and avoiding obstacles. To generate fine-grained human-object interactions, we carefully curate interaction goal guidance using a marker-based body representation and leverage features based on the signed distance field (SDF) to encode human-scene proximity relations. Our method can synthesize realistic and diverse human-object interactions (e.g.,~sitting on a chair and then getting up) even for out-of-distribution test scenarios with different object shapes, orientations, starting body positions, and poses. Experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of both motion naturalness and diversity. Code and video results are available at: https://zkf1997.github.io/DIMOS.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2305.00936.pdf' target='_blank'>https://arxiv.org/pdf/2305.00936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihun Cha, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00936">Generating Texture for 3D Human Avatar from a Single Image using Sampling and Refinement Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been significant progress in generating an animatable 3D human avatar from a single image. However, recovering texture for the 3D human avatar from a single image has been relatively less addressed. Because the generated 3D human avatar reveals the occluded texture of the given image as it moves, it is critical to synthesize the occluded texture pattern that is unseen from the source image. To generate a plausible texture map for 3D human avatars, the occluded texture pattern needs to be synthesized with respect to the visible texture from the given image. Moreover, the generated texture should align with the surface of the target 3D mesh. In this paper, we propose a texture synthesis method for a 3D human avatar that incorporates geometry information. The proposed method consists of two convolutional networks for the sampling and refining process. The sampler network fills in the occluded regions of the source image and aligns the texture with the surface of the target 3D mesh using the geometry information. The sampled texture is further refined and adjusted by the refiner network. To maintain the clear details in the given image, both sampled and refined texture is blended to produce the final texture map. To effectively guide the sampler network to achieve its goal, we designed a curriculum learning scheme that starts from a simple sampling task and gradually progresses to the task where the alignment needs to be considered. We conducted experiments to show that our method outperforms previous methods qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2302.13279.pdf' target='_blank'>https://arxiv.org/pdf/2302.13279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingchao Yang, Takafumi Taketomi, Yoshihiro Kanamori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13279">Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial makeup enriches the beauty of not only real humans but also virtual characters; therefore, makeup for 3D facial models is highly in demand in productions. However, painting directly on 3D faces and capturing real-world makeup are costly, and extracting makeup from 2D images often struggles with shading effects and occlusions. This paper presents the first method for extracting makeup for 3D facial models from a single makeup portrait. Our method consists of the following three steps. First, we exploit the strong prior of 3D morphable models via regression-based inverse rendering to extract coarse materials such as geometry and diffuse/specular albedos that are represented in the UV space. Second, we refine the coarse materials, which may have missing pixels due to occlusions. We apply inpainting and optimization. Finally, we extract the bare skin, makeup, and an alpha matte from the diffuse albedo. Our method offers various applications for not only 3D facial models but also 2D portrait images. The extracted makeup is well-aligned in the UV space, from which we build a large-scale makeup dataset and a parametric makeup model for 3D faces. Our disentangled materials also yield robust makeup transfer and illumination-aware makeup interpolation/removal without a reference image.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2301.06471.pdf' target='_blank'>https://arxiv.org/pdf/2301.06471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Lavit Nicora, Sebastian Beyrodt, Dimitra Tsovaltzi, Fabrizio Nunnari, Patrick Gebhard, Matteo Malosio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06471">Towards social embodied cobots: The integration of an industrial cobot with a social virtual agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of the physical capabilities of an industrial collaborative robot with a social virtual character may represent a viable solution to enhance the workers' perception of the system as an embodied social entity and increase social engagement and well-being at the workplace. An online study was setup using prerecorded video interactions in order to pilot potential advantages of different embodied configurations of the cobot-avatar system in terms of perceptions of Social Presence, cobot-avatar Unity and Social Role of the system, and explore the relation of these. In particular, two different configurations were explored and compared: the virtual character was displayed either on a tablet strapped onto the base of the cobot or on a large TV screen positioned at the back of the workcell. The results imply that participants showed no clear preference based on the constructs, and both configurations fulfill these basic criteria. In terms of the relations between the constructs, there were strong correlations between perception of Social Presence, Unity and Social Role (Collegiality). This gives a valuable insight into the role of these constructs in the perception of cobots as embodied social entities, and towards building cobots that support well-being at the workplace.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2203.04179.pdf' target='_blank'>https://arxiv.org/pdf/2203.04179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Hanisch, Evelyn Muschter, Admantini Hatzipanayioti, Shu-Chen Li, Thorsten Strufe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.04179">Understanding Person Identification through Gait</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait recognition is the process of identifying humans from their bipedal locomotion such as walking or running. As such, gait data is privacy sensitive information and should be anonymized where possible. With the rise of higher quality gait recording techniques, such as depth cameras or motion capture suits, an increasing amount of detailed gait data is captured and processed. The introduction and rise of the Metaverse is an example of a potentially popular application scenario in which the gait of users is transferred onto digital avatars. As a first step towards developing effective anonymization techniques for high-quality gait data, we study different aspects of movement data to quantify their contribution to gait recognition. We first extract categories of features from the literature on human gait perception and then design experiments for each category to assess how much the information they contain contributes to recognition success. We evaluated the utility of gait perturbation by means of naturalness ratings in a user study. Our results show that gait anonymization will be challenging, as the data is highly redundant and inter-dependent.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2507.19359.pdf' target='_blank'>https://arxiv.org/pdf/2507.19359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanmiao Liu, Esam Ghaleb, AslÄ± ÃzyÃ¼rek, Zerrin Yumak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19359">SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at https://semgesture.github.io/.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2507.18758.pdf' target='_blank'>https://arxiv.org/pdf/2507.18758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Liu, Shengjun Zhang, Chensheng Dai, Yang Chen, Hao Liu, Chen Li, Yueqi Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18758">Learning Efficient and Generalizable Human Representation with Human Gaussian Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling animatable human avatars from videos is a long-standing and challenging problem. While conventional methods require per-instance optimization, recent feed-forward methods have been proposed to generate 3D Gaussians with a learnable network. However, these methods predict Gaussians for each frame independently, without fully capturing the relations of Gaussians from different timestamps. To address this, we propose Human Gaussian Graph to model the connection between predicted Gaussians and human SMPL mesh, so that we can leverage information from all frames to recover an animatable human representation. Specifically, the Human Gaussian Graph contains dual layers where Gaussians are the first layer nodes and mesh vertices serve as the second layer nodes. Based on this structure, we further propose the intra-node operation to aggregate various Gaussians connected to one mesh vertex, and inter-node operation to support message passing among mesh node neighbors. Experimental results on novel view synthesis and novel pose animation demonstrate the efficiency and generalization of our method.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2505.20582.pdf' target='_blank'>https://arxiv.org/pdf/2505.20582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhao, Chunjiang Liu, Haoyu Chen, Bhiksha Raj, Min Xu, Tadas Baltrusaitis, Mitch Rundle, HsiangTao Wu, Kamran Ghasedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20582">Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face reenactment and portrait relighting are essential tasks in portrait editing, yet they are typically addressed independently, without much synergy. Most face reenactment methods prioritize motion control and multiview consistency, while portrait relighting focuses on adjusting shading effects. To take advantage of both geometric consistency and illumination awareness, we introduce Total-Editing, a unified portrait editing framework that enables precise control over appearance, motion, and lighting. Specifically, we design a neural radiance field decoder with intrinsic decomposition capabilities. This allows seamless integration of lighting information from portrait images or HDR environment maps into synthesized portraits. We also incorporate a moving least squares based deformation field to enhance the spatiotemporal coherence of avatar motion and shading effects. With these innovations, our unified framework significantly improves the quality and realism of portrait editing results. Further, the multi-source nature of Total-Editing supports more flexible applications, such as illumination transfer from one portrait to another, or portrait animation with customized backgrounds.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2505.05672.pdf' target='_blank'>https://arxiv.org/pdf/2505.05672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05672">TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2505.05376.pdf' target='_blank'>https://arxiv.org/pdf/2505.05376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rachmadio Noval Lazuardi, Artem Sevastopolsky, Egor Zakharov, Matthias Niessner, Vanessa Sklyarova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05376">GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel method that reconstructs hair strands directly from colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair strand reconstruction is a fundamental problem in computer vision and graphics that can be used for high-fidelity digital avatar synthesis, animation, and AR/VR applications. However, accurately recovering hair strands from raw scan data remains challenging due to human hair's complex and fine-grained structure. Existing methods typically rely on RGB captures, which can be sensitive to the environment and can be a challenging domain for extracting the orientation of guiding strands, especially in the case of challenging hairstyles. To reconstruct the hair purely from the observed geometry, our method finds sharp surface features directly on the scan and estimates strand orientation through a neural 2D line detector applied to the renderings of scan shading. Additionally, we incorporate a diffusion prior trained on a diverse set of synthetic hair scans, refined with an improved noise schedule, and adapted to the reconstructed contents via a scan-specific text prompt. We demonstrate that this combination of supervision signals enables accurate reconstruction of both simple and intricate hairstyles without relying on color information. To facilitate further research, we introduce Strands400, the largest publicly available dataset of hair strands with detailed surface geometry extracted from real-world data, which contains reconstructed hair strands from the scans of 400 subjects.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2504.15835.pdf' target='_blank'>https://arxiv.org/pdf/2504.15835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Wu, Malte Prinzler, Xiaogang Jin, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15835">Text-based Animatable 3D Avatars with Morphable Model Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of high-quality, animatable 3D head avatars from text has enormous potential in content creation applications such as games, movies, and embodied virtual assistants. Current text-to-3D generation methods typically combine parametric head models with 2D diffusion models using score distillation sampling to produce 3D-consistent results. However, they struggle to synthesize realistic details and suffer from misalignments between the appearance and the driving parametric model, resulting in unnatural animation results. We discovered that these limitations stem from ambiguities in the 2D diffusion predictions during 3D avatar distillation, specifically: i) the avatar's appearance and geometry is underconstrained by the text input, and ii) the semantic alignment between the predictions and the parametric head model is insufficient because the diffusion model alone cannot incorporate information from the parametric model. In this work, we propose a novel framework, AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with morphable model alignment, and introduce two key strategies to address these challenges. First, we tackle appearance and geometry ambiguities by utilizing prior information from a pretrained text-to-3D model to initialize a 3D avatar with robust appearance, geometry, and rigging relationships to the morphable model. Second, we refine the initial 3D avatar for dynamic expressions using a ControlNet that is conditioned on semantic and normal maps of the morphable model to ensure accurate alignment. As a result, our method outperforms existing approaches in terms of synthesis quality, alignment, and animation fidelity. Our experiments show that the proposed method advances the state of the art in text-based, animatable 3D head avatar generation.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2504.04842.pdf' target='_blank'>https://arxiv.org/pdf/2504.04842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04842">FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2502.19441.pdf' target='_blank'>https://arxiv.org/pdf/2502.19441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengtian Li, Shengxiang Yao, Chen Kai, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19441">AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Gaussian-based human body reconstruction have achieved notable success in creating animatable avatars. However, there are ongoing challenges to fully exploit the SMPL model's prior knowledge and enhance the visual fidelity of these models to achieve more refined avatar reconstructions. In this paper, we introduce AniGaussian which addresses the above issues with two insights. First, we propose an innovative pose guided deformation strategy that effectively constrains the dynamic Gaussian avatar with SMPL pose guidance, ensuring that the reconstructed model not only captures the detailed surface nuances but also maintains anatomical correctness across a wide range of motions. Second, we tackle the expressiveness limitations of Gaussian models in representing dynamic human bodies. We incorporate rigid-based priors from previous works to enhance the dynamic transform capabilities of the Gaussian model. Furthermore, we introduce a split-with-scale strategy that significantly improves geometry quality. The ablative study experiment demonstrates the effectiveness of our innovative model design. Through extensive comparisons with existing methods, AniGaussian demonstrates superior performance in both qualitative result and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2502.01553.pdf' target='_blank'>https://arxiv.org/pdf/2502.01553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiluo Wei, Gareth Tyson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01553">Virtual Stars, Real Fans: Understanding the VTuber Ecosystem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Livestreaming by VTubers -- animated 2D/3D avatars controlled by real individuals -- have recently garnered substantial global followings and achieved significant monetary success. Despite prior research highlighting the importance of realism in audience engagement, VTubers deliberately conceal their identities, cultivating dedicated fan communities through virtual personas. While previous studies underscore that building a core fan community is essential to a streamer's success, we lack an understanding of the characteristics of viewers of this new type of streamer. Gaining a deeper insight into these viewers is critical for VTubers to enhance audience engagement, foster a more robust fan base, and attract a larger viewership. To address this gap, we conduct a comprehensive analysis of VTuber viewers on Bilibili, a leading livestreaming platform where nearly all VTubers in China stream. By compiling a first-of-its-kind dataset covering 2.7M livestreaming sessions, we investigate the characteristics, engagement patterns, and influence of VTuber viewers. Our research yields several valuable insights, which we then leverage to develop a tool to "recommend" future subscribers to VTubers. By reversing the typical approach of recommending streams to viewers, this tool assists VTubers in pinpointing potential future fans to pay more attention to, and thereby effectively growing their fan community.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2411.16729.pdf' target='_blank'>https://arxiv.org/pdf/2411.16729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Siyuan Zhao, Naye Ji, Zhaohan Wang, Jingmei Wu, Fuxing Gao, Zhenqing Ye, Leyao Yan, Lanxin Dai, Weidong Geng, Xin Lyu, Bozuo Zhao, Dingguo Yu, Hui Du, Bin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16729">DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation using transformer-based generative models represents a rapidly advancing area within virtual human creation. However, existing models face significant challenges due to their quadratic time and space complexities, limiting scalability and efficiency. To address these limitations, we introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamba-2 architecture. DiM-Gestor features a dual-component framework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping module, both built on the Mamba-2. The fuzzy feature extractor, integrated with a Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit, continuous speech features. These features are synthesized into a unified latent representation and then processed by the speech-to-gesture mapping module. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced Mamba-2 mechanism to uniformly apply transformations across all sequence tokens. This enables precise modeling of the nuanced interplay between speech features and gesture dynamics. We utilize a diffusion model to train and infer diverse gesture outputs. Extensive subjective and objective evaluations conducted on the newly released Chinese Co-Speech Gestures dataset corroborate the efficacy of our proposed model. Compared with Transformer-based architecture, the assessments reveal that our approach delivers competitive results and significantly reduces memory usage, approximately 2.4 times, and enhances inference speeds by 2 to 4 times. Additionally, we released the CCG dataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six styles across five scenarios) of 3D full-body skeleton gesture motion performed by professional Chinese TV broadcasters.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2409.14778.pdf' target='_blank'>https://arxiv.org/pdf/2409.14778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Egor Zakharov, Vanessa Sklyarova, Michael Black, Giljoo Nam, Justus Thies, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14778">Human Hair Reconstruction with Strand-Aligned 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2409.14393.pdf' target='_blank'>https://arxiv.org/pdf/2409.14393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14393">MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crafting a single, versatile physics-based controller that can breathe life into interactive characters across a wide spectrum of scenarios represents an exciting frontier in character animation. An ideal controller should support diverse control modalities, such as sparse target keyframes, text instructions, and scene information. While previous works have proposed physically simulated, scene-aware control models, these systems have predominantly focused on developing controllers that each specializes in a narrow set of tasks and control modalities. This work presents MaskedMimic, a novel approach that formulates physics-based character control as a general motion inpainting problem. Our key insight is to train a single unified model to synthesize motions from partial (masked) motion descriptions, such as masked keyframes, objects, text descriptions, or any combination thereof. This is achieved by leveraging motion tracking data and designing a scalable training method that can effectively utilize diverse motion descriptions to produce coherent animations. Through this process, our approach learns a physics-based controller that provides an intuitive control interface without requiring tedious reward engineering for all behaviors of interest. The resulting controller supports a wide range of control modalities and enables seamless transitions between disparate tasks. By unifying character control through motion inpainting, MaskedMimic creates versatile virtual characters. These characters can dynamically adapt to complex scenes and compose diverse motions on demand, enabling more interactive and immersive experiences.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2409.11602.pdf' target='_blank'>https://arxiv.org/pdf/2409.11602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuxuan Zhang, Bermet Burkanova, Lawrence H. Kim, Lauren Yip, Ugo Cupcic, StÃ©phane LallÃ©e, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11602">React to This! How Humans Challenge Interactive Agents using Nonverbal Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How do people use their faces and bodies to test the interactive abilities of a robot? Making lively, believable agents is often seen as a goal for robots and virtual agents but believability can easily break down. In this Wizard-of-Oz (WoZ) study, we observed 1169 nonverbal interactions between 20 participants and 6 types of agents. We collected the nonverbal behaviors participants used to challenge the characters physically, emotionally, and socially. The participants interacted freely with humanoid and non-humanoid forms: a robot, a human, a penguin, a pufferfish, a banana, and a toilet. We present a human behavior codebook of 188 unique nonverbal behaviors used by humans to test the virtual characters. The insights and design strategies drawn from video observations aim to help build more interaction-aware and believable robots, especially when humans push them to their limits.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2408.10588.pdf' target='_blank'>https://arxiv.org/pdf/2408.10588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijing Shao, Duotun Wang, Qing-Yao Tian, Yao-Dong Yang, Hengyu Meng, Zeyu Cai, Bo Dong, Yu Zhang, Kang Zhang, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10588">DEGAS: Detailed Expressions on Full-Body Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although neural rendering has made significant advances in creating lifelike, animatable full-body and head avatars, incorporating detailed expressions into full-body avatars remains largely unexplored. We present DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for full-body avatars with rich facial expressions. Trained on multiview videos of a given subject, our method learns a conditional variational autoencoder that takes both the body motion and facial expression as driving signals to generate Gaussian maps in the UV layout. To drive the facial expressions, instead of the commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to adopt the expression latent space trained solely on 2D portrait images, bridging the gap between 2D talking faces and 3D avatars. Leveraging the rendering capability of 3DGS and the rich expressiveness of the expression latent space, the learned avatars can be reenacted to reproduce photorealistic rendering images with subtle and accurate facial expressions. Experiments on an existing dataset and our newly proposed dataset of full-body talking avatars demonstrate the efficacy of our method. We also propose an audio-driven extension of our method with the help of 2D talking faces, opening new possibilities for interactive AI agents.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2408.06019.pdf' target='_blank'>https://arxiv.org/pdf/2408.06019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06019">HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2408.00370.pdf' target='_blank'>https://arxiv.org/pdf/2408.00370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Bozuo Zhao, Jingmei Wu, Yanbing Jiang, Hui Du, Zhenqing Ye, Jiayang Zhu, WeiFan Zhong, Leyao Yan, Xiaomeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00370">DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2 framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation is an emerging domain within virtual human creation, where current methods predominantly utilize Transformer-based architectures that necessitate extensive memory and are characterized by slow inference speeds. In response to these limitations, we propose \textit{DiM-Gestures}, a novel end-to-end generative model crafted to create highly personalized 3D full-body gestures solely from raw speech audio, employing Mamba-based architectures. This model integrates a Mamba-based fuzzy feature extractor with a non-autoregressive Adaptive Layer Normalization (AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba framework and a WavLM pre-trained model, autonomously derives implicit, continuous fuzzy features, which are then unified into a singular latent feature. This feature is processed by the AdaLN Mamba-2, which implements a uniform conditional mechanism across all tokens to robustly model the interplay between the fuzzy features and the resultant gesture sequence. This innovative approach guarantees high fidelity in gesture-speech synchronization while maintaining the naturalness of the gestures. Employing a diffusion model for training and inference, our framework has undergone extensive subjective and objective evaluations on the ZEGGS and BEAT datasets. These assessments substantiate our model's enhanced performance relative to contemporary state-of-the-art methods, demonstrating competitive outcomes with the DiTs architecture (Persona-Gestors) while optimizing memory usage and accelerating inference speed.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2407.07992.pdf' target='_blank'>https://arxiv.org/pdf/2407.07992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Yun, Jose Luis Ponton, Alejandro Beacco, Carlos Andujar, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07992">Exploring the Role of Expected Collision Feedback in Crowded Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An increasing number of virtual reality applications require environments that emulate real-world conditions. These environments often involve dynamic virtual humans showing realistic behaviors. Understanding user perception and navigation among these virtual agents is key for designing realistic and effective environments featuring groups of virtual humans. While collision risk significantly influences human locomotion in the real world, this risk is largely absent in virtual settings. This paper studies the impact of the expected collision feedback on user perception and interaction with virtual crowds. We examine the effectiveness of commonly used collision feedback techniques (auditory cues and tactile vibrations) as well as inducing participants to expect that a physical bump with a real person might occur, as if some virtual humans actually correspond to real persons embodied into them and sharing the same physical space. Our results indicate that the expected collision feedback significantly influences both participant behavior (encompassing global navigation and local movements) and subjective perceptions of presence and copresence. Specifically, the introduction of a perceived risk of actual collision was found to significantly impact global navigation strategies and increase the sense of presence. Auditory cues had a similar effect on global navigation and additionally enhanced the sense of copresence. In contrast, vibrotactile feedback was primarily effective in influencing local movements.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2406.19070.pdf' target='_blank'>https://arxiv.org/pdf/2406.19070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Xuan, Xinyang Li, Gongxin Yao, Shiwei Zhou, Donghui Sun, Xiaoxin Chen, Yu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19070">FAGhead: Fully Animate Gaussian Head from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity reconstruction of 3D human avatars has a wild application in visual reality. In this paper, we introduce FAGhead, a method that enables fully controllable human portraits from monocular videos. We explicit the traditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to reconstruct with complex expressions. Furthermore, we employ a novel Point-based Learnable Representation Field (PLRF) with learnable Gaussian point positions to enhance reconstruction performance. Meanwhile, to effectively manage the edges of avatars, we introduced the alpha rendering to supervise the alpha value of each pixel. Extensive experimental results on the open-source datasets and our capturing datasets demonstrate that our approach is able to generate high-fidelity 3D head avatars and fully control the expression and pose of the virtual avatars, which is outperforming than existing works.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2406.04254.pdf' target='_blank'>https://arxiv.org/pdf/2406.04254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salvatore Esposito, Qingshan Xu, Kacper Kania, Charlie Hewitt, Octave Mariotti, Lohit Petikam, Julien Valentin, Arno Onken, Oisin Mac Aodha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04254">GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections. Most existing approaches predict volumetric density to render multi-view consistent images. By employing volumetric rendering using neural radiance fields, they inherit a key limitation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes. To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner. Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF). This allows us to introduce useful priors to generate valid meshes. However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios. To alleviate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF. Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes. For evaluation, we introduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges presented by real-world datasets, which often lack 3D consistency and do not cover all camera angles. Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2405.11993.pdf' target='_blank'>https://arxiv.org/pdf/2405.11993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11993">GGAvatar: Geometric Adjustment of Gaussian Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2404.00524.pdf' target='_blank'>https://arxiv.org/pdf/2404.00524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00524">TexVocab: Texture Vocabulary-conditioned Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at https://texvocab.github.io/.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2403.09326.pdf' target='_blank'>https://arxiv.org/pdf/2403.09326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Xiaohang Zhan, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09326">HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current text-to-avatar methods often rely on implicit representations (e.g., NeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit and animate in graphics software. This paper introduces a novel framework for generating stylized head avatars from text guidance, which leverages locally learnable mesh deformation and 2D diffusion priors to achieve high-quality digital assets for attribute-preserving manipulation. Given a template mesh, our method represents mesh deformation with per-face Jacobians and adaptively modulates local deformation using a learnable vector field. This vector field enables anisotropic scaling while preserving the rotation of vertices, which can better express identity and geometric details. We employ landmark- and contour-based regularization terms to balance the expressiveness and plausibility of generated avatars from multiple views without relying on any specific shape prior. Our framework can generate realistic shapes and textures that can be further edited via text, while supporting seamless editing using the preserved attributes from the template mesh, such as 3DMM parameters, blendshapes, and UV coordinates. Extensive experiments demonstrate that our framework can generate diverse and expressive head avatars with high-quality meshes that artists can easily manipulate in graphics software, facilitating downstream applications such as efficient asset creation and animation with preserved attributes.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2402.10636.pdf' target='_blank'>https://arxiv.org/pdf/2402.10636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunsoo Cha, Byungjun Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10636">PEGASUS: Personalized Generative 3D Avatars with Composable Attributes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PEGASUS, a method for constructing a personalized generative 3D face avatar from monocular video sources. Our generative 3D avatar enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) while preserving the identity. Our approach consists of two stages: synthetic database generation and constructing a personalized generative avatar. We generate a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing the attributes from monocular videos of diverse identities. Then, we build a person-specific generative 3D avatar that can modify its attributes continuously while preserving its identity. Through extensive experiments, we demonstrate that our method of generating a synthetic database and creating a 3D generative avatar is the most effective in preserving identity while achieving high realism. Subsequently, we introduce a zero-shot approach to achieve the same goal of generative modeling more efficiently by leveraging a previously constructed personalized generative model.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2401.12979.pdf' target='_blank'>https://arxiv.org/pdf/2401.12979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeksoo Kim, Byungjun Kim, Shunsuke Saito, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12979">GALA: Generating Animatable Layered Assets from a Single Scan</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2401.08739.pdf' target='_blank'>https://arxiv.org/pdf/2401.08739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08739">EgoGen: An Egocentric Synthetic Data Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2312.04784.pdf' target='_blank'>https://arxiv.org/pdf/2312.04784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Rao, Eduardo Perez Pellitero, Benjamin Busam, Yiren Zhou, Jifei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04784">Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D avatar generation excel with multi-view supervision for photorealistic models. However, monocular counterparts lag in quality despite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB is a fully-differentiable pipeline that learns high-fidelity 3D human avatars from just a single RGB video. A pose-conditioned deformable NeRF is optimized to volumetrically represent a human subject in canonical T-pose. The canonical representation is then leveraged to efficiently associate neural textures using 2D-3D correspondences. This enables the separation of diffused color generation and lighting correction branches that jointly compose an RGB prediction. The design allows to control intermediate results for human pose, body shape, texture, and lighting with text prompts. An image-conditioned diffusion model thereby helps to animate appearance and pose of the 3D avatar to create video sequences with previously unseen human motion. Extensive experiments show that ReCaLaB outperforms previous monocular approaches in terms of image quality for image synthesis tasks. Moreover, natural language offers an intuitive user interface for creative manipulation of 3D human avatars.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2312.02902.pdf' target='_blank'>https://arxiv.org/pdf/2312.02902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo PÃ©rez-Pellitero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02902">HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, a model that uses 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit 3DGS representation with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, surpassing baselines by up to 2dB, while accelerating rendering speed by over x10.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2311.17113.pdf' target='_blank'>https://arxiv.org/pdf/2311.17113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo PÃ©rez-Pellitero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17113">Human Gaussian Splatting: Real-time Rendering of Animatable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the problem of real-time rendering of photorealistic human body avatars learned from multi-view videos. While the classical approaches to model and render virtual humans generally use a textured mesh, recent research has developed neural body representations that achieve impressive visual quality. However, these models are difficult to render in real-time and their quality degrades when the character is animated with body poses different than the training observations. We propose an animatable human model based on 3D Gaussian Splatting, that has recently emerged as a very efficient alternative to neural radiance fields. The body is represented by a set of gaussian primitives in a canonical space which is deformed with a coarse to fine approach that combines forward skinning and local non-rigid refinement. We describe how to learn our Human Gaussian Splatting (HuGS) model in an end-to-end fashion from multi-view observations, and evaluate it against the state-of-the-art approaches for novel pose synthesis of clothed body. Our method achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4 dataset while being able to render in real-time (80 fps for 512x512 resolution).
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2310.16287.pdf' target='_blank'>https://arxiv.org/pdf/2310.16287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejas S. Prabhune, Peter Wu, Bohan Yu, Gopala K. Anumanchipalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16287">Towards Streaming Speech-to-Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Streaming speech-to-avatar synthesis creates real-time animations for a virtual character from audio data. Accurate avatar representations of speech are important for the visualization of sound in linguistics, phonetics, and phonology, visual feedback to assist second language acquisition, and virtual embodiment for paralyzed patients. Previous works have highlighted the capability of deep articulatory inversion to perform high-quality avatar animation using electromagnetic articulography (EMA) features. However, these models focus on offline avatar synthesis with recordings rather than real-time audio, which is necessary for live avatar visualization or embodiment. To address this issue, we propose a method using articulatory inversion for streaming high quality facial and inner-mouth avatar animation from real-time audio. Our approach achieves 130ms average streaming latency for every 0.1 seconds of audio with a 0.792 correlation with ground truth articulations. Finally, we show generated mouth and tongue animations to demonstrate the efficacy of our methodology.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2309.05782.pdf' target='_blank'>https://arxiv.org/pdf/2309.05782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Grishchenko, Geng Yan, Eduard Gabriel Bazavan, Andrei Zanfir, Nikolai Chinaev, Karthik Raveendran, Matthias Grundmann, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05782">Blendshapes GHUM: Real-time Monocular Facial Blendshape Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Blendshapes GHUM, an on-device ML pipeline that predicts 52 facial blendshape coefficients at 30+ FPS on modern mobile phones, from a single monocular RGB image and enables facial motion capture applications like virtual avatars. Our main contributions are: i) an annotation-free offline method for obtaining blendshape coefficients from real-world human scans, ii) a lightweight real-time model that predicts blendshape coefficients based on facial landmarks.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2308.05995.pdf' target='_blank'>https://arxiv.org/pdf/2308.05995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05995">Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2307.14770.pdf' target='_blank'>https://arxiv.org/pdf/2307.14770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Wu, Hao Xu, Xiangjun Tang, Yue Shangguan, Hongbo Fu, Xiaogang Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14770">3DPortraitGAN: Learning One-Quarter Headshot 3D GANs from a Single-View Portrait Dataset with Diverse Body Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D-aware face generators are typically trained on 2D real-life face image datasets that primarily consist of near-frontal face data, and as such, they are unable to construct one-quarter headshot 3D portraits with complete head, neck, and shoulder geometry. Two reasons account for this issue: First, existing facial recognition methods struggle with extracting facial data captured from large camera angles or back views. Second, it is challenging to learn a distribution of 3D portraits covering the one-quarter headshot region from single-view data due to significant geometric deformation caused by diverse body poses. To this end, we first create the dataset 360Â°-Portrait-HQ (360Â°PHQ for short) which consists of high-quality single-view real portraits annotated with a variety of camera parameters (the yaw angles span the entire 360Â° range) and body poses. We then propose 3DPortraitGAN, the first 3D-aware one-quarter headshot portrait generator that learns a canonical 3D avatar distribution from the 360Â°PHQ dataset with body pose self-learning. Our model can generate view-consistent portrait images from all camera angles with a canonical one-quarter headshot 3D representation. Our experiments show that the proposed framework can accurately predict portrait body poses and generate view-consistent, realistic portrait images with complete geometry from all camera angles.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2307.00804.pdf' target='_blank'>https://arxiv.org/pdf/2307.00804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongjin Luo, Dong Du, Heming Zhu, Yizhou Yu, Hongbo Fu, Xiaoguang Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00804">SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling 3D avatars benefits various application scenarios such as AR/VR, gaming, and filming. Character faces contribute significant diversity and vividity as a vital component of avatars. However, building 3D character face models usually requires a heavy workload with commercial tools, even for experienced artists. Various existing sketch-based tools fail to support amateurs in modeling diverse facial shapes and rich geometric details. In this paper, we present SketchMetaFace - a sketching system targeting amateur users to model high-fidelity 3D faces in minutes. We carefully design both the user interface and the underlying algorithm. First, curvature-aware strokes are adopted to better support the controllability of carving facial details. Second, considering the key problem of mapping a 2D sketch map to a 3D model, we develop a novel learning-based method termed "Implicit and Depth Guided Mesh Modeling" (IDGMM). It fuses the advantages of mesh, implicit, and depth representations to achieve high-quality results with high efficiency. In addition, to further support usability, we present a coarse-to-fine 2D sketching interface design and a data-driven stroke suggestion tool. User studies demonstrate the superiority of our system over existing modeling tools in terms of the ease to use and visual quality of results. Experimental analyses also show that IDGMM reaches a better trade-off between accuracy and efficiency. SketchMetaFace is available at https://zhongjinluo.github.io/SketchMetaFace/.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2306.17123.pdf' target='_blank'>https://arxiv.org/pdf/2306.17123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-En Lin, Alex Trevithick, Keli Cheng, Michel Sarkis, Mohsen Ghafoorian, Ning Bi, Gerhard Reitmayr, Ravi Ramamoorthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17123">PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Portrait synthesis creates realistic digital avatars which enable users to interact with others in a compelling way. Recent advances in StyleGAN and its extensions have shown promising results in synthesizing photorealistic and accurate reconstruction of human faces. However, previous methods often focus on frontal face synthesis and most methods are not able to handle large head rotations due to the training data distribution of StyleGAN. In this work, our goal is to take as input a monocular video of a face, and create an editable dynamic portrait able to handle extreme head poses. The user can create novel viewpoints, edit the appearance, and animate the face. Our method utilizes pivotal tuning inversion (PTI) to learn a personalized video prior from a monocular video sequence. Then we can input pose and expression coefficients to MLPs and manipulate the latent vectors to synthesize different viewpoints and expressions of the subject. We also propose novel loss functions to further disentangle pose and expression in the latent space. Our algorithm shows much better performance over previous approaches on monocular video datasets, and it is also capable of running in real-time at 54 FPS on an RTX 3080.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2306.16233.pdf' target='_blank'>https://arxiv.org/pdf/2306.16233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiara Gorlini, Laurits Dixen, Paolo Burelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16233">Investigating the Uncanny Valley Phenomenon Through the Temporal Dynamics of Neural Responses to Virtual Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Uncanny Valley phenomenon refers to the feeling of unease that arises when interacting with characters that appear almost, but not quite, human-like. First theorised by Masahiro Mori in 1970, it has since been widely observed in different contexts from humanoid robots to video games, in which it can result in players feeling uncomfortable or disconnected from the game, leading to a lack of immersion and potentially reducing the overall enjoyment. The phenomenon has been observed and described mostly through behavioural studies based on self-reported scales of uncanny feeling: however, there is still no consensus on its cognitive and perceptual origins, which limits our understanding of its impact on player experience. In this paper, we present a study aimed at identifying the mechanisms that trigger the uncanny response by collecting and analysing both self-reported feedback and EEG data.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2306.08990.pdf' target='_blank'>https://arxiv.org/pdf/2306.08990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Radek DanÄÄek, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael J. Black, Timo Bolkart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08990">Emotional Speech-Driven Animation with Content-Emotion Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2306.07579.pdf' target='_blank'>https://arxiv.org/pdf/2306.07579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricong Huang, Peiwen Lai, Yipeng Qin, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07579">Parametric Implicit Face Representation for Audio-Driven Facial Reenactment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven facial reenactment is a crucial technique that has a range of applications in film-making, virtual avatars and video conferences. Existing works either employ explicit intermediate face representations (e.g., 2D facial landmarks or 3D face models) or implicit ones (e.g., Neural Radiance Fields), thus suffering from the trade-offs between interpretability and expressive power, hence between controllability and quality of the results. In this work, we break these trade-offs with our novel parametric implicit face representation and propose a novel audio-driven facial reenactment framework that is both controllable and can generate high-quality talking heads. Specifically, our parametric implicit representation parameterizes the implicit representation with interpretable parameters of 3D face models, thereby taking the best of both explicit and implicit methods. In addition, we propose several new techniques to improve the three components of our framework, including i) incorporating contextual information into the audio-to-expression parameters encoding; ii) using conditional image synthesis to parameterize the implicit representation and implementing it with an innovative tri-plane structure for efficient learning; iii) formulating facial reenactment as a conditional image inpainting problem and proposing a novel data augmentation technique to improve model generalizability. Extensive experiments demonstrate that our method can generate more realistic results than previous methods with greater fidelity to the identities and talking styles of speakers.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2305.11870.pdf' target='_blank'>https://arxiv.org/pdf/2305.11870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11870">Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a 3D generation pipeline that uses diffusion models to generate realistic human digital avatars. Due to the wide variety of human identities, poses, and stochastic details, the generation of 3D human meshes has been a challenging problem. To address this, we decompose the problem into 2D normal map generation and normal map-based 3D reconstruction. Specifically, we first simultaneously generate realistic normal maps for the front and backside of a clothed human, dubbed dual normal maps, using a pose-conditional diffusion model. For 3D reconstruction, we "carve" the prior SMPL-X mesh to a detailed 3D mesh according to the normal maps through mesh optimization. To further enhance the high-frequency details, we present a diffusion resampling scheme on both body and facial regions, thus encouraging the generation of realistic digital avatars. We also seamlessly incorporate a recent text-to-image diffusion model to support text-based human identity control. Our method, namely, Chupa, is capable of generating realistic 3D clothed humans with better perceptual quality and identity variety.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2304.02001.pdf' target='_blank'>https://arxiv.org/pdf/2304.02001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, Kwan-Yee Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02001">MonoHuman: Animatable Human Neural Field from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual avatars with free-view control is crucial for various applications like virtual reality and digital entertainment. Previous studies have attempted to utilize the representation power of the neural radiance field (NeRF) to reconstruct the human body from monocular videos. Recent works propose to graft a deformation network into the NeRF to further model the dynamics of the human neural field for animating vivid human motions. However, such pipelines either rely on pose-dependent representations or fall short of motion coherency due to frame-independent optimization, making it difficult to generalize to unseen pose sequences realistically. In this paper, we propose a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under arbitrary novel poses. Our key insight is to model the deformation field with bi-directional constraints and explicitly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Specifically, we first propose a Shared Bidirectional Deformation module, which creates a pose-independent generalizable deformation field by disentangling backward and forward deformation correspondences into shared skeletal motion weight and separate non-rigid motions. Then, we devise a Forward Correspondence Search module, which queries the correspondence feature of keyframes to guide the rendering network. The rendered results are thus multi-view consistent with high fidelity, even under challenging novel pose settings. Extensive experiments demonstrate the superiority of our proposed MonoHuman over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2303.17606.pdf' target='_blank'>https://arxiv.org/pdf/2303.17606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17606">AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural implicit fields are powerful for representing 3D scenes and generating high-quality novel views, but it remains challenging to use such implicit representations for creating a 3D human avatar with a specific identity and artistic style that can be easily animated. Our proposed method, AvatarCraft, addresses this challenge by using diffusion models to guide the learning of geometry and texture for a neural avatar based on a single text prompt. We carefully design the optimization framework of neural implicit fields, including a coarse-to-fine multi-bounding box training strategy, shape regularization, and diffusion-based constraints, to produce high-quality geometry and texture. Additionally, we make the human avatar animatable by deforming the neural implicit field with an explicit warping field that maps the target human mesh to a template human mesh, both represented using parametric human models. This simplifies animation and reshaping of the generated avatar by controlling pose and shape parameters. Extensive experiments on various text descriptions show that AvatarCraft is effective and robust in creating human avatars and rendering novel views, poses, and shapes. Our project page is: https://avatar-craft.github.io/.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2303.06537.pdf' target='_blank'>https://arxiv.org/pdf/2303.06537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungbok Shin, Sanghyun Hong, Niklas Elmqvist
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06537">Perceptual Pat: A Virtual Human System for Iterative Visualization Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing a visualization is often a process of iterative refinement where the designer improves a chart over time by adding features, improving encodings, and fixing mistakes. However, effective design requires external critique and evaluation. Unfortunately, such critique is not always available on short notice and evaluation can be costly. To address this need, we present Perceptual Pat, an extensible suite of AI and computer vision techniques that forms a virtual human visual system for supporting iterative visualization design. The system analyzes snapshots of a visualization using an extensible set of filters - including gaze maps, text recognition, color analysis, etc - and generates a report summarizing the findings. The web-based Pat Design Lab provides a version tracking system that enables the designer to track improvements over time. We validate Perceptual Pat using a longitudinal qualitative study involving 4 professional visualization designers that used the tool over a few days to design a new visualization.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2301.10047.pdf' target='_blank'>https://arxiv.org/pdf/2301.10047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10047">DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture synthesis is a field of growing interest in virtual human creation. However, a critical challenge is the inherent intricate one-to-many mapping between speech and gestures. Previous studies have explored and achieved significant progress with generative models. Notwithstanding, most synthetic gestures are still vastly less natural. This paper presents DiffMotion, a novel speech-driven gesture synthesis architecture based on diffusion models. The model comprises an autoregressive temporal encoder and a denoising diffusion probability Module. The encoder extracts the temporal context of the speech input and historical gestures. The diffusion module learns a parameterized Markov chain to gradually convert a simple distribution into a complex distribution and generates the gestures according to the accompanied speech. Compared with baselines, objective and subjective evaluations confirm that our approach can produce natural and diverse gesticulation and demonstrate the benefits of diffusion-based models on speech-driven gesture synthesis.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/1904.01509.pdf' target='_blank'>https://arxiv.org/pdf/1904.01509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanfu Yan, Ke Lu, Jian Xue, Pengcheng Gao, Jiayi Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1904.01509">FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expression analysis based on machine learning requires large number of well-annotated data to reflect different changes in facial motion. Publicly available datasets truly help to accelerate research in this area by providing a benchmark resource, but all of these datasets, to the best of our knowledge, are limited to rough annotations for action units, including only their absence, presence, or a five-level intensity according to the Facial Action Coding System. To meet the need for videos labeled in great detail, we present a well-annotated dataset named FEAFA for Facial Expression Analysis and 3D Facial Animation. One hundred and twenty-two participants, including children, young adults and elderly people, were recorded in real-world conditions. In addition, 99,356 frames were manually labeled using Expression Quantitative Tool developed by us to quantify 9 symmetrical FACS action units, 10 asymmetrical (unilateral) FACS action units, 2 symmetrical FACS action descriptors and 2 asymmetrical FACS action descriptors, and each action unit or action descriptor is well-annotated with a floating point number between 0 and 1. To provide a baseline for use in future research, a benchmark for the regression of action unit values based on Convolutional Neural Networks are presented. We also demonstrate the potential of our FEAFA dataset for 3D facial animation. Almost all state-of-the-art algorithms for facial animation are achieved based on 3D face reconstruction. We hence propose a novel method that drives virtual characters only based on action unit value regression of the 2D video frames of source actors.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2509.17803.pdf' target='_blank'>https://arxiv.org/pdf/2509.17803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nabila Amadou, Kazi Injamamul Haque, Zerrin Yumak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17803">Effect of Appearance and Animation Realism on the Perception of Emotionally Expressive Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Virtual Human technology is growing with several potential applications in health, education, business and telecommunications. Investigating the perception of these virtual humans can help guide to develop better and more effective applications. Recent developments show that the appearance of the virtual humans reached to a very realistic level. However, there is not yet adequate analysis on the perception of appearance and animation realism for emotionally expressive virtual humans. In this paper, we designed a user experiment and analyzed the effect of a realistic virtual human's appearance realism and animation realism in varying emotion conditions. We found that higher appearance realism and higher animation realism leads to higher social presence and higher attractiveness ratings. We also found significant effects of animation realism on perceived realism and emotion intensity levels. Our study sheds light into how appearance and animation realism effects the perception of highly realistic virtual humans in emotionally expressive scenarios and points out to future directions.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2509.10147.pdf' target='_blank'>https://arxiv.org/pdf/2509.10147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nenad Tomasev, Matija Franklin, Joel Z. Leibo, Julian Jacobs, William A. Cunningham, Iason Gabriel, Simon Osindero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10147">Virtual Agent Economies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. We propose the "sandbox economy" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). Our current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting us with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here we discuss a number of possible design choices that may lead to safely steerable AI agent markets. In particular, we consider auction mechanisms for fair resource allocation and preference resolution, the design of AI "mission economies" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, we argue for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2509.01681.pdf' target='_blank'>https://arxiv.org/pdf/2509.01681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Ilyes Lakhal, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01681">GaussianGAN: Real-Time Photorealistic controllable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2508.19527.pdf' target='_blank'>https://arxiv.org/pdf/2508.19527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiting Gao, Dan Song, Diqiong Jiang, Chao Xue, An-An Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19527">MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2508.15988.pdf' target='_blank'>https://arxiv.org/pdf/2508.15988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Ilyes Lakhal, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15988">Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The diversity of sign representation is essential for Sign Language Production (SLP) as it captures variations in appearance, facial expressions, and hand movements. However, existing SLP models are often unable to capture diversity while preserving visual quality and modelling non-manual attributes such as emotions. To address this problem, we propose a novel approach that leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital avatars from a generated reference image. We propose a novel sign feature aggregation module that explicitly models the non-manual features (\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands). We show that our proposed module ensures the preservation of linguistic content while seamlessly using reference images with different ethnic backgrounds to ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2508.12438.pdf' target='_blank'>https://arxiv.org/pdf/2508.12438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaron Aloni, Rotem Shalev-Arkushin, Yonatan Shafir, Guy Tevet, Ohad Fried, Amit Haim Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12438">Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic facial expression generation from natural language is a crucial task in Computer Graphics, with applications in Animation, Virtual Avatars, and Human-Computer Interaction. However, current generative models suffer from datasets that are either speech-driven or limited to coarse emotion labels, lacking the nuanced, expressive descriptions needed for fine-grained control, and were captured using elaborate and expensive equipment. We hence present a new dataset of facial motion sequences featuring nuanced performances and semantic annotation. The data is easily collected using commodity equipment and LLM-generated natural language instructions, in the popular ARKit blendshape format. This provides riggable motion, rich with expressive performances and labels. We accordingly train two baseline models, and evaluate their performance for future benchmarking. Using our Express4D dataset, the trained models can learn meaningful text-to-expression motion generation and capture the many-to-many mapping of the two modalities. The dataset, code, and video examples are available on our webpage: https://jaron1990.github.io/Express4D/
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2507.22153.pdf' target='_blank'>https://arxiv.org/pdf/2507.22153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Wilson, Vincent Bindschaedler, Sophie JÃ¶rg, Sean Sheikholeslam, Kevin Butler, Eakta Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22153">Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic 3D avatar generation has rapidly improved in recent years, and realistic avatars that match a user's true appearance are more feasible in Mixed Reality (MR) than ever before. Yet, there are known risks to sharing one's likeness online, and photorealistic MR avatars could exacerbate these risks. If user likenesses were to be shared broadly, there are risks for cyber abuse or targeted fraud based on user appearances. We propose an alternate avatar rendering scheme for broader social MR -- synthesizing realistic avatars that preserve a user's demographic identity while being distinct enough from the individual user to protect facial biometric information. We introduce a methodology for privatizing appearance by isolating identity within the feature space of identity-encoding generative models. We develop two algorithms that then obfuscate identity: \epsmethod{} provides differential privacy guarantees and \thetamethod{} provides fine-grained control for the level of identity offset. These methods are shown to successfully generate de-identified virtual avatars across multiple generative architectures in 2D and 3D. With these techniques, it is possible to protect user privacy while largely preserving attributes related to sense of self. Employing these techniques in public settings could enable the use of photorealistic avatars broadly in MR, maintaining high realism and immersion without privacy risk.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2507.09862.pdf' target='_blank'>https://arxiv.org/pdf/2507.09862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09862">SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2507.00472.pdf' target='_blank'>https://arxiv.org/pdf/2507.00472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00472">ARIG: Autoregressive Interactive Head Generation for Real-time Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2506.21632.pdf' target='_blank'>https://arxiv.org/pdf/2506.21632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Li, Donggang Jia, Markus Hadwiger, Ivan Viola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21632">SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an interactive human avatar and the background from a monocular video of a dynamic human scene is highly challenging. In this work we adopt a strategy of point cloud decoupling and joint optimization to achieve the decoupled reconstruction of backgrounds and human bodies while preserving the interactivity of human motion. We introduce a position texture to subdivide the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human point cloud. To capture fine details of human dynamics and deformations, we incorporate a convolutional neural network structure to predict human body point cloud features based on texture. This strategy makes our approach free of hyperparameter tuning for densification and efficiently represents human points with half the point cloud of HUGS. This approach ensures high-quality human reconstruction and reduces GPU resource consumption during training. As a result, our method surpasses the previous state-of-the-art HUGS in reconstruction metrics while maintaining the ability to generalize to novel poses and views. Furthermore, our technique achieves real-time rendering at over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning (LBS) weights for human transformation. Additionally, this work demonstrates that this framework can be extended to animal scene reconstruction when an accurately-posed model of an animal is available.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2505.06131.pdf' target='_blank'>https://arxiv.org/pdf/2505.06131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Hou, Yuting Xiao, Xiangyang Xue, Taiping Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06131">ELA-ZSON: Efficient Layout-Aware Zero-Shot Object Navigation Agent with Hierarchical Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ELA-ZSON, an efficient layout-aware zero-shot object navigation (ZSON) approach designed for complex multi-room indoor environments.
  By planning hierarchically leveraging a global topologigal map with layout information and local imperative approach with detailed scene representation memory, ELA-ZSON achieves both efficient and effective navigation.
  The process is managed by an LLM-powered agent, ensuring seamless effective planning and navigation, without the need for human interaction, complex rewards, or costly training.
  Our experimental results on the MP3D benchmark achieves 85\% object navigation success rate (SR) and 79\% success rate weighted by path length (SPL) (over 40\% point improvement in SR and 60\% improvement in SPL compared to exsisting methods). Furthermore, we validate the robustness of our approach through virtual agent and real-world robotic deployment, showcasing its capability in practical scenarios. See https://anonymous.4open.science/r/ELA-ZSON-C67E/ for details.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2504.12292.pdf' target='_blank'>https://arxiv.org/pdf/2504.12292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Schoneveld, Zhe Chen, Davide Davoli, Jiapeng Tang, Saimon Terazawa, Ko Nishino, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12292">SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach. Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2504.07949.pdf' target='_blank'>https://arxiv.org/pdf/2504.07949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kefan Chen, Sergiu Oprea, Justin Theiss, Sreyas Mohan, Srinath Sridhar, Aayush Prakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07949">InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rising interest from the community in digital avatars coupled with the importance of expressions and gestures in communication, modeling natural avatar behavior remains an important challenge across many industries such as teleconferencing, gaming, and AR/VR. Human hands are the primary tool for interacting with the environment and essential for realistic human behavior modeling, yet existing 3D hand and head avatar models often overlook the crucial aspect of hand-body interactions, such as between hand and face. We present InteracttAvatar, the first model to faithfully capture the photorealistic appearance of dynamic hand and non-rigid hand-face interactions. Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian Splatting as well as a dynamic refinement module, captures pose-dependent change, e.g. the fine wrinkles and complex shadows that occur during articulation. Importantly, our hand-face interaction module models the subtle geometry and appearance dynamics that underlie common gestures. Through experiments of novel view synthesis, self reenactment and cross-identity reenactment, we demonstrate that InteracttAvatar can reconstruct hand and hand-face interactions from monocular or multiview videos with high-fidelity details and be animated with novel poses.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2504.01559.pdf' target='_blank'>https://arxiv.org/pdf/2504.01559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yahui Li, Zhi Zeng, Liming Pang, Guixuan Zhang, Shuwu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01559">RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling animatable human avatars from monocular or multi-view videos has been widely studied, with recent approaches leveraging neural radiance fields (NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in novel-view and novel-pose synthesis. However, existing methods often struggle to accurately capture the dynamics of loose clothing, as they primarily rely on global pose conditioning or static per-frame representations, leading to oversmoothing and temporal inconsistencies in non-rigid regions. To address this, We propose RealityAvatar, an efficient framework for high-fidelity digital human modeling, specifically targeting loosely dressed avatars. Our method leverages 3D Gaussian Splatting to capture complex clothing deformations and motion dynamics while ensuring geometric consistency. By incorporating a motion trend module and a latentbone encoder, we explicitly model pose-dependent deformations and temporal variations in clothing behavior. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach in capturing fine-grained clothing deformations and motion-driven shape variations. Our method significantly enhances structural fidelity and perceptual quality in dynamic human reconstruction, particularly in non-rigid regions, while achieving better consistency across temporal frames.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2503.09293.pdf' target='_blank'>https://arxiv.org/pdf/2503.09293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Moreau, Mohammed Brahimi, Richard Shaw, Athanasios Papaioannou, Thomas Tanay, Zhensong Zhang, Eduardo PÃ©rez-Pellitero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09293">Better Together: Unified Motion Capture and 3D Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Better Together, a method that simultaneously solves the human pose estimation problem while reconstructing a photorealistic 3D human avatar from multi-view videos. While prior art usually solves these problems separately, we argue that joint optimization of skeletal motion with a 3D renderable body model brings synergistic effects, i.e. yields more precise motion capture and improved visual quality of real-time rendering of avatars. To achieve this, we introduce a novel animatable avatar with 3D Gaussians rigged on a personalized mesh and propose to optimize the motion sequence with time-dependent MLPs that provide accurate and temporally consistent pose estimates. We first evaluate our method on highly challenging yoga poses and demonstrate state-of-the-art accuracy on multi-view human pose estimation, reducing error by 35% on body joints and 45% on hand joints compared to keypoint-based methods. At the same time, our method significantly boosts the visual quality of animatable avatars (+2dB PSNR on novel view synthesis) on diverse challenging subjects.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2502.03069.pdf' target='_blank'>https://arxiv.org/pdf/2502.03069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Rasch, Julia TÃ¶ws, Teresa Hirzle, Florian MÃ¼ller, Martin Schmitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03069">CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI's contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2501.06903.pdf' target='_blank'>https://arxiv.org/pdf/2501.06903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06903">Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participant's consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2501.01174.pdf' target='_blank'>https://arxiv.org/pdf/2501.01174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumyaratna Debnath, Harish Katti, Shashikant Verma, Shanmuganathan Raman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01174">L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While 2D pose estimation has advanced our ability to interpret body movements in animals and primates, it is limited by the lack of depth information, constraining its application range. 3D pose estimation provides a more comprehensive solution by incorporating spatial depth, yet creating extensive 3D pose datasets for animals is challenging due to their dynamic and unpredictable behaviours in natural settings. To address this, we propose a hybrid approach that utilizes rigged avatars and the pipeline to generate synthetic datasets to acquire the necessary 3D annotations for training. Our method introduces a simple attention-based MLP network for converting 2D poses to 3D, designed to be independent of the input image to ensure scalability for poses in natural environments. Additionally, we identify that existing anatomical keypoint detectors are insufficient for accurate pose retargeting onto arbitrary avatars. To overcome this, we present a lookup table based on a deep pose estimation method using a synthetic collection of diverse actions rigged avatars perform. Our experiments demonstrate the effectiveness and efficiency of this lookup table-based retargeting approach. Overall, we propose a comprehensive framework with systematically synthesized datasets for lifting poses from 2D to 3D and then utilize this to re-target motion from wild settings onto arbitrary avatars.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2408.13084.pdf' target='_blank'>https://arxiv.org/pdf/2408.13084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernhard Hilpert, Claudio Alves da Silva, Leon Christidis, Chirag Bhuvaneshwara, Patrick Gebhard, Fabrizio Nunnari, Dimitra Tsovaltzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13084">Avatar Visual Similarity for Social HCI: Increasing Self-Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-awareness is a critical factor in social human-human interaction and, hence, in social HCI interaction. Increasing self-awareness through mirrors or video recordings is common in face-to-face trainings, since it influences antecedents of self-awareness like explicit identification and implicit affective identification (affinity). However, increasing self-awareness has been scarcely examined in virtual trainings with virtual avatars, which allow for adjusting the similarity, e.g. to avoid negative effects of self-consciousness. Automatic visual similarity in avatars is an open issue related to high costs. It is important to understand which features need to be manipulated and which degree of similarity is necessary for self-awareness to leverage the added value of using avatars for self-awareness. This article examines the relationship between avatar visual similarity and increasing self-awareness in virtual training environments. We define visual similarity based on perceptually important facial features for human-human identification and develop a theory-based methodology to systematically manipulate visual similarity of virtual avatars and support self-awareness. Three personalized versions of virtual avatars with varying degrees of visual similarity to participants were created (weak, medium and strong facial features manipulation). In a within-subject study (N=33), we tested effects of degree of similarity on perceived similarity, explicit identification and implicit affective identification (affinity). Results show significant differences between the weak similarity manipulation, and both the strong manipulation and the random avatar for all three antecedents of self-awareness. An increasing degree of avatar visual similarity influences antecedents of self-awareness in virtual environments.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2408.01826.pdf' target='_blank'>https://arxiv.org/pdf/2408.01826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Lin, Zhaoxin Fan, Xianjia Wu, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01826">GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven talking head generation is a critical yet challenging task with applications in augmented reality and virtual human modeling. While recent approaches using autoregressive and diffusion-based models have achieved notable progress, they often suffer from modality inconsistencies, particularly misalignment between audio and mesh, leading to reduced motion diversity and lip-sync accuracy. To address this, we propose GLDiTalker, a novel speech-driven 3D facial animation model based on a Graph Latent Diffusion Transformer. GLDiTalker resolves modality misalignment by diffusing signals within a quantized spatiotemporal latent space. It employs a two-stage training pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion diversity. Together, these stages enable GLDiTalker to generate realistic, temporally stable 3D facial animations. Extensive evaluations on standard benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving superior results in both lip-sync accuracy and motion diversity.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2408.01732.pdf' target='_blank'>https://arxiv.org/pdf/2408.01732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Tan, Xize Cheng, Lingyu Xiong, Lei Zhu, Xiandong Li, Xianjia Wu, Kai Gong, Minglei Li, Yi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01732">Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2407.21686.pdf' target='_blank'>https://arxiv.org/pdf/2407.21686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyeongsik Moon, Takaaki Shiratori, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21686">Expressive Whole-Body 3D Gaussian Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expression and hand motions are necessary to express our emotions and interact with the world. Nevertheless, most of the 3D human avatars modeled from a casually captured video only support body motions without facial expressions and hand motions.In this work, we present ExAvatar, an expressive whole-body 3D human avatar learned from a short monocular video. We design ExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and 3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of facial expressions and poses in the video and 2) the absence of 3D observations, such as 3D scans and RGBD images. The limited diversity in the video makes animations with novel facial expressions and poses non-trivial. In addition, the absence of 3D observations could cause significant ambiguity in human parts that are not observed in the video, which can result in noticeable artifacts under novel motions. To address them, we introduce our hybrid representation of the mesh and 3D Gaussians. Our hybrid representation treats each 3D Gaussian as a vertex on the surface with pre-defined connectivity information (i.e., triangle faces) between them following the mesh topology of SMPL-X. It makes our ExAvatar animatable with novel facial expressions by driven by the facial expression space of SMPL-X. In addition, by using connectivity-based regularizers, we significantly reduce artifacts in novel facial expressions and poses.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2407.19593.pdf' target='_blank'>https://arxiv.org/pdf/2407.19593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ShahRukh Athar, Shunsuke Saito, Zhengyu Yang, Stanislav Pidhorsky, Chen Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19593">Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic avatars for individuals traditionally involves extensive capture sessions with complex and expensive studio devices like the LightStage system. While recent strides in neural representations have enabled the generation of photorealistic and animatable 3D avatars from quick phone scans, they have the capture-time lighting baked-in, lack facial details and have missing regions in areas such as the back of the ears. Thus, they lag in quality compared to studio-captured avatars. In this paper, we propose a method that bridges this gap by generating studio-like illuminated texture maps from short, monocular phone captures. We do this by parameterizing the phone texture maps using the $W^+$ space of a StyleGAN2, enabling near-perfect reconstruction. Then, we finetune a StyleGAN2 by sampling in the $W^+$ parameterized space using a very small set of studio-captured textures as an adversarial training signal. To further enhance the realism and accuracy of facial details, we super-resolve the output of the StyleGAN2 using carefully designed diffusion model that is guided by image gradients of the phone-captured texture map. Once trained, our method excels at producing studio-like facial texture maps from casual monocular smartphone videos. Demonstrating its capabilities, we showcase the generation of photorealistic, uniformly lit, complete avatars from monocular phone captures. The project page can be found at http://shahrukhathar.github.io/2024/07/22/Bridging.html
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2407.15070.pdf' target='_blank'>https://arxiv.org/pdf/2407.15070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelang Xu, Zhaoqi Su, Qingyao Wu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15070">GPHM: Gaussian Parametric Head Model for Monocular Head Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity 3D human head avatars is crucial for applications in VR/AR, digital human, and film production. Recent advances have leveraged morphable face models to generate animated head avatars from easily accessible data, representing varying identities and expressions within a low-dimensional parametric space. However, existing methods often struggle with modeling complex appearance details, e.g., hairstyles, and suffer from low rendering quality and efficiency. In this paper we introduce a novel approach, 3D Gaussian Parametric Head Model, which employs 3D Gaussians to accurately represent the complexities of the human head, allowing precise control over both identity and expression. The Gaussian model can handle intricate details, enabling realistic representations of varying appearances and complex expressions. Furthermore, we presents a well-designed training framework to ensure smooth convergence, providing a robust guarantee for learning the rich content. Our method achieves high-quality, photo-realistic rendering with real-time efficiency, making it a valuable contribution to the field of parametric head models. Finally, we apply the 3D Gaussian Parametric Head Model to monocular video or few-shot head avatar reconstruction tasks, which enables instant reconstruction of high-quality 3D head avatars even when input data is extremely limited, surpassing previous methods in terms of reconstruction quality and training speed.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2407.04545.pdf' target='_blank'>https://arxiv.org/pdf/2407.04545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04545">Gaussian Eigen Models for Human Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current personalized neural head avatars face a trade-off: lightweight models lack detail and realism, while high-quality, animatable avatars require significant computational resources, making them unsuitable for commodity devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars. GEM utilizes 3D Gaussian primitives for representing the appearance combined with Gaussian splatting for rendering. Building on the success of mesh-based 3D morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases for representing the head appearance of a specific subject. In particular, we construct linear bases to represent the position, scale, rotation, and opacity of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives of a specific head shape by a linear combination of the basis vectors, only requiring a low-dimensional parameter vector that contains the respective coefficients. We propose to construct these linear bases (GEM) by distilling high-quality compute-intense CNN-based Gaussian avatar models that can generate expression-dependent appearance changes like wrinkles. These high-quality models are trained on multi-view videos of a subject and are distilled using a series of principal component analyses. Once we have obtained the bases that represent the animatable appearance space of a specific human, we learn a regressor that takes a single RGB image as input and predicts the low-dimensional parameter vector that corresponds to the shown facial expression. In a series of experiments, we compare GEM's self-reenactment and cross-person reenactment results to state-of-the-art 3D avatar methods, demonstrating GEM's higher visual quality and better generalization to new expressions.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2405.19609.pdf' target='_blank'>https://arxiv.org/pdf/2405.19609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Zhaolong Wang, Xiangru Lin, Zongqing Lu, Yuxi Zhao, Hanqing Wei, Jingrui Ye, Yu Zhang, Zhijing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19609">SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering photorealistic and drivable full-body avatars is crucial for numerous applications, including virtual reality, 3D games, and tele-presence. Most methods, whether reconstruction or generation, require large numbers of human motion sequences and corresponding textured meshes. To easily learn a drivable avatar, a reasonable parametric body model with unified topology is paramount. However, existing human body datasets either have images or textured models and lack parametric models which fit clothes well. We propose a new parametric model SMPLX-Lite-D, which can fit detailed geometry of the scanned mesh while maintaining stable geometry in the face, hand and foot regions. We present SMPLX-Lite dataset, the most comprehensive clothing avatar dataset with multi-view RGB sequences, keypoints annotations, textured scanned meshes, and textured SMPLX-Lite-D models. With the SMPLX-Lite dataset, we train a conditional variational autoencoder model that takes human pose and facial keypoints as input, and generates a photorealistic drivable human avatar.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2405.11270.pdf' target='_blank'>https://arxiv.org/pdf/2405.11270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11270">HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2404.19038.pdf' target='_blank'>https://arxiv.org/pdf/2404.19038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyong Wang, Xiangyu Liang, Wangguandong Zheng, Dan Niu, Haifeng Xia, Siyu Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19038">Embedded Representation Learning Network for Animating Styled Video Portrait</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The talking head generation recently attracted considerable attention due to its widespread application prospects, especially for digital avatars and 3D animation design. Inspired by this practical demand, several works explored Neural Radiance Fields (NeRF) to synthesize the talking heads. However, these methods based on NeRF face two challenges: (1) Difficulty in generating style-controllable talking heads. (2) Displacement artifacts around the neck in rendered images. To overcome these two challenges, we propose a novel generative paradigm \textit{Embedded Representation Learning Network} (ERLNet) with two learning stages. First, the \textit{ audio-driven FLAME} (ADF) module is constructed to produce facial expression and head pose sequences synchronized with content audio and style video. Second, given the sequence deduced by the ADF, one novel \textit{dual-branch fusion NeRF} (DBF-NeRF) explores these contents to render the final images. Extensive empirical studies demonstrate that the collaboration of these two stages effectively facilitates our method to render a more realistic talking head than the existing algorithms.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2404.14745.pdf' target='_blank'>https://arxiv.org/pdf/2404.14745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Wang, Caoyuan Ma, Guopeng Li, Hanrui Xu, Yuke Li, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14745">You Think, You ACT: The New Task of Arbitrary Text to Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text to Motion aims to generate human motions from texts. Existing settings rely on limited Action Texts that include action labels, which limits flexibility and practicability in scenarios difficult to describe directly. This paper extends limited Action Texts to arbitrary ones. Scene texts without explicit action labels can enhance the practicality of models in complex and diverse industries such as virtual human interaction, robot behavior generation, and film production, while also supporting the exploration of potential implicit behavior patterns. However, newly introduced Scene Texts may yield multiple reasonable output results, causing significant challenges in existing data, framework, and evaluation. To address this practical issue, we first create a new dataset HUMANML3D++ by extending texts of the largest existing dataset HUMANML3D. Secondly, we propose a simple yet effective framework that extracts action instructions from arbitrary texts and subsequently generates motions. Furthermore, we also benchmark this new setting with multi-solution metrics to address the inadequacies of existing single-solution metrics. Extensive experiments indicate that Text to Motion in this realistic setting is challenging, fostering new research in this practical direction.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2404.06152.pdf' target='_blank'>https://arxiv.org/pdf/2404.06152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06152">HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2404.02411.pdf' target='_blank'>https://arxiv.org/pdf/2404.02411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhao, Nan Gao, Zhi Zeng, Guixuan Zhang, Jie Liu, Shuwu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02411">A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have shown great success in generating high-quality co-speech gestures for interactive humanoid robots or digital avatars from noisy input with the speech audio or text as conditions. However, they rarely focus on providing rich editing capabilities for content creators other than high-level specialized measures like style conditioning. To resolve this, we propose a unified framework utilizing diffusion inversion that enables multi-level editing capabilities for co-speech gesture generation without re-training. The method takes advantage of two key capabilities of invertible diffusion models. The first is that through inversion, we can reconstruct the intermediate noise from gestures and regenerate new gestures from the noise. This can be used to obtain gestures with high-level similarities to the original gestures for different speech conditions. The second is that this reconstruction reduces activation caching requirements during gradient calculation, making the direct optimization on input noises possible on current hardware with limited memory. With different loss functions designed for, e.g., joint rotation or velocity, we can control various low-level details by automatically tweaking the input noises through optimization. Extensive experiments on multiple use cases show that this framework succeeds in unifying high-level and low-level co-speech gesture editing.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2404.00300.pdf' target='_blank'>https://arxiv.org/pdf/2404.00300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seoyeon Bae, Yoon Kyung Lee, Jungcheol Lee, Jaeheon Kim, Haeseong Jeon, Seung-Hwan Lim, Byung-Cheol Kim, Sowon Hahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00300">Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A growth mindset has shown promising outcomes for increasing empathy ability. However, stimulating a growth mindset in VR-based empathy interventions is under-explored. In the present study, we implemented prosocial VR content, Our Neighbor Hero, focusing on embodying a virtual character to modulate players' mindsets. The virtual body served as a stepping stone, enabling players to identify with the character and cultivate a growth mindset as they followed mission instructions. We considered several implementation factors to assist players in positioning within the VR experience, including positive feedback, content difficulty, background lighting, and multimodal feedback. We conducted an experiment to investigate the intervention's effectiveness in increasing empathy. Our findings revealed that the VR content and mindset training encouraged participants to improve their growth mindsets and empathic motives. This VR content was developed for college students to enhance their empathy and teamwork skills. It has the potential to improve collaboration in organizational and community environments.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2403.13570.pdf' target='_blank'>https://arxiv.org/pdf/2403.13570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Deng, Duomin Wang, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13570">Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel learning approach for feed-forward one-shot 4D head avatar synthesis. Different from existing methods that often learn from reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view videos to learn a 4D head synthesizer in a data-driven manner, avoiding reliance on inaccurate 3DMM reconstruction that could be detrimental to the synthesis performance. The key idea is to first learn a 3D head synthesizer using synthetic multi-view images to convert monocular real videos into multi-view ones, and then utilize the pseudo multi-view videos to learn a 4D head synthesizer via cross-view self-reenactment. By leveraging a simple vision transformer backbone with motion-aware cross-attentions, our method exhibits superior performance compared to previous methods in terms of reconstruction fidelity, geometry consistency, and motion control accuracy. We hope our method offers novel insights into integrating 3D priors with 2D supervisions for improved 4D head avatar creation.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2402.06385.pdf' target='_blank'>https://arxiv.org/pdf/2402.06385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dragos Costea, Alina Marcu, Cristina Lazar, Marius Leordeanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06385">Maia: A Real-time Non-Verbal Chat for Human-AI Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling face-to-face communication in computer vision, which focuses on recognizing and analyzing nonverbal cues and behaviors during interactions, serves as the foundation for our proposed alternative to text-based Human-AI interaction. By leveraging nonverbal visual communication, through facial expressions, head and body movements, we aim to enhance engagement and capture the user's attention through a novel improvisational element, that goes beyond mirroring gestures. Our goal is to track and analyze facial expressions, and other nonverbal cues in real-time, and use this information to build models that can predict and understand human behavior. Operating in real-time and requiring minimal computational resources, our approach signifies a major leap forward in making AI interactions more natural and accessible. We offer three different complementary approaches, based on retrieval, statistical, and deep learning techniques. A key novelty of our work is the integration of an artistic component atop an efficient human-computer interaction system, using art as a medium to transmit emotions. Our approach is not art-specific and can be adapted to various paintings, animations, and avatars. In our experiments, we compare state-of-the-art diffusion models as mediums for emotion translation in 2D, and our 3D avatar, Maia, that we introduce in this work, with not just facial movements but also body motions for a more natural and engaging experience. We demonstrate the effectiveness of our approach in translating AI-generated emotions into human-relatable expressions, through both human and automatic evaluation procedures, highlighting its potential to significantly enhance the naturalness and engagement of Human-AI interactions across various applications.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2402.03188.pdf' target='_blank'>https://arxiv.org/pdf/2402.03188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Wilson, Frederick Shic, Sophie JÃ¶rg, Eakta Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03188">Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss terms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in face swapping have enabled the automatic generation of highly realistic faces. Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes. Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process. We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect. We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes. We confirm that viewed face swaps do elicit uncanny responses from viewers. Our proposed improvements significant reduce viewing angle errors between face swaps and their source material. Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks. Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications. Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2312.15430.pdf' target='_blank'>https://arxiv.org/pdf/2312.15430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianqiang Ren, Chao He, Lin Liu, Jiahao Chen, Yutong Wang, Yafei Song, Jianfang Li, Tangli Xue, Siqi Hu, Tao Chen, Kunkun Zheng, Jianjing Xiang, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15430">Make-A-Character: High Quality Text-to-3D Character Generation within Minutes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing demand for customized and expressive 3D characters with the emergence of AI agents and Metaverse, but creating 3D characters using traditional computer graphics tools is a complex and time-consuming task. To address these challenges, we propose a user-friendly framework named Make-A-Character (Mach) to create lifelike 3D avatars from text descriptions. The framework leverages the power of large language and vision models for textual intention understanding and intermediate image generation, followed by a series of human-oriented visual perception and 3D generation modules. Our system offers an intuitive approach for users to craft controllable, realistic, fully-realized 3D characters that meet their expectations within 2 minutes, while also enabling easy integration with existing CG pipeline for dynamic expressiveness. For more information, please visit the project page at https://human3daigc.github.io/MACH/.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2311.18729.pdf' target='_blank'>https://arxiv.org/pdf/2311.18729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18729">Portrait4D: Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing one-shot 4D head synthesis methods usually learn from monocular videos with the aid of 3DMM reconstruction, yet the latter is evenly challenging which restricts them from reasonable 4D head synthesis. We present a method to learn one-shot 4D head synthesis via large-scale synthetic data. The key is to first learn a part-wise 4D generative model from monocular images via adversarial learning, to synthesize multi-view images of diverse identities and full motions as training data; then leverage a transformer-based animatable triplane reconstructor to learn 4D head reconstruction using the synthetic data. A novel learning strategy is enforced to enhance the generalizability to real images by disentangling the learning process of 3D reconstruction and reenactment. Experiments demonstrate our superiority over the prior art.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2311.15672.pdf' target='_blank'>https://arxiv.org/pdf/2311.15672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xihe Yang, Xingyu Chen, Daiheng Gao, Shaohui Wang, Xiaoguang Han, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15672">HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As for human avatar reconstruction, contemporary techniques commonly necessitate the acquisition of costly data and struggle to achieve satisfactory results from a small number of casual images. In this paper, we investigate this task from a few-shot unconstrained photo album. The reconstruction of human avatars from such data sources is challenging because of limited data amount and dynamic articulated poses. For handling dynamic data, we integrate a skinning mechanism with deep marching tetrahedra (DMTet) to form a drivable tetrahedral representation, which drives arbitrary mesh topologies generated by the DMTet for the adaptation of unconstrained images. To effectively mine instructive information from few-shot data, we devise a two-phase optimization method with few-shot reference and few-shot guidance. The former focuses on aligning avatar identity with reference images, while the latter aims to generate plausible appearances for unseen regions. Overall, our framework, called HaveFun, can undertake avatar reconstruction, rendering, and animation. Extensive experiments on our developed benchmarks demonstrate that HaveFun exhibits substantially superior performance in reconstructing the human body and hand. Project website: https://seanchenxy.github.io/HaveFunWeb/.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2309.15311.pdf' target='_blank'>https://arxiv.org/pdf/2309.15311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Che-Jui Chang, Samuel S. Sohn, Sen Zhang, Rajath Jayashankar, Muhammad Usman, Mubbasir Kapadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15311">The Importance of Multimodal Emotion Conditioning and Affect Consistency for Embodied Conversational Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous studies regarding the perception of emotions for embodied virtual agents have shown the effectiveness of using virtual characters in conveying emotions through interactions with humans. However, creating an autonomous embodied conversational agent with expressive behaviors presents two major challenges. The first challenge is the difficulty of synthesizing the conversational behaviors for each modality that are as expressive as real human behaviors. The second challenge is that the affects are modeled independently, which makes it difficult to generate multimodal responses with consistent emotions across all modalities. In this work, we propose a conceptual framework, ACTOR (Affect-Consistent mulTimodal behaviOR generation), that aims to increase the perception of affects by generating multimodal behaviors conditioned on a consistent driving affect. We have conducted a user study with 199 participants to assess how the average person judges the affects perceived from multimodal behaviors that are consistent and inconsistent with respect to a driving affect. The result shows that among all model conditions, our affect-consistent framework receives the highest Likert scores for the perception of driving affects. Our statistical analysis suggests that making a modality affect-inconsistent significantly decreases the perception of driving affects. We also observe that multimodal behaviors conditioned on consistent affects are more expressive compared to behaviors with inconsistent affects. Therefore, we conclude that multimodal emotion conditioning and affect consistency are vital to enhancing the perception of affects for embodied conversational agents.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2308.11261.pdf' target='_blank'>https://arxiv.org/pdf/2308.11261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadegh Aliakbarian, Fatemeh Saleh, David Collier, Pashmina Cameron, Darren Cosker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11261">HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating both plausible and accurate full body avatar motion is the key to the quality of immersive experiences in mixed reality scenarios. Head-Mounted Devices (HMDs) typically only provide a few input signals, such as head and hands 6-DoF. Recently, different approaches achieved impressive performance in generating full body motion given only head and hands signal. However, to the best of our knowledge, all existing approaches rely on full hand visibility. While this is the case when, e.g., using motion controllers, a considerable proportion of mixed reality experiences do not involve motion controllers and instead rely on egocentric hand tracking. This introduces the challenge of partial hand visibility owing to the restricted field of view of the HMD. In this paper, we propose the first unified approach, HMD-NeMo, that addresses plausible and accurate full body motion generation even when the hands may be only partially visible. HMD-NeMo is a lightweight neural network that predicts the full body motion in an online and real-time fashion. At the heart of HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask tokens that encourage plausible motion in the absence of hand observations. We perform extensive analysis of the impact of different components in HMD-NeMo and introduce a new state-of-the-art on AMASS dataset through our evaluation.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2308.10843.pdf' target='_blank'>https://arxiv.org/pdf/2308.10843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mireille Fares, Catherine Pelachaud, Nicolas Obin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10843">TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of transferring the behavior expressivity style of a virtual agent to another one while preserving behaviors shape as they carry communicative meaning. Behavior expressivity style is viewed here as the qualitative properties of behaviors. We propose TranSTYLer, a multimodal transformer based model that synthesizes the multimodal behaviors of a source speaker with the style of a target speaker. We assume that behavior expressivity style is encoded across various modalities of communication, including text, speech, body gestures, and facial expressions. The model employs a style and content disentanglement schema to ensure that the transferred style does not interfere with the meaning conveyed by the source behaviors. Our approach eliminates the need for style labels and allows the generalization to styles that have not been seen during the training phase. We train our model on the PATS corpus, which we extended to include dialog acts and 2D facial landmarks. Objective and subjective evaluations show that our model outperforms state of the art models in style transfer for both seen and unseen styles during training. To tackle the issues of style and content leakage that may arise, we propose a methodology to assess the degree to which behavior and gestures associated with the target style are successfully transferred, while ensuring the preservation of the ones related to the source content.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2307.11025.pdf' target='_blank'>https://arxiv.org/pdf/2307.11025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Wan, Zhicong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11025">Investigating VTubing as a Reconstruction of Streamer Self-Presentation: Identity, Performance, and Gender</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VTubers, or Virtual YouTubers, are live streamers who create streaming content using animated 2D or 3D virtual avatars. In recent years, there has been a significant increase in the number of VTuber creators and viewers across the globe. This practise has drawn research attention into topics such as viewers' engagement behaviors and perceptions, however, as animated avatars offer more identity and performance flexibility than traditional live streaming where one uses their own body, little research has focused on how this flexibility influences how creators present themselves. This research thus seeks to fill this gap by presenting results from a qualitative study of 16 Chinese-speaking VTubers' streaming practices. The data revealed that the virtual avatars that were used while live streaming afforded creators opportunities to present themselves using inflated presentations and resulted in inclusive interactions with viewers. The results also unveiled the inflated, and often sexualized, gender expressions of VTubers while they were situated in misogynistic environments. The socio-technical facets of VTubing were found to potentially reduce sexual harassment and sexism, whilst also raising self-objectification concerns.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2307.10533.pdf' target='_blank'>https://arxiv.org/pdf/2307.10533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillermo Colin, Joseph Byrnes, Youngwoo Sim, Patrick Wensing, Joao Ramos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10533">Whole-Body Dynamic Telelocomotion: A Step-to-Step Dynamics Approach to Human Walking Reference Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperated humanoid robots hold significant potential as physical avatars for humans in hazardous and inaccessible environments, with the goal of channeling human intelligence and sensorimotor skills through these robotic counterparts. Precise coordination between humans and robots is crucial for accomplishing whole-body behaviors involving locomotion and manipulation. To progress successfully, dynamic synchronization between humans and humanoid robots must be achieved. This work enhances advancements in whole-body dynamic telelocomotion, addressing challenges in robustness. By embedding the hybrid and underactuated nature of bipedal walking into a virtual human walking interface, we achieve dynamically consistent walking gait generation. Additionally, we integrate a reactive robot controller into a whole-body dynamic telelocomotion framework. Thus, allowing the realization of telelocomotion behaviors on the full-body dynamics of a bipedal robot. Real-time telelocomotion simulation experiments validate the effectiveness of our methods, demonstrating that a trained human pilot can dynamically synchronize with a simulated bipedal robot, achieving sustained locomotion, controlling walking speeds within the range of 0.0 m/s to 0.3 m/s, and enabling backward walking for distances of up to 2.0 m. This research contributes to advancing teleoperated humanoid robots and paves the way for future developments in synchronized locomotion between humans and bipedal robots.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2304.00471.pdf' target='_blank'>https://arxiv.org/pdf/2304.00471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo-Kyeong Kim, Jaemin Kang, Daeun Seo, Hancheol Park, Shinkook Choi, Hyoung-Kyu Song, Hyungshin Kim, Sungsu Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00471">A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual humans have gained considerable attention in numerous industries, e.g., entertainment and e-commerce. As a core technology, synthesizing photorealistic face frames from target speech and facial identity has been actively studied with generative adversarial networks. Despite remarkable results of modern talking-face generation models, they often entail high computational burdens, which limit their efficient deployment. This study aims to develop a lightweight model for speech-driven talking-face synthesis. We build a compact generator by removing the residual blocks and reducing the channel width from Wav2Lip, a popular talking-face generator. We also present a knowledge distillation scheme to stably yet effectively train the small-capacity generator without adversarial learning. We reduce the number of parameters and MACs by 28$\times$ while retaining the performance of the original model. Moreover, to alleviate a severe performance drop when converting the whole generator to INT8 precision, we adopt a selective quantization method that uses FP16 for the quantization-sensitive layers and INT8 for the other layers. Using this mixed precision, we achieve up to a 19$\times$ speedup on edge GPUs without noticeably compromising the generation quality.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2302.10433.pdf' target='_blank'>https://arxiv.org/pdf/2302.10433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Ordonez-Apraez, Mario Martin, Antonio Agudo, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10433">On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a comprehensive study on discrete morphological symmetries of dynamical systems, which are commonly observed in biological and artificial locomoting systems, such as legged, swimming, and flying animals/robots/virtual characters. These symmetries arise from the presence of one or more planes/axis of symmetry in the system's morphology, resulting in harmonious duplication and distribution of body parts. Significantly, we characterize how morphological symmetries extend to symmetries in the system's dynamics, optimal control policies, and in all proprioceptive and exteroceptive measurements related to the system's dynamics evolution. In the context of data-driven methods, symmetry represents an inductive bias that justifies the use of data augmentation or symmetric function approximators. To tackle this, we present a theoretical and practical framework for identifying the system's morphological symmetry group $\G$ and characterizing the symmetries in proprioceptive and exteroceptive data measurements. We then exploit these symmetries using data augmentation and $\G$-equivariant neural networks. Our experiments on both synthetic and real-world applications provide empirical evidence of the advantageous outcomes resulting from the exploitation of these symmetries, including improved sample efficiency, enhanced generalization, and reduction of trainable parameters.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2302.00883.pdf' target='_blank'>https://arxiv.org/pdf/2302.00883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00883">Synthesizing Physical Character-Scene Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Movement is how people interact with and affect their environment. For realistic character animation, it is necessary to synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent's movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interactions require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method learns scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character's movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. By randomizing the properties of the objects and their placements during training, our method is able to generalize beyond the objects and scenarios depicted in the training dataset, producing natural character-scene interactions for a wide variety of object shapes and placements. The approach takes physics-based character motion generation a step closer to broad applicability.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2211.12499.pdf' target='_blank'>https://arxiv.org/pdf/2211.12499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timo Bolkart, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12499">Instant Volumetric Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Instant Volumetric Head Avatars (INSTA), a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2209.00776.pdf' target='_blank'>https://arxiv.org/pdf/2209.00776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanhang Yan, Yu Sun, Qian Bao, Jinhui Pang, Wu Liu, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00776">WOC: A Handy Webcam-based 3D Online Chatroom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop WOC, a webcam-based 3D virtual online chatroom for multi-person interaction, which captures the 3D motion of users and drives their individual 3D virtual avatars in real-time. Compared to the existing wearable equipment-based solution, WOC offers convenient and low-cost 3D motion capture with a single camera. To promote the immersive chat experience, WOC provides high-fidelity virtual avatar manipulation, which also supports the user-defined characters. With the distributed data flow service, the system delivers highly synchronized motion and voice for all users. Deployed on the website and no installation required, users can freely experience the virtual online chat at https://yanch.cloud.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2509.19965.pdf' target='_blank'>https://arxiv.org/pdf/2509.19965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19965">SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <https://novicemm.github.io/synchrorama>.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2508.20623.pdf' target='_blank'>https://arxiv.org/pdf/2508.20623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Xin, Xiaolin Zhang, Yanbin Liu, Peng Zhang, Caifeng Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20623">AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2508.13537.pdf' target='_blank'>https://arxiv.org/pdf/2508.13537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shikun Zhang, Cunjian Chen, Yiqun Wang, Qiuhong Ke, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13537">EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2508.02376.pdf' target='_blank'>https://arxiv.org/pdf/2508.02376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matus Krajcovic, Peter Demcak, Eduard Kuric
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02376">Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied conversational agents (ECAs) are increasingly more realistic and capable of dynamic conversations. In online surveys, anthropomorphic agents could help address issues like careless responding and satisficing, which originate from the lack of personal engagement and perceived accountability. However, there is a lack of understanding of how ECAs in user experience research may affect participant engagement, satisfaction, and the quality of responses. As a proof of concept, we propose an instrument that enables the incorporation of conversations with a virtual avatar into surveys, using on AI-driven video generation, speech recognition, and Large Language Models. In our between-subjects study, 80 participants (UK, stratified random sample of general population) either talked to a voice-based agent with an animated video avatar, or interacted with a chatbot. Across surveys based on two self-reported psychometric tests, 2,265 conversation responses were obtained. Statistical comparison of results indicates that embodied agents can contribute significantly to more informative, detailed responses, as well as higher yet more time-efficient engagement. Furthermore, qualitative analysis provides valuable insights for causes of no significant change to satisfaction, linked to personal preferences, turn-taking delays and Uncanny Valley reactions. These findings support the pursuit and development of new methods toward human-like agents for the transformation of online surveys into more natural interactions resembling in-person interviews.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2507.22326.pdf' target='_blank'>https://arxiv.org/pdf/2507.22326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qun Ma, Xiao Xue, Ming Zhang, Yifan Shen, Zihan Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22326">An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse service is a product of the convergence between Metaverse and service systems, designed to address service-related challenges concerning digital avatars, digital twins, and digital natives within Metaverse. With the rise of large language models (LLMs), agents now play a pivotal role in Metaverse service ecosystem, serving dual functions: as digital avatars representing users in the virtual realm and as service assistants (or NPCs) providing personalized support. However, during the modeling of Metaverse service ecosystems, existing LLM-based agents face significant challenges in bridging virtual-world services with real-world services, particularly regarding issues such as character data fusion, character knowledge association, and ethical safety concerns. This paper proposes an explainable emotion alignment framework for LLM-based agents in Metaverse Service Ecosystem. It aims to integrate factual factors into the decision-making loop of LLM-based agents, systematically demonstrating how to achieve more relational fact alignment for these agents. Finally, a simulation experiment in the Offline-to-Offline food delivery scenario is conducted to evaluate the effectiveness of this framework, obtaining more realistic social emergence.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2506.11829.pdf' target='_blank'>https://arxiv.org/pdf/2506.11829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana MÃ¼ller, Anja Richert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11829">The Space Between Us: A Methodological Framework for Researching Bonding and Proxemics in Situated Group-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a multimethod framework for studying spatial and social dynamics in real-world group-agent interactions with socially interactive agents. Drawing on proxemics and bonding theories, the method combines subjective self-reports and objective spatial tracking. Applied in two field studies in a museum (N = 187) with a robot and a virtual agent, the paper addresses the challenges in aligning human perception and behavior. We focus on presenting an open source, scalable, and field-tested toolkit for future studies.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2506.10462.pdf' target='_blank'>https://arxiv.org/pdf/2506.10462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana MÃ¼ller, Sabina Jeschke, Anja Richert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10462">Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the impact of a group-adaptive conversation design in two socially interactive agents (SIAs) through two real-world studies. Both SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped with a conversational artificial intelligence (CAI) backend combining hybrid retrieval and generative models. The studies were carried out in an in-the-wild setting with a total of $N = 188$ participants who interacted with the SIAs - in dyads, triads or larger groups - at a German museum. Although the results did not reveal a significant effect of the group-sensitive conversation design on perceived satisfaction, the findings provide valuable insights into the challenges of adapting CAI for multi-party interactions and across different embodiments (robot vs.\ virtual agent), highlighting the need for multimodal strategies beyond linguistic pluralization. These insights contribute to the fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and broader Human-Machine Interaction (HMI), providing insights for future research on effective dialogue adaptation in group settings.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2505.08293.pdf' target='_blank'>https://arxiv.org/pdf/2505.08293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhuo Yin, Yuk Hang Tsui, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08293">M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2505.01932.pdf' target='_blank'>https://arxiv.org/pdf/2505.01932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinmu Wang, Xiang Gao, Xiyun Song, Heather Yu, Zongfang Lin, Liang Peng, Xianfeng Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01932">OT-Talk: Animating 3D Talking Head with Optimal Transportation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating 3D head meshes using audio inputs has significant applications in AR/VR, gaming, and entertainment through 3D avatars. However, bridging the modality gap between speech signals and facial dynamics remains a challenge, often resulting in incorrect lip syncing and unnatural facial movements. To address this, we propose OT-Talk, the first approach to leverage optimal transportation to optimize the learning model in talking head animation. Building on existing learning frameworks, we utilize a pre-trained Hubert model to extract audio features and a transformer model to process temporal sequences. Unlike previous methods that focus solely on vertex coordinates or displacements, we introduce Chebyshev Graph Convolution to extract geometric features from triangulated meshes. To measure mesh dissimilarities, we go beyond traditional mesh reconstruction errors and velocity differences between adjacent frames. Instead, we represent meshes as probability measures and approximate their surfaces. This allows us to leverage the sliced Wasserstein distance for modeling mesh variations. This approach facilitates the learning of smooth and accurate facial motions, resulting in coherent and natural facial animations. Our experiments on two public audio-mesh datasets demonstrate that our method outperforms state-of-the-art techniques both quantitatively and qualitatively in terms of mesh reconstruction accuracy and temporal alignment. In addition, we conducted a user perception study with 20 volunteers to further assess the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2504.14373.pdf' target='_blank'>https://arxiv.org/pdf/2504.14373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Zhuo Su, Jian Wang, Shuang Li, Xu Chang, Zhaohu Li, Yang Zhao, Guidong Wang, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14373">SEGA: Drivable 3D Gaussian Head Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2503.21886.pdf' target='_blank'>https://arxiv.org/pdf/2503.21886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pilseo Park, Ze Zhang, Michel Sarkis, Ning Bi, Xiaoming Liu, Yiying Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21886">Refined Geometry-guided Head Avatar Reconstruction from Monocular RGB Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity reconstruction of head avatars from monocular videos is highly desirable for virtual human applications, but it remains a challenge in the fields of computer graphics and computer vision. In this paper, we propose a two-phase head avatar reconstruction network that incorporates a refined 3D mesh representation. Our approach, in contrast to existing methods that rely on coarse template-based 3D representations derived from 3DMM, aims to learn a refined mesh representation suitable for a NeRF that captures complex facial nuances. In the first phase, we train 3DMM-stored NeRF with an initial mesh to utilize geometric priors and integrate observations across frames using a consistent set of latent codes. In the second phase, we leverage a novel mesh refinement procedure based on an SDF constructed from the density field of the initial NeRF. To mitigate the typical noise in the NeRF density field without compromising the features of the 3DMM, we employ Laplace smoothing on the displacement field. Subsequently, we apply a second-phase training with these refined meshes, directing the learning process of the network towards capturing intricate facial details. Our experiments demonstrate that our method further enhances the NeRF rendering based on the initial mesh and achieves performance superior to state-of-the-art methods in reconstructing high-fidelity head avatars with such input.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2501.14646.pdf' target='_blank'>https://arxiv.org/pdf/2501.14646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujian Liu, Shidang Xu, Jing Guo, Dingbin Wang, Zairan Wang, Xianfeng Tan, Xiaoli Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14646">SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating talking avatar driven by audio remains a significant challenge. Existing methods typically require high computational costs and often lack sufficient facial detail and realism, making them unsuitable for applications that demand high real-time performance and visual quality. Additionally, while some methods can synchronize lip movement, they still face issues with consistency between facial expressions and upper body movement, particularly during silent periods. In this paper, we introduce SyncAnimation, the first NeRF-based method that achieves audio-driven, stable, and real-time generation of speaking avatar by combining generalized audio-to-pose matching and audio-to-expression synchronization. By integrating AudioPose Syncer and AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression generation, progressively producing audio-synchronized upper body, head, and lip shapes. Furthermore, the High-Synchronization Human Renderer ensures seamless integration of the head and upper body, and achieves audio-sync lip. The project page can be found at https://syncanimation.github.io
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2501.07104.pdf' target='_blank'>https://arxiv.org/pdf/2501.07104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07104">RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures low-frequency motion and surface deformation of the avatar. Due to the limitations of LBS formula, the human skeleton is hard to control complex non-rigid transformations. We then design a pose-related Gaussian rectification module to learn fine-detailed non-rigid deformations, further improving the realism and expressiveness of the avatar. We conduct extensive experiments on public datasets, RMAvatar shows state-of-the-art performance on both rendering quality and quantitative evaluations. Please see our project page at https://rm-avatar.github.io.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2412.00845.pdf' target='_blank'>https://arxiv.org/pdf/2412.00845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghan Chen, Yang Cong, Jiayue Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00845">SAGA: Surface-Aligned Gaussian Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a Surface-Aligned Gaussian representation for creating animatable human avatars from monocular videos,aiming at improving the novel view and pose synthesis performance while ensuring fast training and real-time rendering. Recently,3DGS has emerged as a more efficient and expressive alternative to NeRF, and has been used for creating dynamic human avatars. However,when applied to the severely ill-posed task of monocular dynamic reconstruction, the Gaussians tend to overfit the constantly changing regions such as clothes wrinkles or shadows since these regions cannot provide consistent supervision, resulting in noisy geometry and abrupt deformation that typically fail to generalize under novel views and poses.To address these limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns the Gaussians with a mesh to enforce well-defined geometry and consistent deformation, thereby improving generalization under novel views and poses. Unlike existing strict alignment methods that suffer from limited expressive power and low realism,SAGA employs a two-stage alignment strategy where the Gaussians are first adhered on while then detached from the mesh, thus facilitating both good geometry and high expressivity. In the Adhered Stage, we improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow on the mesh, in contrast to existing methods that rigidly bind Gaussians to fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh Alignment regularization, which allows us to unleash the expressivity by detaching the Gaussians but maintain the geometric alignment by minimizing their location and orientation offsets from the bound triangles. Finally, since the Gaussians may drift outside the bound triangles during optimization, an efficient Walking-on-Mesh strategy is proposed to dynamically update the bound triangles.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2407.00229.pdf' target='_blank'>https://arxiv.org/pdf/2407.00229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirban Mukherjee, Venkat Suprabath Bitra, Vignesh Bondugula, Tarun Reddy Tallapureddy, Dinesh Babu Jayagopi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00229">SemUV: Deep Learning based semantic manipulation over UV texture map of virtual human heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing and manipulating virtual human heads is essential across various applications, including AR, VR, gaming, human-computer interaction and VFX. Traditional graphic-based approaches require manual effort and resources to achieve accurate representation of human heads. While modern deep learning techniques can generate and edit highly photorealistic images of faces, their focus remains predominantly on 2D facial images. This limitation makes them less suitable for 3D applications. Recognizing the vital role of editing within the UV texture space as a key component in the 3D graphics pipeline, our work focuses on this aspect to benefit graphic designers by providing enhanced control and precision in appearance manipulation. Research on existing methods within the UV texture space is limited, complex, and poses challenges. In this paper, we introduce SemUV: a simple and effective approach using the FFHQ-UV dataset for semantic manipulation directly within the UV texture space. We train a StyleGAN model on the publicly available FFHQ-UV dataset, and subsequently train a boundary for interpolation and semantic feature manipulation. Through experiments comparing our method with 2D manipulation technique, we demonstrate its superior ability to preserve identity while effectively modifying semantic features such as age, gender, and facial hair. Our approach is simple, agnostic to other 3D components such as structure, lighting, and rendering, and also enables seamless integration into standard 3D graphics pipelines without demanding extensive domain expertise, time, or resources.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2406.16478.pdf' target='_blank'>https://arxiv.org/pdf/2406.16478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucie Galland, Catherine Pelachaud, Florian Pecune
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16478">EMMI -- Empathic Multimodal Motivational Interviews Dataset: Analyses and Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of multimodal interaction in therapy can yield a comprehensive understanding of therapist and patient behavior that can be used to develop a multimodal virtual agent supporting therapy. This investigation aims to uncover how therapists skillfully blend therapy's task goal (employing classical steps of Motivational Interviewing) with the social goal (building a trusting relationship and expressing empathy). Furthermore, we seek to categorize patients into various ``types'' requiring tailored therapeutic approaches. To this intent, we present multimodal annotations of a corpus consisting of simulated motivational interviewing conversations, wherein actors portray the roles of patients and therapists. We introduce EMMI, composed of two publicly available MI corpora, AnnoMI and the Motivational Interviewing Dataset, for which we add multimodal annotations. We analyze these annotations to characterize functional behavior for developing a virtual agent performing motivational interviews emphasizing social and empathic behaviors. Our analysis found three clusters of patients expressing significant differences in behavior and adaptation of the therapist's behavior to those types. This shows the importance of a therapist being able to adapt their behavior depending on the current situation within the dialog and the type of user.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2405.18565.pdf' target='_blank'>https://arxiv.org/pdf/2405.18565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keiichi Ihara, Kyzyl Monteiro, Mehrad Faridan, Rubaiat Habib Kazi, Ryo Suzuki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18565">Video2MR: Automatically Generating Mixed Reality 3D Instructions by Augmenting Extracted Motion from 2D Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Video2MR, a mixed reality system that automatically generates 3D sports and exercise instructions from 2D videos. Mixed reality instructions have great potential for physical training, but existing works require substantial time and cost to create these 3D experiences. Video2MR overcomes this limitation by transforming arbitrary instructional videos available online into MR 3D avatars with AI-enabled motion capture (DeepMotion). Then, it automatically enhances the avatar motion through the following augmentation techniques: 1) contrasting and highlighting differences between the user and avatar postures, 2) visualizing key trajectories and movements of specific body parts, 3) manipulation of time and speed using body motion, and 4) spatially repositioning avatars for different perspectives. Developed on Hololens 2 and Azure Kinect, we showcase various use cases, including yoga, dancing, soccer, tennis, and other physical exercises. The study results confirm that Video2MR provides more engaging and playful learning experiences, compared to existing 2D video instructions.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2404.18628.pdf' target='_blank'>https://arxiv.org/pdf/2404.18628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Maiorca, Seyed Abolfazl Ghasemzadeh, Thierry Ravet, FranÃ§ois Cresson, Thierry Dutoit, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18628">Self-Avatar Animation in Virtual Reality: Impact of Motion Signals Artifacts on the Full-Body Pose Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) applications have revolutionized user experiences by immersing individuals in interactive 3D environments. These environments find applications in numerous fields, including healthcare, education, or architecture. A significant aspect of VR is the inclusion of self-avatars, representing users within the virtual world, which enhances interaction and embodiment. However, generating lifelike full-body self-avatar animations remains challenging, particularly in consumer-grade VR systems, where lower-body tracking is often absent. One method to tackle this problem is by providing an external source of motion information that includes lower body information such as full Cartesian positions estimated from RGB(D) cameras. Nevertheless, the limitations of these systems are multiples: the desynchronization between the two motion sources and occlusions are examples of significant issues that hinder the implementations of such systems. In this paper, we aim to measure the impact on the reconstruction of the articulated self-avatar's full-body pose of (1) the latency between the VR motion features and estimated positions, (2) the data acquisition rate, (3) occlusions, and (4) the inaccuracy of the position estimation algorithm. In addition, we analyze the motion reconstruction errors using ground truth and 3D Cartesian coordinates estimated from \textit{YOLOv8} pose estimation. These analyzes show that the studied methods are significantly sensitive to any degradation tested, especially regarding the velocity reconstruction error.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2404.16040.pdf' target='_blank'>https://arxiv.org/pdf/2404.16040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megan A. Witherow, Norou Diawara, Janice Keener, John W. Harrington, Khan M. Iftekharuddin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16040">Pilot Study to Discover Candidate Biomarkers for Autism based on Perception and Production of Facial Expressions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: Facial expression production and perception in autism spectrum disorder (ASD) suggest potential presence of behavioral biomarkers that may stratify individuals on the spectrum into prognostic or treatment subgroups. Construct validity and group discriminability have been recommended as criteria for identification of candidate stratification biomarkers.
  Methods: In an online pilot study of 11 children and young adults diagnosed with ASD and 11 age- and gender-matched neurotypical (NT) individuals, participants recognize and mimic static and dynamic facial expressions of 3D avatars. Webcam-based eye-tracking (ET) and facial video tracking (VT), including activation and asymmetry of action units (AUs) from the Facial Action Coding System (FACS) are collected. We assess validity of constructs for each dependent variable (DV) based on the expected response in the NT group. Then, the Boruta statistical method identifies DVs that are significant to group discriminability (ASD or NT).
  Results: We identify one candidate ET biomarker (percentage gaze duration to the face while mimicking static 'disgust' expression) and 14 additional DVs of interest for future study, including 4 ET DVs, 5 DVs related to VT AU activation, and 4 DVs related to AU asymmetry in VT. Based on a power analysis, we provide sample size recommendations for future studies.
  Conclusion: This pilot study provides a framework for ASD stratification biomarker discovery based on perception and production of facial expressions.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2404.01296.pdf' target='_blank'>https://arxiv.org/pdf/2404.01296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01296">MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: https://syntec-research.github.io/MagicMirror
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2404.00368.pdf' target='_blank'>https://arxiv.org/pdf/2404.00368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Liu, Qiong Cao, Yandong Wen, Huaiguang Jiang, Changxing Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00368">Towards Variable and Coordinated Holistic Co-Speech Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of generating lifelike holistic co-speech motions for 3D avatars, focusing on two key aspects: variability and coordination. Variability allows the avatar to exhibit a wide range of motions even with similar speech content, while coordination ensures a harmonious alignment among facial expressions, hand gestures, and body poses. We aim to achieve both with ProbTalk, a unified probabilistic framework designed to jointly model facial, hand, and body movements in speech. ProbTalk builds on the variational autoencoder (VAE) architecture and incorporates three core designs. First, we introduce product quantization (PQ) to the VAE, which enriches the representation of complex holistic motion. Second, we devise a novel non-autoregressive model that embeds 2D positional encoding into the product-quantized representation, thereby preserving essential structure information of the PQ codes. Last, we employ a secondary stage to refine the preliminary prediction, further sharpening the high-frequency details. Coupling these three designs enables ProbTalk to generate natural and diverse holistic co-speech motions, outperforming several state-of-the-art methods in qualitative and quantitative evaluations, particularly in terms of realism. Our code and model will be released for research purposes at https://feifeifeiliu.github.io/probtalk/.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2403.07314.pdf' target='_blank'>https://arxiv.org/pdf/2403.07314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megan A. Witherow, Crystal Butler, Winston J. Shields, Furkan Ilgin, Norou Diawara, Janice Keener, John W. Harrington, Khan M. Iftekharuddin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07314">Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more. However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert. To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss encourages feature correlation with AUs while discouraging correlation with subject identities for improved generalization. We train BeCoME-Net for unilateral and bilateral AU detection and compare with state-of-the-art approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty healthy adult volunteers complete expression recognition and mimicry tasks in an online feasibility study while webcam-based eye-tracking and video are collected. We test validity of multiple constructs, including face preference during recognition and AUs during mimicry.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2403.05218.pdf' target='_blank'>https://arxiv.org/pdf/2403.05218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05218">3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech. Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning. However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes. To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process. Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based Graph Convolution Encoder applied to the facial mesh. This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW benchmark.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2402.05448.pdf' target='_blank'>https://arxiv.org/pdf/2402.05448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bumsoo Kim, Sanghyun Byun, Yonghoon Jung, Wonseop Shin, Sareer UI Amin, Sanghyun Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05448">Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2312.12096.pdf' target='_blank'>https://arxiv.org/pdf/2312.12096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunjie Luo, Fei Luo, Yusen Wang, Enxu Zhao, Chunxia Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12096">DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing a dynamic human with loose clothing is an important but difficult task. To address this challenge, we propose a method named DLCA-Recon to create human avatars from monocular videos. The distance from loose clothing to the underlying body rapidly changes in every frame when the human freely moves and acts. Previous methods lack effective geometric initialization and constraints for guiding the optimization of deformation to explain this dramatic change, resulting in the discontinuous and incomplete reconstruction surface. To model the deformation more accurately, we propose to initialize an estimated 3D clothed human in the canonical space, as it is easier for deformation fields to learn from the clothed human than from SMPL. With both representations of explicit mesh and implicit SDF, we utilize the physical connection information between consecutive frames and propose a dynamic deformation field (DDF) to optimize deformation fields. DDF accounts for contributive forces on loose clothing to enhance the interpretability of deformations and effectively capture the free movement of loose clothing. Moreover, we propagate SMPL skinning weights to each individual and refine pose and skinning weights during the optimization to improve skinning transformation. Based on more reasonable initialization and DDF, we can simulate real-world physics more accurately. Extensive experiments on public and our own datasets validate that our method can produce superior results for humans with loose clothing compared to the SOTA methods.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2311.11614.pdf' target='_blank'>https://arxiv.org/pdf/2311.11614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixiang Lin, Jianke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11614">Semantic-Preserved Point-based Human Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable realistic experience in AR/VR and digital entertainment, we present the first point-based human avatar model that embodies the entirety expressive range of digital humans. We employ two MLPs to model pose-dependent deformation and linear skinning (LBS) weights. The representation of appearance relies on a decoder and the features that attached to each point. In contrast to alternative implicit approaches, the oriented points representation not only provides a more intuitive way to model human avatar animation but also significantly reduces both training and inference time. Moreover, we propose a novel method to transfer semantic information from the SMPL-X model to the points, which enables to better understand human body movements. By leveraging the semantic information of points, we can facilitate virtual try-on and human avatar composition through exchanging the points of same category across different subjects. Experimental results demonstrate the efficacy of our presented method.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2311.05521.pdf' target='_blank'>https://arxiv.org/pdf/2311.05521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, Yan-Pei Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05521">BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing photorealistic 4D human head avatars from videos is essential for VR/AR, telepresence, and video game applications. Although existing Neural Radiance Fields (NeRF)-based methods achieve high-fidelity results, the computational expense limits their use in real-time applications. To overcome this limitation, we introduce BakedAvatar, a novel representation for real-time neural head avatar synthesis, deployable in a standard polygon rasterization pipeline. Our approach extracts deformable multi-layer meshes from learned isosurfaces of the head and computes expression-, pose-, and view-dependent appearances that can be baked into static textures for efficient rasterization. We thus propose a three-stage pipeline for neural head avatar synthesis, which includes learning continuous deformation, manifold, and radiance fields, extracting layered meshes and textures, and fine-tuning texture details with differential rasterization. Experimental results demonstrate that our representation generates synthesis results of comparable quality to other state-of-the-art methods while significantly reducing the inference time required. We further showcase various head avatar synthesis results from monocular videos, including view synthesis, face reenactment, expression editing, and pose editing, all at interactive frame rates.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2311.04218.pdf' target='_blank'>https://arxiv.org/pdf/2311.04218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijuan Liu, Xiangyu Xu, Zhijie Lin, Jiabin Liang, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04218">Towards Garment Sewing Pattern Reconstruction from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Garment sewing pattern represents the intrinsic rest shape of a garment, and is the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. Code, dataset, and pre-trained models are available at: https://sewformer.github.io.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2309.10902.pdf' target='_blank'>https://arxiv.org/pdf/2309.10902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiffany D. Do, Steve Zelenty, Mar Gonzalez-Franco, Ryan P. McMahan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10902">VALID: A perceptually validated Virtual Avatar Library for Inclusion and Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As consumer adoption of immersive technologies grows, virtual avatars will play a prominent role in the future of social computing. However, as people begin to interact more frequently through virtual avatars, it is important to ensure that the research community has validated tools to evaluate the effects and consequences of such technologies. We present the first iteration of a new, freely available 3D avatar library called the Virtual Avatar Library for Inclusion and Diversity (VALID), which includes 210 fully rigged avatars with a focus on advancing racial diversity and inclusion. We present a detailed process for creating, iterating, and validating avatars of diversity. Through a large online study (n=132) with participants from 33 countries, we provide statistically validated labels for each avatar's perceived race and gender. Through our validation study, we also advance knowledge pertaining to the perception of an avatar's race. In particular, we found that avatars of some races were more accurately identified by participants of the same race.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2308.13551.pdf' target='_blank'>https://arxiv.org/pdf/2308.13551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyue Yao, Mingjie Sun, Bingliang Li, Fengyu Yang, Junle Wang, Ruimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13551">Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as references for motion generation. Then, we introduce a hyper-parameter that coordinates the similarity between dancers by masking poses to prevent the generation of sequences that are over-diverse or consistent. To avoid the rigidity of movements, we design a Dance Pre-generated stage to pre-generate these masked poses instead of filling them with zeros. After that, a Dance Motion Transfer stage is adopted with leader sequences and music, in which a multi-conditional sampling formula is rewritten to transfer the pre-generated poses into a sequence with a partner style. In practice, to address the lack of multi-person datasets, we introduce AIST-M, a new dataset for partner dancer generation, which is publicly availiable. Comprehensive evaluations on our AIST-M dataset demonstrate that the proposed DanY can synthesize satisfactory partner dancer results with controllable diversity.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2308.05112.pdf' target='_blank'>https://arxiv.org/pdf/2308.05112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Zhang, Jie Chen, Qiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05112">Explicifying Neural Implicit Fields for Efficient Dynamic Human Avatar Modeling via a Neural Explicit Surface</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a technique for efficiently modeling dynamic humans by explicifying the implicit neural fields via a Neural Explicit Surface (NES). Implicit neural fields have advantages over traditional explicit representations in modeling dynamic 3D content from sparse observations and effectively representing complex geometries and appearances. Implicit neural fields defined in 3D space, however, are expensive to render due to the need for dense sampling during volumetric rendering. Moreover, their memory efficiency can be further optimized when modeling sparse 3D space. To overcome these issues, the paper proposes utilizing Neural Explicit Surface (NES) to explicitly represent implicit neural fields, facilitating memory and computational efficiency. To achieve this, the paper creates a fully differentiable conversion between the implicit neural fields and the explicit rendering interface of NES, leveraging the strengths of both implicit and explicit approaches. This conversion enables effective training of the hybrid representation using implicit methods and efficient rendering by integrating the explicit rendering interface with a newly proposed rasterization-based neural renderer that only incurs a texture color query once for the initial ray interaction with the explicit surface, resulting in improved inference efficiency. NES describes dynamic human geometries with pose-dependent neural implicit surface deformation fields and their dynamic neural textures both in 2D space, which is a more memory-efficient alternative to traditional 3D methods, reducing redundancy and computational load. The comprehensive experiments show that NES performs similarly to previous 3D approaches, with greatly improved rendering speed and reduced memory cost.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2308.03610.pdf' target='_blank'>https://arxiv.org/pdf/2308.03610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, Min Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03610">AvatarVerse: High-quality & Stable 3D Avatar Creation from Text and Pose</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating expressive, diverse and high-quality 3D avatars from highly customized text descriptions and pose guidance is a challenging task, due to the intricacy of modeling and texturing in 3D that ensure details and various styles (realistic, fictional, etc). We present AvatarVerse, a stable pipeline for generating expressive high-quality 3D avatars from nothing but text descriptions and pose guidance. In specific, we introduce a 2D diffusion model conditioned on DensePose signal to establish 3D pose control of avatars through 2D images, which enhances view consistency from partially observed scenarios. It addresses the infamous Janus Problem and significantly stablizes the generation process. Moreover, we propose a progressive high-resolution 3D synthesis strategy, which obtains substantial improvement over the quality of the created 3D avatars. To this end, the proposed AvatarVerse pipeline achieves zero-shot 3D modeling of 3D avatars that are not only more expressive, but also in higher quality and fidelity than previous works. Rigorous qualitative evaluations and user studies showcase AvatarVerse's superiority in synthesizing high-fidelity 3D avatars, leading to a new standard in high-quality and stable 3D avatar creation. Our project page is: https://avatarverse3d.github.io
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2307.05501.pdf' target='_blank'>https://arxiv.org/pdf/2307.05501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruslan Isaev, Radmir Gumerov, Gulzada Esenalieva, Remudin Reshid Mekuria, Ermek Doszhanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05501">HIVA: Holographic Intellectual Voice Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Holographic Intellectual Voice Assistant (HIVA) aims to facilitate human computer interaction using audiovisual effects and 3D avatar. HIVA provides complete information about the university, including requests of various nature: admission, study issues, fees, departments, university structure and history, canteen, human resources, library, student life and events, information about the country and the city, etc. There are other ways for receiving the data listed above: the university's official website and other supporting apps, HEI (Higher Education Institution) official social media, directly asking the HEI staff, and other channels. However, HIVA provides the unique experience of "face-to-face" interaction with an animated 3D mascot, helping to get a sense of 'real-life' communication. The system includes many sub-modules and connects a family of applications such as mobile applications, Telegram chatbot, suggestion categorization, and entertainment services. The Voice assistant uses Russian language NLP models and tools, which are pipelined for the best user experience.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2303.11463.pdf' target='_blank'>https://arxiv.org/pdf/2303.11463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio MartÃ­n Serrano, RubÃ©n Izquierdo, IvÃ¡n GarcÃ­a Daza, Miguel Ãngel Sotelo, David FernÃ¡ndez Llorca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11463">Digital twin in virtual reality for human-vehicle interactions in the context of autonomous driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the results of tests of interactions between real humans and simulated vehicles in a virtual scenario. Human activity is inserted into the virtual world via a virtual reality interface for pedestrians. The autonomous vehicle is equipped with a virtual Human-Machine interface (HMI) and drives through the digital twin of a real crosswalk. The HMI was combined with gentle and aggressive braking maneuvers when the pedestrian intended to cross. The results of the interactions were obtained through questionnaires and measurable variables such as the distance to the vehicle when the pedestrian initiated the crossing action. The questionnaires show that pedestrians feel safer whenever HMI is activated and that varying the braking maneuver does not influence their perception of danger as much, while the measurable variables show that both HMI activation and the gentle braking maneuver cause the pedestrian to cross earlier.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2303.00744.pdf' target='_blank'>https://arxiv.org/pdf/2303.00744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Saunders, Vinay Namboodiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00744">READ Avatars: Realistic Emotion-controllable Audio Driven Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present READ Avatars, a 3D-based approach for generating 2D avatars that are driven by audio input with direct and granular control over the emotion. Previous methods are unable to achieve realistic animation due to the many-to-many nature of audio to expression mappings. We alleviate this issue by introducing an adversarial loss in the audio-to-expression generation process. This removes the smoothing effect of regression-based models and helps to improve the realism and expressiveness of the generated avatars. We note furthermore, that audio should be directly utilized when generating mouth interiors and that other 3D-based methods do not attempt this. We address this with audio-conditioned neural textures, which are resolution-independent. To evaluate the performance of our method, we perform quantitative and qualitative experiments, including a user study. We also propose a new metric for comparing how well an actor's emotion is reconstructed in the generated avatar. Our results show that our approach outperforms state of the art audio-driven avatar generation methods across several metrics. A demo video can be found at \url{https://youtu.be/QSyMl3vV0pA}
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2302.01880.pdf' target='_blank'>https://arxiv.org/pdf/2302.01880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kelly Mack, Rai Ching Ling Hsu, AndrÃ©s Monroy-HernÃ¡ndez, Brian A. Smith, Fannie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01880">Towards Inclusive Avatars: Disability Representation in Avatar Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital avatars are an important part of identity representation, but there is little work on understanding how to represent disability. We interviewed 18 people with disabilities and related identities about their experiences and preferences in representing their identities with avatars. Participants generally preferred to represent their disability identity if the context felt safe and platforms supported their expression, as it was important for feeling authentically represented. They also utilized avatars in strategic ways: as a means to signal and disclose current abilities, access needs, and to raise awareness. Some participants even found avatars to be a more accessible way to communicate than alternatives. We discuss how avatars can support disability identity representation because of their easily customizable format that is not strictly tied to reality. We conclude with design recommendations for creating platforms that better support people in representing their disability and other minoritized identities.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2212.02978.pdf' target='_blank'>https://arxiv.org/pdf/2212.02978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mia Chiquier, Carl Vondrick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02978">Muscles in Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion is created by, and constrained by, our muscles. We take a first step at building computer vision methods that represent the internal muscle activity that causes motion. We present a new dataset, Muscles in Action (MIA), to learn to incorporate muscle activity into human motion representations. The dataset consists of 12.5 hours of synchronized video and surface electromyography (sEMG) data of 10 subjects performing various exercises. Using this dataset, we learn a bidirectional representation that predicts muscle activation from video, and conversely, reconstructs motion from muscle activation. We evaluate our model on in-distribution subjects and exercises, as well as on out-of-distribution subjects and exercises. We demonstrate how advances in modeling both modalities jointly can serve as conditioning for muscularly consistent motion generation. Putting muscles into computer vision systems will enable richer models of virtual humans, with applications in sports, fitness, and AR/VR.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2211.11903.pdf' target='_blank'>https://arxiv.org/pdf/2211.11903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Purva Tendulkar, DÃ­dac SurÃ­s, Carl Vondrick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11903">FLEX: Full-Body Grasping Without Full-Body Grasps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in AR/VR, video games and robotics. Towards this goal, we address the task of generating a virtual human -- hands and full body -- grasping everyday objects. Existing methods approach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object positions and orientations, or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasping data. Our key insight is to leverage the existence of both full-body pose and hand grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are superior to baselines both quantitatively and qualitatively. See our webpage for more details: https://flex.cs.columbia.edu/.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2201.12894.pdf' target='_blank'>https://arxiv.org/pdf/2201.12894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhi Cheng, Nan Wu, Songqing Chen, Bo Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.12894">Will Metaverse be NextG Internet? Vision, Hype, and Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse, with the combination of the prefix "meta" (meaning transcending) and the word "universe", has been deemed as the next-generation (NextG) Internet. It aims to create a shared virtual space that connects all virtual worlds via the Internet, where users, represented as digital avatars, can communicate and collaborate as if they are in the physical world. Nevertheless, there is still no unified definition of the Metaverse. This article first presents our vision of what the key requirements of Metaverse should be and reviews what has been heavily advocated by the industry and the positions of various high-tech companies. It then briefly introduces existing social virtual reality (VR) platforms that can be viewed as early prototypes of Metaverse and conducts a reality check by diving into the network operation and performance of two representative platforms, Workrooms from Meta and AltspaceVR from Microsoft. Finally, it concludes by discussing several opportunities and future directions for further innovation.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2004.01451.pdf' target='_blank'>https://arxiv.org/pdf/2004.01451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerard Llorach, Maartje M. E. Hendrikse, Giso Grimm, Volker Hohmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2004.01451">Comparison of a Head-Mounted Display and a Curved Screen in a Multi-Talker Audiovisual Listening Task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introduction: Virtual audiovisual technology and its methodology has yet to be established for psychoacoustic research. This study examined the effects of different audiovisual conditions on preference when listening to multi-talker conversations. The study's goal is to explore and assess audiovisual technologies in the context of hearing research. Methods: The participants listened to audiovisual conversations between four talkers. Two displays were tested and compared: a curved screen (CS) and a head-mounted display (HMD). Using three visual conditions (audio-only, virtual characters and video recordings), three groups of participants were tested: seventeen young normal-hearing, ten older normal-hearing, and ten older hearing-impaired listeners. Results: Open interviews showed that the CS was preferred over the HMD for older normal-hearing participants and that video recordings were the preferred visual condition. Young and older hearing-impaired participants did not show a preference between the CS and the HMD. Conclusions: CSs and video recordings should be the preferred audiovisual setup of laboratories and clinics, although HMDs and virtual characters can be used for hearing research when necessary and suitable.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2509.11411.pdf' target='_blank'>https://arxiv.org/pdf/2509.11411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Zioulis, Nikolaos Kotarelas, Georgios Albanis, Spyridon Thermos, Anargyros Chatzitofis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11411">On the Skinning of Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2508.19754.pdf' target='_blank'>https://arxiv.org/pdf/2508.19754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wu, Yufan Wu, Wen Li, Yuxi Lu, Kairui Feng, Xuanhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19754">FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2508.09402.pdf' target='_blank'>https://arxiv.org/pdf/2508.09402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Von Ralph Dane Marquez Herbuela, Yukie Nagai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09402">Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many individuals especially those with autism spectrum disorder (ASD), alexithymia, or other neurodivergent profiles face challenges in recognizing, expressing, or interpreting emotions. To support more inclusive and personalized emotion technologies, we present a real-time multimodal emotion estimation system that combines neurophysiological EEG, ECG, blood volume pulse (BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial expressions, and speech) in a unified arousal-valence 2D interface to track moment-to-moment emotional states. This architecture enables interpretable, user-specific analysis and supports applications in emotion education, neuroadaptive feedback, and interaction support for neurodiverse users. Two demonstration scenarios illustrate its application: (1) passive media viewing (2D or VR videos) reveals cortical and autonomic responses to affective content, and (2) semi-scripted conversations with a facilitator or virtual agent capture real-time facial and vocal expressions. These tasks enable controlled and naturalistic emotion monitoring, making the system well-suited for personalized feedback and neurodiversity-informed interaction design.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2508.08930.pdf' target='_blank'>https://arxiv.org/pdf/2508.08930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juyeong Hwang, Seong-Eun Hon, JaeYoung Seon, Hyeongyeop Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08930">How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive Reasoning for Embodied Head Rotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural head rotation is critical for believable embodied virtual agents, yet this micro-level behavior remains largely underexplored. While head-rotation prediction algorithms could, in principle, reproduce this behavior, they typically focus on visually salient stimuli and overlook the cognitive motives that guide head rotation. This yields agents that look at conspicuous objects while overlooking obstacles or task-relevant cues, diminishing realism in a virtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning framework for Embodied Head Rotation, a data-agnostic framework that produces context-aware head movements without task-specific training or hand-tuned heuristics. A controlled VR study (N=20) identifies five motivational drivers of human head movements: Interest, Information Seeking, Safety, Social Schema, and Habit. SCORE encodes these drivers as symbolic predicates, perceives the scene with a Vision-Language Model (VLM), and plans head poses with a Large Language Model (LLM). The framework employs a hybrid workflow: the VLM-LLM reasoning is executed offline, after which a lightweight FastVLM performs online validation to suppress hallucinations while maintaining responsiveness to scene dynamics. The result is an agent that predicts not only where to look but also why, generalizing to unseen scenes and multi-agent crowds while retaining behavioral plausibility.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2508.04505.pdf' target='_blank'>https://arxiv.org/pdf/2508.04505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daisheng Jin, Ying He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04505">MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing realistic 3D human avatars from monocular videos is a challenging task due to the limited geometric information and complex non-rigid motion involved. We present MonoCloth, a new method for reconstructing and animating clothed human avatars from monocular videos. To overcome the limitations of monocular input, we introduce a part-based decomposition strategy that separates the avatar into body, face, hands, and clothing. This design reflects the varying levels of reconstruction difficulty and deformation complexity across these components. Specifically, we focus on detailed geometry recovery for the face and hands. For clothing, we propose a dedicated cloth simulation module that captures garment deformation using temporal motion cues and geometric constraints. Experimental results demonstrate that MonoCloth improves both visual reconstruction quality and animation realism compared to existing methods. Furthermore, thanks to its part-based design, MonoCloth also supports additional tasks such as clothing transfer, underscoring its versatility and practical utility.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2507.23454.pdf' target='_blank'>https://arxiv.org/pdf/2507.23454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marta BieÅkiewicz, Julia Ayache, Panayiotis Charalambous, Cristina Becchio, Marco Corragio, Bertram Taetz, Francesco De Lellis, Antonio Grotta, Anna Server, Daniel Rammer, Richard Kulpa, Franck Multon, Azucena Garcia-Palacios, Jessica Sutherland, Kathleen Bryson, StÃ©phane Donikian, Didier Stricker, BenoÃ®t Bardy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23454">Breaking the mould of Social Mixed Reality - State-of-the-Art and Glossary</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article explores a critical gap in Mixed Reality (MR) technology: while advances have been made, MR still struggles to authentically replicate human embodiment and socio-motor interaction. For MR to enable truly meaningful social experiences, it needs to incorporate multi-modal data streams and multi-agent interaction capabilities. To address this challenge, we present a comprehensive glossary covering key topics such as Virtual Characters and Autonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges of Social MR within Neuroscience, Embodiment, and Technology. Our aim is to drive the transformative evolution of MR technologies that prioritize human-centric innovation, fostering richer digital connections. We advocate for MR systems that enhance social interaction and collaboration between humans and virtual autonomous agents, ensuring inclusivity, ethical design and psychological safety in the process.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2507.17327.pdf' target='_blank'>https://arxiv.org/pdf/2507.17327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao He, Jianqiang Ren, Jianjing Xiang, Xiejie Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17327">CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of large foundation models, AIGC, cloud rendering, and real-time motion capture technologies, digital humans are now capable of achieving synchronized facial expressions and body movements, engaging in intelligent dialogues driven by natural language, and enabling the fast creation of personalized avatars. While current mainstream approaches to digital humans primarily focus on 3D models and 2D video-based representations, interactive 2D cartoon-style digital humans have received relatively less attention. Compared to 3D digital humans that require complex modeling and high rendering costs, and 2D video-based solutions that lack flexibility and real-time interactivity, 2D cartoon-style Live2D models offer a more efficient and expressive alternative. By simulating 3D-like motion through layered segmentation without the need for traditional 3D modeling, Live2D enables dynamic and real-time manipulation. In this technical report, we present CartoonAlive, an innovative method for generating high-quality Live2D digital humans from a single input portrait image. CartoonAlive leverages the shape basis concept commonly used in 3D face modeling to construct facial blendshapes suitable for Live2D. It then infers the corresponding blendshape weights based on facial keypoints detected from the input image. This approach allows for the rapid generation of a highly expressive and visually accurate Live2D model that closely resembles the input portrait, within less than half a minute. Our work provides a practical and scalable solution for creating interactive 2D cartoon characters, opening new possibilities in digital content creation and virtual character animation. The project homepage is https://human3daigc.github.io/CartoonAlive_webpage/.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2507.16542.pdf' target='_blank'>https://arxiv.org/pdf/2507.16542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiong Wu, Yan Dong, Zipeng Zhang, Ruochen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16542">The Effect of Scale Consistency between Real and Virtual Spaces on Immersion in Exhibition Hybrid Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In exhibition hybrid spaces, scale consistency between real and virtual spaces is crucial for user immersion. However, there is currently a lack of systematic research to determine appropriate virtual-to-real mapping ratios. This study developed an immersive interaction system based on Intel 3D Athlete Tracking body mapping technology. Two experiments investigated the impact of virtual space and virtual avatar scale on immersion. Experiment 1 investigated 30 participants' preferences for virtual space scale, while Experiment 2 tested the effect of 6 different virtual avatar sizes (25%-150%) on immersion. A 5-point Likert scale was used to assess immersion, followed by analysis of variance and Tukey HSD post-hoc tests. Experiment 1 showed that participants preferred a virtual space ratio of 130% (mean 127.29%, SD 8.55%). Experiment 2 found that virtual avatar sizes within the 75%-100% range produced optimal immersion (p < 0.05). Immersion decreased significantly when virtual avatar sizes deviated from users' actual height (below 50% or above 125%). Participants were more sensitive to size changes in the 25%-75% range, while perception was weaker for changes in the 75%-100% range. Virtual environments slightly larger than real space (130%) and virtual avatars slightly smaller than users (75%-100%) optimize user immersion. These findings have been applied in the Intel Global Trade Center exhibition hall, demonstrating actionable insights for designing hybrid spaces that enhance immersion and coherence.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2507.10469.pdf' target='_blank'>https://arxiv.org/pdf/2507.10469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikko Korkiakoski, Saeid Sheikhi, Jesper Nyman, Jussi Saariniemi, Kalle Tapio, Panos Kostakos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10469">An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in artificial intelligence (AI) have significantly enhanced the realism and interactivity of non-player characters (NPCs) in virtual reality (VR), creating more engaging and believable user experiences. This paper evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their perceived realism, usability, and system performance. The simulator features two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage participants in a scenario to determine the suspect's guilt or innocence. A user study with 18 participants assessed the system using the System Usability Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent Believability Questionnaire, alongside latency measurements for speech-to-text (STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency. Results showed an average cycle latency of 7 seconds, influenced by the increasing conversational context. Believability scored 6.67 out of 10, with high ratings in behavior, social relationships, and intelligence but moderate scores in emotion and personality. The system achieved a SUS score of 79.44, indicating good usability. These findings demonstrate the potential of large language models to improve NPC realism and interaction in VR while highlighting challenges in reducing system latency and enhancing emotional depth. This research contributes to the development of more sophisticated AI-driven NPCs, revealing the need for performance optimization to achieve increasingly immersive virtual experiences.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2506.23777.pdf' target='_blank'>https://arxiv.org/pdf/2506.23777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Du, Kiran Chhatre, Christopher Peters, Brian Keegan, Rachel McDonnell, Cathy Ennis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23777">Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of virtual humans increasingly leverages automated synthesis of speech and gestures, enabling expressive, adaptable agents that effectively engage users. However, the independent development of voice and gesture generation technologies, alongside the growing popularity of virtual reality (VR), presents significant questions about the integration of these signals and their ability to convey emotional detail in immersive environments. In this paper, we evaluate the influence of real and synthetic gestures and speech, alongside varying levels of immersion (VR vs. 2D displays) and emotional contexts (positive, neutral, negative) on user perceptions. We investigate how immersion affects the perceived match between gestures and speech and the impact on key aspects of user experience, including emotional and empathetic responses and the sense of co-presence. Our findings indicate that while VR enhances the perception of natural gesture-voice pairings, it does not similarly improve synthetic ones - amplifying the perceptual gap between them. These results highlight the need to reassess gesture appropriateness and refine AI-driven synthesis for immersive environments. Supplementary video: https://youtu.be/WMfjIB1X-dc
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2506.14268.pdf' target='_blank'>https://arxiv.org/pdf/2506.14268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Aymerich-Franch, Tarek Taha, Takahiro Miyashita, Hiroko Kamide, Hiroshi Ishiguro, Paolo Dario
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14268">Public Acceptance of Cybernetic Avatars in the service sector: Evidence from a Large-Scale Survey in Dubai</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybernetic avatars are hybrid interaction robots or digital representations that combine autonomous capabilities with teleoperated control. This study investigates the acceptance of cybernetic avatars in the highly multicultural society of Dubai, with particular emphasis on robotic avatars for customer service. Specifically, we explore how acceptance varies as a function of robot appearance (e.g., android, robotic-looking, cartoonish), deployment settings (e.g., shopping malls, hotels, hospitals), and functional tasks (e.g., providing information, patrolling). To this end, we conducted a large-scale survey with over 1,000 participants. Overall, cybernetic avatars received a high level of acceptance, with physical robot avatars receiving higher acceptance than digital avatars. In terms of appearance, robot avatars with a highly anthropomorphic robotic appearance were the most accepted, followed by cartoonish designs and androids. Animal-like appearances received the lowest level of acceptance. Among the tasks, providing information and guidance was rated as the most valued. Shopping malls, airports, public transport stations, and museums were the settings with the highest acceptance, whereas healthcare-related spaces received lower levels of support. An analysis by community cluster revealed among others that Emirati respondents showed significantly greater acceptance of android appearances compared to the overall sample, while participants from the 'Other Asia' cluster were significantly more accepting of cartoonish appearances. Our study underscores the importance of incorporating citizen feedback into the design and deployment of cybernetic avatars from the early stages to enhance acceptance of this technology in society.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2506.05806.pdf' target='_blank'>https://arxiv.org/pdf/2506.05806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Yu, Zhaonian Wang, Yihan Pan, Meng Cheng, Hao Yang, Chao Wang, Tao Xie, Xiaoming Xu, Xiaoming Wei, Xunliang Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05806">LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based models have gained wide adoption in the virtual human generation due to their outstanding expressiveness. However, their substantial computational requirements have constrained their deployment in real-time interactive avatar applications, where stringent speed, latency, and duration requirements are paramount. We present a novel audio-driven portrait video generation framework based on the diffusion model to address these challenges. Firstly, we propose robust variable-length video generation to reduce the minimum time required to generate the initial video clip or state transitions, which significantly enhances the user experience. Secondly, we propose a consistency model training strategy for Audio-Image-to-Video to ensure real-time performance, enabling a fast few-step generation. Model quantization and pipeline parallelism are further employed to accelerate the inference speed. To mitigate the stability loss incurred by the diffusion process and model quantization, we introduce a new inference strategy tailored for long-duration video generation. These methods ensure real-time performance and low latency while maintaining high-fidelity output. Thirdly, we incorporate class labels as a conditional input to seamlessly switch between speaking, listening, and idle states. Lastly, we design a novel mechanism for fine-grained facial expression control to exploit our model's inherent capacity. Extensive experiments demonstrate that our approach achieves low-latency, fluid, and authentic two-way communication. On an NVIDIA RTX 4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45 FPS at a resolution of 512x512, with an initial video generation latency of 140 ms and 215 ms, respectively.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2505.23301.pdf' target='_blank'>https://arxiv.org/pdf/2505.23301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rim Rekik, Stefanie Wuhrer, Ludovic Hoyet, Katja Zibrek, Anne-HÃ©lÃ¨ne Olivier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23301">Quality assessment of 3D human animation: Subjective and objective evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual human animations have a wide range of applications in virtual and augmented reality. While automatic generation methods of animated virtual humans have been developed, assessing their quality remains challenging. Recently, approaches introducing task-oriented evaluation metrics have been proposed, leveraging neural network training. However, quality assessment measures for animated virtual humans that are not generated with parametric body models have yet to be developed. In this context, we introduce a first such quality assessment measure leveraging a novel data-driven framework. First, we generate a dataset of virtual human animations together with their corresponding subjective realism evaluation scores collected with a user study. Second, we use the resulting dataset to learn predicting perceptual evaluation scores. Results indicate that training a linear regressor on our dataset results in a correlation of 90%, which outperforms a state of the art deep learning baseline.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2505.04387.pdf' target='_blank'>https://arxiv.org/pdf/2505.04387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amin Fadaeinejad, Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amaury Depierre, Nikolaus F. Troje, Marcus A. Brubaker, Marc-AndrÃ© Carbonneau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04387">Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic 3D head assets for virtual characters that match a precise artistic vision remains labor-intensive. We present a novel framework that streamlines this process by providing artists with intuitive control over generated 3D heads. Our approach uses a geometry-aware texture synthesis pipeline that learns correlations between head geometry and skin texture maps across different demographics. The framework offers three levels of artistic control: manipulation of overall head geometry, adjustment of skin tone while preserving facial characteristics, and fine-grained editing of details such as wrinkles or facial hair. Our pipeline allows artists to make edits to a single texture map using familiar tools, with our system automatically propagating these changes coherently across the remaining texture maps needed for realistic rendering. Experiments demonstrate that our method produces diverse results with clean geometries. We showcase practical applications focusing on intuitive control for artists, including skin tone adjustments and simplified editing workflows for adding age-related details or removing unwanted features from scanned models. This integrated approach aims to streamline the artistic workflow in virtual character creation.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2505.02694.pdf' target='_blank'>https://arxiv.org/pdf/2505.02694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurtis Haut, Masum Hasan, Thomas Carroll, Ronald Epstein, Taylan Sen, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02694">AI Standardized Patient Improves Human Conversations in Advanced Cancer Care</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Serious illness communication (SIC) in end-of-life care faces challenges such as emotional stress, cultural barriers, and balancing hope with honesty. Despite its importance, one of the few available ways for clinicians to practice SIC is with standardized patients, which is expensive, time-consuming, and inflexible. In this paper, we present SOPHIE, an AI-powered standardized patient simulation and automated feedback system. SOPHIE combines large language models (LLMs), a lifelike virtual avatar, and automated, personalized feedback based on clinical literature to provide remote, on-demand SIC training. In a randomized control study with healthcare students and professionals, SOPHIE users demonstrated significant improvement across three critical SIC domains: Empathize, Be Explicit, and Empower. These results suggest that AI-driven tools can enhance complex interpersonal communication skills, offering scalable, accessible solutions to address a critical gap in clinician education.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2505.00421.pdf' target='_blank'>https://arxiv.org/pdf/2505.00421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00421">Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2504.13386.pdf' target='_blank'>https://arxiv.org/pdf/2504.13386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Radek DanÄÄek, Carolin Schmitt, Senya Polikovsky, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13386">Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without rich expressions, whereas stochastic models generate diverse expressions but with lower lip-sync quality. To get the best of both, we seek a stochastic model with accurate lip-sync. To that end, we develop a new approach based on the following observation: if a method generates realistic 3D lip motions, it should be possible to infer the spoken audio from the lip motion. The inferred speech should match the original input audio, and erroneous predictions create a novel supervision signal for training 3D talking head avatars with accurate lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under Neural Differentiable Elocution Reconstruction), a 3D talking head avatar framework that introduces a novel supervision mechanism via differentiable sound production. First, we train a novel mesh-to-speech model that regresses audio from facial animation. Then, we incorporate this model into a diffusion-based talking avatar framework. During training, the mesh-to-speech model takes the generated animation and produces a sound that is compared to the input speech, creating a differentiable analysis-by-audio-synthesis supervision loop. Our extensive qualitative and quantitative experiments demonstrate that THUNDER significantly improves the quality of the lip-sync of talking head avatars while still allowing for generation of diverse, high-quality, expressive facial animations. The code and models will be available at https://thunder.is.tue.mpg.de/
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2504.09018.pdf' target='_blank'>https://arxiv.org/pdf/2504.09018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Yin, Chenxinran Shen, Robert Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09018">Entertainers Between Real and Virtual -- Investigating Viewer Interaction, Engagement, and Relationships with Avatarized Virtual Livestreamers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual YouTubers (VTubers) are avatar-based livestreamers that are voiced and played by human actors. VTubers have been popular in East Asia for years and have more recently seen widespread international growth. Despite their emergent popularity, research has been scarce into the interactions and relationships that exist between avatarized VTubers and their viewers, particularly in contrast to non-avatarized streamers. To address this gap, we performed in-depth interviews with self-reported VTuber viewers (n=21). Our findings first reveal that the avatarized nature of VTubers fosters new forms of theatrical engagement, as factors of the virtual blend with the real to create a mixture of fantasy and realism in possible livestream interactions. Avatarization furthermore results in a unique audience perception regarding the identity of VTubers - an identity which comprises a dynamic, distinct mix of the real human (the voice actor/actress) and the virtual character. Our findings suggest that each of these dual identities both individually and symbiotically affect viewer interactions and relationships with VTubers. Whereas the performer's identity mediates social factors such as intimacy, relatability, and authenticity, the virtual character's identity offers feelings of escapism, novelty in interactions, and a sense of continuity beyond the livestream. We situate our findings within existing livestreaming literature to highlight how avatarization drives unique, character-based interactions as well as reshapes the motivations and relationships that viewers form with livestreamers. Finally, we provide suggestions and recommendations for areas of future exploration to address the challenges involved in present livestreamed avatarized entertainment.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2503.22728.pdf' target='_blank'>https://arxiv.org/pdf/2503.22728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Fu, Ziqi Ni, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22728">Dual Audio-Centric Modality Coupling for Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2503.17306.pdf' target='_blank'>https://arxiv.org/pdf/2503.17306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meisam Jamshidi Seikavandi, Jostein Fimland, Maria Jung Barrett, Paolo Burelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17306">Exploring the Temporal Dynamics of Facial Mimicry in Emotion Processing Using Action Units</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial mimicry - the automatic, unconscious imitation of others' expressions - is vital for emotional understanding. This study investigates how mimicry differs across emotions using Face Action Units from videos and participants' responses. Dynamic Time Warping quantified the temporal alignment between participants' and stimuli's facial expressions, revealing significant emotional variations. Post-hoc tests indicated greater mimicry for 'Fear' than 'Happy' and reduced mimicry for 'Anger' compared to 'Fear'. The mimicry correlations with personality traits like Extraversion and Agreeableness were significant, showcasing subtle yet meaningful connections. These findings suggest specific emotions evoke stronger mimicry, with personality traits playing a secondary role in emotional alignment. Notably, our results highlight how personality-linked mimicry mechanisms extend beyond interpersonal communication to affective computing applications, such as remote human-human interactions and human-virtual-agent scenarios. Insights from temporal facial mimicry - e.g., designing digital agents that adaptively mirror user expressions - enable developers to create empathetic, personalized systems, enhancing emotional resonance and user engagement.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2503.17032.pdf' target='_blank'>https://arxiv.org/pdf/2503.17032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17032">TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we "bake" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2503.06324.pdf' target='_blank'>https://arxiv.org/pdf/2503.06324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuya Izumi, Shuhey Koyama, Yoichi Ochiai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06324">AnimeGaze: Real-Time Mutual Gaze Synthesis for Anime-Style Avatars in Physical Environments via Behind-Display Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars on displays lack the ability to engage with the physical environment through gaze. To address this limitation, we propose a gaze synthesis method that enables animated avatars to establish gaze communication with the physical environment using a camera-behind-the-display system. The system uses a display that rapidly alternates between visible and transparent states. During the transparent state, a camera positioned behind the display captures the physical environment. This configuration physically aligns the position of the avatar's eyes with the camera, enabling two-way gaze communication with people and objects in the physical environment. Building on this system, we developed a framework for mutual gaze communication between avatars and people. The framework detects the user's gaze and dynamically synthesizes the avatar's gaze towards people or objects in the environment. This capability was integrated into an AI agent system to generate real-time, context-aware gaze behaviors during conversations, enabling more seamless and natural interactions. To evaluate the system, we conducted a user study to assess its effectiveness in supporting physical gaze awareness and generating human-like gaze behaviors. The results show that the behind-display approach significantly enhances the user's perception of being observed and attended to by the avatar. By bridging the gap between virtual avatars and the physical environment through enhanced gaze interactions, our system offers a promising avenue for more immersive and human-like AI-mediated communication in everyday environments.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2503.05196.pdf' target='_blank'>https://arxiv.org/pdf/2503.05196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzhi Guo, Yixiao Chen, Dongye Xiaonuo, Zeyu Tian, Dongdong Weng, Le Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05196">STGA: Selective-Training Gaussian Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose selective-training Gaussian head avatars (STGA) to enhance the details of dynamic head Gaussian. The dynamic head Gaussian model is trained based on the FLAME parameterized model. Each Gaussian splat is embedded within the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before training, our selection strategy calculates the 3D Gaussian splat to be optimized in each frame. The parameters of these 3D Gaussian splats are optimized in the training of each frame, while those of the other splats are frozen. This means that the splats participating in the optimization process differ in each frame, to improve the realism of fine details. Compared with network-based methods, our method achieves better results with shorter training time. Compared with mesh-based methods, our method produces more realistic details within the same training time. Additionally, the ablation experiment confirms that our method effectively enhances the quality of details.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2502.19455.pdf' target='_blank'>https://arxiv.org/pdf/2502.19455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingzhou Mu, Baiji Liu, Ruonan Zhang, Guiming Mo, Jiawei Jin, Kai Zhang, Haozhi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19455">FLAP: Fully-controllable Audio-driven Portrait Video Generation through 3D head conditioned diffusion model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based video generation techniques have significantly improved zero-shot talking-head avatar generation, enhancing the naturalness of both head motion and facial expressions. However, existing methods suffer from poor controllability, making them less applicable to real-world scenarios such as filmmaking and live streaming for e-commerce. To address this limitation, we propose FLAP, a novel approach that integrates explicit 3D intermediate parameters (head poses and facial expressions) into the diffusion model for end-to-end generation of realistic portrait videos. The proposed architecture allows the model to generate vivid portrait videos from audio while simultaneously incorporating additional control signals, such as head rotation angles and eye-blinking frequency. Furthermore, the decoupling of head pose and facial expression allows for independent control of each, offering precise manipulation of both the avatar's pose and facial expressions. We also demonstrate its flexibility in integrating with existing 3D head generation methods, bridging the gap between 3D model-based approaches and end-to-end diffusion techniques. Extensive experiments show that our method outperforms recent audio-driven portrait video models in both naturalness and controllability.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2502.07030.pdf' target='_blank'>https://arxiv.org/pdf/2502.07030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07030">PrismAvatar: Real-time animated 3D neural head avatars on edge devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PrismAvatar: a 3D head avatar model which is designed specifically to enable real-time animation and rendering on resource-constrained edge devices, while still enjoying the benefits of neural volumetric rendering at training time. By integrating a rigged prism lattice with a 3D morphable head model, we use a hybrid rendering model to simultaneously reconstruct a mesh-based head and a deformable NeRF model for regions not represented by the 3DMM. We then distill the deformable NeRF into a rigged mesh and neural textures, which can be animated and rendered efficiently within the constraints of the traditional triangle rendering pipeline. In addition to running at 60 fps with low memory usage on mobile devices, we find that our trained models have comparable quality to state-of-the-art 3D avatar models on desktop devices.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2501.16870.pdf' target='_blank'>https://arxiv.org/pdf/2501.16870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Josep Lopez Camunas, Cristina Bustos, Yanjun Zhu, Raquel Ros, Agata Lapedriza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16870">Experimenting with Affective Computing Models in Video Interviews with Spanish-speaking Older Adults</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding emotional signals in older adults is crucial for designing virtual assistants that support their well-being. However, existing affective computing models often face significant limitations: (1) limited availability of datasets representing older adults, especially in non-English-speaking populations, and (2) poor generalization of models trained on younger or homogeneous demographics. To address these gaps, this study evaluates state-of-the-art affective computing models -- including facial expression recognition, text sentiment analysis, and smile detection -- using videos of older adults interacting with either a person or a virtual avatar. As part of this effort, we introduce a novel dataset featuring Spanish-speaking older adults engaged in human-to-human video interviews. Through three comprehensive analyses, we investigate (1) the alignment between human-annotated labels and automatic model outputs, (2) the relationships between model outputs across different modalities, and (3) individual variations in emotional signals. Using both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover limited agreement between human annotations and model predictions, weak consistency across modalities, and significant variability among individuals. These findings highlight the shortcomings of generalized emotion perception models and emphasize the need of incorporating personal variability and cultural nuances into future systems.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2501.05755.pdf' target='_blank'>https://arxiv.org/pdf/2501.05755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madhurananda Pahar, Fuxiang Tao, Bahman Mirheidari, Nathan Pevy, Rebecca Bright, Swapnil Gadgil, Lise Sproson, Dorota Braun, Caitlin Illingworth, Daniel Blackburn, Heidi Christensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05755">CognoSpeak: an automatic, remote assessment of early cognitive decline in real-world conversational speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The early signs of cognitive decline are often noticeable in conversational speech, and identifying those signs is crucial in dealing with later and more serious stages of neurodegenerative diseases. Clinical detection is costly and time-consuming and although there has been recent progress in the automatic detection of speech-based cues, those systems are trained on relatively small databases, lacking detailed metadata and demographic information. This paper presents CognoSpeak and its associated data collection efforts. CognoSpeak asks memory-probing long and short-term questions and administers standard cognitive tasks such as verbal and semantic fluency and picture description using a virtual agent on a mobile or web platform. In addition, it collects multimodal data such as audio and video along with a rich set of metadata from primary and secondary care, memory clinics and remote settings like people's homes. Here, we present results from 126 subjects whose audio was manually transcribed. Several classic classifiers, as well as large language model-based classifiers, have been investigated and evaluated across the different types of prompts. We demonstrate a high level of performance; in particular, we achieved an F1-score of 0.873 using a DistilBERT model to discriminate people with cognitive impairment (dementia and people with mild cognitive impairment (MCI)) from healthy volunteers using the memory responses, fluency tasks and cookie theft picture description. CognoSpeak is an automatic, remote, low-cost, repeatable, non-invasive and less stressful alternative to existing clinical cognitive assessments.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2412.12093.pdf' target='_blank'>https://arxiv.org/pdf/2412.12093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12093">CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints $-$ for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2412.12061.pdf' target='_blank'>https://arxiv.org/pdf/2412.12061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farnaz Nouraei, Keith Rebello, Mina Fallah, Prasanth Murali, Haley Matuszak, Valerie Jap, Andrea Parker, Michael Paasche-Orlow, Timothy Bickmore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12061">Virtual Agent-Based Communication Skills Training to Facilitate Health Persuasion Among Peers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many laypeople are motivated to improve the health behavior of their family or friends but do not know where to start, especially if the health behavior is potentially stigmatizing or controversial. We present an approach that uses virtual agents to coach community-based volunteers in health counseling techniques, such as motivational interviewing, and allows them to practice these skills in role-playing scenarios. We use this approach in a virtual agent-based system to increase COVID-19 vaccination by empowering users to influence their social network. In a between-subjects comparative design study, we test the effects of agent system interactivity and role-playing functionality on counseling outcomes, with participants evaluated by standardized patients and objective judges. We find that all versions are effective at producing peer counselors who score adequately on a standardized measure of counseling competence, and that participants were significantly more satisfied with interactive virtual agents compared to passive viewing of the training material. We discuss design implications for interpersonal skills training systems based on our findings.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2412.10458.pdf' target='_blank'>https://arxiv.org/pdf/2412.10458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhao, Dongdong Weng, Qiuxin Du, Zeyu Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10458">Motion Generation Review: Exploring Deep Learning for Lifelike Animation with Manifold</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion generation involves creating natural sequences of human body poses, widely used in gaming, virtual reality, and human-computer interaction. It aims to produce lifelike virtual characters with realistic movements, enhancing virtual agents and immersive experiences. While previous work has focused on motion generation based on signals like movement, music, text, or scene background, the complexity of human motion and its relationships with these signals often results in unsatisfactory outputs. Manifold learning offers a solution by reducing data dimensionality and capturing subspaces of effective motion. In this review, we present a comprehensive overview of manifold applications in human motion generation, one of the first in this domain. We explore methods for extracting manifolds from unstructured data, their application in motion generation, and discuss their advantages and future directions. This survey aims to provide a broad perspective on the field and stimulate new approaches to ongoing challenges.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2412.10209.pdf' target='_blank'>https://arxiv.org/pdf/2412.10209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10209">GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2412.07739.pdf' target='_blank'>https://arxiv.org/pdf/2412.07739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay P. Namboodiri, Benjamin E Lundell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07739">GASP: Gaussian Avatars with Synthetic Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360$^\circ$ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page (https://microsoft.github.io/GASP/) for results.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2412.02421.pdf' target='_blank'>https://arxiv.org/pdf/2412.02421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwei Pan, Yang Li, Hongsheng Li, Kwan-Yee Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02421">TimeWalker: Personalized Neural Space for Lifelong Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized 'time traveling' in a breeze.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2411.11102.pdf' target='_blank'>https://arxiv.org/pdf/2411.11102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Annalena Bea Aicher, Yuki Matsuda, Keichii Yasumoto, Wolfgang Minker, Elisabeth AndrÃ©, Stefan Ultes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11102">Exploring the Impact of Non-Verbal Virtual Agent Behavior on User Engagement in Argumentative Dialogues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Engaging in discussions that involve diverse perspectives and exchanging arguments on a controversial issue is a natural way for humans to form opinions. In this process, the way arguments are presented plays a crucial role in determining how engaged users are, whether the interaction takes place solely among humans or within human-agent teams. This is of great importance as user engagement plays a crucial role in determining the success or failure of cooperative argumentative discussions. One main goal is to maintain the user's motivation to participate in a reflective opinion-building process, even when addressing contradicting viewpoints. This work investigates how non-verbal agent behavior, specifically co-speech gestures, influences the user's engagement and interest during an ongoing argumentative interaction. The results of a laboratory study conducted with 56 participants demonstrate that the agent's co-speech gestures have a substantial impact on user engagement and interest and the overall perception of the system. Therefore, this research offers valuable insights for the design of future cooperative argumentative virtual agents.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2411.06719.pdf' target='_blank'>https://arxiv.org/pdf/2411.06719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Osman Akar, Yushan Han, Yizhou Chen, Weixian Lan, Benn Gallagher, Ronald Fedkiw, Joseph Teran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06719">Shallow Signed Distance Functions for Kinematic Collision Bodies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present learning-based implicit shape representations designed for real-time avatar collision queries arising in the simulation of clothing. Signed distance functions (SDFs) have been used for such queries for many years due to their computational efficiency. Recently deep neural networks have been used for implicit shape representations (DeepSDFs) due to their ability to represent multiple shapes with modest memory requirements compared to traditional representations over dense grids. However, the computational expense of DeepSDFs prevents their use in real-time clothing simulation applications. We design a learning-based representation of SDFs for human avatars whoes bodies change shape kinematically due to joint-based skinning. Rather than using a single DeepSDF for the entire avatar, we use a collection of extremely computationally efficient (shallow) neural networks that represent localized deformations arising from changes in body shape induced by the variation of a single joint. This requires a stitching process to combine each shallow SDF in the collection together into one SDF representing the signed closest distance to the boundary of the entire body. To achieve this we augment each shallow SDF with an additional output that resolves whether or not the individual shallow SDF value is referring to a closest point on the boundary of the body, or to a point on the interior of the body (but on the boundary of the individual shallow SDF). Our model is extremely fast and accurate and we demonstrate its applicability with real-time simulation of garments driven by animated characters.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2410.20789.pdf' target='_blank'>https://arxiv.org/pdf/2410.20789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaonuo Dongye, Hanzhi Guo, Le Luo, Haiyan Jiang, Yihua Bao, Zeyu Tian, Dongdong Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20789">LoDAvatar: Hierarchical Embedding and Adaptive Levels of Detail with Gaussian Splatting for Enhanced Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of virtual reality, the demand for 3D human avatars is increasing. The emergence of Gaussian Splatting technology has enabled the rendering of Gaussian avatars with superior visual quality and reduced computational costs. Despite numerous methods researchers propose for implementing drivable Gaussian avatars, limited attention has been given to balancing visual quality and computational costs. In this paper, we introduce LoDAvatar, a method that introduces levels of detail into Gaussian avatars through hierarchical embedding and selective detail enhancement methods. The key steps of LoDAvatar encompass data preparation, Gaussian embedding, Gaussian optimization, and selective detail enhancement. We conducted experiments involving Gaussian avatars at various levels of detail, employing both objective assessments and subjective evaluations. The outcomes indicate that incorporating levels of detail into Gaussian avatars can decrease computational costs during rendering while upholding commendable visual quality, thereby enhancing runtime frame rates. We advocate adopting LoDAvatar to render multiple dynamic Gaussian avatars or extensive Gaussian scenes to balance visual quality and computational costs.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2410.17262.pdf' target='_blank'>https://arxiv.org/pdf/2410.17262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqing Wang, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17262">EmoGene: Audio-Driven Emotional 3D Talking-Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking-head generation is a crucial and useful technology for virtual human interaction and film-making. While recent advances have focused on improving image fidelity and lip synchronization, generating accurate emotional expressions remains underexplored. In this paper, we introduce EmoGene, a novel framework for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Our approach employs a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks, which are concatenated with emotional embedding in a motion-to-emotion module to produce emotional landmarks. These landmarks drive a Neural Radiance Fields (NeRF)-based emotion-to-video module to render realistic emotional talking-head videos. Additionally, we propose a pose sampling method to generate natural idle-state (non-speaking) videos for silent audio inputs. Extensive experiments demonstrate that EmoGene outperforms previous methods in generating high-fidelity emotional talking-head videos.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2410.16503.pdf' target='_blank'>https://arxiv.org/pdf/2410.16503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saif Punjwani, Larry Heck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16503">Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for Allocentric Avatar Gesture Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of high-quality, multimodal training data severely hinders the creation of lifelike avatar animations for conversational AI in virtual environments. Existing datasets often lack the intricate synchronization between speech, facial expressions, and body movements that characterize natural human communication. To address this critical gap, we introduce Allo-AVA, a large-scale dataset specifically designed for text and audio-driven avatar gesture animation in an allocentric (third person point-of-view) context. Allo-AVA consists of $\sim$1,250 hours of diverse video content, complete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely maps these keypoints to precise timestamps, enabling accurate replication of human movements (body and facial gestures) in synchronization with speech. This comprehensive resource enables the development and evaluation of more natural, context-aware avatar animation models, potentially transforming applications ranging from virtual reality to digital assistants.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2410.13503.pdf' target='_blank'>https://arxiv.org/pdf/2410.13503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Wagner, Mario Botsch, Ulrich Schwanecke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13503">NePHIM: A Neural Physics-Based Head-Hand Interaction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the increasing use of virtual avatars, the animation of head-hand interactions has recently gained attention. To this end, we present a novel volumetric and physics-based interaction simulation. In contrast to previous work, our simulation incorporates temporal effects such as collision paths, respects anatomical constraints, and can detect and simulate skin pulling. As a result, we can achieve more natural-looking interaction animations and take a step towards greater realism. However, like most complex and computationally expensive simulations, ours is not real-time capable even on high-end machines. Therefore, we train small and efficient neural networks as accurate approximations that achieve about 200 FPS on consumer GPUs, about 50 FPS on CPUs, and are learned in less than four hours for one person. In general, our focus is not to generalize the approximation networks to low-resolution head models but to adapt them to more detailed personalized avatars. Nevertheless, we show that these networks can learn to approximate our head-hand interaction model for multiple identities while maintaining computational efficiency.
  Since the quality of the simulations can only be judged subjectively, we conducted a comprehensive user study which confirms the improved realism of our approach. In addition, we provide extensive visual results and inspect the neural approximations quantitatively. All data used in this work has been recorded with a multi--view camera rig and will be made available upon publication. We will also publish relevant implementations.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2410.03714.pdf' target='_blank'>https://arxiv.org/pdf/2410.03714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubens Montanha, Giovana Raupp, Ana Carolina Schmitt, Gabriel Schneider, Victor Araujo, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03714">Perceptual Analysis of Groups of Virtual Humans Animated using Interactive Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual humans (VH) have been used in Computer Graphics (CG) for many years, and perception studies have been applied to understand how people perceive them. Some studies have already examined how realism impacts the comfort of viewers. In some cases, the user's comfort is related to human identification. For example, people from a specific group may look positively at others from the same group. Gender is one of those characteristics that have in-group advantages. For example, in terms of VHs, studies have shown that female humans are more likely to recognize emotions in female VHs than in male VHs. However, there are many other variables that can impact the user perception. To aid this discussion, we conducted a study on how people perceive comfort and realism in relation to interactive VHs with different genders and expressing negative, neutral, or positive emotions in groups. We created a virtual environment for participants to interact with groups of VHs, which are interactive and should evolve in real-time, using a popular game engine. To animate the characters, we opted for cartoon figures that are animated by tracking the facial expressions of actors, using available game engine platforms to conduct the driven animation. Our results indicate that the emotion of the VH group impacts both comfort and realism perception, even by using simple cartoon characters in an interactive environment. Furthermore, the findings suggest that individuals reported feeling better with a positive emotion compared to a negative emotion, and that negative emotion recognition is impacted by the gender of the VHs group. Additionally, although we used simple characters, the results are consistent with the perception obtained when analysing realistic the state-of-the-art virtual humans, which positive emotions tend to be more correctly recognized than negative ones.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2408.15762.pdf' target='_blank'>https://arxiv.org/pdf/2408.15762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Fonseca Silva, Paulo Ricardo Knob, Rubens Halbig Montanha, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15762">Evaluating and Comparing Crowd Simulations: Perspectives from a Crowd Authoring Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd simulation is a research area widely used in diverse fields, including gaming and security, assessing virtual agent movements through metrics like time to reach their goals, speed, trajectories, and densities. This is relevant for security applications, for instance, as different crowd configurations can determine the time people spend in environments trying to evacuate them. In this work, we extend WebCrowds, an authoring tool for crowd simulation, to allow users to build scenarios and evaluate them through a set of metrics. The aim is to provide a quantitative metric that can, based on simulation data, select the best crowd configuration in a certain environment. We conduct experiments to validate our proposed metric in multiple crowd simulation scenarios and perform a comparison with another metric found in the literature. The results show that experts in the domain of crowd scenarios agree with our proposed quantitative metric.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2407.11174.pdf' target='_blank'>https://arxiv.org/pdf/2407.11174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pramish Paudel, Anubhav Khanal, Ajad Chhatkuli, Danda Pani Paudel, Jyoti Tandukar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11174">iHuman: Instant Animatable Digital Humans From Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized 3D avatars require an animatable representation of digital humans. Doing so instantly from monocular videos offers scalability to broad class of users and wide-scale applications. In this paper, we present a fast, simple, yet effective method for creating animatable 3D digital humans from monocular videos. Our method utilizes the efficiency of Gaussian splatting to model both 3D geometry and appearance. However, we observed that naively optimizing Gaussian splats results in inaccurate geometry, thereby leading to poor animations. This work achieves and illustrates the need of accurate 3D mesh-type modelling of the human body for animatable digitization through Gaussian splats. This is achieved by developing a novel pipeline that benefits from three key aspects: (a) implicit modelling of surface's displacements and the color's spherical harmonics; (b) binding of 3D Gaussians to the respective triangular faces of the body template; (c) a novel technique to render normals followed by their auxiliary supervision. Our exhaustive experiments on three different benchmark datasets demonstrates the state-of-the-art results of our method, in limited time settings. In fact, our method is faster by an order of magnitude (in terms of training time) than its closest competitor. At the same time, we achieve superior rendering and 3D reconstruction performance under the change of poses.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2407.08095.pdf' target='_blank'>https://arxiv.org/pdf/2407.08095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Steenstra, Farnaz Nouraei, Mehdi Arjmand, Timothy W. Bickmore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08095">Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered Motivational Interviewing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLM-powered virtual agent matches human counselors' empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2406.16815.pdf' target='_blank'>https://arxiv.org/pdf/2406.16815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Liu, Junshu Tang, Chu Zheng, Shijie Zhang, Jinkun Hao, Junwei Zhu, Dongjin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16815">ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity 3D garment synthesis from text is desirable yet challenging for digital avatar creation. Recent diffusion-based approaches via Score Distillation Sampling (SDS) have enabled new possibilities but either intricately couple with human body or struggle to reuse. We introduce ClotheDreamer, a 3D Gaussian-based method for generating wearable, production-ready 3D garment assets from text prompts. We propose a novel representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate optimization. DCGS represents clothed avatar as one Gaussian model but freezes body Gaussian splats. To enhance quality and completeness, we incorporate bidirectional SDS to supervise clothed avatar and garment RGBD renderings respectively with pose conditions and propose a new pruning strategy for loose clothing. Our approach can also support custom clothing templates as input. Benefiting from our design, the synthetic 3D garment can be easily applied to virtual try-on and support physically accurate animation. Extensive experiments showcase our method's superior and competitive performance. Our project page is at https://ggxxii.github.io/clothedreamer.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2405.20786.pdf' target='_blank'>https://arxiv.org/pdf/2405.20786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Feng, Wenchao Ma, Quankai Gao, Xianwei Zheng, Nan Xue, Huijuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20786">Stratified Avatar Generation from Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating 3D full-body avatars from AR/VR devices is essential for creating immersive experiences in AR/VR applications. This task is challenging due to the limited input from Head Mounted Devices, which capture only sparse observations from the head and hands. Predicting the full-body avatars, particularly the lower body, from these sparse observations presents significant difficulties. In this paper, we are inspired by the inherent property of the kinematic tree defined in the Skinned Multi-Person Linear (SMPL) model, where the upper body and lower body share only one common ancestor node, bringing the potential of decoupled reconstruction. We propose a stratified approach to decouple the conventional full-body avatar reconstruction pipeline into two stages, with the reconstruction of the upper body first and a subsequent reconstruction of the lower body conditioned on the previous stage. To implement this straightforward idea, we leverage the latent diffusion model as a powerful probabilistic generator, and train it to follow the latent distribution of decoupled motions explored by a VQ-VAE encoder-decoder model. Extensive experiments on AMASS mocap dataset demonstrate our state-of-the-art performance in the reconstruction of full-body motions.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2405.16204.pdf' target='_blank'>https://arxiv.org/pdf/2405.16204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phong Tran, Egor Zakharov, Long-Nhat Ho, Liwen Hu, Adilbek Karmanov, Aviral Agarwal, McLean Goldwhite, Ariana Bermudez Venegas, Anh Tuan Tran, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16204">VOODOO XP: Expressive One-Shot Head Reenactment for VR Telepresence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce VOODOO XP: a 3D-aware one-shot head reenactment method that can generate highly expressive facial expressions from any input driver video and a single 2D portrait. Our solution is real-time, view-consistent, and can be instantly used without calibration or fine-tuning. We demonstrate our solution on a monocular video setting and an end-to-end VR telepresence system for two-way communication. Compared to 2D head reenactment methods, 3D-aware approaches aim to preserve the identity of the subject and ensure view-consistent facial geometry for novel camera poses, which makes them suitable for immersive applications. While various facial disentanglement techniques have been introduced, cutting-edge 3D-aware neural reenactment techniques still lack expressiveness and fail to reproduce complex and fine-scale facial expressions. We present a novel cross-reenactment architecture that directly transfers the driver's facial expressions to transformer blocks of the input source's 3D lifting module. We show that highly effective disentanglement is possible using an innovative multi-stage self-supervision approach, which is based on a coarse-to-fine strategy, combined with an explicit face neutralization and 3D lifted frontalization during its initial training stage. We further integrate our novel head reenactment solution into an accessible high-fidelity VR telepresence system, where any person can instantly build a personalized neural head avatar from any photo and bring it to life using the headset. We demonstrate state-of-the-art performance in terms of expressiveness and likeness preservation on a large set of diverse subjects and capture conditions.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2404.12888.pdf' target='_blank'>https://arxiv.org/pdf/2404.12888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12888">Learn2Talk: 3D Talking Face Learns from 2D Talking Face</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2404.07991.pdf' target='_blank'>https://arxiv.org/pdf/2404.07991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07991">GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2403.11700.pdf' target='_blank'>https://arxiv.org/pdf/2403.11700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Can Liu, Di Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11700">Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread popularity of internet celebrity marketing all over the world, short video production has gradually become a popular way of presenting products information. However, the traditional video production industry usually includes series of procedures as script writing, video filming in a professional studio, video clipping, special effects rendering, customized post-processing, and so forth. Not to mention that multilingual videos is not accessible for those who could not speak multilingual languages. These complicated procedures usually needs a professional team to complete, and this made short video production costly in both time and money. This paper presents an intelligent system that supports the automatic generation of talking avatar videos, namely Virbo. With simply a user-specified script, Virbo could use a deep generative model to generate a target talking videos. Meanwhile, the system also supports multimodal inputs to customize the video with specified face, specified voice and special effects. This system also integrated a multilingual customization module that supports generate multilingual talking avatar videos in a batch with hundreds of delicate templates and creative special effects. Through a series of user studies and demo tests, we found that Virbo can generate talking avatar videos that maintained a high quality of videos as those from a professional team while reducing the entire production costs significantly. This intelligent system will effectively promote the video production industry and facilitate the internet marketing neglecting of language barriers and cost challenges.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2403.08363.pdf' target='_blank'>https://arxiv.org/pdf/2403.08363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthikeya Puttur Venkatraj, Wo Meijer, Monica PerusquÃ­a-HernÃ¡ndez, Gijs Huisman, Abdallah El Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08363">ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2403.07122.pdf' target='_blank'>https://arxiv.org/pdf/2403.07122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Mal, Nina DÃ¶llinger, Erik Wolf, Stephan Wenninger, Mario Botsch, Carolin Wienrich, Marc Erich Latoschik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07122">Am I the Odd One? Exploring (In)Congruencies in the Realism of Avatars and Virtual Others in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual humans play a pivotal role in social virtual environments, shaping users' VR experiences. The diversity in available options and users' preferences can result in a heterogeneous mix of appearances among a group of virtual humans. The resulting variety in higher-order anthropomorphic and realistic cues introduces multiple (in)congruencies, eventually impacting the plausibility of the experience. In this work, we consider the impact of (in)congruencies in the realism of a group of virtual humans, including co-located others and one's self-avatar. In a 2 x 3 mixed design, participants embodied either (1) a personalized realistic or (2) a customized stylized self-avatar across three consecutive VR exposures in which they were accompanied by a group of virtual others being either (1) all realistic, (2) all stylized, or (3) mixed. Our results indicate groups of virtual others of higher realism, i.e., potentially more congruent with participants' real-world experiences and expectations, were considered more human-like, increasing the feeling of co-presence and the impression of interaction possibilities. (In)congruencies concerning the homogeneity of the group did not cause considerable effects. Furthermore, our results indicate that a self-avatar's congruence with the participant's real-world experiences concerning their own physical body yielded notable benefits for virtual body ownership and self-identification for realistic personalized avatars. Notably, the incongruence between a stylized self-avatar and a group of realistic virtual others resulted in diminished ratings of self-location and self-identification. We conclude on the implications of our findings and discuss our results within current theories of VR experiences, considering (in)congruent visual cues and their impact on the perception of virtual others, self-representation, and spatial presence.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2401.11078.pdf' target='_blank'>https://arxiv.org/pdf/2401.11078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11078">UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2312.13091.pdf' target='_blank'>https://arxiv.org/pdf/2312.13091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andre Carbonneau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13091">MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrinsic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: https://ubisoft-laforge.github.io/character/mosar/
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2312.12877.pdf' target='_blank'>https://arxiv.org/pdf/2312.12877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12877">Relightable and Animatable Neural Avatars from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lightweight creation of 3D digital avatars is a highly desirable but challenging task. With only sparse videos of a person under unknown illumination, we propose a method to create relightable and animatable neural avatars, which can be used to synthesize photorealistic images of humans under novel viewpoints, body poses, and lighting. The key challenge here is to disentangle the geometry, material of the clothed body, and lighting, which becomes more difficult due to the complex geometry and shadow changes caused by body motions. To solve this ill-posed problem, we propose novel techniques to better model the geometry and shadow changes. For geometry change modeling, we propose an invertible deformation field, which helps to solve the inverse skinning problem and leads to better geometry quality. To model the spatial and temporal varying shading cues, we propose a pose-aware part-wise light visibility network to estimate light occlusion. Extensive experiments on synthetic and real datasets show that our approach reconstructs high-quality geometry and generates realistic shadows under different body poses. Code and data are available at \url{https://wenbin-lin.github.io/RelightableAvatar-page/}.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2312.06790.pdf' target='_blank'>https://arxiv.org/pdf/2312.06790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Araujo, Angelo Brandelli Costa, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06790">Evaluating the Uncanny Valley Effect in Dark Colored Skin Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of technology, the design of virtual humans has led to a very realistic user experience, such as in movies, video games, and simulations. As a result, virtual humans are becoming increasingly similar to real humans. However, following the Uncanny Valley (UV) theory, users tend to feel discomfort when watching entities with anthropomorphic traits that differ from real humans. This phenomenon is related to social identity theory, where the observer looks for something familiar. In Computer Graphics (CG), techniques used to create virtual humans with dark skin tones often rely on approaches initially developed for rendering characters with white skin tones. Furthermore, most CG characters portrayed in various media, including movies and games, predominantly exhibit white skin tones. Consequently, it is pertinent to explore people's perceptions regarding different groups of virtual humans. Thus, this paper aims to examine and evaluate the human perception of CG characters from different media, comparing two types of skin colors. The findings indicate that individuals felt more comfortable and perceived less realism when watching characters with dark colored skin than those with white colored skin. Our central hypothesis is that dark colored characters, rendered with classical developed algorithms, are considered more cartoon than realistic and placed on the left of the Valley in the UV chart.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2312.05311.pdf' target='_blank'>https://arxiv.org/pdf/2312.05311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jalees Nehvi, Berna Kabadayi, Julien Valentin, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05311">360Â° Volumetric Portrait Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose 360Â° Volumetric Portrait (3VP) Avatar, a novel method for reconstructing 360Â° photo-realistic portrait avatars of human subjects solely based on monocular video inputs. State-of-the-art monocular avatar reconstruction methods rely on stable facial performance capturing. However, the common usage of 3DMM-based facial tracking has its limits; side-views can hardly be captured and it fails, especially, for back-views, as required inputs like facial landmarks or human parsing masks are missing. This results in incomplete avatar reconstructions that only cover the frontal hemisphere. In contrast to this, we propose a template-based tracking of the torso, head and facial expressions which allows us to cover the appearance of a human subject from all sides. Thus, given a sequence of a subject that is rotating in front of a single camera, we train a neural volumetric representation based on neural radiance fields. A key challenge to construct this representation is the modeling of appearance changes, especially, in the mouth region (i.e., lips and teeth). We, therefore, propose a deformation-field-based blend basis which allows us to interpolate between different appearance states. We evaluate our approach on captured real-world data and compare against state-of-the-art monocular reconstruction methods. In contrast to those, our method is the first monocular technique that reconstructs an entire 360Â° avatar.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2312.03590.pdf' target='_blank'>https://arxiv.org/pdf/2312.03590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubens Montanha, Giovana Raupp, Vitoria Gonzalez, Yanny Partichelli, AndrÃ© Bins, Marcos Ferreira, Victor Araujo, Soraia Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03590">Revisiting Micro and Macro Expressions in Computer Graphics Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the reproduction of two studies focused on the perception of micro and macro expressions of Virtual Humans (VHs) generated by Computer Graphics (CG), first described in 2014 and replicated in 2021. The 2014 study referred to a VH realistic, whereas, in 2021, it referred to a VH cartoon. In our work, we replicate the study by using a realistic CG character. Our main goals are to compare the perceptions of micro and macro expressions between levels of realism (2021 cartoon versus 2023 realistic) and between realistic characters in different periods (i.e., 2014 versus 2023). In one of our results, people more easily recognized micro expressions in realistic VHs than in a cartoon VH. In another result, we show that the participants' perception was similar for both micro and macro expressions in 2014 and 2023.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2312.03428.pdf' target='_blank'>https://arxiv.org/pdf/2312.03428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubens Montanha, Victor Araujo, Paulo Knob, Greice Pinho, Gabriel Fonseca, Vitor Peres, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03428">Crafting Realistic Virtual Humans: Unveiling Perspectives on Human Perception, Crowds, and Embodied Conversational Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Humans (VHs) were first developed more than 50 years ago and have undergone significant advancements since then. In the past, creating and animating VHs was a complex task. However, contemporary commercial and freely available technology now empowers users, programmers, and designers to create and animate VHs with relative ease. These technologies have even reached a point where they can replicate the authentic characteristics and behaviors of real actors, resulting in VHs that are visually convincing and behaviorally lifelike. This paper explores three closely related research areas in the context of virtual humans and discusses the far-reaching implications of highly realistic characters within these domains.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2311.05844.pdf' target='_blank'>https://arxiv.org/pdf/2311.05844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minki Kang, Wooseok Han, Eunho Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05844">Face-StyleSpeech: Enhancing Zero-shot Speech Synthesis from Face Images with Improved Face-to-Speech Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating speech from a face image is crucial for developing virtual humans capable of interacting using their unique voices, without relying on pre-recorded human speech. In this paper, we propose Face-StyleSpeech, a zero-shot Text-To-Speech (TTS) synthesis model that generates natural speech conditioned on a face image rather than reference speech. We hypothesize that learning entire prosodic features from a face image poses a significant challenge. To address this, our TTS model incorporates both face and prosody encoders. The prosody encoder is specifically designed to model speech style characteristics that are not fully captured by the face image, allowing the face encoder to focus on extracting speaker-specific features such as timbre. Experimental results demonstrate that Face-StyleSpeech effectively generates more natural speech from a face image than baselines, even for unseen faces. Samples are available on our demo page.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2309.09314.pdf' target='_blank'>https://arxiv.org/pdf/2309.09314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Taeil Jin, Sung-Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09314">MOVIN: Real-time Motion Capture using a Single LiDAR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in technology have brought forth new forms of interactive applications, such as the social metaverse, where end users interact with each other through their virtual avatars. In such applications, precise full-body tracking is essential for an immersive experience and a sense of embodiment with the virtual avatar. However, current motion capture systems are not easily accessible to end users due to their high cost, the requirement for special skills to operate them, or the discomfort associated with wearable devices. In this paper, we present MOVIN, the data-driven generative method for real-time motion capture with global tracking, using a single LiDAR sensor. Our autoregressive conditional variational autoencoder (CVAE) model learns the distribution of pose variations conditioned on the given 3D point cloud from LiDAR.As a central factor for high-accuracy motion capture, we propose a novel feature encoder to learn the correlation between the historical 3D point cloud data and global, local pose features, resulting in effective learning of the pose prior. Global pose features include root translation, rotation, and foot contacts, while local features comprise joint positions and rotations. Subsequently, a pose generator takes into account the sampled latent variable along with the features from the previous frame to generate a plausible current pose. Our framework accurately predicts the performer's 3D global information and local joint details while effectively considering temporally coherent movements across frames. We demonstrate the effectiveness of our architecture through quantitative and qualitative evaluations, comparing it against state-of-the-art methods. Additionally, we implement a real-time application to showcase our method in real-world scenarios. MOVIN dataset is available at \url{https://movin3d.github.io/movin_pg2023/}.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2308.14404.pdf' target='_blank'>https://arxiv.org/pdf/2308.14404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Forouzan Farzinnejad, Javad Rasti, Navid Khezrian, Jens Grubert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14404">The Effect of an Exergame on the Shadow Play Skill Based on Muscle Memory for Young Female Participants: The Case of Forehand Drive in Table Tennis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning and practicing table tennis with traditional methods is a long, tedious process and may even lead to the internalization of incorrect techniques if not supervised by a coach. To overcome these issues, the presented study proposes an exergame with the aim of enhancing young female novice players' performance by boosting muscle memory, making practice more interesting, and decreasing the probability of faulty training. Specifically, we propose an exergame based on skeleton tracking and a virtual avatar to support correct shadow practice to learn forehand drive technique without the presence of a coach. We recruited 44 schoolgirls aged between 8 and 12 years without a background in playing table tennis and divided them into control and experimental groups. We examined their stroke skills (via the Mott-Lockhart test) and the error coefficient of their forehand drives (using a ball machine) in the pretest, post-test, and follow-up tests (10 days after the post-test). Our results showed that the experimental group had progress in the short and long term, while the control group had an improvement only in the short term. Further, the scale of improvement in the experimental group was significantly higher than in the control group. Given that the early stages of learning, particularly in girls children, are important in the internalization of individual skills in would-be athletes, this method could support promoting correct training for young females.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2308.03022.pdf' target='_blank'>https://arxiv.org/pdf/2308.03022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masum Hasan, Cengiz Ozel, Sammy Potter, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03022">SAPIEN: Affective Virtual Agents Powered by Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this demo paper, we introduce SAPIEN, a platform for high-fidelity virtual agents driven by large language models that can hold open domain conversations with users in 13 different languages, and display emotions through facial expressions and voice. The platform allows users to customize their virtual agent's personality, background, and conversation premise, thus providing a rich, immersive interaction experience. Furthermore, after the virtual meeting, the user can choose to get the conversation analyzed and receive actionable feedback on their communication skills. This paper illustrates an overview of the platform and discusses the various application domains of this technology, ranging from entertainment to mental health, communication training, language learning, education, healthcare, and beyond. Additionally, we consider the ethical implications of such realistic virtual agent representations and the potential challenges in ensuring responsible use.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2306.15213.pdf' target='_blank'>https://arxiv.org/pdf/2306.15213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurtis Haut, Caleb Wohn, Benjamin Kane, Tom Carroll, Catherine Guigno, Varun Kumar, Ron Epstein, Lenhart Schubert, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15213">Validating a virtual human and automated feedback system for training doctor-patient communication skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication between a clinician and their patient is critical for delivering healthcare maximizing outcomes. Unfortunately, traditional communication training approaches that use human standardized patients and expert coaches are difficult to scale. Here, we present the development and validation of a scalable, easily accessible, digital tool known as the Standardized Online Patient for Health Interaction Education (SOPHIE) for practicing and receiving feedback on doctor-patient communication skills. SOPHIE was validated by conducting an experiment with 30 participants. We found that participants who underwent SOPHIE performed significantly better than the control in overall communication, aggregate scores, empowering the patient, and showing empathy ($p < 0.05$ in all cases). One day, we hope that SOPHIE will help make communication training resources more accessible by providing a scalable option to supplement existing resources.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2306.09532.pdf' target='_blank'>https://arxiv.org/pdf/2306.09532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel van de Panne, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09532">Hierarchical Planning and Control for Box Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans perform everyday tasks using a combination of locomotion and manipulation skills. Building a system that can handle both skills is essential to creating virtual humans. We present a physically-simulated human capable of solving box rearrangement tasks, which requires a combination of both skills. We propose a hierarchical control architecture, where each level solves the task at a different level of abstraction, and the result is a physics-based simulated virtual human capable of rearranging boxes in a cluttered environment. The control architecture integrates a planner, diffusion models, and physics-based motion imitation of sparse motion clips using deep reinforcement learning. Boxes can vary in size, weight, shape, and placement height. Code and trained control policies are provided.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2305.16411.pdf' target='_blank'>https://arxiv.org/pdf/2305.16411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhen Weng, Zeyu Wang, Serena Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16411">ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in text-to-image generation have enabled significant progress in zero-shot 3D shape generation. This is achieved by score distillation, a methodology that uses pre-trained text-to-image diffusion models to optimize the parameters of a 3D neural presentation, e.g. Neural Radiance Field (NeRF). While showing promising results, existing methods are often not able to preserve the geometry of complex shapes, such as human bodies. To address this challenge, we present ZeroAvatar, a method that introduces the explicit 3D human body prior to the optimization process. Specifically, we first estimate and refine the parameters of a parametric human body from a single image. Then during optimization, we use the posed parametric body as additional geometry constraint to regularize the diffusion model as well as the underlying density field. Lastly, we propose a UV-guided texture regularization term to further guide the completion of texture on invisible body parts. We show that ZeroAvatar significantly enhances the robustness and 3D consistency of optimization-based image-to-3D avatar generation, outperforming existing zero-shot image-to-3D methods.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2305.14345.pdf' target='_blank'>https://arxiv.org/pdf/2305.14345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeksoo Kim, Shunsuke Saito, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14345">NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models have been recently extended to synthesizing 3D digital humans. However, previous approaches treat clothed humans as a single chunk of geometry without considering the compositionality of clothing and accessories. As a result, individual items cannot be naturally composed into novel identities, leading to limited expressiveness and controllability of generative 3D avatars. While several methods attempt to address this by leveraging synthetic data, the interaction between humans and objects is not authentic due to the domain gap, and manual asset creation is difficult to scale for a wide variety of objects. In this work, we present a novel framework for learning a compositional generative model of humans and objects (backpacks, coats, scarves, and more) from real-world 3D scans. Our compositional model is interaction-aware, meaning the spatial relationship between humans and objects, and the mutual shape change by physical contact is fully incorporated. The key challenge is that, since humans and objects are in contact, their 3D scans are merged into a single piece. To decompose them without manual annotations, we propose to leverage two sets of 3D scans of a single person with and without objects. Our approach learns to decompose objects and naturally compose them back into a generative human model in an unsupervised manner. Despite our simple setup requiring only the capture of a single subject with objects, our experiments demonstrate the strong generalization of our model by enabling the natural composition of objects to diverse identities in various poses and the composition of multiple objects, which is unseen in training data. https://taeksuu.github.io/ncho/
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2304.12809.pdf' target='_blank'>https://arxiv.org/pdf/2304.12809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katie Seaborn, Somang Nam, Julia Keckeis, Tatsuya Itagaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12809">Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Japanese notion of "kawaii" or expressions of cuteness, vulnerability, and/or charm is a global cultural export. Work has explored kawaii-ness as a design feature and factor of user experience in the visual appearance, nonverbal behaviour, and sound of robots and virtual characters. In this initial work, we consider whether voices can be kawaii by exploring the vocal qualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an age-inclusive model of kawaii, we ran a user perceptions study on the kawaii-ness of younger- and older-sounding Japanese computer voices. We found that kawaii-ness intersected with perceptions of gender and age, i.e., gender ambiguous and girlish, as well as VA features, i.e., fluency and artificiality. We propose an initial model of kawaii vocalics to be validated through the identification and study of vocal qualities, cognitive appraisals, behavioural responses, and affective reports.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2304.12483.pdf' target='_blank'>https://arxiv.org/pdf/2304.12483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aashish Rai, Hiresh Gupta, Ayush Pandey, Francisco Vicente Carrasco, Shingo Jason Takagi, Amaury Aubel, Daeil Kim, Aayush Prakash, Fernando de la Torre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12483">Towards Realistic Generative 3D Face Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face model to produce high-quality albedo and precise 3D shape leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision. Moreover, this approach outperforms the state-of-the-art (SOTA) methods in the well-known NoW benchmark for shape reconstruction. It also outperforms the SOTA reconstruction models in recovering rendered faces' identities across novel poses by an average of 10%. Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2304.11113.pdf' target='_blank'>https://arxiv.org/pdf/2304.11113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhan Chen, Matthew O'Toole, Gaurav Bharaj, Pablo Garrido
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11113">Implicit Neural Head Synthesis via Controllable Local Deformation Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality reconstruction of controllable 3D head avatars from 2D videos is highly desirable for virtual human applications in movies, games, and telepresence. Neural implicit fields provide a powerful representation to model 3D head avatars with personalized shape, expressions, and facial parts, e.g., hair and mouth interior, that go beyond the linear 3D morphable model (3DMM). However, existing methods do not model faces with fine-scale facial features, or local control of facial parts that extrapolate asymmetric expressions from monocular videos. Further, most condition only on 3DMM parameters with poor(er) locality, and resolve local features with a global neural field. We build on part-based implicit shape models that decompose a global deformation field into local ones. Our novel formulation models multiple implicit deformation fields with local semantic rig-like control via 3DMM-based parameters, and representative facial landmarks. Further, we propose a local control loss and attention mask mechanism that promote sparsity of each learned deformation field. Our formulation renders sharper locally controllable nonlinear deformations than previous implicit monocular approaches, especially mouth interior, asymmetric expressions, and facial details.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2210.01781.pdf' target='_blank'>https://arxiv.org/pdf/2210.01781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boxiao Pan, Bokui Shen, Davis Rempe, Despoina Paschalidou, Kaichun Mo, Yanchao Yang, Leonidas J. Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.01781">COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to forecast human-environment collisions from egocentric observations is vital to enable collision avoidance in applications such as VR, AR, and wearable assistive robotics. In this work, we introduce the challenging problem of predicting collisions in diverse environments from multi-view egocentric videos captured from body-mounted cameras. Solving this problem requires a generalizable perception system that can classify which human body joints will collide and estimate a collision region heatmap to localize collisions in the environment. To achieve this, we propose a transformer-based model called COPILOT to perform collision prediction and localization simultaneously, which accumulates information across multi-view inputs through a novel 4D space-time-viewpoint attention mechanism. To train our model and enable future research on this task, we develop a synthetic data generation framework that produces egocentric videos of virtual humans moving and colliding within diverse 3D environments. This framework is then used to establish a large-scale dataset consisting of 8.6M egocentric RGBD frames. Extensive experiments show that COPILOT generalizes to unseen synthetic as well as real-world scenes. We further demonstrate COPILOT outputs are useful for downstream collision avoidance through simple closed-loop control. Please visit our project webpage at https://sites.google.com/stanford.edu/copilot.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2509.13013.pdf' target='_blank'>https://arxiv.org/pdf/2509.13013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaofeng Liu, Hengsen Li, Ruoyu Gao, Xuetong Li, Zhiyuan Ma, Tao Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13013">Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2509.05582.pdf' target='_blank'>https://arxiv.org/pdf/2509.05582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiling Ye, Cong Zhou, Xiubao Zhang, Haifeng Shen, Weihong Deng, Quan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05582">Reconstruction and Reenactment Separated Method for Realistic Gaussian Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore a reconstruction and reenactment separated framework for 3D Gaussians head, which requires only a single portrait image as input to generate controllable avatar. Specifically, we developed a large-scale one-shot gaussian head generator built upon WebSSL and employed a two-stage training approach that significantly enhances the capabilities of generalization and high-frequency texture reconstruction. During inference, an ultra-lightweight gaussian avatar driven by control signals enables high frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further demonstrate that the proposed framework follows the scaling law, whereby increasing the parameter scale of the reconstruction module leads to improved performance. Moreover, thanks to the separation design, driving efficiency remains unaffected. Finally, extensive quantitative and qualitative experiments validate that our approach outperforms current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2508.14920.pdf' target='_blank'>https://arxiv.org/pdf/2508.14920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilya Fedorov, Dmitry Korobchenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14920">Human Feedback Driven Dynamic Speech Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes to explore a new area of dynamic speech emotion recognition. Unlike traditional methods, we assume that each audio track is associated with a sequence of emotions active at different moments in time. The study particularly focuses on the animation of emotional 3D avatars. We propose a multi-stage method that includes the training of a classical speech emotion recognition model, synthetic generation of emotional sequences, and further model improvement based on human feedback. Additionally, we introduce a novel approach to modeling emotional mixtures based on the Dirichlet distribution. The models are evaluated based on ground-truth emotions extracted from a dataset of 3D facial animations. We compare our models against the sliding window approach. Our experimental results show the effectiveness of Dirichlet-based approach in modeling emotional mixtures. Incorporating human feedback further improves the model quality while providing a simplified annotation procedure.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2508.08429.pdf' target='_blank'>https://arxiv.org/pdf/2508.08429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dalton Omens, Allise Thurman, Jihun Yu, Ronald Fedkiw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08429">Improving Facial Rig Semantics for Tracking and Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider retargeting a tracked facial performance to either another person or to a virtual character in a game or virtual reality (VR) environment. We remove the difficulties associated with identifying and retargeting the semantics of one rig framework to another by utilizing the same framework (3DMM, FLAME, MetaHuman, etc.) for both subjects. Although this does not constrain the choice of framework when retargeting from one person to another, it does force the tracker to use the game/VR character rig when retargeting to a game/VR character. We utilize volumetric morphing in order to fit facial rigs to both performers and targets; in addition, a carefully chosen set of Simon-Says expressions is used to calibrate each rig to the motion signatures of the relevant performer or target. Although a uniform set of Simon-Says expressions can likely be used for all person to person retargeting, we argue that person to game/VR character retargeting benefits from Simon-Says expressions that capture the distinct motion signature of the game/VR character rig. The Simon-Says calibrated rigs tend to produce the desired expressions when exercising animation controls (as expected). Unfortunately, these well-calibrated rigs still lead to undesirable controls when tracking a performance (a well-behaved function can have an arbitrarily ill-conditioned inverse), even though they typically produce acceptable geometry reconstructions. Thus, we propose a fine-tuning approach that modifies the rig used by the tracker in order to promote the output of more semantically meaningful animation controls, facilitating high efficacy retargeting. In order to better address real-world scenarios, the fine-tuning relies on implicit differentiation so that the tracker can be treated as a (potentially non-differentiable) black box.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2507.23597.pdf' target='_blank'>https://arxiv.org/pdf/2507.23597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Dong, Longteng Duan, Jie Song, Michael J. Black, Andreas Geiger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23597">MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see https://zj-dong.github.io/MoGA/.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2507.19568.pdf' target='_blank'>https://arxiv.org/pdf/2507.19568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>You Wu, Philip E. Bourne, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19568">Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) has sparked immense interest in drug discovery, but most current approaches only digitize existing high-throughput experiments. They remain constrained by conventional pipelines. As a result, they do not address the fundamental challenges of predicting drug effects in humans. Similarly, biomedical digital twins, largely grounded in real-world data and mechanistic models, are tailored for late-phase drug development and lack the resolution to model molecular interactions or their systemic consequences, limiting their impact in early-stage discovery. This disconnect between early discovery and late development is one of the main drivers of high failure rates in drug discovery. The true promise of AI lies not in augmenting current experiments but in enabling virtual experiments that are impossible in the real world: testing novel compounds directly in silico in the human body. Recent advances in AI, high-throughput perturbation assays, and single-cell and spatial omics across species now make it possible to construct programmable virtual humans: dynamic, multiscale models that simulate drug actions from molecular to phenotypic levels. By bridging the translational gap, programmable virtual humans offer a transformative path to optimize therapeutic efficacy and safety earlier than ever before. This perspective introduces the concept of programmable virtual humans, explores their roles in a new paradigm of drug discovery centered on human physiology, and outlines key opportunities, challenges, and roadmaps for their realization.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2507.18155.pdf' target='_blank'>https://arxiv.org/pdf/2507.18155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SeungJun Moon, Hah Min Lew, Seungeun Lee, Ji-Su Kang, Gyeong-Moon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18155">GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2507.16562.pdf' target='_blank'>https://arxiv.org/pdf/2507.16562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megha Quamara, Viktor Schmuck, Cristina Iani, Axel Primavesi, Alexander Plaum, Luca Vigano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16562">Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the findings of a user study that evaluated the social acceptance of eXtended Reality (XR) agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves user interaction with a virtual avatar, enabled by a modular toolkit. The interactions are designed to provide tailored training for journalists in digital-remote settings, especially for sensitive or dangerous scenarios, without requiring specialized end-user equipment like headsets. Our research adapts and extends the Almere model, representing social acceptance through existing attributes such as perceived ease of use and perceived usefulness, along with added ones like dependability and security in the user-agent interaction. The XR agent was tested through a controlled experiment in a real-world setting, with data collected on users' perceptions. Our findings, based on quantitative and qualitative measurements involving questionnaires, contribute to the understanding of user perceptions and acceptance of XR agent solutions within a specific social context, while also identifying areas for the improvement of XR systems.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2506.23739.pdf' target='_blank'>https://arxiv.org/pdf/2506.23739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lisa Marie Otto, Michael Kaiser, Daniel Seebacher, Steffen MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23739">Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe and realistic interactions between automated driving systems and vulnerable road users (VRUs) in urban environments requires advanced testing methodologies. This paper presents a test environment that combines a Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the feasibility of cyber-physical (CP) testing of vehicle-pedestrian and vehicle-cyclist interactions. Building upon previous work focused on pedestrian localization, we further validate a human pose estimation (HPE) approach through a comparative analysis of real-world (RW) and virtual representations of VRUs. The study examines the perception of full-body motion using a commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is generated in Unreal Engine 5, where VRUs are animated in real time and projected onto a screen to stimulate the camera. The proposed stimulation technique ensures the correct perspective, enabling realistic vehicle perception. To assess the accuracy and consistency of HPE across RW and CP domains, we analyze the reliability of detections as well as variations in movement trajectories and joint estimation stability. The validation includes dynamic test scenarios where human avatars, both walking and cycling, are monitored under controlled conditions. Our results show a strong alignment in HPE between RW and CP test conditions for stable motion patterns, while notable inaccuracies persist under dynamic movements and occlusions, particularly for complex cyclist postures. These findings contribute to refining CP testing approaches for evaluating next-generation AI-based vehicle perception and to enhancing interaction models of automated vehicles and VRUs in CP environments.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2506.21780.pdf' target='_blank'>https://arxiv.org/pdf/2506.21780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anya Osborne, Sabrina Fielder, Lee Taber, Tara Lamb, Joshua McVeigh-Schultz, Katherine Isbister
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21780">Avatars and Environments for Meetings in Social VR: What Styles and Choices Matter to People in Group Creativity Tasks?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the COVID-19 pandemic, many professional entities shifted toward remote collaboration and video conferencing (VC) tools. Social virtual reality (VR) platforms present an alternative to VC for meetings and collaborative activities. Well-crafted social VR environments could enhance feelings of co-presence and togetherness at meetings, helping reduce the need for carbon-intensive travel to face-to-face meetings. This research contributes to creating meeting tools in VR by exploring the effects of avatar styles and virtual environments on groups creative performance using the Mozilla Hubs platform. We present the results of two sequential studies. Study One surveys avatar and environment preferences in various VR meeting contexts (N=87). Study Two applies these findings to the design of a between-subjects and within-subjects research where participants (N=40) perform creativity tasks in pairs as embodied avatars in different virtual settings using VR headsets. We discuss the design implications of avatar appearances and meeting settings on teamwork.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2506.09411.pdf' target='_blank'>https://arxiv.org/pdf/2506.09411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaclav Knapp, Matyas Bohacek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09411">Synthetic Human Action Video Data Generation with Pose Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In video understanding tasks, particularly those involving human motion, synthetic data generation often suffers from uncanny features, diminishing its effectiveness for training. Tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving have thus been unable to exploit the full potential of synthetic data. This paper proposes a method for generating synthetic human action video data using pose transfer (specifically, controllable 3D Gaussian avatar models). We evaluate this method on the Toyota Smarthome and NTU RGB+D datasets and show that it improves performance in action recognition tasks. Moreover, we demonstrate that the method can effectively scale few-shot datasets, making up for groups underrepresented in the real training data and adding diverse backgrounds. We open-source the method along with RANDOM People, a dataset with videos and avatars of novel human identities for pose transfer crowd-sourced from the internet.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2506.06271.pdf' target='_blank'>https://arxiv.org/pdf/2506.06271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Schmidt, Simon Giebenhain, Matthias Niessner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06271">BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce BecomingLit, a novel method for reconstructing relightable, high-resolution head avatars that can be rendered from novel viewpoints at interactive rates. Therefore, we propose a new low-cost light stage capture setup, tailored specifically towards capturing faces. Using this setup, we collect a novel dataset consisting of diverse multi-view sequences of numerous subjects under varying illumination conditions and facial expressions. By leveraging our new dataset, we introduce a new relightable avatar representation based on 3D Gaussian primitives that we animate with a parametric head model and an expression-dependent dynamics module. We propose a new hybrid neural shading approach, combining a neural diffuse BRDF with an analytical specular term. Our method reconstructs disentangled materials from our dynamic light stage recordings and enables all-frequency relighting of our avatars with both point lights and environment maps. In addition, our avatars can easily be animated and controlled from monocular videos. We validate our approach in extensive experiments on our dataset, where we consistently outperform existing state-of-the-art methods in relighting and reenactment by a significant margin.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2506.03099.pdf' target='_blank'>https://arxiv.org/pdf/2506.03099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chetwin Low, Weimin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03099">TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2505.21566.pdf' target='_blank'>https://arxiv.org/pdf/2505.21566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gao Huayu, Huang Tengjiu, Ye Xiaolong, Tsuyoshi Okita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21566">Diffusion Model-based Activity Completion for AI Motion Capture from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-based motion capture is an emerging technology that offers a cost-effective alternative to traditional motion capture systems. However, current AI motion capture methods rely entirely on observed video sequences, similar to conventional motion capture. This means that all human actions must be predefined, and movements outside the observed sequences are not possible. To address this limitation, we aim to apply AI motion capture to virtual humans, where flexible actions beyond the observed sequences are required. We assume that while many action fragments exist in the training data, the transitions between them may be missing. To bridge these gaps, we propose a diffusion-model-based action completion technique that generates complementary human motion sequences, ensuring smooth and continuous movements. By introducing a gate module and a position-time embedding module, our approach achieves competitive results on the Human3.6M dataset. Our experimental results show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size (16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural and coherent motion sequences. Additionally, we propose a method for extracting sensor data, including acceleration and angular velocity, from human motion sequences.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2504.20403.pdf' target='_blank'>https://arxiv.org/pdf/2504.20403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Liu, Yifang Men, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20403">Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2504.17614.pdf' target='_blank'>https://arxiv.org/pdf/2504.17614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Leaf, David Sebastian Minor, Gilles Daviet, Nuttapong Chentanez, Greg Klar, Ed Quigley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17614">Bolt: Clothing Virtual Characters at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clothing virtual characters is a time-consuming and often manual process. Outfits can be composed of multiple garments, and each garment must be fitted to the unique shape of a character. Since characters can vary widely in size and shape, fitting outfits to many characters is a combinatorially large problem. We present Bolt, a system designed to take outfits originally authored on a source body and fit them to new body shapes via a three stage transfer, drape, and rig process. First, our new garment transfer method transforms each garment's 3D mesh positions to the new character, then optimizes the garment's 2D sewing pattern while maintaining key features of the original seams and boundaries. Second, our system simulates the transferred garments to progressively drape and untangle each garment in the outfit. Finally, the garments are rigged to the new character. This entire process is automatic, making it feasible to clothe characters at scale with no human intervention. Clothed characters are then ready for immediate use in applications such as gaming, animation, synthetic generation, and more.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2504.12999.pdf' target='_blank'>https://arxiv.org/pdf/2504.12999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rendong Zhang, Alexandra Watkins, Nilanjan Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12999">GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation with Unity Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic avatars have become essential for immersive applications in virtual reality (VR) and augmented reality (AR), enabling lifelike interactions in areas such as training simulations, telemedicine, and virtual collaboration. These avatars bridge the gap between the physical and digital worlds, improving the user experience through realistic human representation. However, existing avatar creation techniques face significant challenges, including high costs, long creation times, and limited utility in virtual applications. Manual methods, such as MetaHuman, require extensive time and expertise, while automatic approaches, such as NeRF-based pipelines often lack efficiency, detailed facial expression fidelity, and are unable to be rendered at a speed sufficent for real-time applications. By involving several cutting-edge modern techniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar creation pipeline that leverages monocular video input to create a scalable and efficient photorealistic avatar directly compatible with the Unity game engine. Our pipeline incorporates a novel Gaussian splatting technique with customized preprocessing that enables the user of "in the wild" monocular video capture, detailed facial expression reconstruction and embedding within a fully rigged avatar model. Additionally, we present a Unity-integrated Gaussian Splatting Avatar Editor, offering a user-friendly environment for VR/AR application development. Experimental results validate the effectiveness of our preprocessing pipeline in standardizing custom data for 3DGS training and demonstrate the versatility of Gaussian avatars in Unity, highlighting the scalability and practicality of our approach.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2504.08296.pdf' target='_blank'>https://arxiv.org/pdf/2504.08296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, Praagya Bahuguna, Mark Chan, Khushi Hora, Lijian Yang, Yongqi Liang, Runhe Bian, Yunlei Liu, Isabela Campillo Valencia, Patricia Morales Tredinick, Ilia Kozlov, Sijia Jiang, Peiwen Huang, Na Chen, Xuanxuan Liu, Anyi Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08296">Generative AI for Film Creation: A Survey of Recent Advances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI (GenAI) is transforming filmmaking, equipping artists with tools like text-to-image and image-to-video diffusion, neural radiance fields, avatar generation, and 3D synthesis. This paper examines the adoption of these technologies in filmmaking, analyzing workflows from recent AI-driven films to understand how GenAI contributes to character creation, aesthetic styling, and narration. We explore key strategies for maintaining character consistency, achieving stylistic coherence, and ensuring motion continuity. Additionally, we highlight emerging trends such as the growing use of 3D generation and the integration of real footage with AI-generated elements.
  Beyond technical advancements, we examine how GenAI is enabling new artistic expressions, from generating hard-to-shoot footage to dreamlike diffusion-based morphing effects, abstract visuals, and unworldly objects. We also gather artists' feedback on challenges and desired improvements, including consistency, controllability, fine-grained editing, and motion refinement. Our study provides insights into the evolving intersection of AI and filmmaking, offering a roadmap for researchers and artists navigating this rapidly expanding field.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2504.04968.pdf' target='_blank'>https://arxiv.org/pdf/2504.04968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayang Huang, Lingjie Li, Kang Zhang, David Yip
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04968">The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the art project The Dream Within Huang Long Cave, an AI-driven interactive and immersive narrative experience. The project offers new insights into AI technology, artistic practice, and psychoanalysis. Inspired by actual geographical landscapes and familial archetypes, the work combines psychoanalytic theory and computational technology, providing an artistic response to the concept of the non-existence of the Big Other. The narrative is driven by a combination of a large language model (LLM) and a realistic digital character, forming a virtual agent named YELL. Through dialogue and exploration within a cave automatic virtual environment (CAVE), the audience is invited to unravel the language puzzles presented by YELL and help him overcome his life challenges. YELL is a fictional embodiment of the Big Other, modeled after the artist's real father. Through a cross-temporal interaction with this digital father, the project seeks to deconstruct complex familial relationships. By demonstrating the non-existence of the Big Other, we aim to underscore the authenticity of interpersonal emotions, positioning art as a bridge for emotional connection and understanding within family dynamics.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2503.18408.pdf' target='_blank'>https://arxiv.org/pdf/2503.18408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Wu, Ruiqi Zhang, Jie Chen, Hui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18408">Fast and Physically-based Neural Explicit Surface for Relightable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling relightable human avatars from sparse-view videos is crucial for AR/VR applications. Current methods use neural implicit representations to capture dynamic geometry and reflectance, which incur high costs due to the need for dense sampling in volume rendering. To overcome these challenges, we introduce Physically-based Neural Explicit Surface (PhyNES), which employs compact neural material maps based on the Neural Explicit Surface (NES) representation. PhyNES organizes human models in a compact 2D space, enhancing material disentanglement efficiency. By connecting Signed Distance Fields to explicit surfaces, PhyNES enables efficient geometry inference around a parameterized human shape model. This approach models dynamic geometry, texture, and material maps as 2D neural representations, enabling efficient rasterization. PhyNES effectively captures physical surface attributes under varying illumination, enabling real-time physically-based rendering. Experiments show that PhyNES achieves relighting quality comparable to SOTA methods while significantly improving rendering speed, memory efficiency, and reconstruction quality.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2503.16432.pdf' target='_blank'>https://arxiv.org/pdf/2503.16432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young-Ho Bae, Casey C. Bennett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16432">Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game "Dont Starve Together", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2503.15100.pdf' target='_blank'>https://arxiv.org/pdf/2503.15100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristina Fiani, Pejman Saeghe, Mark McGill, Mohamed Khamis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15100">Exploring the Perspectives of Social VR-Aware Non-Parent Adults and Parents on Children's Use of Social Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social Virtual Reality (VR), where people meet in virtual spaces via 3D avatars, is used by children and adults alike. Children experience new forms of harassment in social VR where it is often inaccessible to parental oversight. To date, there is limited understanding of how parents and non-parent adults within the child social VR ecosystem perceive the appropriateness of social VR for different age groups and the measures in place to safeguard children. We present results of a mixed-methods questionnaire (N=149 adults, including 79 parents) focusing on encounters with children in social VR and perspectives towards children's use of social VR. We draw novel insights on the frequency of social VR use by children under 13 and current use of, and future aspirations for, child protection interventions. Compared to non-parent adults, parents familiar with social VR propose lower minimum ages and are more likely to allow social VR without supervision. Adult users experience immaturity from children in social VR, while children face abuse, encounter age-inappropriate behaviours and self-disclose to adults. We present directions to enhance the safety of social VR through pre-planned controls, real-time oversight, post-event insight and the need for evidence-based guidelines to support parents and platforms around age-appropriate interventions.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2503.00842.pdf' target='_blank'>https://arxiv.org/pdf/2503.00842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daye Kim, Sebin Lee, Yoonseo Jun, Yujin Shin, Jungjin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00842">VTuber's Atelier: The Design Space, Challenges, and Opportunities for VTubing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VTubing, the practice of live streaming using virtual avatars, has gained worldwide popularity among streamers seeking to maintain anonymity. While previous research has primarily focused on the social and cultural aspects of VTubing, there is a noticeable lack of studies examining the practical challenges VTubers face in creating and operating their avatars. To address this gap, we surveyed VTubers' equipment and expanded the live-streaming design space by introducing six new dimensions related to avatar creation and control. Additionally, we conducted interviews with 16 professional VTubers to comprehensively explore their practices, strategies, and challenges throughout the VTubing process. Our findings reveal that VTubers face significant burdens compared to real-person streamers due to fragmented tools and the multi-tasking nature of VTubing, leading to unique workarounds. Finally, we summarize these challenges and propose design opportunities to improve the effectiveness and efficiency of VTubing.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2502.20577.pdf' target='_blank'>https://arxiv.org/pdf/2502.20577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MD Wahiduzzaman Khan, Mingshan Jia, Xiaolin Zhang, En Yu, Caifeng Shan, Kaska Musial-Gabrys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20577">InstaFace: Identity-Preserving Facial Editing with Single Image Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial appearance editing is crucial for digital avatars, AR/VR, and personalized content creation, driving realistic user experiences. However, preserving identity with generative models is challenging, especially in scenarios with limited data availability. Traditional methods often require multiple images and still struggle with unnatural face shifts, inconsistent hair alignment, or excessive smoothing effects. To overcome these challenges, we introduce a novel diffusion-based framework, InstaFace, to generate realistic images while preserving identity using only a single image. Central to InstaFace, we introduce an efficient guidance network that harnesses 3D perspectives by integrating multiple 3DMM-based conditionals without introducing additional trainable parameters. Moreover, to ensure maximum identity retention as well as preservation of background, hair, and other contextual features like accessories, we introduce a novel module that utilizes feature embeddings from a facial recognition model and a pre-trained vision-language model. Quantitative evaluations demonstrate that our method outperforms several state-of-the-art approaches in terms of identity preservation, photorealism, and effective control of pose, expression, and lighting.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2502.11642.pdf' target='_blank'>https://arxiv.org/pdf/2502.11642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyumin Shim, Sangmin Lee, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11642">GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce GaussianMotion, a novel human rendering model that generates fully animatable scenes aligned with textual descriptions using Gaussian Splatting. Although existing methods achieve reasonable text-to-3D generation of human bodies using various 3D representations, they often face limitations in fidelity and efficiency, or primarily focus on static models with limited pose control. In contrast, our method generates fully animatable 3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score distillation, achieving high fidelity and efficient rendering for arbitrary poses. By densely generating diverse random poses during optimization, our deformable 3D human model learns to capture a wide range of natural motions distilled from a pose-conditioned diffusion model in an end-to-end manner. Furthermore, we propose Adaptive Score Distillation that effectively balances realistic detail and smoothness to achieve optimal 3D results. Experimental results demonstrate that our approach outperforms existing baselines by producing high-quality textures in both static and animated results, and by generating diverse 3D human models from various textual inputs.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2502.09617.pdf' target='_blank'>https://arxiv.org/pdf/2502.09617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wen, Alexander G. Schwing, Shenlong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09617">LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable rendering of an animatable human avatar from sparse inputs relies on data priors and inductive biases extracted from training on large data to avoid scene-specific optimization and to enable fast reconstruction. This raises two main challenges: First, unlike iterative gradient-based adjustment in scene-specific optimization, generalizable methods must reconstruct the human shape representation in a single pass at inference time. Second, rendering is preferably computationally efficient yet of high resolution. To address both challenges we augment the recently proposed dual shape representation, which combines the benefits of a mesh and Gaussian points, in two ways. To improve reconstruction, we propose an iterative feedback update framework, which successively improves the canonical human shape representation during reconstruction. To achieve computationally efficient yet high-resolution rendering, we study a coupled-multi-resolution Gaussians-on-Mesh representation. We evaluate the proposed approach on the challenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an animatable representation from sparse inputs in less than 1s, renders views with 95.1FPS at $1024 \times 1024$, and achieves PSNR/LPIPS*/FID of 24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in rendering quality.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2501.17792.pdf' target='_blank'>https://arxiv.org/pdf/2501.17792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17792">CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through these experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2501.17085.pdf' target='_blank'>https://arxiv.org/pdf/2501.17085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17085">Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and realistic crowd rendering is an important element of many real-time graphics applications such as Virtual Reality (VR) and games. To this end, Levels of Detail (LOD) avatar representations such as polygonal meshes, image-based impostors, and point clouds have been proposed and evaluated. More recently, 3D Gaussian Splatting has been explored as a potential method for real-time crowd rendering. In this paper, we present a two-alternative forced choice (2AFC) experiment that aims to determine the perceived quality of 3D Gaussian avatars. Three factors were explored: Motion, LOD (i.e., #Gaussians), and the avatar height in Pixels (corresponding to the viewing distance). Participants viewed pairs of animated 3D Gaussian avatars and were tasked with choosing the most detailed one. Our findings can inform the optimization of LOD strategies in Gaussian-based crowd rendering, thereby helping to achieve efficient rendering while maintaining visual quality in real-time applications.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2501.08609.pdf' target='_blank'>https://arxiv.org/pdf/2501.08609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaleab A. Kinfu, Carolina Pacheco, Alice D. Sperry, Deana Crocetti, Bahar TunÃ§genÃ§, Stewart H. Mostofsky, RenÃ© Vidal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08609">Computerized Assessment of Motor Imitation for Distinguishing Autism in Video (CAMI-2DNet)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motor imitation impairments are commonly reported in individuals with autism spectrum conditions (ASCs), suggesting that motor imitation could be used as a phenotype for addressing autism heterogeneity. Traditional methods for assessing motor imitation are subjective, labor-intensive, and require extensive human training. Modern Computerized Assessment of Motor Imitation (CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video data, are less subjective. However, they rely on labor-intensive data normalization and cleaning techniques, and human annotations for algorithm training. To address these challenges, we propose CAMI-2DNet, a scalable and interpretable deep learning-based approach to motor imitation assessment in video data, which eliminates the need for data normalization, cleaning and annotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a motion encoding that is disentangled from nuisance factors such as body shape and camera views. To learn a disentangled representation, we employ synthetic data generated by motion retargeting of virtual characters through the reshuffling of motion, body shape, and camera views, as well as real participant data. To automatically assess how well an individual imitates an actor, we compute a similarity score between their motion encodings, and use it to discriminate individuals with ASCs from neurotypical (NT) individuals. Our comparative analysis demonstrates that CAMI-2DNet has a strong correlation with human scores while outperforming CAMI-2D in discriminating ASC vs NT children. Moreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater practicality by operating directly on video data and without the need for ad-hoc data normalization and human annotations.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2411.15732.pdf' target='_blank'>https://arxiv.org/pdf/2411.15732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyang Qian, Yuan Sun, Yu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15732">DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and Precise Editing with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating and editing dynamic 3D head avatars are crucial tasks in virtual reality and film production. However, existing methods often suffer from facial distortions, inaccurate head movements, and limited fine-grained editing capabilities. To address these challenges, we present DynamicAvatars, a dynamic model that generates photorealistic, moving 3D head avatars from video clips and parameters associated with facial positions and expressions. Our approach enables precise editing through a novel prompt-based editing model, which integrates user-provided prompts with guiding parameters derived from large language models (LLMs). To achieve this, we propose a dual-tracking framework based on Gaussian Splatting and introduce a prompt preprocessing module to enhance editing stability. By incorporating a specialized GAN algorithm and connecting it to our control module, which generates precise guiding parameters from LLMs, we successfully address the limitations of existing methods. Additionally, we develop a dynamic editing strategy that selectively utilizes specific training datasets to improve the efficiency and adaptability of the model for dynamic editing tasks.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2410.15343.pdf' target='_blank'>https://arxiv.org/pdf/2410.15343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Tang Tsui, Yu-Rou Tuan, Jia-You Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15343">POSE: Pose estimation Of virtual Sync Exhibit system</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work is a portable MetaVerse implementation, and we use 3D pose estimation with AI to make virtual avatars do synchronized actions and interact with the environment. The motivation is that we find it inconvenient to use joysticks and sensors when playing with fitness rings. In order to replace joysticks and reduce costs, we developed a platform that can control virtual avatars through pose estimation to identify the movements of real people, and we also implemented a multi-process to achieve modularization and reduce the overall latency.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2410.12051.pdf' target='_blank'>https://arxiv.org/pdf/2410.12051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cindy Xu, Mengyu Chen, Pranav Deshpande, Elvir Azanli, Runqing Yang, Joseph Ligman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12051">Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel system designed to enhance customer service in the financial and retail sectors through a context-aware 3D virtual agent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our approach focuses on enabling data-driven and empathetic interactions that ensure customer satisfaction by introducing situational awareness of the physical location, personalized interactions based on customer profiles, and rigorous privacy and security standards. We discuss our design considerations critical for deployment in real-world customer service environments, addressing challenges in user data management and sensitive information handling. We also outline the system architecture and key features unique to banking and retail environments. Our work demonstrates the potential of integrating MR and VLMs in service industries, offering practical insights in customer service delivery while maintaining high standards of security and personalization.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2409.18083.pdf' target='_blank'>https://arxiv.org/pdf/2409.18083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mirela Ostrek, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18083">Stable Video Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in the field of generative AI and text-to-image methods in particular have transformed the way we interact with and perceive computer-generated imagery today. In parallel, much progress has been made in 3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we present SVP, a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific fine-tuning of a general 2D stable diffusion model which we lift to a video model by providing temporal 3DMM sequences as conditioning and by introducing a temporal denoising procedure. As an output, this model generates temporally smooth imagery of a person with 3DMM-based controls, i.e., a person-specific avatar. The facial appearance of this person-specific avatar can be edited and morphed to text-defined celebrities, without any fine-tuning at test time. The method is analyzed quantitatively and qualitatively, and we show that our method outperforms state-of-the-art monocular head avatar methods.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2409.16147.pdf' target='_blank'>https://arxiv.org/pdf/2409.16147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16147">Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars, providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements, the creation of controllable 3DGS-based head avatars remains time-intensive, often requiring tens of minutes to hours. To expedite this process, we here introduce the "Gaussian Deja-vu" framework, which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing, we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians, ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods, producing the avatar in minutes.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2409.00012.pdf' target='_blank'>https://arxiv.org/pdf/2409.00012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanhyuk Park, Jungbin Cho, Junwan Kim, Seongmin Lee, Jungsu Kim, Sanghoon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00012">AVIN-Chat: An Audio-Visual Interactive Chatbot System with Emotional State Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents an audio-visual interactive chatbot (AVIN-Chat) system that allows users to have face-to-face conversations with 3D avatars in real-time. Compared to the previous chatbot services, which provide text-only or speech-only communications, the proposed AVIN-Chat can offer audio-visual communications providing users with a superior experience quality. In addition, the proposed AVIN-Chat emotionally speaks and expresses according to the user's emotional state. Thus, it enables users to establish a strong bond with the chatbot system, increasing the user's immersion. Through user subjective tests, it is demonstrated that the proposed system provides users with a higher sense of immersion than previous chatbot systems. The demonstration video is available at https://www.youtube.com/watch?v=Z74uIV9k7_k.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2408.16110.pdf' target='_blank'>https://arxiv.org/pdf/2408.16110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubens Halbig Montanha, Giovana Nascimento Raupp, Ana Carolina Policarpo Schmitt, Victor FlÃ¡vio de Andrade Araujo, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16110">Micro and macro facial expressions by driven animations in realistic Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer Graphics (CG) advancements have allowed the creation of more realistic Virtual Humans (VH) through modern techniques for animating the VH body and face, thereby affecting perception. From traditional methods, including blend shapes, to driven animations using facial and body tracking, these advancements can potentially enhance the perception of comfort and realism in relation to VHs. Previously, Psychology studied facial movements in humans, with some works separating expressions into macro and micro expressions. Also, some previous CG studies have analyzed how macro and micro expressions are perceived, replicating psychology studies in VHs, encompassing studies with realistic and cartoon VHs, and exploring different VH technologies. However, instead of using facial tracking animation methods, these previous studies animated the VHs using blendshapes interpolation. To understand how the facial tracking technique alters the perception of VHs, this paper extends the study to macro and micro expressions, employing two datasets to transfer real facial expressions to VHs and analyze how their expressions are perceived. Our findings suggest that transferring facial expressions from real actors to VHs significantly diminishes the accuracy of emotion perception compared to VH facial animations created by artists.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2408.07005.pdf' target='_blank'>https://arxiv.org/pdf/2408.07005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingju Liu, Hyeongwoo Kim, Gaurav Bharaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07005">Content and Style Aware Audio-Driven Facial Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven 3D facial animation has several virtual humans applications for content creation and editing. While several existing methods provide solutions for speech-driven animation, precise control over content (what) and style (how) of the final performance is still challenging. We propose a novel approach that takes as input an audio, and the corresponding text to extract temporally-aligned content and disentangled style representations, in order to provide controls over 3D facial animation. Our method is trained in two stages, that evolves from audio prominent styles (how it sounds) to visual prominent styles (how it looks). We leverage a high-resource audio dataset in stage I to learn styles that control speech generation in a self-supervised learning framework, and then fine-tune this model with low-resource audio/3D mesh pairs in stage II to control 3D vertex generation. We employ a non-autoregressive seq2seq formulation to model sentence-level dependencies, and better mouth articulations. Our method provides flexibility that the style of a reference audio and the content of a source audio can be combined to enable audio style transfer. Similarly, the content can be modified, e.g. muting or swapping words, that enables style-preserving content editing.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2407.21047.pdf' target='_blank'>https://arxiv.org/pdf/2407.21047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akin Caliskan, Berkay Kicanaoglu, Hyeongwoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21047">PAV: Personalized Head Avatar from Unstructured Video Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose PAV, Personalized Head Avatar for the synthesis of human faces under arbitrary viewpoints and facial expressions. PAV introduces a method that learns a dynamic deformable neural radiance field (NeRF), in particular from a collection of monocular talking face videos of the same character under various appearance and shape changes. Unlike existing head NeRF methods that are limited to modeling such input videos on a per-appearance basis, our method allows for learning multi-appearance NeRFs, introducing appearance embedding for each input video via learnable latent neural features attached to the underlying geometry. Furthermore, the proposed appearance-conditioned density formulation facilitates the shape variation of the character, such as facial hair and soft tissues, in the radiance field prediction. To the best of our knowledge, our approach is the first dynamic deformable NeRF framework to model appearance and shape variations in a single unified network for multi-appearances of the same subject. We demonstrate experimentally that PAV outperforms the baseline method in terms of visual rendering quality in our quantitative and qualitative studies on various subjects.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2407.11998.pdf' target='_blank'>https://arxiv.org/pdf/2407.11998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Chen, Heng Wang, Sainan Sun, Zhiyuan Chen, Zhenkun Liu, Shuhua Cao, Li Yang, Minghui Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11998">Custom Cloth Creation and Virtual Try-on for Everyone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This demo showcases a simple tool that utilizes AIGC technology, enabling both professional designers and regular users to easily customize clothing for their digital avatars. Customization options include changing clothing colors, textures, logos, and patterns. Compared with traditional 3D modeling processes, our approach significantly enhances efficiency and interactivity and reduces production costs.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2406.15889.pdf' target='_blank'>https://arxiv.org/pdf/2406.15889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyu Chen, Marko Peljhan, Misha Sra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15889">ConnectVR: A Trigger-Action Interface for Creating Agent-based Interactive VR Stories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The demand for interactive narratives is growing with increasing popularity of VR and video gaming. This presents an opportunity to create interactive storytelling experiences that allow players to engage with a narrative from a first person perspective, both, immersively in VR and in 3D on a computer. However, for artists and storytellers without programming experience, authoring such experiences is a particularly complex task as it involves coding a series of story events (character animation, movements, time control, dialogues, etc.) to be connected and triggered by a variety of player behaviors. In this work, we present ConnectVR, a trigger-action interface to enable non-technical creators design agent-based narrative experiences. Our no-code authoring method specifically focuses on the design of narratives driven by a series of cause-effect relationships triggered by the player's actions. We asked 15 participants to use ConnectVR in a preliminary workshop study as well as two artists to extensively use our system to create VR narrative projects in a three-week in-depth study. Our findings shed light on the creative opportunities facilitated by ConnectVR's trigger-action approach, particularly its capability to establish chained behavioral effects between virtual characters and objects. The results of both studies underscore the positive feedback from participants regarding our system's capacity to not only support creativity but also to simplify the creation of interactive narrative experiences. Results indicate compatibility with non-technical narrative creator's workflows, showcasing its potential to enhance the overall creative process in the realm of VR narrative design.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2406.15074.pdf' target='_blank'>https://arxiv.org/pdf/2406.15074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suvadeep Mukherjee, Verena Distler, Gabriele Lenzini, Pedro Cardoso-Leite
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15074">Balancing The Perception of Cheating Detection, Privacy and Fairness: A Mixed-Methods Study of Visual Data Obfuscation in Remote Proctoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote proctoring technology, a cheating-preventive measure, often raises privacy and fairness concerns that may affect test-takers' experiences and the validity of test results. Our study explores how selectively obfuscating information in video recordings can protect test-takers' privacy while ensuring effective and fair cheating detection. Interviews with experts (N=9) identified four key video regions indicative of potential cheating behaviors: the test-taker's face, body, background and the presence of individuals in the background. Experts recommended specific obfuscation methods for each region based on privacy significance and cheating behavior frequency, ranging from conventional blurring to advanced methods like replacement with deepfake, 3D avatars and silhouetting. We then conducted a vignette experiment with potential test-takers (N=259, non-experts) to evaluate their perceptions of cheating detection, visual privacy and fairness, using descriptions and examples of still images for each expert-recommended combination of video regions and obfuscation methods. Our results indicate that the effectiveness of obfuscation methods varies by region. Tailoring remote proctoring with region-specific advanced obfuscation methods can improve the perceptions of privacy and fairness compared to the conventional methods, though it may decrease perceived information sufficiency for detecting cheating. However, non-experts preferred conventional blurring for videos they were more willing to share, highlighting a gap between the perceived effectiveness of the advanced obfuscation methods and their practical acceptance. This study contributes to the field of user-centered privacy by suggesting promising directions to address current remote proctoring challenges and guiding future research.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2406.13093.pdf' target='_blank'>https://arxiv.org/pdf/2406.13093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wuxinlin Cheng, Cheng Wan, Yupeng Cao, Sihan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13093">RITA: A Real-time Interactive Talking Avatars Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RITA presents a high-quality real-time interactive framework built upon generative models, designed with practical applications in mind. Our framework enables the transformation of user-uploaded photos into digital avatars that can engage in real-time dialogue interactions. By leveraging the latest advancements in generative modeling, we have developed a versatile platform that not only enhances the user experience through dynamic conversational avatars but also opens new avenues for applications in virtual reality, online education, and interactive gaming. This work showcases the potential of integrating computer vision and natural language processing technologies to create immersive and interactive digital personas, pushing the boundaries of how we interact with digital content.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2405.13336.pdf' target='_blank'>https://arxiv.org/pdf/2405.13336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingrong Cheng, Xu Li, Xinghui Fu, Fei Xia, Zhongqian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13336">SIGGesture: Generalized Co-Speech Gesture Synthesis via Semantic Injection with Large-Scale Pre-Training Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automated synthesis of high-quality 3D gestures from speech is of significant value in virtual humans and gaming. Previous methods focus on synthesizing gestures that are synchronized with speech rhythm, yet they frequently overlook the inclusion of semantic gestures. These are sparse and follow a long-tailed distribution across the gesture sequence, making them difficult to learn in an end-to-end manner. Moreover, generating gestures, rhythmically aligned with speech, faces a significant issue that cannot be generalized to in-the-wild speeches. To address these issues, we introduce SIGGesture, a novel diffusion-based approach for synthesizing realistic gestures that are of both high quality and semantically pertinent. Specifically, we firstly build a strong diffusion-based foundation model for rhythmical gesture synthesis by pre-training it on a collected large-scale dataset with pseudo labels. Secondly, we leverage the powerful generalization capabilities of Large Language Models (LLMs) to generate proper semantic gestures for the various speech content. Finally, we propose a semantic injection module to infuse semantic information into the synthesized results during diffusion reverse process. Extensive experiments demonstrate that the proposed SIGGesture significantly outperforms existing baselines and shows excellent generalization and controllability.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2405.13042.pdf' target='_blank'>https://arxiv.org/pdf/2405.13042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Qian Zhou, David Ledo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13042">StoryVerse: Towards Co-authoring Dynamic Plot with LLM-based Character Simulation via Narrative Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated plot generation for games enhances the player's experience by providing rich and immersive narrative experience that adapts to the player's actions. Traditional approaches adopt a symbolic narrative planning method which limits the scale and complexity of the generated plot by requiring extensive knowledge engineering work. Recent advancements use Large Language Models (LLMs) to drive the behavior of virtual characters, allowing plots to emerge from interactions between characters and their environments. However, the emergent nature of such decentralized plot generation makes it difficult for authors to direct plot progression. We propose a novel plot creation workflow that mediates between a writer's authorial intent and the emergent behaviors from LLM-driven character simulation, through a novel authorial structure called "abstract acts". The writers define high-level plot outlines that are later transformed into concrete character action sequences via an LLM-based narrative planning process, based on the game world state. The process creates "living stories" that dynamically adapt to various game world states, resulting in narratives co-created by the author, character simulation, and player. We present StoryVerse as a proof-of-concept system to demonstrate this plot creation workflow. We showcase the versatility of our approach with examples in different stories and game environments.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2405.02672.pdf' target='_blank'>https://arxiv.org/pdf/2405.02672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Kuffner dos Anjos, JoÃ£o Madeiras Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02672">Effects of Realism and Representation on Self-Embodied Avatars in Immersive Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) has recently gained traction with many new and ever more affordable devices being released. The increase in popularity of this paradigm of interaction has given birth to new applications and has attracted casual consumers to experience VR. Providing a self-embodied representation (avatar) of users' full bodies inside shared virtual spaces can improve the VR experience and make it more engaging to both new and experienced users . This is especially important in fully immersive systems, where the equipment completely occludes the real world making self awareness problematic. Indeed, the feeling of presence of the user is highly influenced by their virtual representations, even though small flaws could lead to uncanny valley side-effects. Following previous research, we would like to assess whether using a third-person perspective could also benefit the VR experience, via an improved spatial awareness of the user's virtual surroundings. In this paper we investigate realism and perspective of self-embodied representation in VR setups in natural tasks, such as walking and avoiding obstacles. We compare both First and Third-Person perspectives with three different levels of realism in avatar representation. These range from a stylized abstract avatar, to a "realistic" mesh-based humanoid representation and a point-cloud rendering. The latter uses data captured via depth-sensors and mapped into a virtual self inside the Virtual Environment. We present a throughout evaluation and comparison of these different representations, describing a series of guidelines for self-embodied VR applications. The effects of the uncanny valley are also discussed in the context of navigation and reflex-based tasks.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2402.13027.pdf' target='_blank'>https://arxiv.org/pdf/2402.13027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kourosh Parand, Saeed Setayeshi, Mir Mohsen Pedram, Ali Yoonesi, Aida Pakniyat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13027">Solving the decision-making differential equations from eye fixation data in Unity software by using Hermite Long-Short-Term Memory neural network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cognitive decision-making processes are crucial aspects of human behavior, influencing various personal and professional domains. This research delves into the application of differential equations in analyzing decision-making accuracy by leveraging eye-tracking data within a virtual industrial town setting. The study unveils a systematic approach to transforming raw data into a differential equation, essential for deciphering the relationship between eye movements during decision-making processes.
  Mathematical relationship extraction and variable-parameter definition pave the way for deriving a differential equation that encapsulates the growth of fixations on characters. The key factors in this equation encompass the fixation rate $(Î»)$ and separation rate $(Î¼)$, reflecting user interaction dynamics and their impact on decision-making complexities tied to user engagement with virtual characters.
  For a comprehensive grasp of decision dynamics, solving this differential equation requires initial fixation counts, fixation rate, and separation rate. The formulation of differential equations incorporates various considerations such as engagement duration, character-player distance, relative speed, and character attributes, enabling the representation of fixation changes, speed dynamics, distance variations, and the effects of character attributes.
  This comprehensive analysis not only enhances our comprehension of decision-making processes but also provides a foundational framework for predictive modeling and data-driven insights for future research and applications in cognitive science and virtual reality environments.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2312.12634.pdf' target='_blank'>https://arxiv.org/pdf/2312.12634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Payam Jome Yazdian, Rachel Lagasse, Hamid Mohammadi, Eric Liu, Li Cheng, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12634">MotionScript: Natural Language Descriptions for Expressive 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotionScript, a novel framework for generating highly detailed, natural language descriptions of 3D human motions. Unlike existing motion datasets that rely on broad action labels or generic captions, MotionScript provides fine-grained, structured descriptions that capture the full complexity of human movement including expressive actions (e.g., emotions, stylistic walking) and interactions beyond standard motion capture datasets. MotionScript serves as both a descriptive tool and a training resource for text-to-motion models, enabling the synthesis of highly realistic and diverse human motions from text. By augmenting motion datasets with MotionScript captions, we demonstrate significant improvements in out-of-distribution motion generation, allowing large language models (LLMs) to generate motions that extend beyond existing data. Additionally, MotionScript opens new applications in animation, virtual human simulation, and robotics, providing an interpretable bridge between intuitive descriptions and motion synthesis. To the best of our knowledge, this is the first attempt to systematically translate 3D motion into structured natural language without requiring training data.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2311.12707.pdf' target='_blank'>https://arxiv.org/pdf/2311.12707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hye Sun Yun, Mehdi Arjmand, Phillip Sherlock, Michael K. Paasche-Orlow, James W. Griffith, Timothy Bickmore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12707">Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standardized, validated questionnaires are vital tools in research and healthcare, offering dependable self-report data. Prior work has revealed that virtual agent-administered questionnaires are almost equivalent to self-administered ones in an electronic form. Despite being an engaging method, repeated use of virtual agent-administered questionnaires in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose using large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants interacted with our agent system and responded daily for two weeks to one of the following questionnaires: a standardized depression questionnaire, question variants generated by LLMs, or question variants accompanied by LLM-generated small talk. The responses were compared to a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found that the variants were significantly less repetitive than repeated administrations of the same standardized questionnaire. Our findings highlight the potential of LLM-generated variants to invigorate agent-administered questionnaires and foster engagement and interest, without compromising their validity.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2310.13482.pdf' target='_blank'>https://arxiv.org/pdf/2310.13482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyan Yu, Shihuan Wang, Dong zhang, Yingying Zhang, Chaoqun Cen, Zhixiang you, Xiaobing zou, Hongzhu Deng, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13482">HSVRS: A Virtual Reality System of the Hide-and-Seek Game to Enhance Gaze Fixation Ability for Autistic Children</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous children diagnosed with Autism Spectrum Disorder (ASD) exhibit abnormal eye gaze pattern in communication and social interaction. Due to the high cost of ASD interventions and a shortage of professional therapists, researchers have explored the use of virtual reality (VR) systems as a supplementary intervention for autistic children. This paper presents the design of a novel VR-based system called the Hide and Seek Virtual Reality System (HSVRS). The HSVRS allows children with ASD to enhance their ocular gaze abilities while engaging in a hide-and-seek game with a virtual avatar. By employing face and voice manipulation technology, the HSVRS provides the option to customize the appearance and voice of the avatar, making it resemble someone familiar to the child, such as their parents. We conducted a pilot study at the Third Affiliated Hospital of Sun Yat-sen University, China, to evaluate the feasibility of HSVRS as an auxiliary intervention for children with autism (N=24). Through the analysis of subjective questionnaires completed by the participants' parents and objective eye gaze data, we observed that children in the VR-assisted intervention group demonstrated better performance compared to those in the control group. Furthermore, our findings indicate that the utilization of face and voice manipulation techniques to personalize avatars in hide-and-seek games can enhance the efficiency and effectiveness of the system.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2310.04731.pdf' target='_blank'>https://arxiv.org/pdf/2310.04731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katie Seaborn, Katja Rogers, Somang Name, Miu Kojima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04731">Kawaii Game Vocalics: A Preliminary Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kawaii is the Japanese concept of cute++, a global export with local characteristics. Recent work has explored kawaii as a feature of user experience (UX) with social robots, virtual characters, and voice assistants, i.e., kawaii vocalics. Games have a long history of incorporating characters that use voice as a means of expressing kawaii. Nevertheless, no work to date has evaluated kawaii game voices or mapped out a model of kawaii game vocalics. In this work, we explored whether and how a model of kawaii vocalics maps onto game character voices. We conducted an online perceptions study (N=157) using 18 voices from kawaii characters in Japanese games. We replicated the results for computer voice and discovered nuanced relationships between gender and age, especially youthfulness, agelessness, gender ambiguity, and gender neutrality. We provide our initial model and advocate for future work on character visuals and within play contexts.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2308.10385.pdf' target='_blank'>https://arxiv.org/pdf/2308.10385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanseob Kim, Bin Han, Jieun Kim, Muhammad Firdaus Syawaludin, Gerard Jounghyun Kim, Jae-In Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10385">Engaged and Affective Virtual Agents: Their Impact on Social Presence, Trustworthiness, and Decision-Making in the Group Discussion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates how different virtual agent (VA) behaviors influence subjects' perceptions and group decision-making. Participants carried out experimental group discussions with a VA exhibiting varying levels of engagement and affective behavior. Engagement refers to the VA's focus on the group task, whereas affective behavior reflects the VA's emotional state. The findings revealed that VA's engagements effectively captured participants' attention even in the group setting and enhanced group synergy, thereby facilitating more in-depth discussion and producing better consensus. On the other hand, VA's affective behavior negatively affected the perceived social presence and trustworthiness. Consequently, in the context of group discussion, participants preferred the engaged and non-affective VA to the non-engaged and affective VA. The study provides valuable insights for improving the VA's behavioral design as a team member for collaborative tasks.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2308.04868.pdf' target='_blank'>https://arxiv.org/pdf/2308.04868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Canela, Pol Caselles, Ibrar Malik, Eduard Ramon, Jaime GarcÃ­a, Jordi SÃ¡nchez-Riera, Gil Triginer, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04868">InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in full-head reconstruction have been obtained by optimizing a neural field through differentiable surface or volume rendering to represent a single scene. While these techniques achieve an unprecedented accuracy, they take several minutes, or even hours, due to the expensive optimization process required. In this work, we introduce InstantAvatar, a method that recovers full-head avatars from few images (down to just one) in a few seconds on commodity hardware. In order to speed up the reconstruction process, we propose a system that combines, for the first time, a voxel-grid neural field representation with a surface renderer. Notably, a naive combination of these two techniques leads to unstable optimizations that do not converge to valid solutions. In order to overcome this limitation, we present a novel statistical model that learns a prior distribution over 3D head signed distance functions using a voxel-grid based architecture. The use of this prior model, in combination with other design choices, results into a system that achieves 3D head reconstructions with comparable accuracy as the state-of-the-art with a 100x speed-up.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2306.10656.pdf' target='_blank'>https://arxiv.org/pdf/2306.10656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kenta Oono, Nontawat Charoenphakdee, Kotatsu Bito, Zhengyan Gao, Hideyoshi Igata, Masashi Yoshikawa, Yoshiaki Ota, Hiroki Okui, Kei Akita, Shoichiro Yamaguchi, Yohei Sugawara, Shin-ichi Maeda, Kunihiko Miyoshi, Yuki Saito, Koki Tsuda, Hiroshi Maruyama, Kohei Hayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10656">Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying the relationship between healthcare attributes, lifestyles, and personality is vital for understanding and improving physical and mental well-being. Machine learning approaches are promising for modeling their relationships and offering actionable suggestions. In this paper, we propose the Virtual Human Generative Model (VHGM), a novel deep generative model capable of estimating over 2,000 attributes across healthcare, lifestyle, and personality domains. VHGM leverages masked modeling to learn the joint distribution of attributes, enabling accurate predictions and robust conditional sampling. We deploy VHGM as a web service, showcasing its versatility in driving diverse healthcare applications aimed at improving user well-being. Through extensive quantitative evaluations, we demonstrate VHGM's superior performance in attribute imputation and high-quality sample generation compared to existing baselines. This work highlights VHGM as a powerful tool for personalized healthcare and lifestyle management, with broad implications for data-driven health solutions.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2306.10350.pdf' target='_blank'>https://arxiv.org/pdf/2306.10350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Xiang Zhou, Yukang Cao, Wensen Feng, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10350">MA-NeRF: Motion-Assisted Neural Radiance Fields for Face Synthesis from Sparse Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of photorealistic 3D face avatar synthesis from sparse images. Existing Parametric models for face avatar reconstruction struggle to generate details that originate from inputs. Meanwhile, although current NeRF-based avatar methods provide promising results for novel view synthesis, they fail to generalize well for unseen expressions. We improve from NeRF and propose a novel framework that, by leveraging the parametric 3DMM models, can reconstruct a high-fidelity drivable face avatar and successfully handle the unseen expressions. At the core of our implementation are structured displacement feature and semantic-aware learning module. Our structured displacement feature will introduce the motion prior as an additional constraints and help perform better for unseen expressions, by constructing displacement volume. Besides, the semantic-aware learning incorporates multi-level prior, e.g., semantic embedding, learnable latent code, to lift the performance to a higher level. Thorough experiments have been doen both quantitatively and qualitatively to demonstrate the design of our framework, and our method achieves much better results than the current state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2304.04897.pdf' target='_blank'>https://arxiv.org/pdf/2304.04897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04897">Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method that enables synthesizing novel views and novel poses of arbitrary human performers from sparse multi-view images. A key ingredient of our method is a hybrid appearance blending module that combines the advantages of the implicit body NeRF representation and image-based rendering. Existing generalizable human NeRF methods that are conditioned on the body model have shown robustness against the geometric variation of arbitrary human performers. Yet they often exhibit blurry results when generalized onto unseen identities. Meanwhile, image-based rendering shows high-quality results when sufficient observations are available, whereas it suffers artifacts in sparse-view settings. We propose Neural Image-based Avatars (NIA) that exploits the best of those two methods: to maintain robustness under new articulations and self-occlusions while directly leveraging the available (sparse) source view colors to preserve appearance details of new subject identities. Our hybrid design outperforms recent methods on both in-domain identity generalization as well as challenging cross-dataset generalization settings. Also, in terms of the pose generalization, our method outperforms even the per-subject optimized animatable NeRF methods. The video results are available at https://youngjoongunc.github.io/nia
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2303.09375.pdf' target='_blank'>https://arxiv.org/pdf/2303.09375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Svitov, Dmitrii Gudkov, Renat Bashirov, Victor Lempitsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09375">DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DINAR, an approach for creating realistic rigged fullbody avatars from single RGB images. Similarly to previous works, our method uses neural textures combined with the SMPL-X body model to achieve photo-realistic quality of avatars while keeping them easy to animate and fast to infer. To restore the texture, we use a latent diffusion model and show how such model can be trained in the neural texture space. The use of the diffusion model allows us to realistically reconstruct large unseen regions such as the back of a person given the frontal view. The models in our pipeline are trained using 2D images and videos only. In the experiments, our approach achieves state-of-the-art rendering quality and good generalization to new poses and viewpoints. In particular, the approach improves state-of-the-art on the SnapshotPeople public benchmark.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2303.06981.pdf' target='_blank'>https://arxiv.org/pdf/2303.06981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, Anastasiia Ternova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06981">CAstelet in Virtual reality for shadOw AVatars (CAVOAV)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>After an overview of the use of digital shadows in computing science research projects with cultural and social impacts and a focus on recent researches and insights on virtual theaters, this paper introduces a research mixing the manipulation of shadow avatars and the building of a virtual theater setup inspired by traditional shadow theater (or ``castelet'' in french) in a mixed reality environment. It describes the virtual 3D setup, the nature of the shadow avatars and the issues of directing believable interactions between virtual avatars and physical performers on stage. Two modalities of shadow avatars direction are exposed. Some results of the research are illustrated in two use cases: the development of theatrical creativity in mixed reality through pedagogical workshops; and an artistic achievement in ''The Shadow'' performance, after H. C. Andersen.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2301.09702.pdf' target='_blank'>https://arxiv.org/pdf/2301.09702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Guo, Amy R. Reibman, Edward J. Delp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09702">Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2207.07621.pdf' target='_blank'>https://arxiv.org/pdf/2207.07621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, Egor Zakharov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.07621">MegaPortraits: One-shot Megapixel Neural Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we advance the neural head avatar technology to the megapixel resolution while focusing on the particularly challenging task of cross-driving synthesis, i.e., when the appearance of the driving image is substantially different from the animated source image. We propose a set of new neural architectures and training methods that can leverage both medium-resolution video data and high-resolution image data to achieve the desired levels of rendered image quality and generalization to novel views and motion. We demonstrate that suggested architectures and methods produce convincing high-resolution neural avatars, outperforming the competitors in the cross-driving scenario. Lastly, we show how a trained high-resolution neural avatar model can be distilled into a lightweight student model which runs in real-time and locks the identities of neural avatars to several dozens of pre-defined source images. Real-time operation and identity lock are essential for many practical applications head avatar systems.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2509.15372.pdf' target='_blank'>https://arxiv.org/pdf/2509.15372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yudong Huang, Avneet Singh, Mark Roman Miller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15372">Experience Level Influences User's Criteria for Avatar Animation Realism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The sense of realism in avatar animation is a widely pursued goal in social VR applications. A common approach to enhancing realism is improving the match between avatar motion and real-world human movement. However, experience with existing VR platforms may reshape users' expectations, suggesting that matching reality is not the only path to enhancing the sense of realism. This study examines how different levels of experience with a social VR platform influence users' criteria for evaluating the realism of avatar animation. Participants were shown a set of animations varying in the degree they reflected real-world motion and motion seen on the social VR platform VRChat. Results showed that users with no VRChat experience found animations recorded on VRChat unnatural and unrealistic, but experienced users in fact rated these animations as more likely to come from a real person than the motion-capture animations. Additionally, highly experienced users recognized the intent to imitate VRChat's style and noted the differences from genuine in-platform animations. All these results suggest users' expectations of and criteria for realistic animation were shaped by their experience level. The findings support the idea that realism in avatar animation does not solely depend on mimicking real-world movement. Experience with VR platforms can shape how users expect, perceive, and evaluate animation realism. This insight can inform the design of more immersive VR environments and virtual humans in the future.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2509.14132.pdf' target='_blank'>https://arxiv.org/pdf/2509.14132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julia S. Dollis, Iago A. Brito, Fernanda B. Färber, Pedro S. F. B. Ribeiro, Rafael T. Sousa, Arlindo R. Galvão Filho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14132">When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While virtual reality (VR) excels at simulating physical environments, its effectiveness for training complex interpersonal skills is limited by a lack of psychologically plausible virtual humans. This is a critical gap in high-stakes domains like medical education, where communication is a core competency. This paper introduces a framework that integrates large language models (LLMs) into immersive VR to create medically coherent virtual patients with distinct, consistent personalities, built on a modular architecture that decouples personality from clinical data. We evaluated our system in a mixed-method, within-subjects study with licensed physicians who engaged in simulated consultations. Results demonstrate that the approach is not only feasible but is also perceived by physicians as a highly rewarding and effective training enhancement. Furthermore, our analysis uncovers critical design principles, including a ``realism-verbosity paradox" where less communicative agents can seem more artificial, and the need for challenges to be perceived as authentic to be instructive. This work provides a validated framework and key insights for developing the next generation of socially intelligent VR training environments.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2509.12525.pdf' target='_blank'>https://arxiv.org/pdf/2509.12525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>T. James Brandt, Cecilia Xi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12525">The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI powers a growing wave of companion chatbots, yet principles for fostering genuine connection remain unsettled. We test two routes: visible user authorship versus covert language-style mimicry. In a preregistered 3x2 experiment (N = 162), we manipulated user-controlled avatar generation (none, premade, user-generated) and Language Style Matching (LSM) (static vs. adaptive). Generating an avatar boosted rapport ($ω^2$ = .040, p = .013), whereas adaptive LSM underperformed static style on personalization and satisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t = 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony erodes connection when perceived as incoherent, destabilizing persona. To explain, we propose a stability-and-legibility account: visible authorship fosters natural interaction, while covert mimicry risks incoherence. Our findings suggest designers should prioritize legible, user-driven personalization and limit stylistic shifts rather than rely on opaque mimicry.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2508.15500.pdf' target='_blank'>https://arxiv.org/pdf/2508.15500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fulden Ece UÄur, Rafael Redondo, Albert Barreiro, Stefan Hristov, Roger MarÃ­
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15500">MExECON: Multi-view Extended Explicit Clothed humans Optimized via Normal integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents MExECON, a novel pipeline for 3D reconstruction of clothed human avatars from sparse multi-view RGB images. Building on the single-view method ECON, MExECON extends its capabilities to leverage multiple viewpoints, improving geometry and body pose estimation. At the core of the pipeline is the proposed Joint Multi-view Body Optimization (JMBO) algorithm, which fits a single SMPL-X body model jointly across all input views, enforcing multi-view consistency. The optimized body model serves as a low-frequency prior that guides the subsequent surface reconstruction, where geometric details are added via normal map integration. MExECON integrates normal maps from both front and back views to accurately capture fine-grained surface details such as clothing folds and hairstyles. All multi-view gains are achieved without requiring any network re-training. Experimental results show that MExECON consistently improves fidelity over the single-view baseline and achieves competitive performance compared to modern few-shot 3D reconstruction methods.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2508.10586.pdf' target='_blank'>https://arxiv.org/pdf/2508.10586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Birgit Nierula, Mustafa Tevfik Lafci, Anna Melnik, Mert AkgÃ¼l, Farelle Toumaleu Siewe, Sebastian Bosse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10586">Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proxemics, the study of spatial behavior, is fundamental to social interaction and increasingly relevant for virtual reality (VR) applications. While previous research has established that users respond to personal space violations in VR similarly as in real-world settings, phase-specific physiological responses and the modulating effects of facial expressions remain understudied. We investigated physiological and subjective responses to personal space violations by virtual avatars, to understand how threatening facial expressions and interaction phases (approach vs. standing) influence these responses. Sixteen participants experienced a 2x2 factorial design manipulating Personal Space (intrusion vs. respect) and Facial Expression (neutral vs. angry) while we recorded skin conductance response (SCR), heart rate variability (HRV), and discomfort ratings. Personal space boundaries were individually calibrated using a stop-distance procedure. Results show that SCR responses are significantly higher during the standing phase compared to the approach phase when personal space was violated, indicating that prolonged proximity within personal space boundaries is more physiologically arousing than the approach itself. Angry facial expressions significantly reduced HRV, reflecting decreased parasympathetic activity, and increased discomfort ratings, but did not amplify SCR responses. These findings demonstrate that different physiological modalities capture distinct aspects of proxemic responses: SCR primarily reflects spatial boundary violations, while HRV responds to facial threat cues. Our results provide insights for developing comprehensive multi-modal assessments of social behavior in virtual environments and inform the design of more realistic avatar interactions.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2508.09973.pdf' target='_blank'>https://arxiv.org/pdf/2508.09973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geonhee Sim, Gyeongsik Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09973">PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two major approaches exist for creating animatable human avatars. The first, a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a single person, achieving personalization through a disentangled identity representation. However, modeling pose-driven deformations, such as non-rigid cloth deformations, requires numerous pose-rich videos, which are costly and impractical to capture in daily life. The second, a diffusion-based approach, learns pose-driven deformations from large-scale in-the-wild videos but struggles with identity preservation and pose-dependent identity entanglement. We present PERSONA, a framework that combines the strengths of both approaches to obtain a personalized 3D human avatar with pose-driven deformations from a single image. PERSONA leverages a diffusion-based approach to generate pose-rich videos from the input image and optimizes a 3D avatar based on them. To ensure high authenticity and sharp renderings across diverse poses, we introduce balanced sampling and geometry-weighted optimization. Balanced sampling oversamples the input image to mitigate identity shifts in diffusion-generated training videos. Geometry-weighted optimization prioritizes geometry constraints over image loss, preserving rendering quality in diverse poses.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2508.04904.pdf' target='_blank'>https://arxiv.org/pdf/2508.04904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Hu, Qiwen Xiong, Zhenzhen Qin, Brandon Watanabe, Yujing Wang, Mirjana Prpa, Ilmi Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04904">Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Root Cause Analysis (RCA) is a critical tool for investigating adverse events in healthcare and improving patient safety. However, existing RCA training programs are often limited by high resource demands, leading to insufficient training and inconsistent implementation. To address this challenge, we present an AI-powered 3D simulation game that helps healthcare professionals develop RCA skills through interactive, immersive simulations. This approach offers a cost-effective, scalable, and accessible alternative to traditional training. The prototype simulates an RCA investigation following a death in the ICU, where learners interview five virtual avatars representing ICU team members to investigate the incident and complete a written report. The system enables natural, life-like interactions with avatars via large language models (LLMs), emotional text-to-speech, and AI-powered animations. An additional LLM component provides formative and summative feedback to support continual improvement. We conclude by outlining plans to empirically evaluate the system's efficacy.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2508.01743.pdf' target='_blank'>https://arxiv.org/pdf/2508.01743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyao Zhang, Omar Faruk, Robert Porzel, Dennis KÃ¼ster, Tanja Schultz, Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01743">Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An increasing number of online interaction settings now provide the possibility to visually represent oneself via an animated avatar instead of a video stream. Benefits include protecting the communicator's privacy while still providing a means to express their individuality. In consequence, there has been a surge in means for avatar-based personalization, ranging from classic human representations to animals, food items, and more. However, using avatars also has drawbacks. Depending on the human-likeness of the avatar and the corresponding disparities between the avatar and the original expresser, avatars may elicit discomfort or even hinder effective nonverbal communication by distorting emotion perception. This study examines the relationship between the human-likeness of virtual avatars and emotion perception for Ekman's six "basic emotions". Research reveals that avatars with varying degrees of human-likeness have distinct effects on emotion perception. High human-likeness avatars, such as human avatars, tend to elicit more negative emotional responses from users, a phenomenon that is consistent with the concept of Uncanny Valley in aesthetics, which suggests that closely resembling humans can provoke negative emotional responses. Conversely, a raccoon avatar and a shark avatar, known as cuteness, which exhibit moderate human similarity in this study, demonstrate a positive influence on emotion perception. Our initial results suggest that the human-likeness of avatars is an important factor for emotion perception. The results from the follow-up study further suggest that the cuteness of avatars and their natural facial status may also play a significant role in emotion perception and elicitation. We discuss practical implications for strategically conveying specific human behavioral messages through avatars in multiple applications, such as business and counseling.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2508.01381.pdf' target='_blank'>https://arxiv.org/pdf/2508.01381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Onat Vuran, Hsuan-I Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01381">ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reconstruction of multi-layer 3D garments typically requires expensive multi-view capture setups and specialized 3D editing efforts. To support the creation of life-like clothed human avatars, we introduce ReMu for reconstructing multi-layer clothed humans in a new setup, Image Layers, which captures a subject wearing different layers of clothing with a single RGB camera. To reconstruct physically plausible multi-layer 3D garments, a unified 3D representation is necessary to model these garments in a layered manner. Thus, we first reconstruct and align each garment layer in a shared coordinate system defined by the canonical body pose. Afterwards, we introduce a collision-aware optimization process to address interpenetration and further refine the garment boundaries leveraging implicit neural fields. It is worth noting that our method is template-free and category-agnostic, which enables the reconstruction of 3D garments in diverse clothing styles. Through our experiments, we show that our method reconstructs nearly penetration-free 3D clothed humans and achieves competitive performance compared to category-specific methods. Project page: https://eth-ait.github.io/ReMu/
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2507.15072.pdf' target='_blank'>https://arxiv.org/pdf/2507.15072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maisha Maimuna, Minhaz Bin Farukee, Sama Nikanfar, Mahfuza Siddiqua, Ayon Roy, Fillia Makedon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15072">NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial warehouses are congested with moving forklifts, shelves and personnel, making robot teleoperation particularly risky and demanding for blind and low-vision (BLV) operators. Although accessible teleoperation plays a key role in inclusive workforce participation, systematic research on its use in industrial environments is limited, and few existing studies barely address multimodal guidance designed for BLV users. We present a novel multimodal guidance simulator that enables BLV users to control a mobile robot through a high-fidelity warehouse environment while simultaneously receiving synchronized visual, auditory, and haptic feedback. The system combines a navigation mesh with regular re-planning so routes remain accurate avoiding collisions as forklifts and human avatars move around the warehouse. Users with low vision are guided with a visible path line towards destination; navigational voice cues with clockwise directions announce upcoming turns, and finally proximity-based haptic feedback notifies the users of static and moving obstacles in the path. This real-time, closed-loop system offers a repeatable testbed and algorithmic reference for accessible teleoperation research. The simulator's design principles can be easily adapted to real robots due to the alignment of its navigation, speech, and haptic modules with commercial hardware, supporting rapid feasibility studies and deployment of inclusive telerobotic tools in actual warehouses.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2507.09230.pdf' target='_blank'>https://arxiv.org/pdf/2507.09230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>G. Kutay TÃ¼rkoglu, Julian Tanke, Iheb Belgacem, Lev Markhasin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09230">EgoAnimate: Generating Human Animations from Egocentric top-down Views</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An ideal digital telepresence experience requires accurate replication of a person's body, clothing, and movements. To capture and transfer these movements into virtual reality, the egocentric (first-person) perspective can be adopted, which enables the use of a portable and cost-effective device without front-view cameras. However, this viewpoint introduces challenges such as occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views, and none use a generative prior-based approach. Some methods create avatars from a single egocentric image during inference, but still rely on multi-view datasets during training. To our knowledge, this is the first study using a generative backbone to reconstruct animatable avatars from egocentric inputs. Based on Stable Diffusion, our method reduces training burden and improves generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree reconstruction from a frontal image, we introduce a pipeline that generates realistic frontal views from occluded top-down images using ControlNet and a Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic frontal representation and feed it into an image-to-motion model. This enables generation of avatar motions from minimal input, paving the way for more accessible and generalizable telepresence systems.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2507.06060.pdf' target='_blank'>https://arxiv.org/pdf/2507.06060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Symeonidis-Herzig, Ãzge MercanoÄlu Sincan, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06060">VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2505.10072.pdf' target='_blank'>https://arxiv.org/pdf/2505.10072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui-Yang Ju, Sheng-Yen Huang, Yi-Ping Hung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10072">ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based method, has become widely used for facial image stylization. To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1 (stylized video generation), we adopt an improved StyleGAN to generate the stylized video from the input video frames, which overcomes the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN. This process provides a more stable stylized video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, facilitating the synthesis of high-quality animations in the next stage. In Stage 2 (Gaussian blendshapes synthesis), our method learns a stylized neutral head model and a set of expression blendshapes from the generated stylized video. By combining the neutral head model with expression blendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary expressions. We validate the effectiveness of ToonifyGB on benchmark datasets using two representative styles: Arcane and Pixar.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2505.01319.pdf' target='_blank'>https://arxiv.org/pdf/2505.01319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Pan, Karan Singh, Luiz Gustavo Hafemann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01319">Model See Model Do: Speech-Driven Facial Animation with Style Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D facial animation plays a key role in applications such as virtual avatars, gaming, and digital content creation. While existing methods have made significant progress in achieving accurate lip synchronization and generating basic emotional expressions, they often struggle to capture and effectively transfer nuanced performance styles. We propose a novel example-based generation framework that conditions a latent diffusion model on a reference style clip to produce highly expressive and temporally coherent facial animations. To address the challenge of accurately adhering to the style reference, we introduce a novel conditioning mechanism called style basis, which extracts key poses from the reference and additively guides the diffusion generation process to fit the style without compromising lip synchronization quality. This approach enables the model to capture subtle stylistic cues while ensuring that the generated animations align closely with the input speech. Extensive qualitative, quantitative, and perceptual evaluations demonstrate the effectiveness of our method in faithfully reproducing the desired style while achieving superior lip synchronization across various speech scenarios.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2504.20016.pdf' target='_blank'>https://arxiv.org/pdf/2504.20016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linshi Li, Hanlin Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20016">Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In child-centered design, directly engaging children is crucial for deeply understanding their experiences. However, current research often prioritizes adult perspectives, as interviewing children involves unique challenges such as environmental sensitivities and the need for trust-building. AI-powered virtual humans (VHs) offer a promising approach to facilitate engaging and multimodal interactions with children. This study establishes key design guidelines for LLM-powered virtual humans tailored to child interviews, standardizing multimodal elements including color schemes, voice characteristics, facial features, expressions, head movements, and gestures. Using ChatGPT-based prompt engineering, we developed three distinct Human-AI workflows (LLM-Auto, LLM-Interview, and LLM-Analyze) and conducted a user study involving 15 children aged 6 to 12. The results indicated that the LLM-Analyze workflow outperformed the others by eliciting longer responses, achieving higher user experience ratings, and promoting more effective child engagement.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2504.10010.pdf' target='_blank'>https://arxiv.org/pdf/2504.10010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Kocur, Niels Henze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10010">Investigating Environments' and Avatars' Effects on Thermal Perception in Virtual Reality to Reduce Energy Consumption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding thermal regulation and subjective perception of temperature is crucial for improving thermal comfort and human energy consumption in times of global warming. Previous work shows that an environment's color temperature affects the experienced temperature. As virtual reality (VR) enables visual immersion, recent work suggests that a VR scene's color temperature also affects experienced temperature. In addition, virtual avatars representing thermal cues influence users' thermal perception and even the body temperature. As immersive technology becomes increasingly prevalent in daily life, leveraging thermal cues to enhance thermal comfort - without relying on actual thermal energy - presents a promising opportunity. Understanding these effects is crucial for optimizing virtual experiences and promoting sustainable energy practices. Therefore, we propose three controlled experiments to learn more about thermal effects caused by virtual worlds and avatars.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2503.19334.pdf' target='_blank'>https://arxiv.org/pdf/2503.19334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ghazanfar Ali, Hong-Quan Le, Junho Kim, Seoung-won Hwang, Jae-In Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19334">Design of Seamless Multi-modal Interaction Framework for Intelligent Virtual Agents in Wearable Mixed Reality Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the design of a multimodal interaction framework for intelligent virtual agents in wearable mixed reality environments, especially for interactive applications at museums, botanical gardens, and similar places. These places need engaging and no-repetitive digital content delivery to maximize user involvement. An intelligent virtual agent is a promising mode for both purposes. Premises of framework is wearable mixed reality provided by MR devices supporting spatial mapping. We envisioned a seamless interaction framework by integrating potential features of spatial mapping, virtual character animations, speech recognition, gazing, domain-specific chatbot and object recognition to enhance virtual experiences and communication between users and virtual agents. By applying a modular approach and deploying computationally intensive modules on cloud-platform, we achieved a seamless virtual experience in a device with limited resources. Human-like gaze and speech interaction with a virtual agent made it more interactive. Automated mapping of body animations with the content of a speech made it more engaging. In our tests, the virtual agents responded within 2-4 seconds after the user query. The strength of the framework is flexibility and adaptability. It can be adapted to any wearable MR device supporting spatial mapping.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2503.15489.pdf' target='_blank'>https://arxiv.org/pdf/2503.15489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elvis Kimara, Kunle S. Oguntoye, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15489">PersonaAI: Leveraging Retrieval-Augmented Generation and Personalized Context for AI-Driven Digital Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces PersonaAI, a cutting-edge application that leverages Retrieval-Augmented Generation (RAG) and the LLAMA model to create highly personalized digital avatars capable of accurately mimicking individual personalities. Designed as a cloud-based mobile application, PersonaAI captures user data seamlessly, storing it in a secure database for retrieval and analysis. The result is a system that provides context-aware, accurate responses to user queries, enhancing the potential of AI-driven personalization.
  Why should you care? PersonaAI combines the scalability of RAG with the efficiency of prompt-engineered LLAMA3, offering a lightweight, sustainable alternative to traditional large language model (LLM) training methods. The system's novel approach to data collection, utilizing real-time user interactions via a mobile app, ensures enhanced context relevance while maintaining user privacy. By open-sourcing our implementation, we aim to foster adaptability and community-driven development.
  PersonaAI demonstrates how AI can transform interactions by merging efficiency, scalability, and personalization, making it a significant step forward in the future of digital avatars and personalized AI.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2503.15225.pdf' target='_blank'>https://arxiv.org/pdf/2503.15225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelo Di Porzio, Marco Coraggio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15225">A Personalized Data-Driven Generative Model of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of autonomous virtual avatars (in extended reality) and robots in human group activities - such as rehabilitation therapy, sports, and manufacturing - is expected to increase as these technologies become more pervasive. Designing cognitive architectures and control strategies to drive these agents requires realistic models of human motion. However, existing models only provide simplified descriptions of human motor behavior. In this work, we propose a fully data-driven approach, based on Long Short-Term Memory neural networks, to generate original motion that captures the unique characteristics of specific individuals. We validate the architecture using real data of scalar oscillatory motion. Extensive analyses show that our model effectively replicates the velocity distribution and amplitude envelopes of the individual it was trained on, remaining different from other individuals, and outperforming state-of-the-art models in terms of similarity to human data.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2503.14943.pdf' target='_blank'>https://arxiv.org/pdf/2503.14943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wang, Ivan Molodetskikh, Ondrej Texler, Dimitar Dinev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14943">3D Engine-ready Photorealistic Avatars via Dynamic Textures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the digital and physical worlds become more intertwined, there has been a lot of interest in digital avatars that closely resemble their real-world counterparts. Current digitization methods used in 3D production pipelines require costly capture setups, making them impractical for mass usage among common consumers. Recent academic literature has found success in reconstructing humans from limited data using implicit representations (e.g., voxels used in NeRFs), which are able to produce impressive videos. However, these methods are incompatible with traditional rendering pipelines, making it difficult to use them in applications such as games. In this work, we propose an end-to-end pipeline that builds explicitly-represented photorealistic 3D avatars using standard 3D assets. Our key idea is the use of dynamically-generated textures to enhance the realism and visually mask deficiencies in the underlying mesh geometry. This allows for seamless integration with current graphics pipelines while achieving comparable visual quality to state-of-the-art 3D avatar generation methods.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2503.14408.pdf' target='_blank'>https://arxiv.org/pdf/2503.14408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parisa Ghanad Torshizi, Laura B. Hensel, Ari Shapiro, Stacy C. Marsella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14408">Large Language Models for Virtual Human Gesture Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gestures convey a wide variety of meanings and play an important role in face-to-face human interactions. These gestures significantly influence the addressee's engagement, recall, comprehension, and attitudes toward the speaker. Similarly, they impact interactions between humans and embodied virtual agents. The process of selecting and animating meaningful gestures has thus become a key focus in the design of these agents. However, automating this gesture selection process poses a significant challenge. Prior gesture generation techniques have varied from fully automated, data-driven methods, which often struggle to produce contextually meaningful gestures, to more manual approaches that require crafting specific gesture expertise and are time-consuming and lack generalizability. In this paper, we leverage the semantic capabilities of Large Language Models to develop a gesture selection approach that suggests meaningful, appropriate co-speech gestures. We first describe how information on gestures is encoded into GPT-4. Then, we conduct a study to evaluate alternative prompting approaches for their ability to select meaningful, contextually relevant gestures and to align them appropriately with the co-speech utterance. Finally, we detail and demonstrate how this approach has been implemented within a virtual agent system, automating the selection and subsequent animation of the selected gestures for enhanced human-agent interactions.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2502.15956.pdf' target='_blank'>https://arxiv.org/pdf/2502.15956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canxuan Gang, Yiran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15956">Human Motion Prediction, Reconstruction, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2502.02468.pdf' target='_blank'>https://arxiv.org/pdf/2502.02468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akash Haridas, Imran N. Junejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02468">High-Fidelity Human Avatars from Laptop Webcams using Edge Compute</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applications of generating photo-realistic human avatars are many, however, high-fidelity avatar generation traditionally required expensive professional camera rigs and artistic labor, but recent research has enabled constructing them automatically from smartphones with RGB and IR sensors. However, these new methods still rely on the presence of high-resolution cameras on modern smartphones and often require offloading the processing to powerful servers with GPUs. Modern applications such as video conferencing call for the ability to generate these avatars from consumer-grade laptop webcams using limited compute available on-device. In this work, we develop a novel method based on 3D morphable models, landmark detection, photo-realistic texture GANs, and differentiable rendering to tackle the problem of low webcam image quality and edge computation. We build an automatic system to generate high-fidelity animatable avatars under these limitations, leveraging the neural compute capabilities of mobile chips.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2412.16151.pdf' target='_blank'>https://arxiv.org/pdf/2412.16151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bharat Vyas, Carol O'Sullivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16151">Shape Shifters: Does Body Shape Change the Perception of Small-Scale Crowd Motions?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The animation of realistic virtual avatars in crowd scenarios is an important element of immersive virtual environments. However, achieving this realism requires attention to multiple factors, such as their visual appearance and motion cues. We investigated how body shape diversity influences the perception of motion clones in virtual crowds. A physics-based model was used to simulate virtual avatars in a small-scale crowd of size twelve. Participants viewed side-by-side video clips of these virtual crowds: one featuring all unique motions (Baseline) and the other containing motion clones (i.e., the same motion used to animate two or more avatars in the crowd). We also varied the levels of body shape and motion diversity. Our findings revealed that body shape diversity did not influence participants' ratings of motion clone detection, and motion variety had a greater impact on their perception of the crowd. Further research is needed to investigate how other visual factors interact with motion in order to enhance the perception of virtual crowd realism.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2412.07912.pdf' target='_blank'>https://arxiv.org/pdf/2412.07912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bora Tarlan, Nisa Erdal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07912">How Can I Assist You Today?: A Comparative Analysis of a Humanoid Robot and a Virtual Human Avatar in Human Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores human perceptions of intelligent agents by comparing interactions with a humanoid robot and a virtual human avatar, both utilizing GPT-3 for response generation. The study aims to understand how physical and virtual embodiments influence perceptions of anthropomorphism, animacy, likeability, and perceived intelligence. The uncanny valley effect was also investigated in the scope of this study based on the two agents' human-likeness and affinity. Conducted with ten participants from Sabanci University, the experiment involved tasks that sought advice, followed by assessments using the Godspeed Questionnaire Series and structured interviews. Results revealed no significant difference in anthropomorphism between the humanoid robot and the virtual human avatar, but the humanoid robot was perceived as more likable and slightly more intelligent, highlighting the importance of physical presence and interactive gestures. These findings suggest that while virtual avatars can achieve high human-likeness, physical embodiment enhances likeability and perceived intelligence. However, the study's scope was insufficient to claim the existence of the uncanny valley effect in the participants' interactions. The study offers practical insights for designing future intelligent assistants, emphasizing the need for integrating physical elements and sophisticated communicative behaviors to improve user experience and acceptance.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2412.04433.pdf' target='_blank'>https://arxiv.org/pdf/2412.04433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shota Sasaki, Jane Wu, Ko Nishino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04433">PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel clothed human model that can be learned from multiview RGB videos, with a particular emphasis on recovering physically accurate body and cloth movements. Our method, Position Based Dynamic Gaussians (PBDyG), realizes ``movement-dependent'' cloth deformation via physical simulation, rather than merely relying on ``pose-dependent'' rigid transformations. We model the clothed human holistically but with two distinct physical entities in contact: clothing modeled as 3D Gaussians, which are attached to a skinned SMPL body that follows the movement of the person in the input videos. The articulation of the SMPL body also drives physically-based simulation of the clothes' Gaussians to transform the avatar to novel poses. In order to run position based dynamics simulation, physical properties including mass and material stiffness are estimated from the RGB videos through Dynamic 3D Gaussian Splatting. Experiments demonstrate that our method not only accurately reproduces appearance but also enables the reconstruction of avatars wearing highly deformable garments, such as skirts or coats, which have been challenging to reconstruct using existing methods.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2411.18949.pdf' target='_blank'>https://arxiv.org/pdf/2411.18949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Zhou, Ding Ding, Shengyu Wang, Chuhan Shi, Xiangyu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18949">Study on the Influence of Embodied Avatars on Gait Parameters in Virtual Environments and Real World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we compare the virtual and real gait parameters to investigate the effect of appearances of embodied avatars and virtual reality experience on gait in physical and virtual environments. We developed a virtual environment simulation and gait detection system for analyzing gait. The system transfers real-life scenarios into a realistic presentation in the virtual environment and provides look-alike same-age and old-age avatars for participants. We conducted an empirical study and used subjective questionnaires to evaluate participants' feelings about the virtual reality experience. Also, the paired sample t-test and neural network were implemented to analyze gait differences. The results suggest that there are disparities in gait between virtual and real environments. Also, the appearance of embodied avatars could influence the gait parameters in the virtual environment. Moreover, the experience of embodying old-age avatars affects the gait in the real world.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2411.10940.pdf' target='_blank'>https://arxiv.org/pdf/2411.10940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Hsiang Lien, Benedictus Kent Chandra, Robin Fischer, Ya-Hui Tang, Shiann-Jang Wang, Wei-En Hsu, Li-Chen Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10940">A Monocular SLAM-based Multi-User Positioning System with Image Occlusion in Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, with the rapid development of augmented reality (AR) technology, there is an increasing demand for multi-user collaborative experiences. Unlike for single-user experiences, ensuring the spatial localization of every user and maintaining synchronization and consistency of positioning and orientation across multiple users is a significant challenge. In this paper, we propose a multi-user localization system based on ORB-SLAM2 using monocular RGB images as a development platform based on the Unity 3D game engine. This system not only performs user localization but also places a common virtual object on a planar surface (such as table) in the environment so that every user holds a proper perspective view of the object. These generated virtual objects serve as reference points for multi-user position synchronization. The positioning information is passed among every user's AR devices via a central server, based on which the relative position and movement of other users in the space of a specific user are presented via virtual avatars all with respect to these virtual objects. In addition, we use deep learning techniques to estimate the depth map of an image from a single RGB image to solve occlusion problems in AR applications, making virtual objects appear more natural in AR scenes.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2411.05653.pdf' target='_blank'>https://arxiv.org/pdf/2411.05653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leon O. H. Kroczek, Alexander May, Selina Hettenkofer, Andreas Ruider, Bernd Ludwig, Andreas MÃ¼hlberger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05653">The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities in conversational tasks. Embodying an LLM as a virtual human allows users to engage in face-to-face social interactions in Virtual Reality. However, the influence of person- and task-related factors in social interactions with LLM-controlled agents remains unclear. In this study, forty-six participants interacted with a virtual agent whose persona was manipulated as extravert or introvert in three different conversational tasks (small talk, knowledge test, convincing). Social-evaluation, emotional experience, and realism were assessed using ratings. Interactive engagement was measured by quantifying participants' words and conversational turns. Finally, we measured participants' willingness to ask the agent for help during the knowledge test. Our findings show that the extraverted agent was more positively evaluated, elicited a more pleasant experience and greater engagement, and was assessed as more realistic compared to the introverted agent. Whereas persona did not affect the tendency to ask for help, participants were generally more confident in the answer when they had help of the LLM. Variation of personality traits of LLM-controlled embodied virtual agents, therefore, affects social-emotional processing and behavior in virtual interactions. Embodied virtual agents allow the presentation of naturalistic social encounters in a virtual environment.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2411.01512.pdf' target='_blank'>https://arxiv.org/pdf/2411.01512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alvaro Budria, Adrian Lopez-Rodriguez, Oscar Lorente, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01512">InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present InstantGeoAvatar, a method for efficient and effective learning from monocular video of detailed 3D geometry and appearance of animatable implicit human avatars. Our key observation is that the optimization of a hash grid encoding to represent a signed distance function (SDF) of the human subject is fraught with instabilities and bad local minima. We thus propose a principled geometry-aware SDF regularization scheme that seamlessly fits into the volume rendering pipeline and adds negligible computational overhead. Our regularization scheme significantly outperforms previous approaches for training SDFs on hash grids. We obtain competitive results in geometry reconstruction and novel view synthesis in as little as five minutes of training time, a significant reduction from the several hours required by previous work. InstantGeoAvatar represents a significant leap forward towards achieving interactive reconstruction of virtual avatars.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2410.21894.pdf' target='_blank'>https://arxiv.org/pdf/2410.21894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enes Yigitbas, Christian Kaltschmidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21894">Effects of Human Avatar Representation in Virtual Reality on Inter-Brain Connection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Increasing advances in affordable consumer hardware and accessible software frameworks are now bringing Virtual Reality (VR) to the masses. Especially collaborative VR applications where different people can work together are gaining momentum. In this context, human avatars and their representations are a crucial aspect of collaborative VR applications as they represent a digital twin of the end-users and determine how one is perceived in a virtual environment. When it comes to the effect of avatar representation on the end-users of collaborative VR applications, so far mostly questionnaires have been used to assess the quality of avatar representations. A promising alternative to objectively measure the effect of avatar representation is the investigation of inter-brain connections during the usage of a collaborative VR application. However, the combination of immersive VR applications and inter-brain connections has not been fully researched yet. Thus, our work investigates how different human avatar representations (real (RL), full-body (FB), and head-hand (HH)) affect inter-brain connections. For this purpose, we have designed and conducted a hyperscanning study with eight pairs. The main results of our hyperscanning study show that the number of significant sensor pairs was the highest in the RL, medium in the FB, and lowest in the HH condition indicating that an avatar that looks more like a real human enables more significant sensor pairs to appear in an EEG analysis.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2410.17741.pdf' target='_blank'>https://arxiv.org/pdf/2410.17741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zexu Huang, Sarah Monazam Erfani, Siying Lu, Mingming Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17741">Efficient Neural Implicit Representation for 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence, AR/VR, 3D graphics, and the rapidly evolving metaverse. Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110X faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 seconds of training.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2409.18745.pdf' target='_blank'>https://arxiv.org/pdf/2409.18745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Christina Almada Campos, Bruno Vilhena Adorno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18745">A study on the effects of mixed explicit and implicit communications in human-virtual-agent interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication between humans and robots (or virtual agents) is essential for interaction and often inspired by human communication, which uses gestures, facial expressions, gaze direction, and other explicit and implicit means. This work presents an interaction experiment where humans and virtual agents interact through explicit (gestures, manual entries using mouse and keyboard, voice, sound, and information on screen) and implicit (gaze direction, location, facial expressions, and raise of eyebrows) communication to evaluate the effect of mixed explicit-implicit communication against purely explicit communication. Results obtained using Bayesian parameter estimation show that the number of errors and task execution time did not significantly change when mixed explicit and implicit communications were used, and neither the perceived efficiency of the interaction. In contrast, acceptance, sociability, and transparency of the virtual agent increased when using mixed communication modalities (88.3%, 92%, and 92.9% of the effect size posterior distribution of each variable, respectively, were above the upper limit of the region of practical equivalence). This suggests that task-related measures, such as time, number of errors, and perceived efficiency of the interaction, have not been influenced by the communication type in our particular experiment. However, the improvement of subjective measures related to the virtual agent, such as acceptance, sociability, and transparency, suggests that humans are more receptive to mixed explicit and implicit communications.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2409.16404.pdf' target='_blank'>https://arxiv.org/pdf/2409.16404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixin Guo, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16404">FastTalker: Jointly Generating Speech and Conversational Gestures from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human gestures and speech from a text script is critical for creating realistic talking avatars. One solution is to leverage separate pipelines for text-to-speech (TTS) and speech-to-gesture (STG), but this approach suffers from poor alignment of speech and gestures and slow inference times. In this paper, we introduce FastTalker, an efficient and effective framework that simultaneously generates high-quality speech audio and 3D human gestures at high inference speeds. Our key insight is reusing the intermediate features from speech synthesis for gesture generation, as these features contain more precise rhythmic information than features re-extracted from generated speech. Specifically, 1) we propose an end-to-end framework that concurrently generates speech waveforms and full-body gestures, using intermediate speech features such as pitch, onset, energy, and duration directly for gesture decoding; 2) we redesign the causal network architecture to eliminate dependencies on future inputs for real applications; 3) we employ Reinforcement Learning-based Neural Architecture Search (NAS) to enhance both performance and inference speed by optimizing our network architecture. Experimental results on the BEAT2 dataset demonstrate that FastTalker achieves state-of-the-art performance in both speech synthesis and gesture generation, processing speech and gestures in 0.17 seconds per second on an NVIDIA 3090.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2409.16203.pdf' target='_blank'>https://arxiv.org/pdf/2409.16203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunji Chu, Yunseob Shim, Unsang Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16203">Facial Expression-Enhanced TTS: Combining Face Representation and Emotion Intensity for Adaptive Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FEIM-TTS, an innovative zero-shot text-to-speech (TTS) model that synthesizes emotionally expressive speech, aligned with facial images and modulated by emotion intensity. Leveraging deep learning, FEIM-TTS transcends traditional TTS systems by interpreting facial cues and adjusting to emotional nuances without dependence on labeled datasets. To address sparse audio-visual-emotional data, the model is trained using LRS3, CREMA-D, and MELD datasets, demonstrating its adaptability. FEIM-TTS's unique capability to produce high-quality, speaker-agnostic speech makes it suitable for creating adaptable voices for virtual characters. Moreover, FEIM-TTS significantly enhances accessibility for individuals with visual impairments or those who have trouble seeing. By integrating emotional nuances into TTS, our model enables dynamic and engaging auditory experiences for webcomics, allowing visually impaired users to enjoy these narratives more fully. Comprehensive evaluation evidences its proficiency in modulating emotion and intensity, advancing emotional speech synthesis and accessibility. Samples are available at: https://feim-tts.github.io/.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2409.09326.pdf' target='_blank'>https://arxiv.org/pdf/2409.09326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deng Junli, Luo Yihao, Yang Xueting, Li Siyou, Wang Wei, Guo Jinyang, Shi Ping
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09326">LawDNet: Enhanced Audio-Driven Lip Synthesis via Local Affine Warping Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of photorealistic avatar generation, the fidelity of audio-driven lip motion synthesis is essential for realistic virtual interactions. Existing methods face two key challenges: a lack of vivacity due to limited diversity in generated lip poses and noticeable anamorphose motions caused by poor temporal coherence. To address these issues, we propose LawDNet, a novel deep-learning architecture enhancing lip synthesis through a Local Affine Warping Deformation mechanism. This mechanism models the intricate lip movements in response to the audio input by controllable non-linear warping fields. These fields consist of local affine transformations focused on abstract keypoints within deep feature maps, offering a novel universal paradigm for feature warping in networks. Additionally, LawDNet incorporates a dual-stream discriminator for improved frame-to-frame continuity and employs face normalization techniques to handle pose and scene variations. Extensive evaluations demonstrate LawDNet's superior robustness and lip movement dynamism performance compared to previous methods. The advancements presented in this paper, including the methodologies, training data, source codes, and pre-trained models, will be made accessible to the research community.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2407.15956.pdf' target='_blank'>https://arxiv.org/pdf/2407.15956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Alizadeh, Dave Randall, Peter Tolmie, Minha Lee, Yuhui Xu, Sarah Mennicken, MikoÅaj P. WoÅºniak, Dennis Paul, Dominik Pins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15956">Future of Home-living: Designing Smart Spaces for Modern Domestic Life</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of smart home technologies, particularly agentic ones such as conversational agents, robots, and virtual avatars, is reshaping our understanding of home and domestic life. This shift highlights the complexities of modern domestic life, with the household landscape now featuring diverse cohabiting units like co-housing and communal living arrangements. These agentic technologies present specific design challenges and opportunities as they become integrated into everyday routines and activities. Our workshop envisions smart homes as dynamic, user-shaped spaces, focusing on the integration of these technologies into daily life. We aim to explore how these technologies transform household dynamics, especially through boundary fluidity, by uniting researchers and practitioners from fields such as design, sociology, and ethnography. Together, we will develop a taxonomy of challenges and opportunities, providing a structured perspective on the integration of agentic technologies and their impact on contemporary living arrangements.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2406.17582.pdf' target='_blank'>https://arxiv.org/pdf/2406.17582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changyang Li, Lap-Fai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17582">Crafting Dynamic Virtual Activities with Advanced Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the use of large multimodal models (LMMs) for generating virtual activities, leveraging the integration of vision-language modalities to enable the interpretation of virtual environments. This approach not only facilitates the recognition of scene layouts, semantic contexts, and object identities, but also empowers LMMs to abstract the elements of a scene. By correlating these abstractions with massive knowledge about human activities, LMMs are capable of generating adaptive and contextually relevant virtual activities. We propose a structured framework for articulating abstract activity descriptions, with an emphasis on delineating character interactions within the virtual milieu. Utilizing the derived high-level contexts, our methodology proficiently positions virtual characters, ensuring that their interactions and behaviors are realistically and contextually congruent through strategic optimizations. The implications of our findings are significant, offering a novel pathway for enhancing the realism and contextual appropriateness of virtual activities in simulated environments.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2406.10561.pdf' target='_blank'>https://arxiv.org/pdf/2406.10561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Palash Moon, Pushpak Bhattacharyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10561">We Care: Multimodal Depression Detection and Knowledge Infused Mental Health Therapeutic Response Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection of depression through non-verbal cues has gained significant attention. Previous research predominantly centred on identifying depression within the confines of controlled laboratory environments, often with the supervision of psychologists or counsellors. Unfortunately, datasets generated in such controlled settings may struggle to account for individual behaviours in real-life situations. In response to this limitation, we present the Extended D-vlog dataset, encompassing a collection of 1, 261 YouTube vlogs. Additionally, the emergence of large language models (LLMs) like GPT3.5, and GPT4 has sparked interest in their potential they can act like mental health professionals. Yet, the readiness of these LLM models to be used in real-life settings is still a concern as they can give wrong responses that can harm the users. We introduce a virtual agent serving as an initial contact for mental health patients, offering Cognitive Behavioral Therapy (CBT)-based responses. It comprises two core functions: 1. Identifying depression in individuals, and 2. Delivering CBT-based therapeutic responses. Our Mistral model achieved impressive scores of 70.1% and 30.9% for distortion assessment and classification, along with a Bert score of 88.7%. Moreover, utilizing the TVLT model on our Multimodal Extended D-vlog Dataset yielded outstanding results, with an impressive F1-score of 67.8%
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2406.01223.pdf' target='_blank'>https://arxiv.org/pdf/2406.01223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Liu, Ke Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01223">Report on Methods and Applications for Crafting 3D Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an in-depth exploration of 3D human model and avatar generation technology, propelled by the rapid advancements in large-scale models and artificial intelligence. The paper reviews the comprehensive process of 3D human model generation, from scanning to rendering, and highlights the pivotal role these models play in entertainment, VR, AR, healthcare, and education. We underscore the significance of diffusion models in generating high-fidelity images and videos. It emphasizes the indispensable nature of 3D human models in enhancing user experiences and functionalities across various fields. Furthermore, this paper anticipates the potential of integrating large-scale models with deep learning to revolutionize 3D content generation, offering insights into the future prospects of the technology. It concludes by emphasizing the importance of continuous innovation in the field, suggesting that ongoing advancements will significantly expand the capabilities and applications of 3D human models and avatars.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2405.07418.pdf' target='_blank'>https://arxiv.org/pdf/2405.07418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Guillermo FeijÃ³o-GarcÃ­a, Chase Wrenn, Alexandre Gomes de Siqueira, Rashi Ghosh, Jacob Stuart, Heng Yao, Benjamin Lok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07418">Exploring the Effects of User-Agent and User-Designer Similarity in Virtual Human Design to Promote Mental Health Intentions for College Students</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual humans (i.e., embodied conversational agents) have the potential to support college students' mental health, particularly in Science, Technology, Engineering, and Mathematics (STEM) fields where students are at a heightened risk of mental disorders such as anxiety and depression. A comprehensive understanding of students, considering their cultural characteristics, experiences, and expectations, is crucial for creating timely and effective virtual human interventions. To this end, we conducted a user study with 481 computer science students from a major university in North America, exploring how they co-designed virtual humans to support mental health conversations for students similar to them. Our findings suggest that computer science students who engage in co-design processes of virtual humans tend to create agents that closely resemble them demographically--agent-designer demographic similarity. Key factors influencing virtual human design included age, gender, ethnicity, and the matching between appearance and voice. We also observed that the demographic characteristics of virtual human designers, especially ethnicity and gender, tend to be associated with those of the virtual humans they designed. Finally, we provide insights concerning the impact of user-designer demographic similarity in virtual humans' effectiveness in promoting mental health conversations when designers' characteristics are shared explicitly or implicitly. Understanding how virtual humans' characteristics serve users' experiences in mental wellness conversations and the similarity-attraction effects between agents, users, and designers may help tailor virtual humans' design to enhance their acceptance and increase their counseling effectiveness.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2404.12317.pdf' target='_blank'>https://arxiv.org/pdf/2404.12317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbo Yu, Graeme McKinley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12317">Synthetic Participatory Planning of Shard Automated Electric Mobility Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unleashing the synergies among rapidly evolving mobility technologies in a multi-stakeholder setting presents unique challenges and opportunities for addressing urban transportation problems. This paper introduces a novel synthetic participatory method that critically leverages large language models (LLMs) to create digital avatars representing diverse stakeholders to plan shared automated electric mobility systems (SAEMS). These calibratable agents collaboratively identify objectives, envision and evaluate SAEMS alternatives, and strategize implementation under risks and constraints. The results of a Montreal case study indicate that a structured and parameterized workflow provides outputs with higher controllability and comprehensiveness on an SAEMS plan than that generated using a single LLM-enabled expert agent. Consequently, this approach provides a promising avenue for cost-efficiently improving the inclusivity and interpretability of multi-objective transportation planning, suggesting a paradigm shift in how we envision and strategize for sustainable transportation systems.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2401.06957.pdf' target='_blank'>https://arxiv.org/pdf/2401.06957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Nadeem, Raza Imam, Rouqaiah Al-Refai, Meriem Chkir, Mohamad Hoda, Abdulmotaleb El Saddik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06957">EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has grown. Addressing this demand, we introduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation (EVOKE), a lightweight emotion recognition framework designed for the seamless integration of emotion recognition into 3D avatars within virtual environments. Our approach leverages knowledge distillation involving multi-label classification on the publicly available DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Remarkably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results, boasting an accuracy of 87% while demanding far less computational resources. This equilibrium between performance and deployability positions our framework as an ideal choice for virtual environment systems. Furthermore, the multi-label classification outcomes are utilized to map emotions onto custom-designed 3D avatars.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2312.02128.pdf' target='_blank'>https://arxiv.org/pdf/2312.02128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vitor Miguel Xavier Peres, Greice Pinho Dal Molin, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02128">Can we truly transfer an actor's genuine happiness to avatars? An investigation into virtual, real, posed and spontaneous faces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A look is worth a thousand words is a popular phrase. And why is a simple look enough to portray our feelings about something or someone? Behind this question are the theoretical foundations of the field of psychology regarding social cognition and the studies of psychologist Paul Ekman. Facial expressions, as a form of non-verbal communication, are the primary way to transmit emotions between human beings. The set of movements and expressions of facial muscles that convey some emotional state of the individual to their observers are targets of studies in many areas. Our research aims to evaluate Ekman's action units in datasets of real human faces, posed and spontaneous, and virtual human faces resulting from transferring real faces into Computer Graphics faces. In addition, we also conducted a case study with specific movie characters, such as SheHulk and Genius. We intend to find differences and similarities in facial expressions between real and CG datasets, posed and spontaneous faces, and also to consider the actors' genders in the videos. This investigation can help several areas of knowledge, whether using real or virtual human beings, in education, health, entertainment, games, security, and even legal matters. Our results indicate that AU intensities are greater for posed than spontaneous datasets, regardless of gender. Furthermore, there is a smoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a real face is transformed into CG.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2311.15251.pdf' target='_blank'>https://arxiv.org/pdf/2311.15251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Misirlis, Yiannis Nikolaidis, Anna Sabidussi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15251">Should I use metaverse or not? An investigation of university students behavioral intention to use MetaEducation technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse, a burgeoning technological trend that combines virtual and augmented reality, provides users with a fully digital environment where they can assume a virtual identity through a digital avatar and interact with others as they were in the real world. Its applications span diverse domains such as economy (with its entry into the cryptocurrency field), finance, social life, working environment, healthcare, real estate, and education. During the COVID-19 and post-COVID-19 era, universities have rapidly adopted e-learning technologies to provide students with online access to learning content and platforms, rendering previous considerations on integrating such technologies or preparing institutional infrastructures virtually obsolete. In light of this context, the present study proposes a framework for analyzing university students' acceptance and intention to use metaverse technologies in education, drawing upon the Technology Acceptance Model (TAM). The study aims to investigate the relationship between students' intention to use metaverse technologies in education, hereafter referred to as MetaEducation, and selected TAM constructs, including Attitude, Perceived Usefulness, Perceived Ease of Use, Self-efficacy of metaverse technologies in education, and Subjective Norm. Notably, Self-efficacy and Subjective Norm have a positive influence on Attitude and Perceived Usefulness, whereas Perceived Ease of Use does not exhibit a strong correlation with Attitude or Perceived Usefulness. The authors postulate that the weak associations between the study's constructs may be attributed to limited knowledge regarding MetaEducation and its potential benefits. Further investigation and analysis of the study's proposed model are warranted to comprehensively understand the complex dynamics involved in the acceptance and utilization of MetaEducation technologies in the realm of higher education
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2311.15088.pdf' target='_blank'>https://arxiv.org/pdf/2311.15088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Liu, Gizem Kayar, Ken Perlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15088">A GPU-based Hydrodynamic Simulator with Boid Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a hydrodynamic simulation system using the GPU compute shaders of DirectX for simulating virtual agent behaviors and navigation inside a smoothed particle hydrodynamical (SPH) fluid environment with real-time water mesh surface reconstruction. The current SPH literature includes interactions between SPH and heterogeneous meshes but seldom involves interactions between SPH and virtual boid agents. The contribution of the system lies in the combination of the parallel smoothed particle hydrodynamics model with the distributed boid model of virtual agents to enable agents to interact with fluids. The agents based on the boid algorithm influence the motion of SPH fluid particles, and the forces from the SPH algorithm affect the movement of the boids. To enable realistic fluid rendering and simulation in a particle-based system, it is essential to construct a mesh from the particle attributes. Our system also contributes to the surface reconstruction aspect of the pipeline, in which we performed a set of experiments with the parallel marching cubes algorithm per frame for constructing the mesh from the fluid particles in a real-time compute and memory-intensive application, producing a wide range of triangle configurations. We also demonstrate that our system is versatile enough for reinforced robotic agents instead of boid agents to interact with the fluid environment for underwater navigation and remote control engineering purposes.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2308.01889.pdf' target='_blank'>https://arxiv.org/pdf/2308.01889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bavo Van Kerrebroeck, Kristel CrombÃ©, StÃ©phanie Wilain, Marc Leman, Pieter-Jan Maes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01889">The virtual drum circle: polyrhythmic music interactions in extended reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging technologies in the domain of extended reality offer rich, new possibilities for the study and practice of joint music performance. Apart from the technological challenges, bringing music players together in extended reality raises important questions on their performance and embodied coordination. In this study, we designed an extended reality platform to assess a remote, bidirectional polyrhythmic interaction between two players, mediated in real time by their three-dimensional embodied avatars and a shared, virtual drum circle. We leveraged a multi-layered analysis framework to assess their performance quality, embodied co-regulation and first-person interaction experience, using statistical techniques for time-series analysis and mixed-effect regression and focusing on contrasts of visual coupling (not seeing / seeing as avatars / seeing as real) and auditory context (metronome / music). Results reveal that an auditory context with music improved the performance output as measured by a prediction error, increased movement energy and levels of experienced agency. Visual coupling impacted experiential qualities and induced prosocial effects with increased levels of partner realism resulting in increased levels of shared agency and self-other merging. Embodied co-regulation between players was impacted by auditory context and visual coupling, suggesting prediction-based compensatory mechanisms to deal with the novelty, difficulty, and expressivity in the musical interaction. This study contributes to the understanding of music performance in extended reality by using a methodological approach to demonstrate how co-regulation between players is impacted by visual coupling and auditory context and provides a basis and future directions for further action-oriented research.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2308.01492.pdf' target='_blank'>https://arxiv.org/pdf/2308.01492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Blooma John, Ramanathan Subramanian, Jayan Chirayath Kurian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01492">A Virtual Reality Game to Improve Physical and Cognitive Acuity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Virtual Human Benchmark (VHB) game to evaluate and improve physical and cognitive acuity. VHB simulates in 3D the BATAK lightboard game, which is designed to improve physical reaction and hand-eye coordination, on the \textit{Oculus Rift} and \textit{Quest} headsets. The game comprises the \textit{reaction}, \textit{accumulator} and \textit{sequence} modes; \bj{along} with the \textit{reaction} and \textit{accumulator} modes which mimic BATAK functionalities, the \textit{sequence} mode involves the user repeating a sequence of illuminated targets with increasing complexity to train visual memory and cognitive processing. A first version of the game (VHB v1) was evaluated against the real-world BATAK by 20 users, and their feedback was utilized to improve game design and obtain a second version (VHB v2). Another study to evaluate VHB v2 was conducted with 20 users, whose results confirmed that the deign improvements enhanced game usability and user experience in multiple respects. Also, logging and visualization of performance data such as \textit{reaction time}, \textit{speed between targets} and \textit{completed sequence patterns} provides useful data for coaches/therapists monitoring sports/rehabilitation regimens.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2307.12345.pdf' target='_blank'>https://arxiv.org/pdf/2307.12345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Stock, Stephan SchlÃ¶gl, Aleksander Groth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12345">Tell me, what are you most afraid of? Exploring the Effects of Agent Representation on Information Disclosure in Human-Chatbot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-disclosure counts as a key factor influencing successful health treatment, particularly when it comes to building a functioning patient-therapist-connection. To this end, the use of chatbots may be considered a promising puzzle piece that helps foster respective information provision. Several studies have shown that people disclose more information when they are interacting with a chatbot than when they are interacting with another human being. If and how the chatbot is embodied, however, seems to play an important role influencing the extent to which information is disclosed. Here, research shows that people disclose less if the chatbot is embodied with a human avatar in comparison to a chatbot without embodiment. Still, there is only little information available as to whether it is the embodiment with a human face that inhibits disclosure, or whether any type of face will reduce the amount of shared information. The study presented in this paper thus aims to investigate how the type of chatbot embodiment influences self-disclosure in human-chatbot-interaction. We conducted a quasi-experimental study in which $n=178$ participants were asked to interact with one of three settings of a chatbot app. In each setting, the humanness of the chatbot embodiment was different (i.e., human vs. robot vs. disembodied). A subsequent discourse analysis explored difference in the breadth and depth of self-disclosure. Results show that non-human embodiment seems to have little effect on self-disclosure. Yet, our data also shows, that, contradicting to previous work, human embodiment may have a positive effect on the breadth and depth of self-disclosure.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2307.01468.pdf' target='_blank'>https://arxiv.org/pdf/2307.01468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanyu Pan, Guowei Yang, Taijiang Mu, Yu-Kun Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01468">Generating Animatable 3D Cartoon Faces from Single Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the booming of virtual reality (VR) technology, there is a growing need for customized 3D avatars. However, traditional methods for 3D avatar modeling are either time-consuming or fail to retain similarity to the person being modeled. We present a novel framework to generate animatable 3D cartoon faces from a single portrait image. We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN. Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture, which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision. Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer. Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria. Furthermore, we demonstrate the capability of real-time facial animation of our 3D model.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2304.12152.pdf' target='_blank'>https://arxiv.org/pdf/2304.12152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Li Ding, Liang Chen, Kai Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12152">Efficient Halftoning via Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Halftoning aims to reproduce a continuous-tone image with pixels whose intensities are constrained to two discrete levels. This technique has been deployed on every printer, and the majority of them adopt fast methods (e.g., ordered dithering, error diffusion) that fail to render structural details, which determine halftone's quality. Other prior methods of pursuing visual pleasure by searching for the optimal halftone solution, on the contrary, suffer from their high computational cost. In this paper, we propose a fast and structure-aware halftoning method via a data-driven approach. Specifically, we formulate halftoning as a reinforcement learning problem, in which each binary pixel's value is regarded as an action chosen by a virtual agent with a shared fully convolutional neural network (CNN) policy. In the offline phase, an effective gradient estimator is utilized to train the agents in producing high-quality halftones in one action step. Then, halftones can be generated online by one fast CNN inference. Besides, we propose a novel anisotropy suppressing loss function, which brings the desirable blue-noise property. Finally, we find that optimizing SSIM could result in holes in flat areas, which can be avoided by weighting the metric with the contone's contrast map. Experiments show that our framework can effectively train a light-weight CNN, which is 15x faster than previous structure-aware methods, to generate blue-noise halftones with satisfactory visual quality. We also present a prototype of deep multitoning to demonstrate the extensibility of our method.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2304.06678.pdf' target='_blank'>https://arxiv.org/pdf/2304.06678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Viceconti, Maarten De Vos, Sabato Mellone, Liesbet Geris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06678">From the digital twins in healthcare to the Virtual Human Twin: a moon-shot project for digital health research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The idea of a systematic digital representation of the entire known human pathophysiology, which we could call the Virtual Human Twin, has been around for decades. To date, most research groups focused instead on developing highly specialised, highly focused patient-specific models able to predict specific quantities of clinical relevance. While it has facilitated harvesting the low-hanging fruits, this narrow focus is, in the long run, leaving some significant challenges that slow the adoption of digital twins in healthcare. This position paper lays the conceptual foundations for developing the Virtual Human Twin (VHT). The VHT is intended as a distributed and collaborative infrastructure, a collection of technologies and resources (data, models) that enable it, and a collection of Standard Operating Procedures (SOP) that regulate its use. The VHT infrastructure aims to facilitate academic researchers, public organisations, and the biomedical industry in developing and validating new digital twins in healthcare solutions with the possibility of integrating multiple resources if required by the specific context of use. Healthcare professionals and patients can also use the VHT infrastructure for clinical decision support or personalised health forecasting. As the European Commission launched the EDITH coordination and support action to develop a roadmap for the development of the Virtual Human Twin, this position paper is intended as a starting point for the consensus process and a call to arms for all stakeholders.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2303.14255.pdf' target='_blank'>https://arxiv.org/pdf/2303.14255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Mullen, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14255">PACE: Data-Driven Virtual Agent Interaction in Dense and Cluttered Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PACE, a novel method for modifying motion-captured virtual agents to interact with and move throughout dense, cluttered 3D scenes. Our approach changes a given motion sequence of a virtual agent as needed to adjust to the obstacles and objects in the environment. We first take the individual frames of the motion sequence most important for modeling interactions with the scene and pair them with the relevant scene geometry, obstacles, and semantics such that interactions in the agents motion match the affordances of the scene (e.g., standing on a floor or sitting in a chair). We then optimize the motion of the human by directly altering the high-DOF pose at each frame in the motion to better account for the unique geometric constraints of the scene. Our formulation uses novel loss functions that maintain a realistic flow and natural-looking motion. We compare our method with prior motion generating techniques and highlight the benefits of our method with a perceptual study and physical plausibility metrics. Human raters preferred our method over the prior approaches. Specifically, they preferred our method 57.1% of the time versus the state-of-the-art method using existing motions, and 81.0% of the time versus a state-of-the-art motion synthesis method. Additionally, our method performs significantly higher on established physical plausibility and interaction metrics. Specifically, we outperform competing methods by over 1.2% in terms of the non-collision metric and by over 18% in terms of the contact metric. We have integrated our interactive system with Microsoft HoloLens and demonstrate its benefits in real-world indoor scenes. Our project website is available at https://gamma.umd.edu/pace/.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2302.10186.pdf' target='_blank'>https://arxiv.org/pdf/2302.10186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karan Singla, Yeon-Jun Kim, Srinivas Bangalore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10186">E2E Spoken Entity Extraction for Virtual Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-computer conversations, extracting entities such as names, street addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech ignoring the superfluous portions such as carrier phrases, or spell name entities. In the context of dialog from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step approach which first generates lexical transcriptions followed by text-based entity extraction for identifying spoken entities.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2301.02527.pdf' target='_blank'>https://arxiv.org/pdf/2301.02527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bianca Marques, Rui NÃ³brega, Carmen Morgado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02527">Avatar-centred AR Collaborative Mobile Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interaction with the physical environment and different users is essential to foster a collaborative experience. For this, we propose an interaction based on a central point represented by an Augmented Reality marker in which several users can capture the attention and interact with a virtual avatar. The interface provides different game modes, with various challenges, supporting a collaborative mobile interaction. The system fosters various group interactions with a virtual avatar and enables various tasks with playful and didactic components.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2210.04366.pdf' target='_blank'>https://arxiv.org/pdf/2210.04366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Perrine, Trevor Kirkby
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04366">KP-RNN: A Deep Learning Pipeline for Human Motion Prediction and Synthesis of Performance Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digitally synthesizing human motion is an inherently complex process, which can create obstacles in application areas such as virtual reality. We offer a new approach for predicting human motion, KP-RNN, a neural network which can integrate easily with existing image processing and generation pipelines. We utilize a new human motion dataset of performance art, Take The Lead, as well as the motion generation pipeline, the Everybody Dance Now system, to demonstrate the effectiveness of KP-RNN's motion predictions. We have found that our neural network can predict human dance movements effectively, which serves as a baseline result for future works using the Take The Lead dataset. Since KP-RNN can work alongside a system such as Everybody Dance Now, we argue that our approach could inspire new methods for rendering human avatar animation. This work also serves to benefit the visualization of performance art in digital platforms by utilizing accessible neural networks.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2209.01320.pdf' target='_blank'>https://arxiv.org/pdf/2209.01320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddarth Ravichandran, OndÅej Texler, Dimitar Dinev, Hyun Jae Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01320">Synthesizing Photorealistic Virtual Humans Through Cross-modal Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last few decades, many aspects of human life have been enhanced with virtual domains, from the advent of digital assistants such as Amazon's Alexa and Apple's Siri to the latest metaverse efforts of the rebranded Meta. These trends underscore the importance of generating photorealistic visual depictions of humans. This has led to the rapid growth of so-called deepfake and talking-head generation methods in recent years. Despite their impressive results and popularity, they usually lack certain qualitative aspects such as texture quality, lips synchronization, or resolution, and practical aspects such as the ability to run in real-time. To allow for virtual human avatars to be used in practical scenarios, we propose an end-to-end framework for synthesizing high-quality virtual human faces capable of speaking with accurate lip motion with a special emphasis on performance. We introduce a novel network utilizing visemes as an intermediate audio representation and a novel data augmentation strategy employing a hierarchical image synthesis approach that allows disentanglement of the different modalities used to control the global head motion. Our method runs in real-time, and is able to deliver superior results compared to the current state-of-the-art.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2203.09016.pdf' target='_blank'>https://arxiv.org/pdf/2203.09016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rachel Love, Edith Law, Philip R. Cohen, Dana KuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.09016">Natural Language Communication with a Teachable Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational teachable agents offer a promising platform to support learning, both in the classroom and in remote settings. In this context, the agent takes the role of the novice, while the student takes on the role of teacher. This framing is significant for its ability to elicit the ProtÃ©gÃ© effect in the student-teacher, a pedagogical phenomenon known to increase engagement in the teaching task, and also improve cognitive outcomes. In prior work, teachable agents often take a passive role in the learning interaction, and there are few studies in which the agent and student engage in natural language dialogue during the teaching task. This work investigates the effect of teaching modality when interacting with a virtual agent, via the web-based teaching platform, the Curiosity Notebook. A method of teaching the agent by selecting sentences from source material is compared to a method paraphrasing the source material and typing text input to teach. A user study has been conducted to measure the effect teaching modality on the learning outcomes and engagement of the participants. The results indicate that teaching via paraphrasing and text input has a positive effect on learning outcomes for the material covered, and also on aspects of affective engagement. Furthermore, increased paraphrasing effort, as measured by the similarity between the source material and the material the teacher conveyed to the robot, improves learning outcomes for participants.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2103.10316.pdf' target='_blank'>https://arxiv.org/pdf/2103.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohith G, Madhu Vadali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.10316">A Quasi-centralized Collision-free Path Planning Approach for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel quasi-centralized approach for collision-free path planning of multi-robot systems (MRS) in obstacle-ridden environments. A new formation potential fields (FPF) concept is proposed around a virtual agent, located at the center of the formation which ensures self-organization and maintenance of the formation. The path of the virtual agent is centrally planned and the robots at the minima of the FPF are forced to move along with the virtual agent. In the neighborhood of obstacles, individual robots selfishly avoid collisions, thus marginally deviating from the formation. The proposed quasi-centralized approach introduces formation flexibility into the MRS, which enables MRS to effectively navigate in an obstacle-ridden workspace. Methodical analysis of the proposed approach and guidelines for selecting the FPF are presented. Results using a candidate FPF are shown that ensure a pentagonal formation effectively squeezes through a narrow passage avoiding any collisions with the walls.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2508.11872.pdf' target='_blank'>https://arxiv.org/pdf/2508.11872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinxing Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11872">Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical teaching, we observe that few students thoroughly read or fully comprehend the information provided in traditional, text-based course syllabi. As a result, essential details, such as course policies and learning outcomes, are frequently overlooked. To address this challenge, in this paper, we propose a novel approach leveraging AI-generated singing and virtual avatars to present syllabi in a format that is more visually appealing, engaging, and memorable. Especially, we leveraged the open-source tool, HeyGem, to transform textual syllabi into audiovisual presentations, in which digital avatars perform the syllabus content as songs. The proposed approach aims to stimulate students' curiosity, foster emotional connection, and enhance retention of critical course information. Student feedback indicated that AI-sung syllabi significantly improved awareness and recall of key course information.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2505.05475.pdf' target='_blank'>https://arxiv.org/pdf/2505.05475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonwoo Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05475">SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2411.18047.pdf' target='_blank'>https://arxiv.org/pdf/2411.18047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jennifer Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18047">The Trusted Caregiver: The Influence of Eye and Mouth Design Incorporating the Baby Schema Effect in Virtual Humanoid Agents on Older Adults Users' Perception of Trustworthiness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing proportion of the older adult population has made the smart home care industry one of the critical markets for virtual human-like agents. It is crucial to effectively promote a trustworthy human-computer partnership with older adults, enhancing service acceptance and effectiveness. However, few studies have focused on the facial features of the agents themselves, where the "baby schema" effect plays a vital role in enhancing trustworthiness. The eyes and mouth, in particular, attract most of the audience's attention and are especially significant. This study explores the impact of eye and mouth design on users' perception of trustworthiness. Specifically, a virtual humanoid agents model was developed, and based on this, 729 virtual facial images of children were designed. Participants (N=162) were asked to evaluate the impact of variations in the size and positioning of the eyes and mouth regions on the perceived credibility of these virtual agents. The results revealed that when the facial aspect ratio (width and height denoted as W and H, respectively) aligned with the "baby schema" effect (eye size at 0.25W, mouth size at 0.27W, eye height at 0.64H, eye distance at 0.43W, mouth height at 0.74H, and smile arc at 0.043H), the virtual agents achieved the highest facial credibility. This study proposes a design paradigm for the main facial features of virtual humanoid agents, which can increase the trust of older adults during interactions and significantly contribute to the research on the trustworthiness of virtual humanoid agents.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2408.10040.pdf' target='_blank'>https://arxiv.org/pdf/2408.10040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moshe BenBassat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10040">The Practimum-Optimum Algorithm for Manufacturing Scheduling: A Paradigm Shift Leading to Breakthroughs in Scale and Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Practimum-Optimum (P-O) algorithm represents a paradigm shift in developing automatic optimization products for complex real-life business problems such as large-scale manufacturing scheduling. It leverages deep business domain expertise to create a group of virtual human expert (VHE) agents with different "schools of thought" on how to create high-quality schedules. By computerizing them into algorithms, P-O generates many valid schedules at far higher speeds than human schedulers are capable of. Initially, these schedules can also be local optimum peaks far away from high-quality schedules. By submitting these schedules to a reinforced machine learning algorithm (RL), P-O learns the weaknesses and strengths of each VHE schedule, and accordingly derives reward and punishment changes in the Demand Set that will modify the relative priorities for time and resource allocation that jobs received in the prior iteration that led to the current state of the schedule. These cause the core logic of the VHE algorithms to explore, in the subsequent iteration, substantially different parts of the schedules universe and potentially find higher-quality schedules. Using the hill climbing analogy, this may be viewed as a big jump, shifting from a given local peak to a faraway promising start point equipped with knowledge embedded in the demand set for future iterations. This is a fundamental difference from most contemporary algorithms, which spend considerable time on local micro-steps restricted to the neighbourhoods of local peaks they visit. This difference enables a breakthrough in scale and performance for fully automatic manufacturing scheduling in complex organizations. The P-O algorithm is at the heart of Plataine Scheduler that, in one click, routinely schedules 30,000-50,000 tasks for real-life complex manufacturing operations.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2406.01056.pdf' target='_blank'>https://arxiv.org/pdf/2406.01056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Mandava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01056">Virtual avatar generation models as world navigators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SABR-CLIMB, a novel video model simulating human movement in rock climbing environments using a virtual avatar. Our diffusion transformer predicts the sample instead of noise in each diffusion step and ingests entire videos to output complete motion sequences. By leveraging a large proprietary dataset, NAV-22M, and substantial computational resources, we showcase a proof of concept for a system to train general-purpose virtual avatars for complex tasks in robotics, sports, and healthcare.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2403.17631.pdf' target='_blank'>https://arxiv.org/pdf/2403.17631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoxu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17631">AniArtAvatar: Animatable 3D Art Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements. Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D diffusion model to synthesize multi-view images from a single art portrait with a neutral expression. With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface. For avatar animation, we extract control points, transfer the motion with these points, and deform the implicit canonical space. Firstly, we render the front image of the avatar, extract the 2D landmarks, and project them to the 3D space using a trained SDF network. We extract 3D driving landmarks using 3DMM and transfer the motion to the avatar landmarks. To animate the avatar pose, we manually set the body height and bound the head and torso of an avatar with two cages. The head and torso can be animated by transforming the two cages. Our approach is a one-shot pipeline that can be applied to various styles. Experiments demonstrate that our method can generate high-quality 3D art avatars with desired control over different motions.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2305.08326.pdf' target='_blank'>https://arxiv.org/pdf/2305.08326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun-Cheng Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08326">Learner-Centered Analysis in Educational Metaverse Environments: Exploring Value Exchange Systems through Natural Interaction and Text Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the potential developments of self-directed learning in the metaverse in response to Education 4.0 and the Fourth Industrial Revolution. It highlights the importance of education keeping up with technological advancements and adopting learner-centered approaches. Additionally, it focuses on exploring value exchange systems through natural interaction, text mining, and analysis. The metaverse concept extends beyond extended reality (XR) technologies, encompassing digital avatars and shared ecological value. The role of educators in exploring new technologies and leveraging text-mining techniques to enhance learning efficiency is emphasized. The metaverse is presented as a platform for value exchange, necessitating meaningful and valuable content to attract users. Integrating virtual and real-world experiences within the metaverse offers practical applications and contributes to its essence. This paper sheds light on the metaverse's potential to create a learner-centered educational environment and adapt to the evolving landscape of Education 4.0. Its findings, supported by text mining analysis, contribute to understanding the metaverse's role in shaping education in the Fourth Industrial Revolution.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2304.01443.pdf' target='_blank'>https://arxiv.org/pdf/2304.01443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01443">Virtual Avatar Stream: a cost-down approach to the Metaverse experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Metaverse through VR headsets is a rapidly growing concept, but the high cost of entry currently limits access for many users. This project aims to provide an accessible entry point to the immersive Metaverse experience by leveraging web technologies. The platform developed allows users to engage with rendered avatars using only a web browser, microphone, and webcam. By employing the WebGL and MediaPipe face tracking AI model from Google, the application generates real-time 3D face meshes for users. It uses a client-to-client streaming cluster to establish a connection, and clients negotiate SRTP protocol through WebRTC for direct data streaming. Additionally, the project addresses backend challenges through an architecture that is serverless, distributive, auto-scaling, highly resilient, and secure. The platform offers a scalable, hardware-free solution for users to experience a near-immersive Metaverse, with the potential for future integration with game server clusters. This project provides an important step toward a more inclusive Metaverse accessible to a wider audience.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2210.04047.pdf' target='_blank'>https://arxiv.org/pdf/2210.04047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M Seetha Ramaiah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04047">Motion Planning on Visual Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this thesis, we propose an alternative characterization of the notion of Configuration Space, which we call Visual Configuration Space (VCS). This new characterization allows an embodied agent (e.g., a robot) to discover its own body structure and plan obstacle-free motions in its peripersonal space using a set of its own images in random poses. Here, we do not assume any knowledge of geometry of the agent, obstacles or the environment. We demonstrate the usefulness of VCS in (a) building and working with geometry-free models for robot motion planning, (b) explaining how a human baby might learn to reach objects in its peripersonal space through motor babbling, and (c) automatically generating natural looking head motion animations for digital avatars in virtual environments. This work is based on the formalism of manifolds and manifold learning using the agent's images and hence we call it Motion Planning on Visual Manifolds.
