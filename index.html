<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2509.17654.pdf' target='_blank'>https://arxiv.org/pdf/2509.17654.pdf</a></span>   <span><a href='https://github.com/DevChoco/CAP-VTON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sehyun Kim, Hye Jun Lee, Jiwoo Lee, Taemin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17654">Clothing agnostic Pre-inpainting Virtual Try-ON</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of deep learning technology, virtual try-on technology has become an important application value in the fields of e-commerce, fashion, and entertainment. The recently proposed Leffa has improved the texture distortion problem of diffu-sion-based models, but there are limitations in that the bottom detection inaccuracy and the existing clothing silhouette remain in the synthesis results. To solve this problem, this study proposes CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has improved the naturalness and consistency of whole-body clothing syn-thesis by integrating multi-category masking based on Dress Code and skin inpainting based on Stable Diffusion. In particular, a generate skin module was introduced to solve the skin restoration problem that occurs when long-sleeved images are converted into short-sleeved or sleeveless ones, and high-quality restoration was implemented consider-ing the human body posture and color. As a result, CaP-VTON recorded 92.5%, which is 15.4% better than Leffa in short-sleeved synthesis accuracy, and showed the performance of consistently reproducing the style and shape of reference clothing in visual evaluation. These structures maintain model-agnostic properties and are applicable to various diffu-sion-based virtual inspection systems, and can contribute to applications that require high-precision virtual wearing, such as e-commerce, custom styling, and avatar creation.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2509.03883.pdf' target='_blank'>https://arxiv.org/pdf/2509.03883.pdf</a></span>   <span><a href='https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03883">Human Motion Video Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2508.12854.pdf' target='_blank'>https://arxiv.org/pdf/2508.12854.pdf</a></span>   <span><a href='https://github.com/RH-Lin/E3RG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghao Lin, Shuai Shen, Weipeng Hu, Qiaolin He, Aolin Xiong, Li Huang, Haifeng Hu, Yap-peng Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12854">E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Empathetic Response Generation (MERG) is crucial for building emotionally intelligent human-computer interactions. Although large language models (LLMs) have improved text-based ERG, challenges remain in handling multimodal emotional content and maintaining identity consistency. Thus, we propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System based on multimodal LLMs which decomposes MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By integrating advanced expressive speech and video generative models, E3RG delivers natural, emotionally rich, and identity-consistent responses without extra training. Experiments validate the superiority of our system on both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. Our code is available at https://github.com/RH-Lin/E3RG.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2508.08891.pdf' target='_blank'>https://arxiv.org/pdf/2508.08891.pdf</a></span>   <span><a href='https://github.com/deepreasonings/WholeBodyBenchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyi Wang, Yifan Yang, Jun Pei, Lijie Xia, Jianpo Liu, Xiaobing Yuan, Xinhan Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08891">Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation of Whole-body Talking Avatar Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic, fully animatable whole-body avatars from a single portrait is challenging due to limitations in capturing subtle expressions, body movements, and dynamic backgrounds. Current evaluation datasets and metrics fall short in addressing these complexities. To bridge this gap, we introduce the Whole-Body Benchmark Dataset (WB-DH), an open-source, multi-modal benchmark designed for evaluating whole-body animatable avatar generation. Key features include: (1) detailed multi-modal annotations for fine-grained guidance, (2) a versatile evaluation framework, and (3) public access to the dataset and tools at https://github.com/deepreasonings/WholeBodyBenchmark.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2507.23343.pdf' target='_blank'>https://arxiv.org/pdf/2507.23343.pdf</a></span>   <span><a href='https://github.com/zyj-2000/Talker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Zhou, Jiezhang Cao, Zicheng Zhang, Farong Wen, Yanwei Jiang, Jun Jia, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23343">Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven methods for portraits are figuratively known as "Talkers" because of their capability to synthesize speaking mouth shapes and facial movements. Especially with the rapid development of the Text-to-Image (T2I) models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging digital human media. However, challenges persist regarding the quality of these talkers and AGTHs they generate, and comprehensive studies addressing these issues remain limited. To address this gap, this paper presents the largest AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset contains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the AGTHs and give the corresponding distortion categories. In our analysis for subjective experimental results, we evaluate the performance of talkers in terms of generalizability and quality, and also expose the distortions of existing AGTHs. Finally, an objective quality assessment method based on the first frame, Y-T slice and tone-lip consistency is proposed. Experimental results show that this method can achieve state-of-the-art (SOTA) performance in AGTH quality assessment. The work is released at https://github.com/zyj-2000/Talker.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2507.23298.pdf' target='_blank'>https://arxiv.org/pdf/2507.23298.pdf</a></span>   <span><a href='https://github.com/MaAI-Kyoto/MaAI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazushi Kato, Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23298">Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human dialogue, nonverbal information such as nodding and facial expressions is as crucial as verbal information, and spoken dialogue systems are also expected to express such nonverbal behaviors. We focus on nodding, which is critical in an attentive listening system, and propose a model that predicts both its timing and type in real time. The proposed model builds on the voice activity projection (VAP) model, which predicts voice activity from both listener and speaker audio. We extend it to prediction of various types of nodding in a continuous and real-time manner unlike conventional models. In addition, the proposed model incorporates multi-task learning with verbal backchannel prediction and pretraining on general dialogue data. In the timing and type prediction task, the effectiveness of multi-task learning was significantly demonstrated. We confirmed that reducing the processing rate enables real-time operation without a substantial drop in accuracy, and integrated the model into an avatar attentive listening system. Subjective evaluations showed that it outperformed the conventional method, which always does nodding in sync with verbal backchannel. The code and trained models are available at https://github.com/MaAI-Kyoto/MaAI.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2507.20987.pdf' target='_blank'>https://arxiv.org/pdf/2507.20987.pdf</a></span>   <span><a href='https://github.com/deepreasonings/WholeBodyBenchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhan Di, Kristin Qi, Pengqian Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20987">JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2507.16463.pdf' target='_blank'>https://arxiv.org/pdf/2507.16463.pdf</a></span>   <span><a href='https://github.com/DFKI-SignLanguage/MMS-Player' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrizio Nunnari, Shailesh Mishra, Patrick Gebhard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16463">MMS Player: an open source software for parametric data-driven animation of Sign Language avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes the MMS-Player, an open source software able to synthesise sign language animations from a novel sign language representation format called MMS (MultiModal Signstream). The MMS enhances gloss-based representations by adding information on parallel execution of signs, timing, and inflections. The implementation consists of Python scripts for the popular Blender 3D authoring tool and can be invoked via command line or HTTP API. Animations can be rendered as videos or exported in other popular 3D animation exchange formats. The software is freely available under GPL-3.0 license at https://github.com/DFKI-SignLanguage/MMS-Player.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2507.13648.pdf' target='_blank'>https://arxiv.org/pdf/2507.13648.pdf</a></span>   <span><a href='https://github.com/seungjun-moon/epsilon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjun Moon, Sangjoon Yu, Gyeong-Moon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13648">EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of neural radiance fields (NeRF) has paved the way to generate animatable human avatars from a monocular video. However, the sole usage of NeRF suffers from a lack of details, which results in the emergence of hybrid representation that utilizes SMPL-based mesh together with NeRF representation. While hybrid-based models show photo-realistic human avatar generation qualities, they suffer from extremely slow inference due to their deformation scheme: to be aligned with the mesh, hybrid-based models use the deformation based on SMPL skinning weights, which needs high computational costs on each sampled point. We observe that since most of the sampled points are located in empty space, they do not affect the generation quality but result in inference latency with deformation. In light of this observation, we propose EPSilon, a hybrid-based 3D avatar generation scheme with novel efficient point sampling strategies that boost both training and inference. In EPSilon, we propose two methods to omit empty points at rendering; empty ray omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that progress through the empty space. Then, EIO narrows down the sampling interval on the ray, which wipes out the region not occupied by either clothes or mesh. The delicate sampling scheme of EPSilon enables not only great computational cost reduction during deformation but also the designation of the important regions to be sampled, which enables a single-stage NeRF structure without hierarchical sampling. Compared to existing methods, EPSilon maintains the generation quality while using only 3.9% of sampled points and achieves around 20 times faster inference, together with 4 times faster training convergence. We provide video results on https://github.com/seungjun-moon/epsilon.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2507.07591.pdf' target='_blank'>https://arxiv.org/pdf/2507.07591.pdf</a></span>   <span><a href='https://github.com/sunkymepro/StableHairV2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuiyuan Sun, Yuxuan Zhang, Jichao Zhang, Jiaming Liu, Wei Wang, Niculae Sebe, Yao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07591">Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While diffusion-based methods have shown impressive capabilities in capturing diverse and complex hairstyles, their ability to generate consistent and high-quality multi-view outputs -- crucial for real-world applications such as digital humans and virtual avatars -- remains underexplored. In this paper, we propose Stable-Hair v2, a novel diffusion-based multi-view hair transfer framework. To the best of our knowledge, this is the first work to leverage multi-view diffusion models for robust, high-fidelity, and view-consistent hair transfer across multiple perspectives. We introduce a comprehensive multi-view training data generation pipeline comprising a diffusion-based Bald Converter, a data-augment inpainting model, and a face-finetuned multi-view diffusion model to generate high-quality triplet data, including bald images, reference hairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer model integrates polar-azimuth embeddings for pose conditioning and temporal attention layers to ensure smooth transitions between views. To optimize this model, we design a novel multi-stage training strategy consisting of pose-controllable latent IdentityNet training, hair extractor training, and temporal attention training. Extensive experiments demonstrate that our method accurately transfers detailed and realistic hairstyles to source subjects while achieving seamless and consistent results across views, significantly outperforming existing methods and establishing a new benchmark in multi-view hair transfer. Code is publicly available at https://github.com/sunkymepro/StableHairV2.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2507.02900.pdf' target='_blank'>https://arxiv.org/pdf/2507.02900.pdf</a></span>   <span><a href='https://github.com/VineetKumarRakesh/thg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineet Kumar Rakesh, Soumya Mazumdar, Research Pratim Maity, Sarbajit Pal, Amitabha Das, Tapas Samanta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02900">Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2507.02803.pdf' target='_blank'>https://arxiv.org/pdf/2507.02803.pdf</a></span>   <span><a href='https://github.com/gserifi/HyperGaussians' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gent Serifi, Marcel C. BÃ¼hler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02803">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2507.00792.pdf' target='_blank'>https://arxiv.org/pdf/2507.00792.pdf</a></span>   <span><a href='https://github.com/hvoss-techfak/JAX-IK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendric Voss, Stefan Kopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00792">JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/JAX-IK
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2505.20255.pdf' target='_blank'>https://arxiv.org/pdf/2505.20255.pdf</a></span>   <span><a href='https://github.com/MyNiuuu/AniCrafter' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MyNiuuu/AniCrafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyao Niu, Mingdeng Cao, Yifan Zhan, Qingtian Zhu, Mingze Ma, Jiancheng Zhao, Yanhong Zeng, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20255">AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video diffusion models have significantly improved character animation techniques. However, current approaches rely on basic structural conditions such as DWPose or SMPL-X to animate character images, limiting their effectiveness in open-domain scenarios with dynamic backgrounds or challenging human poses. In this paper, we introduce \textbf{AniCrafter}, a diffusion-based human-centric animation model that can seamlessly integrate and animate a given character into open-domain dynamic backgrounds while following given human motion sequences. Built on cutting-edge Image-to-Video (I2V) diffusion architectures, our model incorporates an innovative ''avatar-background'' conditioning mechanism that reframes open-domain human-centric animation as a restoration task, enabling more stable and versatile animation outputs. Experimental results demonstrate the superior performance of our method. Codes are available at https://github.com/MyNiuuu/AniCrafter.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2505.05022.pdf' target='_blank'>https://arxiv.org/pdf/2505.05022.pdf</a></span>   <span><a href='https://github.com/TingtingLiao/soap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Liao, Yujian Zheng, Adilbek Karmanov, Liwen Hu, Leyang Jin, Yuliang Xiu, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05022">SOAP: Style-Omniscient Animatable Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at https://github.com/TingtingLiao/soap.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2504.19056.pdf' target='_blank'>https://arxiv.org/pdf/2504.19056.pdf</a></span>   <span><a href='https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Mahdi Abootorabi, Omid Ghahroodi, Pardis Sadat Zahraei, Hossein Behzadasl, Alireza Mirrokni, Mobina Salimipanah, Arash Rasouli, Bahar Behzadipour, Sara Azarnoush, Benyamin Maleki, Erfan Sadraiye, Kiarash Kiani Feriz, Mahdi Teymouri Nahad, Ali Moghadasi, Abolfazl Eshagh Abianeh, Nizi Nazar, Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Meisam Ahmadi, Ehsaneddin Asgari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19056">Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2504.12909.pdf' target='_blank'>https://arxiv.org/pdf/2504.12909.pdf</a></span>   <span><a href='https://github.com/1231234zhan/mmlphuman' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youyi Zhan, Tianjia Shao, Yin Yang, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12909">Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many works have succeeded in reconstructing Gaussian human avatars from multi-view videos. However, they either struggle to capture pose-dependent appearance details with a single MLP, or rely on a computationally intensive neural network to reconstruct high-fidelity appearance but with rendering performance degraded to non-real-time. We propose a novel Gaussian human avatar representation that can reconstruct high-fidelity pose-dependence appearance with details and meanwhile can be rendered in real time. Our Gaussian avatar is empowered by spatially distributed MLPs which are explicitly located on different positions on human body. The parameters stored in each Gaussian are obtained by interpolating from the outputs of its nearby MLPs based on their distances. To avoid undesired smooth Gaussian property changing during interpolation, for each Gaussian we define a set of Gaussian offset basis, and a linear combination of basis represents the Gaussian property offsets relative to the neutral properties. Then we propose to let the MLPs output a set of coefficients corresponding to the basis. In this way, although Gaussian coefficients are derived from interpolation and change smoothly, the Gaussian offset basis is learned freely without constraints. The smoothly varying coefficients combined with freely learned basis can still produce distinctly different Gaussian property offsets, allowing the ability to learn high-frequency spatial signals. We further use control points to constrain the Gaussians distributed on a surface layer rather than allowing them to be irregularly distributed inside the body, to help the human avatar generalize better when animated under novel poses. Compared to the state-of-the-art method, our method achieves better appearance quality with finer details while the rendering speed is significantly faster under novel views and novel poses.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2503.19207.pdf' target='_blank'>https://arxiv.org/pdf/2503.19207.pdf</a></span>   <span><a href='https://github.com/rongakowang/FRESA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rong Wang, Fabian Prada, Ziyan Wang, Zhongshi Jiang, Chengxiang Yin, Junxuan Li, Shunsuke Saito, Igor Santesteban, Javier Romero, Rohan Joshi, Hongdong Li, Jason Saragih, Yaser Sheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19207">FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2503.15851.pdf' target='_blank'>https://arxiv.org/pdf/2503.15851.pdf</a></span>   <span><a href='https://github.com/ZhenglinZhou/Zero-1-to-A' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglin Zhou, Fan Ma, Hehe Fan, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15851">Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2503.12963.pdf' target='_blank'>https://arxiv.org/pdf/2503.12963.pdf</a></span>   <span><a href='https://github.com/chaolongy/KDTalker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaolong Yang, Kai Yao, Yuyao Yan, Chenru Jiang, Weiguang Zhao, Jie Sun, Guangliang Cheng, Yifei Zhang, Bin Dong, Kaizhu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12963">Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at https://github.com/chaolongy/KDTalker.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2503.12886.pdf' target='_blank'>https://arxiv.org/pdf/2503.12886.pdf</a></span>   <span><a href='https://github.com/gapszju/RGBAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linzhou Li, Yumeng Li, Yanlin Weng, Youyi Zheng, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12886">RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for reconstructing photorealistic, animatable head avatars at speeds sufficient for on-the-fly reconstruction. Unlike prior approaches that utilize linear bases from 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps tracked 3DMM parameters into reduced blendshape weights with an MLP, leading to a compact set of blendshape bases. The learned compact base composition effectively captures essential facial details for specific individuals, and does not rely on the fixed base composition weights of 3DMM, leading to enhanced reconstruction quality and higher efficiency. To further expedite the reconstruction process, we develop a novel color initialization estimation method and a batch-parallel Gaussian rasterization process, achieving state-of-the-art quality with training throughput of about 630 images per second. Moreover, we propose a local-global sampling strategy that enables direct on-the-fly reconstruction, immediately reconstructing the model as video streams in real time while achieving quality comparable to offline settings. Our source code is available at https://github.com/gapszju/RGBAvatar.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2503.06154.pdf' target='_blank'>https://arxiv.org/pdf/2503.06154.pdf</a></span>   <span><a href='https://github.com/wang-zidu/SRM-Hair' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zidu Wang, Jiankuo Zhao, Miao Xu, Xiangyu Zhu, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06154">SRM-Hair: Single Image Head Mesh Reconstruction via 3D Morphable Hair</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Morphable Models (3DMMs) have played a pivotal role as a fundamental representation or initialization for 3D avatar animation and reconstruction. However, extending 3DMMs to hair remains challenging due to the difficulty of enforcing vertex-level consistent semantic meaning across hair shapes. This paper introduces a novel method, Semantic-consistent Ray Modeling of Hair (SRM-Hair), for making 3D hair morphable and controlled by coefficients. The key contribution lies in semantic-consistent ray modeling, which extracts ordered hair surface vertices and exhibits notable properties such as additivity for hairstyle fusion, adaptability, flipping, and thickness modification. We collect a dataset of over 250 high-fidelity real hair scans paired with 3D face data to serve as a prior for the 3D morphable hair. Based on this, SRM-Hair can reconstruct a hair mesh combined with a 3D head from a single image. Note that SRM-Hair produces an independent hair mesh, facilitating applications in virtual avatar creation, realistic animation, and high-fidelity hair rendering. Both quantitative and qualitative experiments demonstrate that SRM-Hair achieves state-of-the-art performance in 3D mesh reconstruction. Our project is available at https://github.com/wang-zidu/SRM-Hair
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2502.17796.pdf' target='_blank'>https://arxiv.org/pdf/2502.17796.pdf</a></span>   <span><a href='https://github.com/aigc3d/LAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yisheng He, Xiaodong Gu, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17796">LAM: Large Avatar Model for One-shot Animatable Gaussian Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. Our experimental results demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks. Our code and video are available at https://aigc3d.github.io/projects/LAM/
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2502.02465.pdf' target='_blank'>https://arxiv.org/pdf/2502.02465.pdf</a></span>   <span><a href='https://github.com/weimengting/RigFace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengting Wei, Tuomas Varanka, Yante Li, Xingxun Jiang, Huai-Qian Khor, Guoying Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02465">Towards Consistent and Controllable Image Synthesis for Face Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face editing methods, essential for tasks like virtual avatars, digital human synthesis and identity preservation, have traditionally been built upon GAN-based techniques, while recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in controlling specific attributes and preserving the consistency of other unchanged attributes especially the identity characteristics. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion (SD) models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involves the combinations of target background, identity and face attributes aimed to edit. We strive to sufficiently disentangle the control of these factors to enable consistency of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Attribute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) A high-consistency FaceFusion method that transfers identity features from the Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models. Code is publicly available at https://github.com/weimengting/RigFace.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2412.19149.pdf' target='_blank'>https://arxiv.org/pdf/2412.19149.pdf</a></span>   <span><a href='https://github.com/liguohao96/EGG3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19149">Generating Editable Head Avatars with 3D Gaussian GANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at https://github.com/liguohao96/EGG3D.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2412.13983.pdf' target='_blank'>https://arxiv.org/pdf/2412.13983.pdf</a></span>   <span><a href='https://github.com/ucwxb/GraphAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobao Wei, Peng Chen, Ming Lu, Hui Chen, Feng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13983">GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size. The code will be released at: https://github.com/ucwxb/GraphAvatar
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2412.11599.pdf' target='_blank'>https://arxiv.org/pdf/2412.11599.pdf</a></span>   <span><a href='https://github.com/silence-tang/GaussianActor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Tang, Hongyu Yang, Hanchen Zhang, Jiaxin Chen, Di Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11599">3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic Gaussian Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in neural implicit representations and differentiable rendering have markedly improved the ability to learn animatable 3D avatars from sparse multi-view RGB videos. However, current methods that map observation space to canonical space often face challenges in capturing pose-dependent details and generalizing to novel poses. While diffusion models have demonstrated remarkable zero-shot capabilities in 2D image generation, their potential for creating animatable 3D avatars from 2D inputs remains underexplored. In this work, we introduce 3D$^2$-Actor, a novel approach featuring a pose-conditioned 3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D rectifying steps. The 2D denoiser, guided by pose cues, generates detailed multi-view images that provide the rich feature set necessary for high-fidelity 3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D rectifier renders images with enhanced 3D consistency through a two-stage projection strategy and a novel local coordinate representation. Additionally, we propose an innovative sampling strategy to ensure smooth temporal continuity across frames in video synthesis. Our method effectively addresses the limitations of traditional numerical solutions in handling ill-posed mappings, producing realistic and animatable 3D human avatars. Experimental results demonstrate that 3D$^2$-Actor excels in high-fidelity avatar modeling and robustly generalizes to novel poses. Code is available at: https://github.com/silence-tang/GaussianActor.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2412.04955.pdf' target='_blank'>https://arxiv.org/pdf/2412.04955.pdf</a></span>   <span><a href='https://github.com/ChenVoid/MGA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04955">MixedGaussianAvatar: Realistically and Geometrically Accurate Head Avatar via Mixed 2D-3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: https://github.com/ChenVoid/MGA/.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2411.16758.pdf' target='_blank'>https://arxiv.org/pdf/2411.16758.pdf</a></span>   <span><a href='https://github.com/MyNiuuu/BAGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyao Niu, Yifan Zhan, Qingtian Zhu, Zhuoxiao Li, Wei Wang, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16758">Bundle Adjusted Gaussian Avatars Deblurring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of 3D human avatars from multi-view videos represents a significant yet challenging task in the field. Recent advancements, including 3D Gaussian Splattings (3DGS), have markedly progressed this domain. Nonetheless, existing techniques necessitate the use of high-quality sharp images, which are often impractical to obtain in real-world settings due to variations in human motion speed and intensity. In this study, we attempt to explore deriving sharp intrinsic 3D human Gaussian avatars from blurry video footage in an end-to-end manner. Our approach encompasses a 3D-aware, physics-oriented model of blur formation attributable to human movement, coupled with a 3D human motion model to clarify ambiguities found in motion-induced blurry images. This methodology facilitates the concurrent learning of avatar model parameters and the refinement of sub-frame motion parameters from a coarse initialization. We have established benchmarks for this task through a synthetic dataset derived from existing multi-view captures, alongside a real-captured dataset acquired through a 360-degree synchronous hybrid-exposure camera system. Comprehensive evaluations demonstrate that our model surpasses existing baselines.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2411.09952.pdf' target='_blank'>https://arxiv.org/pdf/2411.09952.pdf</a></span>   <span><a href='https://github.com/J-X-Chen/GGAvatar/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingxuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09952">GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatar modelling has broad applications in human animation and virtual try-ons. Recent advancements in this field have focused on high-quality and comprehensive human reconstruction but often overlook the separation of clothing from the body. To bridge this gap, this paper introduces GGAvatar (Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular videos. Through advanced parameterized templates and unique phased training, this model effectively achieves decoupled, editable, and realistic reconstruction of clothed humans. Comparative evaluations with other costly models confirm GGAvatar's superior quality and efficiency in modelling both clothed humans and separable garments. The paper also showcases applications in clothing editing, as illustrated in Figure 1, highlighting the model's benefits and the advantages of effective disentanglement. The code is available at https://github.com/J-X-Chen/GGAvatar/.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2411.09066.pdf' target='_blank'>https://arxiv.org/pdf/2411.09066.pdf</a></span>   <span><a href='https://github.com/microsoft/P.910' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ross Cutler, Babak Naderi, Vishak Gopal, Dharmendar Palle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09066">A multidimensional measurement of photorealistic avatar quality of experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic avatars are human avatars that look, move, and talk like real people. The performance of photorealistic avatars has significantly improved recently based on objective metrics such as PSNR, SSIM, LPIPS, FID, and FVD. However, recent photorealistic avatar publications do not provide subjective tests of the avatars to measure human usability factors. We provide an open source test framework to subjectively measure photorealistic avatar performance in ten dimensions: realism, trust, comfortableness using, comfortableness interacting with, appropriateness for work, creepiness, formality, affinity, resemblance to the person, and emotion accuracy. Using telecommunication scenarios, we show that the correlation of nine of these subjective metrics with PSNR, SSIM, LPIPS, FID, and FVD is weak, and moderate for emotion accuracy. The crowdsourced subjective test framework is highly reproducible and accurate when compared to a panel of experts. We analyze a wide range of avatars from photorealistic to cartoon-like and show that some photorealistic avatars are approaching real video performance based on these dimensions. We also find that for avatars above a certain level of realism, eight of these measured dimensions are strongly correlated. This means that avatars that are not as realistic as real video will have lower trust, comfortableness using, comfortableness interacting with, appropriateness for work, formality, and affinity, and higher creepiness compared to real video. In addition, because there is a strong linear relationship between avatar affinity and realism, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario. We suggest several extensions of this test framework for future work and discuss design implications for telecommunication systems. The test framework is available at https://github.com/microsoft/P.910.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2411.04249.pdf' target='_blank'>https://arxiv.org/pdf/2411.04249.pdf</a></span>   <span><a href='https://github.com/sidsunny/pocoloco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharth Seth, Rishabh Dabral, Diogo Luvizon, Marc Habermann, Ming-Hsuan Yang, Christian Theobalt, Adam Kortylewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04249">PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling a human avatar that can plausibly deform to articulations is an active area of research. We present PocoLoco -- the first template-free, point-based, pose-conditioned generative model for 3D humans in loose clothing. We motivate our work by noting that most methods require a parametric model of the human body to ground pose-dependent deformations. Consequently, they are restricted to modeling clothing that is topologically similar to the naked body and do not extend well to loose clothing. The few methods that attempt to model loose clothing typically require either canonicalization or a UV-parameterization and need to address the challenging problem of explicitly estimating correspondences for the deforming clothes. In this work, we formulate avatar clothing deformation as a conditional point-cloud generation task within the denoising diffusion framework. Crucially, our framework operates directly on unordered point clouds, eliminating the need for a parametric model or a clothing template. This also enables a variety of practical applications, such as point-cloud completion and pose-based editing -- important features for virtual human animation. As current datasets for human avatars in loose clothing are far too small for training diffusion models, we release a dataset of two subjects performing various poses in loose clothing with a total of 75K point clouds. By contributing towards tackling the challenging task of effectively modeling loose clothing and expanding the available data for training these models, we aim to set the stage for further innovation in digital humans. The source code is available at https://github.com/sidsunny/pocoloco .
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2410.13726.pdf' target='_blank'>https://arxiv.org/pdf/2410.13726.pdf</a></span>   <span><a href='https://github.com/Hanbo-Cheng/DAWN-pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13726">DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2410.13360.pdf' target='_blank'>https://arxiv.org/pdf/2410.13360.pdf</a></span>   <span><a href='https://github.com/Hoar012/RAP-MLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13360">RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2410.12242.pdf' target='_blank'>https://arxiv.org/pdf/2410.12242.pdf</a></span>   <span><a href='https://github.com/LarsPh/EG-HumanNeRF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaorong Wang, Yoshihiro Kanamori, Yuki Endo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12242">EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior for Sparse View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable neural radiance field (NeRF) enables neural-based digital human rendering without per-scene retraining. When combined with human prior knowledge, high-quality human rendering can be achieved even with sparse input views. However, the inference of these methods is still slow, as a large number of neural network queries on each ray are required to ensure the rendering quality. Moreover, occluded regions often suffer from artifacts, especially when the input views are sparse. To address these issues, we propose a generalizable human NeRF framework that achieves high-quality and real-time rendering with sparse input views by extensively leveraging human prior knowledge. We accelerate the rendering with a two-stage sampling reduction strategy: first constructing boundary meshes around the human geometry to reduce the number of ray samples for sampling guidance regression, and then volume rendering using fewer guided samples. To improve rendering quality, especially in occluded regions, we propose an occlusion-aware attention mechanism to extract occlusion information from the human priors, followed by an image space refinement network to improve rendering quality. Furthermore, for volume rendering, we adopt a signed ray distance function (SRDF) formulation, which allows us to propose an SRDF loss at every sample position to improve the rendering quality further. Our experiments demonstrate that our method outperforms the state-of-the-art methods in rendering quality and has a competitive rendering speed compared with speed-prioritized novel view synthesis methods.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2410.08840.pdf' target='_blank'>https://arxiv.org/pdf/2410.08840.pdf</a></span>   <span><a href='https://github.com/XuanHuang0/GuassianHand' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Huang, Hanhui Li, Wanquan Liu, Xiaodan Liang, Yiqiang Yan, Yuhao Cheng, Chengqiang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08840">Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Project Page: \url{https://github.com/XuanHuang0/GuassianHand}.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2410.07971.pdf' target='_blank'>https://arxiv.org/pdf/2410.07971.pdf</a></span>   <span><a href='https://github.com/xg-chu/GAGAvatar,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xg-chu/GAGAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuangeng Chu, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07971">Generalizable and Animatable Gaussian Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Generalizable and Animatable Gaussian head Avatar (GAGAvatar) for one-shot animatable head avatar reconstruction. Existing methods rely on neural radiance fields, leading to heavy rendering consumption and low reenactment speeds. To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass. The key innovation of our work is the proposed dual-lifting method, which produces high-fidelity 3D Gaussians that capture identity and facial details. Additionally, we leverage global image features and the 3D morphable model to construct 3D Gaussians for controlling expressions. After training, our model can reconstruct unseen identities without specific optimizations and perform reenactment rendering at real-time speeds. Experiments show that our method exhibits superior performance compared to previous methods in terms of reconstruction quality and expression accuracy. We believe our method can establish new benchmarks for future research and advance applications of digital avatars. Code and demos are available https://github.com/xg-chu/GAGAvatar.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2410.03556.pdf' target='_blank'>https://arxiv.org/pdf/2410.03556.pdf</a></span>   <span><a href='https://github.com/baldoarbol/BodyShapeGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baldomero R. Ãrbol, Dan Casas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03556">BodyShapeGPT: SMPL Body Shape Manipulation with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI models provide a wide range of tools capable of performing complex tasks in a fraction of the time it would take a human. Among these, Large Language Models (LLMs) stand out for their ability to generate diverse texts, from literary narratives to specialized responses in different fields of knowledge. This paper explores the use of fine-tuned LLMs to identify physical descriptions of people, and subsequently create accurate representations of avatars using the SMPL-X model by inferring shape parameters. We demonstrate that LLMs can be trained to understand and manipulate the shape space of SMPL, allowing the control of 3D human shapes through natural language. This approach promises to improve human-machine interaction and opens new avenues for customization and simulation in virtual environments.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2409.18057.pdf' target='_blank'>https://arxiv.org/pdf/2409.18057.pdf</a></span>   <span><a href='https://github.com/MingSun-Tse/LightAvatar-TensorFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18057">LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have shown that neural radiance fields (NeRFs) on top of parametric models have reached SOTA quality to build photorealistic head avatars from a monocular video. However, one major limitation of the NeRF-based avatars is the slow rendering speed due to the dense point sampling of NeRF, preventing them from broader utility on resource-constrained devices. We introduce LightAvatar, the first head avatar model based on neural light fields (NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose via a single network forward pass, without using mesh or volume rendering. The proposed approach, while being conceptually appealing, poses a significant challenge towards real-time efficiency and training stability. To resolve them, we introduce dedicated network designs to obtain proper representations for the NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a distillation-based training strategy that uses a pretrained avatar model as teacher to synthesize abundant pseudo data for training. A warping field network is introduced to correct the fitting error in the real data so that the model can learn better. Extensive experiments suggest that our method can achieve new SOTA image quality quantitatively or qualitatively, while being significantly faster than the counterparts, reporting 174.1 FPS (512x512 resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2409.09149.pdf' target='_blank'>https://arxiv.org/pdf/2409.09149.pdf</a></span>   <span><a href='https://github.com/fuqifan/Region-Aware-Cycle-Loss' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifan Fu, Xiaohang Yang, Muhammad Asad, Changjae Oh, Shanxin Yuan, Gregory Slabaugh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09149">Adaptive Multi-Modal Control of Digital Human Hand Synthesis Using a Region-Aware Cycle Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have shown their remarkable ability to synthesize images, including the generation of humans in specific poses. However, current models face challenges in adequately expressing conditional control for detailed hand pose generation, leading to significant distortion in the hand regions. To tackle this problem, we first curate the How2Sign dataset to provide richer and more accurate hand pose annotations. In addition, we introduce adaptive, multi-modal fusion to integrate characters' physical features expressed in different modalities such as skeleton, depth, and surface normal. Furthermore, we propose a novel Region-Aware Cycle Loss (RACL) that enables the diffusion model training to focus on improving the hand region, resulting in improved quality of generated hand gestures. More specifically, the proposed RACL computes a weighted keypoint distance between the full-body pose keypoints from the generated image and the ground truth, to generate higher-quality hand poses while balancing overall pose accuracy. Moreover, we use two hand region metrics, named hand-PSNR and hand-Distance for hand pose generation evaluations. Our experimental evaluations demonstrate the effectiveness of our proposed approach in improving the quality of digital human pose generation using diffusion models, especially the quality of the hand region. The source code is available at https://github.com/fuqifan/Region-Aware-Cycle-Loss.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2409.01502.pdf' target='_blank'>https://arxiv.org/pdf/2409.01502.pdf</a></span>   <span><a href='https://github.com/zshyang/amg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangsihao Yang, Mengyi Shan, Mohammad Farazi, Wenhui Zhu, Yanxi Chen, Xuanzhao Dong, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01502">AMG: Avatar Motion Guided Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation task has gained significant attention with the advancement of deep generative models. Generating realistic videos with human movements is challenging in nature, due to the intricacies of human body topology and sensitivity to visual artifacts. The extensively studied 2D media generation methods take advantage of massive human media datasets, but struggle with 3D-aware control; whereas 3D avatar-based approaches, while offering more freedom in control, lack photorealism and cannot be harmonized seamlessly with background scene. We propose AMG, a method that combines the 2D photorealism and 3D controllability by conditioning video diffusion models on controlled rendering of 3D avatars. We additionally introduce a novel data processing pipeline that reconstructs and renders human avatar movements from dynamic camera videos. AMG is the first method that enables multi-person diffusion video generation with precise control over camera positions, human motions, and background style. We also demonstrate through extensive evaluation that it outperforms existing human video generation methods conditioned on pose sequences or driving videos in terms of realism and adaptability.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2408.15777.pdf' target='_blank'>https://arxiv.org/pdf/2408.15777.pdf</a></span>   <span><a href='https://github.com/wangyanckxx/SurveyFER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Wang, Shaoqi Yan, Yang Liu, Wei Song, Jing Liu, Yang Chang, Xinji Mai, Xiping Hu, Wenqiang Zhang, Zhongxue Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15777">A Survey on Facial Expression Recognition of Static and Dynamic Emotions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expression recognition (FER) aims to analyze emotional states from static images and dynamic sequences, which is pivotal in enhancing anthropomorphic communication among humans, robots, and digital avatars by leveraging AI technologies. As the FER field evolves from controlled laboratory environments to more complex in-the-wild scenarios, advanced methods have been rapidly developed and new challenges and apporaches are encounted, which are not well addressed in existing reviews of FER. This paper offers a comprehensive survey of both image-based static FER (SFER) and video-based dynamic FER (DFER) methods, analyzing from model-oriented development to challenge-focused categorization. We begin with a critical comparison of recent reviews, an introduction to common datasets and evaluation criteria, and an in-depth workflow on FER to establish a robust research foundation. We then systematically review representative approaches addressing eight main challenges in SFER (such as expression disturbance, uncertainties, compound emotions, and cross-domain inconsistency) as well as seven main challenges in DFER (such as key frame sampling, expression intensity variations, and cross-modal alignment). Additionally, we analyze recent advancements, benchmark performances, major applications, and ethical considerations. Finally, we propose five promising future directions and development trends to guide ongoing research. The project page for this paper can be found at https://github.com/wangyanckxx/SurveyFER.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2408.01218.pdf' target='_blank'>https://arxiv.org/pdf/2408.01218.pdf</a></span>   <span><a href='https://github.com/wang-zidu/S2TD-Face' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zidu Wang, Xiangyu Zhu, Jiang Yu, Tianshuo Zhang, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01218">S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from a Single Sketch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D textured face reconstruction from sketches applicable in many scenarios such as animation, 3D avatars, artistic design, missing people search, etc., is a highly promising but underdeveloped research topic. On the one hand, the stylistic diversity of sketches leads to existing sketch-to-3D-face methods only being able to handle pose-limited and realistically shaded sketches. On the other hand, texture plays a vital role in representing facial appearance, yet sketches lack this information, necessitating additional texture control in the reconstruction process. This paper proposes a novel method for reconstructing controllable textured and detailed 3D faces from sketches, named S2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework that directly reconstructs detailed geometry from the input sketch. To keep geometry consistent with the delicate strokes of the sketch, we propose a novel sketch-to-geometry loss that ensures the reconstruction accurately fits the input features like dimples and wrinkles. Our training strategies do not rely on hard-to-obtain 3D face scanning data or labor-intensive hand-drawn sketches. Furthermore, S2TD-Face introduces a texture control module utilizing text prompts to select the most suitable textures from a library and seamlessly integrate them into the geometry, resulting in a 3D detailed face with controllable texture. S2TD-Face surpasses existing state-of-the-art methods in extensive quantitative and qualitative experiments. Our project is available at https://github.com/wang-zidu/S2TD-Face .
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2407.21002.pdf' target='_blank'>https://arxiv.org/pdf/2407.21002.pdf</a></span>   <span><a href='https://github.com/agnJason/XHand' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Gan, Zijie Zhou, Jianke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21002">XHand: Real-time Expressive Hand Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand avatars play a pivotal role in a wide array of digital interfaces, enhancing user immersion and facilitating natural interaction within virtual environments. While previous studies have focused on photo-realistic hand rendering, little attention has been paid to reconstruct the hand geometry with fine details, which is essential to rendering quality. In the realms of extended reality and gaming, on-the-fly rendering becomes imperative. To this end, we introduce an expressive hand avatar, named XHand, that is designed to comprehensively generate hand shape, appearance, and deformations in real-time. To obtain fine-grained hand meshes, we make use of three feature embedding modules to predict hand deformation displacements, albedo, and linear blending skinning weights, respectively. To achieve photo-realistic hand rendering on fine-grained meshes, our method employs a mesh-based neural renderer by leveraging mesh topological consistency and latent codes from embedding modules. During training, a part-aware Laplace smoothing strategy is proposed by incorporating the distinct levels of regularization to effectively maintain the necessary details and eliminate the undesired artifacts. The experimental evaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy of XHand, which is able to recover high-fidelity geometry and texture for hand animations across diverse poses in real-time. To reproduce our results, we will make the full implementation publicly available at https://github.com/agnJason/XHand.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2407.12501.pdf' target='_blank'>https://arxiv.org/pdf/2407.12501.pdf</a></span>   <span><a href='https://github.com/SJTU-Lucy/EmoFace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Qunfen Lin, Zijiao Zeng, Ye Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12501">EmoFace: Audio-driven Emotional 3D Face Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven emotional 3D face animation aims to generate emotionally expressive talking heads with synchronized lip movements. However, previous research has often overlooked the influence of diverse emotions on facial expressions or proved unsuitable for driving MetaHuman models. In response to this deficiency, we introduce EmoFace, a novel audio-driven methodology for creating facial animations with vivid emotional dynamics. Our approach can generate facial expressions with multiple emotions, and has the ability to generate random yet natural blinks and eye movements, while maintaining accurate lip synchronization. We propose independent speech encoders and emotion encoders to learn the relationship between audio, emotion and corresponding facial controller rigs, and finally map into the sequence of controller values. Additionally, we introduce two post-processing techniques dedicated to enhancing the authenticity of the animation, particularly in blinks and eye movements. Furthermore, recognizing the scarcity of emotional audio-visual data suitable for MetaHuman model manipulation, we contribute an emotional audio-visual dataset and derive control parameters for each frames. Our proposed methodology can be applied in producing dialogues animations of non-playable characters (NPCs) in video games, and driving avatars in virtual reality environments. Our further quantitative and qualitative experiments, as well as an user study comparing with existing researches show that our approach demonstrates superior results in driving 3D facial models. The code and sample data are available at https://github.com/SJTU-Lucy/EmoFace.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2407.10707.pdf' target='_blank'>https://arxiv.org/pdf/2407.10707.pdf</a></span>   <span><a href='https://github.com/1231234zhan/InteractRAGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youyi Zhan, Tianjia Shao, He Wang, Yin Yang, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10707">Interactive Rendering of Relightable and Animatable Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating relightable and animatable avatars from multi-view or monocular videos is a challenging task for digital human creation and virtual reality applications. Previous methods rely on neural radiance fields or ray tracing, resulting in slow training and rendering processes. By utilizing Gaussian Splatting, we propose a simple and efficient method to decouple body materials and lighting from sparse-view or monocular avatar videos, so that the avatar can be rendered simultaneously under novel viewpoints, poses, and lightings at interactive frame rates (6.9 fps). Specifically, we first obtain the canonical body mesh using a signed distance function and assign attributes to each mesh vertex. The Gaussians in the canonical space then interpolate from nearby body mesh vertices to obtain the attributes. We subsequently deform the Gaussians to the posed space using forward skinning, and combine the learnable environment light with the Gaussian attributes for shading computation. To achieve fast shadow modeling, we rasterize the posed body mesh from dense viewpoints to obtain the visibility. Our approach is not only simple but also fast enough to allow interactive rendering of avatar animation under environmental light changes. Experiments demonstrate that, compared to previous works, our method can render higher quality results at a faster speed on both synthetic and real datasets.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2407.04345.pdf' target='_blank'>https://arxiv.org/pdf/2407.04345.pdf</a></span>   <span><a href='https://github.com/jsshin98/CanonicalFusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jisu Shin, Junmyeong Lee, Seongmin Lee, Min-Gyu Park, Ju-Mi Kang, Ju Hong Yoon, Hae-Gon Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04345">CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for reconstructing animatable human avatars from multiple images, termed CanonicalFusion. Our central concept involves integrating individual reconstruction results into the canonical space. To be specific, we first predict Linear Blend Skinning (LBS) weight maps and depth maps using a shared-encoder-dual-decoder network, enabling direct canonicalization of the 3D mesh from the predicted depth maps. Here, instead of predicting high-dimensional skinning weights, we infer compressed skinning weights, i.e., 3-dimensional vector, with the aid of pre-trained MLP networks. We also introduce a forward skinning-based differentiable rendering scheme to merge the reconstructed results from multiple images. This scheme refines the initial mesh by reposing the canonical mesh via the forward skinning and by minimizing photometric and geometric errors between the rendered and the predicted results. Our optimization scheme considers the position and color of vertices as well as the joint angles for each image, thereby mitigating the negative effects of pose errors. We conduct extensive experiments to demonstrate the effectiveness of our method and compare our CanonicalFusion with state-of-the-art methods. Our source codes are available at https://github.com/jsshin98/CanonicalFusion.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2407.02129.pdf' target='_blank'>https://arxiv.org/pdf/2407.02129.pdf</a></span>   <span><a href='https://github.com/MIV-XJTU/ReliaAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Qian, Zhenhuan Wei, Jiashuo Li, Xing Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02129">ReliaAvatar: A Robust Real-Time Avatar Animator with Integrated Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently estimating the full-body pose with minimal wearable devices presents a worthwhile research direction. Despite significant advancements in this field, most current research neglects to explore full-body avatar estimation under low-quality signal conditions, which is prevalent in practical usage. To bridge this gap, we summarize three scenarios that may be encountered in real-world applications: standard scenario, instantaneous data-loss scenario, and prolonged data-loss scenario, and propose a new evaluation benchmark. The solution we propose to address data-loss scenarios is integrating the full-body avatar pose estimation problem with motion prediction. Specifically, we present \textit{ReliaAvatar}, a real-time, \textbf{relia}ble \textbf{avatar} animator equipped with predictive modeling capabilities employing a dual-path architecture. ReliaAvatar operates effectively, with an impressive performance rate of 109 frames per second (fps). Extensive comparative evaluations on widely recognized benchmark datasets demonstrate Relia\-Avatar's superior performance in both standard and low data-quality conditions. The code is available at \url{https://github.com/MIV-XJTU/ReliaAvatar}.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2406.11200.pdf' target='_blank'>https://arxiv.org/pdf/2406.11200.pdf</a></span>   <span><a href='https://github.com/zou-group/avatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11200">AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2406.02880.pdf' target='_blank'>https://arxiv.org/pdf/2406.02880.pdf</a></span>   <span><a href='https://github.com/NetEase-Media/ControlTalk' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Zhao, Jiaying Shi, Wenjun Li, Shudong Wang, Shenghui Xu, Zhaoming Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02880">Controllable Talking Face Generation by Implicit Facial Keypoints Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking face generation has garnered significant interest within the domain of digital human research. Existing methods are encumbered by intricate model architectures that are intricately dependent on each other, complicating the process of re-editing image or video inputs. In this work, we present ControlTalk, a talking face generation method to control face expression deformation based on driven audio, which can construct the head pose and facial expression including lip motion for both single image or sequential video inputs in a unified manner. By utilizing a pre-trained video synthesis renderer and proposing the lightweight adaptation, ControlTalk achieves precise and naturalistic lip synchronization while enabling quantitative control over mouth opening shape. Our experiments show that our method is superior to state-of-the-art performance on widely used benchmarks, including HDTF and MEAD. The parameterized adaptation demonstrates remarkable generalization capabilities, effectively handling expression deformation across same-ID and cross-ID scenarios, and extending its utility to out-of-domain portraits, regardless of languages. Code is available at https://github.com/NetEase-Media/ControlTalk.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2405.14869.pdf' target='_blank'>https://arxiv.org/pdf/2405.14869.pdf</a></span>   <span><a href='https://github.com/YuliangXiu/PuzzleAvatar,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14869">PuzzleAvatar: Assembling 3D Avatars from Personal Albums</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating personalized 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if a user could just upload their personal "OOTD" (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel "Album2Human" task by developing PuzzleAvatar, a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, while bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into (separate) learned tokens and instilling these cues into the VLM. In effect, we exploit the learned tokens as "puzzle pieces" from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total of nearly 1K OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and strong robustness. Our code and data are publicly available for research purpose at https://puzzleavatar.is.tue.mpg.de/
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2405.03121.pdf' target='_blank'>https://arxiv.org/pdf/2405.03121.pdf</a></span>   <span><a href='https://github.com/X-LANCE/AniTalker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03121">AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait. Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation. This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements. AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders. This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data. Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations. This method not only demonstrates AniTalker's capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications. Synthetic results can be viewed at https://github.com/X-LANCE/AniTalker.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2404.13957.pdf' target='_blank'>https://arxiv.org/pdf/2404.13957.pdf</a></span>   <span><a href='https://github.com/CUHK-ARISE/ECHO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Man Tik Ng, Hui Tung Tse, Jen-tse Huang, Jingjing Li, Wenxuan Wang, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13957">How Well Can LLMs Echo Us? Evaluating AI Chatbots' Role-Play Ability with ECHO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The role-play ability of Large Language Models (LLMs) has emerged as a popular research direction. However, existing studies focus on imitating well-known public figures or fictional characters, overlooking the potential for simulating ordinary individuals. Such an oversight limits the potential for advancements in digital human clones and non-player characters in video games. To bridge this gap, we introduce ECHO, an evaluative framework inspired by the Turing test. This framework engages the acquaintances of the target individuals to distinguish between human and machine-generated responses. Notably, our framework focuses on emulating average individuals rather than historical or fictional figures, presenting a unique advantage to apply the Turing Test. We evaluated three role-playing LLMs using ECHO, with GPT-3.5 and GPT-4 serving as foundational models, alongside the online application GPTs from OpenAI. Our results demonstrate that GPT-4 more effectively deceives human evaluators, and GPTs achieves a leading success rate of 48.3%. Furthermore, we investigated whether LLMs could discern between human-generated and machine-generated texts. While GPT-4 can identify differences, it could not determine which texts were human-produced. Our code and results of reproducing the role-playing LLMs are made publicly available via https://github.com/CUHK-ARISE/ECHO.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2403.17881.pdf' target='_blank'>https://arxiv.org/pdf/2403.17881.pdf</a></span>   <span><a href='https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17881">Deepfake Generation and Detection: A Benchmark and Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential published works. Finally, we analyze challenges and future research directions of the discussed fields.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2403.16510.pdf' target='_blank'>https://arxiv.org/pdf/2403.16510.pdf</a></span>   <span><a href='https://github.com/ICTMCG/Make-Your-Anchor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16510">Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{https://github.com/ICTMCG/Make-Your-Anchor}.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2403.10335.pdf' target='_blank'>https://arxiv.org/pdf/2403.10335.pdf</a></span>   <span><a href='https://github.com/iSEE-Laboratory/NECA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10335">NECA: Neural Customizable Human Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at https://github.com/iSEE-Laboratory/NECA.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2403.08262.pdf' target='_blank'>https://arxiv.org/pdf/2403.08262.pdf</a></span>   <span><a href='https://github.com/yunminjin2/BiTT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minje Kim, Tae-Kyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08262">BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating personalized hand avatars is important to offer a realistic experience to users on AR / VR platforms. While most prior studies focused on reconstructing 3D hand shapes, some recent work has tackled the reconstruction of hand textures on top of shapes. However, these methods are often limited to capturing pixels on the visible side of a hand, requiring diverse views of the hand in a video or multiple images as input. In this paper, we propose a novel method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the first end-to-end trainable method for relightable, pose-free texture reconstruction of two interacting hands taking only a single RGB image, by three novel components: 1) bi-directional (left $\leftrightarrow$ right) texture reconstruction using the texture symmetry of left / right hands, 2) utilizing a texture parametric model for hand texture recovery, and 3) the overall coarse-to-fine stage pipeline for reconstructing personalized texture of two interacting hands. BiTT first estimates the scene light condition and albedo image from an input image, then reconstructs the texture of both hands through the texture parametric model and bi-directional texture reconstructor. In experiments using InterHand2.6M and RGB2Hands datasets, our method significantly outperforms state-of-the-art hand texture reconstruction methods quantitatively and qualitatively. The code is available at https://github.com/yunminjin2/BiTT
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2403.05087.pdf' target='_blank'>https://arxiv.org/pdf/2403.05087.pdf</a></span>   <span><a href='https://github.com/initialneil/SplattingAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05087">SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2402.06149.pdf' target='_blank'>https://arxiv.org/pdf/2402.06149.pdf</a></span>   <span><a href='https://github.com/ZhenglinZhou/HeadStudio' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06149">HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising results achieved with 2D diffusion priors, current methods struggle to create high-quality and consistent animated avatars efficiently. Previous animatable head models like FLAME have difficulty in accurately representing detailed texture and geometry. Additionally, high-quality 3D static representations face challenges in semantically driving with dynamic priors. In this paper, we introduce \textbf{HeadStudio}, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animatable avatars from text prompts. Firstly, we associate 3D Gaussians with animatable head prior model, facilitating semantic animation on high-quality 3D representations. To ensure consistent animation, we further enhance the optimization from initialization, distillation, and regularization to jointly learn the shape, texture, and animation. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. Moreover, These avatars can be smoothly driven by real-world speech and video. We hope that HeadStudio can enhance digital avatar creation and gain popularity in the community. Code is at: https://github.com/ZhenglinZhou/HeadStudio.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2401.12133.pdf' target='_blank'>https://arxiv.org/pdf/2401.12133.pdf</a></span>   <span><a href='https://github.com/KindOPSTAR/VRMN-bD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>He Zhang, Xinyang Li, Yuanxi Sun, Xinyi Fu, Christine Qiu, John M. Carroll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12133">VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2401.10215.pdf' target='_blank'>https://arxiv.org/pdf/2401.10215.pdf</a></span>   <span><a href='https://github.com/xg-chu/GPAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10215">GPAvatar: Generalizable and Precise Head Avatar from Image(s)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control expressions and postures. Existing methods, categorized into 2D-based warping, mesh-based, and neural rendering approaches, present challenges in maintaining multi-view consistency, incorporating non-facial information, and generalizing to new identities. In this paper, we propose a framework named GPAvatar that reconstructs 3D head avatars from one or several images in a single forward pass. The key idea of this work is to introduce a dynamic point-based expression field driven by a point cloud to precisely and effectively capture expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion module in the tri-planes canonical field to leverage information from multiple input images. The proposed method achieves faithful identity reconstruction, precise expression control, and multi-view consistency, demonstrating promising results for free-viewpoint rendering and novel view synthesis.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2401.04730.pdf' target='_blank'>https://arxiv.org/pdf/2401.04730.pdf</a></span>   <span><a href='https://github.com/FangyunWei/SLRT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronglai Zuo, Fangyun Wei, Zenggui Chen, Brian Mak, Jiaolong Yang, Xin Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04730">A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of this paper is to develop a functional system for translating spoken languages into sign languages, referred to as Spoken2Sign translation. The Spoken2Sign task is orthogonal and complementary to traditional sign language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign translation, we present a simple baseline consisting of three steps: 1) creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2) estimating a 3D sign for each sign video in the dictionary; 3) training a Spoken2Sign model, which is composed of a Text2Gloss translator, a sign connector, and a rendering module, with the aid of the yielded gloss-3D sign dictionary. The translation results are then displayed through a sign avatar. As far as we know, we are the first to present the Spoken2Sign task in an output format of 3D signs. In addition to its capability of Spoken2Sign translation, we also demonstrate that two by-products of our approach-3D keypoint augmentation and multi-view understanding-can assist in keypoint-based sign language understanding. Code and models are available at https://github.com/FangyunWei/SLRT.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2312.02222.pdf' target='_blank'>https://arxiv.org/pdf/2312.02222.pdf</a></span>   <span><a href='https://github.com/XChenZ/invertAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaochen Zhao, Jingxiang Sun, Lizhen Wang, Jinli Suo, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02222">InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While high fidelity and efficiency are central to the creation of digital head avatars, recent methods relying on 2D or 3D generative models often experience limitations such as shape distortion, expression inaccuracy, and identity flickering. Additionally, existing one-shot inversion techniques fail to fully leverage multiple input images for detailed feature extraction. We propose a novel framework, \textbf{Incremental 3D GAN Inversion}, that enhances avatar reconstruction performance using an algorithm designed to increase the fidelity from multiple frames, resulting in improved reconstruction quality proportional to frame count. Our method introduces a unique animatable 3D GAN prior with two crucial modifications for enhanced expression controllability alongside an innovative neural texture encoder that categorizes texture feature spaces based on UV parameterization. Differentiating from traditional techniques, our architecture emphasizes pixel-aligned image-to-image translation, mitigating the need to learn correspondences between observation and canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent networks for temporal data aggregation from multiple frames, boosting geometry and texture detail reconstruction. The proposed paradigm demonstrates state-of-the-art performance on one-shot and few-shot avatar animation tasks. Code will be available at https://github.com/XChenZ/invertAvatar.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2312.01632.pdf' target='_blank'>https://arxiv.org/pdf/2312.01632.pdf</a></span>   <span><a href='https://github.com/chiehwangs/gaussian-head' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Wang, Jiu-Cheng Xie, Xianyan Li, Feng Xu, Chi-Man Pun, Hao Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01632">GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing vivid 3D head avatars for given subjects and realizing a series of animations on them is valuable yet challenging. This paper presents GaussianHead, which models the actional human head with anisotropic 3D Gaussians. In our framework, a motion deformation field and multi-resolution tri-plane are constructed respectively to deal with the head's dynamic geometry and complex texture. Notably, we impose an exclusive derivation scheme on each Gaussian, which generates its multiple doppelgangers through a set of learnable parameters for position transformation. With this design, we can compactly and accurately encode the appearance information of Gaussians, even those fitting the head's particular components with sophisticated structures. In addition, an inherited derivation strategy for newly added Gaussians is adopted to facilitate training acceleration. Extensive experiments show that our method can produce high-fidelity renderings, outperforming state-of-the-art approaches in reconstruction, cross-identity reenactment, and novel view synthesis tasks. Our code is available at: https://github.com/chiehwangs/gaussian-head.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2311.17910.pdf' target='_blank'>https://arxiv.org/pdf/2311.17910.pdf</a></span>   <span><a href='https://github.com/apple/ml-hugs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17910">HUGS: Human Gaussian Splats</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2310.20251.pdf' target='_blank'>https://arxiv.org/pdf/2310.20251.pdf</a></span>   <span><a href='https://github.com/zyj-2000/CUMT_2D_PhotoSpeaker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Zhou, Yaodong Chen, Kaiyue Bi, Lian Xiong, Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20251">An Implementation of Multimodal Fusion System for Intelligent Digital Human Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of artificial intelligence (AI), digital humans have attracted more and more attention and are expected to achieve a wide range of applications in several industries. Then, most of the existing digital humans still rely on manual modeling by designers, which is a cumbersome process and has a long development cycle. Therefore, facing the rise of digital humans, there is an urgent need for a digital human generation system combined with AI to improve development efficiency. In this paper, an implementation scheme of an intelligent digital human generation system with multimodal fusion is proposed. Specifically, text, speech and image are taken as inputs, and interactive speech is synthesized using large language model (LLM), voiceprint extraction, and text-to-speech conversion techniques. Then the input image is age-transformed and a suitable image is selected as the driving image. Then, the modification and generation of digital human video content is realized by digital human driving, novel view synthesis, and intelligent dressing techniques. Finally, we enhance the user experience through style transfer, super-resolution, and quality evaluation. Experimental results show that the system can effectively realize digital human generation. The related code is released at https://github.com/zyj-2000/CUMT_2D_PhotoSpeaker.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2310.05215.pdf' target='_blank'>https://arxiv.org/pdf/2310.05215.pdf</a></span>   <span><a href='https://github.com/JLPM22/MotionMatching' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05215">Motion Matching for Character Animation and Virtual Reality Avatars in Unity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time animation of virtual characters has traditionally been accomplished by playing short sequences of animations structured in the form of a graph. These methods are time-consuming to set up and scale poorly with the number of motions required in modern virtual environments. The ever-increasing need for highly-realistic virtual characters in fields such as entertainment, virtual reality, or the metaverse has led to significant advances in the field of data-driven character animation. Techniques like Motion Matching have provided enough versatility to conveniently animate virtual characters using a selection of features from an animation database. Data-driven methods retain the quality of the captured animations, thus delivering smoother and more natural-looking animations. In this work, we researched and developed a Motion Matching technique for the Unity game engine. In this thesis, we present our findings on how to implement an animation system based on Motion Matching. We also introduce a novel method combining body orientation prediction with Motion Matching to animate avatars for consumer-grade virtual reality systems.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2309.13524.pdf' target='_blank'>https://arxiv.org/pdf/2309.13524.pdf</a></span>   <span><a href='https://github.com/River-Zhang/GTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13524">Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D clothed human avatars from single images is a challenging task, especially when encountering complex poses and loose clothing. Current methods exhibit limitations in performance, largely attributable to their dependence on insufficient 2D image features and inconsistent query methods. Owing to this, we present the Global-correlated 3D-decoupling Transformer for clothed Avatar reconstruction (GTA), a novel transformer-based architecture that reconstructs clothed human avatars from monocular images. Our approach leverages transformer architectures by utilizing a Vision Transformer model as an encoder for capturing global-correlated image features. Subsequently, our innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane features, using learnable embeddings as queries for cross-plane generation. To effectively enhance feature fusion with the tri-plane 3D feature and human body prior, we propose a hybrid prior fusion strategy combining spatial and prior-enhanced queries, leveraging the benefits of spatial localization and human body prior knowledge. Comprehensive experiments on CAPE and THuman2.0 datasets illustrate that our method outperforms state-of-the-art approaches in both geometry and texture reconstruction, exhibiting high robustness to challenging poses and loose clothing, and producing higher-resolution textures. Codes will be available at https://github.com/River-Zhang/GTA.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2309.02434.pdf' target='_blank'>https://arxiv.org/pdf/2309.02434.pdf</a></span>   <span><a href='https://github.com/arthur-qiu/ReliTalk' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Qiu, Zhaoxi Chen, Yuming Jiang, Hang Zhou, Xiangyu Fan, Lei Yang, Wayne Wu, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02434">ReliTalk: Relightable Talking Portrait Generation from a Single Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed great progress in creating vivid audio-driven portraits from monocular videos. However, how to seamlessly adapt the created video avatars to other scenarios with different backgrounds and lighting conditions remains unsolved. On the other hand, existing relighting studies mostly rely on dynamically lighted or multi-view data, which are too expensive for creating video portraits. To bridge this gap, we propose ReliTalk, a novel framework for relightable audio-driven talking portrait generation from monocular videos. Our key insight is to decompose the portrait's reflectance from implicitly learned audio-driven facial normals and images. Specifically, we involve 3D facial priors derived from audio features to predict delicate normal maps through implicit functions. These initially predicted normals then take a crucial part in reflectance decomposition by dynamically estimating the lighting condition of the given video. Moreover, the stereoscopic face representation is refined using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video. Extensive experiments validate the superiority of our proposed framework on both real and synthetic datasets. Our code is released in https://github.com/arthur-qiu/ReliTalk.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2307.09153.pdf' target='_blank'>https://arxiv.org/pdf/2307.09153.pdf</a></span>   <span><a href='https://github.com/lsx0101/OPHAvatars' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoxu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09153">OPHAvatars: One-shot Photo-realistic Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method for synthesizing photo-realistic digital avatars from only one portrait as the reference. Given a portrait, our method synthesizes a coarse talking head video using driving keypoints features. And with the coarse video, our method synthesizes a coarse talking head avatar with a deforming neural radiance field. With rendered images of the coarse avatar, our method updates the low-quality images with a blind face restoration model. With updated images, we retrain the avatar for higher quality. After several iterations, our method can synthesize a photo-realistic animatable 3D neural head avatar. The motivation of our method is deformable neural radiance field can eliminate the unnatural distortion caused by the image2video method. Our method outperforms state-of-the-art methods in quantitative and qualitative studies on various subjects.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2307.02808.pdf' target='_blank'>https://arxiv.org/pdf/2307.02808.pdf</a></span>   <span><a href='https://github.com/zzc-1998/SJTU-H3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Sun, Yingjie Zhou, Haoning Wu, Chunyi Li, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02808">Advancing Zero-Shot Digital Human Quality Assessment through Text-Prompted Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zero-shot performance. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2306.02903.pdf' target='_blank'>https://arxiv.org/pdf/2306.02903.pdf</a></span>   <span><a href='https://github.com/lsx0101/Instruct-Video2Avatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoxu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02903">Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method for synthesizing edited photo-realistic digital avatars with text instructions. Given a short monocular RGB video and text instructions, our method uses an image-conditioned diffusion model to edit one head image and uses the video stylization method to accomplish the editing of other head images. Through iterative training and update (three times or more), our method synthesizes edited photo-realistic animatable 3D neural head avatars with a deformable neural radiance field head synthesis method. In quantitative and qualitative studies on various subjects, our method outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2305.19012.pdf' target='_blank'>https://arxiv.org/pdf/2305.19012.pdf</a></span>   <span><a href='https://github.com/icoz69/StyleAvatar3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19012">StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the diversity of the generated avatars. Additionally, we develop a latent diffusion model within the style space of StyleGAN to enable the generation of avatars based on image inputs. Our approach demonstrates superior performance over current state-of-the-art methods in terms of visual quality and diversity of the produced avatars.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2305.13353.pdf' target='_blank'>https://arxiv.org/pdf/2305.13353.pdf</a></span>   <span><a href='https://github.com/RenderMe-360/RenderMe-360' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwei Pan, Long Zhuo, Jingtan Piao, Huiwen Luo, Wei Cheng, Yuxin Wang, Siming Fan, Shengqi Liu, Lei Yang, Bo Dai, Ziwei Liu, Chen Change Loy, Chen Qian, Wayne Wu, Dahua Lin, Kwan-Yee Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13353">RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing high-fidelity head avatars is a central problem for computer vision and graphics. While head avatar synthesis algorithms have advanced rapidly, the best ones still face great obstacles in real-world scenarios. One of the vital causes is inadequate datasets -- 1) current public datasets can only support researchers to explore high-fidelity head avatars in one or two task directions; 2) these datasets usually contain digital head assets with limited data volume, and narrow distribution over different attributes. In this paper, we present RenderMe-360, a comprehensive 4D human head dataset to drive advance in head avatar research. It contains massive data assets, with 243+ million complete head frames, and over 800k video sequences from 500 different identities captured by synchronized multi-view cameras at 30 FPS. It is a large-scale digital library for head avatars with three key attributes: 1) High Fidelity: all subjects are captured by 60 synchronized, high-resolution 2K cameras in 360 degrees. 2) High Diversity: The collected subjects vary from different ages, eras, ethnicities, and cultures, providing abundant materials with distinctive styles in appearance and geometry. Moreover, each subject is asked to perform various motions, such as expressions and head rotations, which further extend the richness of assets. 3) Rich Annotations: we provide annotations with different granularities: cameras' parameters, matting, scan, 2D/3D facial landmarks, FLAME fitting, and text description.
  Based on the dataset, we build a comprehensive benchmark for head avatar research, with 16 state-of-the-art methods performed on five main tasks: novel view synthesis, novel expression synthesis, hair rendering, hair editing, and talking head generation. Our experiments uncover the strengths and weaknesses of current methods. RenderMe-360 opens the door for future exploration in head avatars.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2305.03302.pdf' target='_blank'>https://arxiv.org/pdf/2305.03302.pdf</a></span>   <span><a href='https://github.com/zhuhao-nju/describe3d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Menghua Wu, Hao Zhu, Linjia Huang, Yiyu Zhuang, Yuanxun Lu, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03302">High-Fidelity 3D Face Generation from Natural Language Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing high-quality 3D face models from natural language descriptions is very valuable for many applications, including avatar creation, virtual reality, and telepresence. However, little research ever tapped into this task. We argue the major obstacle lies in 1) the lack of high-quality 3D face data with descriptive text annotation, and 2) the complex mapping relationship between descriptive language space and shape/appearance space. To solve these problems, we build Describe3D dataset, the first large-scale dataset with fine-grained text descriptions for text-to-3D face generation task. Then we propose a two-stage framework to first generate a 3D face that matches the concrete descriptions, then optimize the parameters in the 3D shape and texture space with abstract description to refine the 3D face model. Extensive experimental results show that our method can produce a faithful 3D face that conforms to the input descriptions with higher accuracy and quality than previous methods. The code and Describe3D dataset are released at https://github.com/zhuhao-nju/describe3d .
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2305.02722.pdf' target='_blank'>https://arxiv.org/pdf/2305.02722.pdf</a></span>   <span><a href='https://github.com/Gumpest/AvatarKD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhang, Weihua Chen, Yichen Lu, Tao Huang, Xiuyu Sun, Jian Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02722">Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation is an effective paradigm for boosting the performance of pocket-size model, especially when multiple teacher models are available, the student would break the upper limit again. However, it is not economical to train diverse teacher models for the disposable distillation. In this paper, we introduce a new concept dubbed Avatars for distillation, which are the inference ensemble models derived from the teacher. Concretely, (1) For each iteration of distillation training, various Avatars are generated by a perturbation transformation. We validate that Avatars own higher upper limit of working capacity and teaching ability, aiding the student model in learning diverse and receptive knowledge perspectives from the teacher model. (2) During the distillation, we propose an uncertainty-aware factor from the variance of statistical differences between the vanilla teacher and Avatars, to adjust Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge Distillation AKD is fundamentally different from existing methods and refines with the innovative view of unequal training. Comprehensive experiments demonstrate the effectiveness of our Avatars mechanism, which polishes up the state-of-the-art distillation methods for dense prediction without more extra computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation, respectively. Code is available at https://github.com/Gumpest/AvatarKD.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2305.00942.pdf' target='_blank'>https://arxiv.org/pdf/2305.00942.pdf</a></span>   <span><a href='https://github.com/LizhenWangT/StyleAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lizhen Wang, Xiaochen Zhao, Jingxiang Sun, Yuxiang Zhang, Hongwen Zhang, Tao Yu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00942">StyleAvatar: Real-time Photo-realistic Portrait Avatar from a Single Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face reenactment methods attempt to restore and re-animate portrait videos as realistically as possible. Existing methods face a dilemma in quality versus controllability: 2D GAN-based methods achieve higher image quality but suffer in fine-grained control of facial attributes compared with 3D counterparts. In this work, we propose StyleAvatar, a real-time photo-realistic portrait avatar reconstruction method using StyleGAN-based networks, which can generate high-fidelity portrait avatars with faithful expression control. We expand the capabilities of StyleGAN by introducing a compositional representation and a sliding window augmentation method, which enable faster convergence and improve translation generalization. Specifically, we divide the portrait scenes into three parts for adaptive adjustments: facial region, non-facial foreground region, and the background. Besides, our network leverages the best of UNet, StyleGAN and time coding for video learning, which enables high-quality video generation. Furthermore, a sliding window augmentation method together with a pre-training strategy are proposed to improve translation generalization and training performance, respectively. The proposed network can converge within two hours while ensuring high image quality and a forward rendering time of only 20 milliseconds. Furthermore, we propose a real-time live system, which further pushes research into applications. Results and experiments demonstrate the superiority of our method in terms of image quality, full portrait video generation, and real-time re-animation compared to existing facial reenactment methods. Training and inference code for this paper are at https://github.com/LizhenWangT/StyleAvatar.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2304.13006.pdf' target='_blank'>https://arxiv.org/pdf/2304.13006.pdf</a></span>   <span><a href='https://github.com/lizhe00/posevocab' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lizhe00/PoseVocab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13006">PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating pose-driven human avatars is about modeling the mapping from the low-frequency driving pose to high-frequency dynamic human appearances, so an effective pose encoding method that can encode high-fidelity human details is essential to human avatar modeling. To this end, we present PoseVocab, a novel pose encoding method that encourages the network to discover the optimal pose embeddings for learning the dynamic human appearance. Given multi-view RGB videos of a character, PoseVocab constructs key poses and latent embeddings based on the training poses. To achieve pose generalization and temporal consistency, we sample key rotations in $so(3)$ of each joint rather than the global pose vectors, and assign a pose embedding to each sampled key rotation. These joint-structured pose embeddings not only encode the dynamic appearances under different key poses, but also factorize the global pose embedding into joint-structured ones to better learn the appearance variation related to the motion of each joint. To improve the representation ability of the pose embedding while maintaining memory efficiency, we introduce feature lines, a compact yet effective 3D representation, to model more fine-grained details of human appearances. Furthermore, given a query pose and a spatial position, a hierarchical query strategy is introduced to interpolate pose embeddings and acquire the conditional pose feature for dynamic human synthesis. Overall, PoseVocab effectively encodes the dynamic details of human appearance and enables realistic and generalized animation under novel poses. Experiments show that our method outperforms other state-of-the-art baselines both qualitatively and quantitatively in terms of synthesis quality. Code is available at https://github.com/lizhe00/PoseVocab.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2304.07849.pdf' target='_blank'>https://arxiv.org/pdf/2304.07849.pdf</a></span>   <span><a href='https://github.com/X-PLUG/ChatPLUG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junfeng Tian, Hehong Chen, Guohai Xu, Ming Yan, Xing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu, Wenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qinghao Ye, Jiejing Zhang, Ji Zhang, Fei Huang, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07849">ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for digital human applications that instruction finetunes on a wide range of dialogue tasks in a unified internet-augmented format. Different from other open-domain dialogue models that focus on large-scale pre-training and scaling up model size or dialogue corpus, we aim to build a powerful and practical dialogue system for digital human with diverse skills and good multi-task generalization by internet-augmented instruction tuning. To this end, we first conduct large-scale pre-training on both common document corpus and dialogue data with curriculum learning, so as to inject various world knowledge and dialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue tasks spanning diverse features of knowledge, personality, multi-turn memory, and empathy, on which we further instruction tune \modelname via unified natural language instruction templates. External knowledge from an internet search is also used during instruction finetuning for alleviating the problem of knowledge hallucinations. We show that \modelname outperforms state-of-the-art Chinese dialogue systems on both automatic and human evaluation, and demonstrates strong multi-task generalization on a variety of text understanding and generation tasks. In addition, we deploy \modelname to real-world applications such as Smart Speaker and Instant Message applications with fast inference. Our models and code will be made publicly available on ModelScope: https://modelscope.cn/models/damo/ChatPLUG-3.7B and Github: https://github.com/X-PLUG/ChatPLUG .
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2304.01186.pdf' target='_blank'>https://arxiv.org/pdf/2304.01186.pdf</a></span>   <span><a href='https://github.com/mayuelala/FollowYourPose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01186">Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating text-editable and pose-controllable character videos have an imperious demand in creating various digital human. Nevertheless, this task has been restricted by the absence of a comprehensive dataset featuring paired video-pose captions and the generative prior models for videos. In this work, we design a novel two-stage training scheme that can utilize easily obtained datasets (i.e.,image pose pair and pose-free video) and the pre-trained text-to-image (T2I) model to obtain the pose-controllable character videos. Specifically, in the first stage, only the keypoint-image pairs are used only for a controllable text-to-image generation. We learn a zero-initialized convolutional encoder to encode the pose information. In the second stage, we finetune the motion of the above network via a pose-free video dataset by adding the learnable temporal self-attention and reformed cross-frame self-attention blocks. Powered by our new designs, our method successfully generates continuously pose-controllable character videos while keeps the editing and concept composition ability of the pre-trained T2I model. The code and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2303.14662.pdf' target='_blank'>https://arxiv.org/pdf/2303.14662.pdf</a></span>   <span><a href='https://github.com/theEricMa/OTAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Zhen Lei, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14662">OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllability, generalizability and efficiency are the major objectives of constructing face avatars represented by neural implicit field. However, existing methods have not managed to accommodate the three requirements simultaneously. They either focus on static portraits, restricting the representation ability to a specific subject, or suffer from substantial computational cost, limiting their flexibility. In this paper, we propose One-shot Talking face Avatar (OTAvatar), which constructs face avatars by a generalized controllable tri-plane rendering solution so that each personalized avatar can be constructed from only one portrait as the reference. Specifically, OTAvatar first inverts a portrait image to a motion-free identity code. Second, the identity code and a motion code are utilized to modulate an efficient CNN to generate a tri-plane formulated volume, which encodes the subject in the desired motion. Finally, volume rendering is employed to generate an image in any view. The core of our solution is a novel decoupling-by-inverting strategy that disentangles identity and motion in the latent code via optimization-based inversion. Benefiting from the efficient tri-plane representation, we achieve controllable rendering of generalized face avatar at $35$ FPS on A100. Experiments show promising performance of cross-identity reenactment on subjects out of the training set and better 3D consistency.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2303.09119.pdf' target='_blank'>https://arxiv.org/pdf/2303.09119.pdf</a></span>   <span><a href='https://github.com/Advocate99/DiffGesture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, Lequan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09119">Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual avatars to make co-speech gestures facilitates various applications in human-machine interaction. The existing methods mainly rely on generative adversarial networks (GANs), which typically suffer from notorious mode collapse and unstable training, thus making it difficult to learn accurate audio-gesture joint distributions. In this work, we propose a novel diffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to effectively capture the cross-modal audio-to-gesture associations and preserve temporal coherence for high-fidelity audio-driven co-speech gesture generation. Specifically, we first establish the diffusion-conditional generation process on clips of skeleton sequences and audio to enable the whole framework. Then, a novel Diffusion Audio-Gesture Transformer is devised to better attend to the information from multiple modalities and model the long-term temporal dependency. Moreover, to eliminate temporal inconsistency, we propose an effective Diffusion Gesture Stabilizer with an annealed noise sampling strategy. Benefiting from the architectural advantages of diffusion models, we further incorporate implicit classifier-free guidance to trade off between diversity and gesture quality. Extensive experiments demonstrate that DiffGesture achieves state-of-theart performance, which renders coherent gestures with better mode coverage and stronger audio correlations. Code is available at https://github.com/Advocate99/DiffGesture.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2303.08500.pdf' target='_blank'>https://arxiv.org/pdf/2303.08500.pdf</a></span>   <span><a href='https://github.com/hmdolatabadi/AVATAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08500">The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: https://github.com/hmdolatabadi/AVATAR.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2303.01765.pdf' target='_blank'>https://arxiv.org/pdf/2303.01765.pdf</a></span>   <span><a href='https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Chen Liu, Muyi Sun, Lincheng Li, Changjie Fan, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01765">Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting natural and diverse 3D hand gestures from the upper body dynamics is a practical yet challenging task in virtual avatar creation. Previous works usually overlook the asymmetric motions between two hands and generate two hands in a holistic manner, leading to unnatural results. In this work, we introduce a novel bilateral hand disentanglement based two-stage 3D hand generation method to achieve natural and diverse 3D hand prediction from body dynamics. In the first stage, we intend to generate natural hand gestures by two hand-disentanglement branches. Considering the asymmetric gestures and motions of two hands, we introduce a Spatial-Residual Memory (SRM) module to model spatial interaction between the body and each hand by residual learning. To enhance the coordination of two hand motions wrt. body dynamics holistically, we then present a Temporal-Motion Memory (TMM) module. TMM can effectively model the temporal association between body dynamics and two hand motions. The second stage is built upon the insight that 3D hand predictions should be non-deterministic given the sequential body postures. Thus, we further diversify our 3D hand predictions based on the initial output from the stage one. Concretely, we propose a Prototypical-Memory Sampling Strategy (PSS) to generate the non-deterministic hand gestures by gradient-based Markov Chain Monte Carlo (MCMC) sampling. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on the B2H dataset and our newly collected TED Hands dataset. The dataset and code are available at https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2302.03397.pdf' target='_blank'>https://arxiv.org/pdf/2302.03397.pdf</a></span>   <span><a href='https://github.com/loong8888/AniPixel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Fan, Jing Zhang, Zhi Hou, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03397">AniPixel: Towards Animatable Pixel-Aligned Human Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although human reconstruction typically results in human-specific avatars, recent 3D scene reconstruction techniques utilizing pixel-aligned features show promise in generalizing to new scenes. Applying these techniques to human avatar reconstruction can result in a volumetric avatar with generalizability but limited animatability due to rendering only being possible for static representations. In this paper, we propose AniPixel, a novel animatable and generalizable human avatar reconstruction method that leverages pixel-aligned features for body geometry prediction and RGB color blending. Technically, to align the canonical space with the target space and the observation space, we propose a bidirectional neural skinning field based on skeleton-driven deformation to establish the target-to-canonical and canonical-to-observation correspondences. Then, we disentangle the canonical body geometry into a normalized neutral-sized body and a subject-specific residual for better generalizability. As the geometry and appearance are closely related, we introduce pixel-aligned features to facilitate the body geometry prediction and detailed surface normals to reinforce the RGB color blending. We also devise a pose-dependent and view direction-related shading module to represent the local illumination variance. Experiments show that AniPixel renders comparable novel views while delivering better novel pose animation results than state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2212.08377.pdf' target='_blank'>https://arxiv.org/pdf/2212.08377.pdf</a></span>   <span><a href='https://github.com/zhengyuf/pointavatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08377">PointAvatar: Deformable Point-based Head Avatars from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to create realistic, animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting in the color estimation, thus they are limited in re-rendering the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2209.09489.pdf' target='_blank'>https://arxiv.org/pdf/2209.09489.pdf</a></span>   <span><a href='https://github.com/zzc-1998/DHHQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Yuzhe Wu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.09489">Perceptual Quality Assessment for Digital Human Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital humans are attracting more and more research interest during the last decade, the generation, representation, rendering, and animation of which have been put into large amounts of effort. However, the quality assessment of digital humans has fallen behind. Therefore, to tackle the challenge of digital human quality assessment issues, we propose the first large-scale quality assessment database for three-dimensional (3D) scanned digital human heads (DHHs). The constructed database consists of 55 reference DHHs and 1,540 distorted DHHs along with the subjective perceptual ratings. Then, a simple yet effective full-reference (FR) projection-based method is proposed to evaluate the visual quality of DHHs. The pretrained Swin Transformer tiny is employed for hierarchical feature extraction and the multi-head attention module is utilized for feature fusion. The experimental results reveal that the proposed method exhibits state-of-the-art performance among the mainstream FR metrics. The database is released at https://github.com/zzc-1998/DHHQA.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2207.14425.pdf' target='_blank'>https://arxiv.org/pdf/2207.14425.pdf</a></span>   <span><a href='https://github.com/hwang1996/3D-Cartoon-Face-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Wenhao Shen, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.14425">3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting conditions by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting conditions, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2207.11243.pdf' target='_blank'>https://arxiv.org/pdf/2207.11243.pdf</a></span>   <span><a href='https://github.com/facebookresearch/multiface' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan Bali, Danielle Belko, Eric Brockmeyer, Lucas Evans, Timothy Godisart, Hyowon Ha, Xuhua Huang, Alexander Hypes, Taylor Koska, Steven Krenn, Stephen Lombardi, Xiaomin Luo, Kevyn McPhail, Laura Millerschoen, Michal Perdoch, Mark Pitts, Alexander Richard, Jason Saragih, Junko Saragih, Takaaki Shiratori, Tomas Simon, Matt Stewart, Autumn Trimble, Xinshuo Weng, David Whitewolf, Chenglei Wu, Shoou-I Yu, Yaser Sheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.11243">Multiface: A Dataset for Neural Face Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic avatars of human faces have come a long way in recent years, yet research along this area is limited by a lack of publicly available, high-quality datasets covering both, dense multi-view camera captures, and rich facial expressions of the captured subjects. In this work, we present Multiface, a new multi-view, high-resolution human face dataset collected from 13 identities at Reality Labs Research for neural face rendering. We introduce Mugsy, a large scale multi-camera apparatus to capture high-resolution synchronized videos of a facial performance. The goal of Multiface is to close the gap in accessibility to high quality data in the academic community and to enable research in VR telepresence. Along with the release of the dataset, we conduct ablation studies on the influence of different model architectures toward the model's interpolation capacity of novel viewpoint and expressions. With a conditional VAE model serving as our baseline, we found that adding spatial bias, texture warp field, and residual connections improves performance on novel view synthesis. Our code and data is available at: https://github.com/facebookresearch/multiface
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2310.16732.pdf' target='_blank'>https://arxiv.org/pdf/2310.16732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiongkuo Min, Xianghe Ma, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16732">A No-Reference Quality Assessment Method for Digital Human Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, digital humans have been widely applied in augmented/virtual reality (A/VR), where viewers are allowed to freely observe and interact with the volumetric content. However, the digital humans may be degraded with various distortions during the procedure of generation and transmission. Moreover, little effort has been put into the perceptual quality assessment of digital humans. Therefore, it is urgent to carry out objective quality assessment methods to tackle the challenge of digital human quality assessment (DHQA). In this paper, we develop a novel no-reference (NR) method based on Transformer to deal with DHQA in a multi-task manner. Specifically, the front 2D projections of the digital humans are rendered as inputs and the vision transformer (ViT) is employed for the feature extraction. Then we design a multi-task module to jointly classify the distortion types and predict the perceptual quality levels of digital humans. The experimental results show that the proposed method well correlates with the subjective ratings and outperforms the state-of-the-art quality assessment methods.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2310.15984.pdf' target='_blank'>https://arxiv.org/pdf/2310.15984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15984">Geometry-Aware Video Quality Assessment for Dynamic Digital Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Digital Humans (DDHs) are 3D digital models that are animated using predefined motions and are inevitably bothered by noise/shift during the generation process and compression distortion during the transmission process, which needs to be perceptually evaluated. Usually, DDHs are displayed as 2D rendered animation videos and it is natural to adapt video quality assessment (VQA) methods to DDH quality assessment (DDH-QA) tasks. However, the VQA methods are highly dependent on viewpoints and less sensitive to geometry-based distortions. Therefore, in this paper, we propose a novel no-reference (NR) geometry-aware video quality assessment method for DDH-QA challenge. Geometry characteristics are described by the statistical parameters estimated from the DDHs' geometry attribute distributions. Spatial and temporal features are acquired from the rendered videos. Finally, all kinds of features are integrated and regressed into quality values. Experimental results show that the proposed method achieves state-of-the-art performance on the DDH-QA database.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2212.12734.pdf' target='_blank'>https://arxiv.org/pdf/2212.12734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Wei Sun, Wei Lu, Xiongkuo Min, Yu Wang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.12734">DDH-QA: A Dynamic Digital Humans Quality Assessment Database</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, large amounts of effort have been put into pushing forward the real-world application of dynamic digital human (DDH). However, most current quality assessment research focuses on evaluating static 3D models and usually ignores motion distortions. Therefore, in this paper, we construct a large-scale dynamic digital human quality assessment (DDH-QA) database with diverse motion content as well as multiple distortions to comprehensively study the perceptual quality of DDHs. Both model-based distortion (noise, compression) and motion-based distortion (binding error, motion unnaturalness) are taken into consideration. Ten types of common motion are employed to drive the DDHs and a total of 800 DDHs are generated in the end. Afterward, we render the video sequences of the distorted DDHs as the evaluation media and carry out a well-controlled subjective experiment. Then a benchmark experiment is conducted with the state-of-the-art video quality assessment (VQA) methods and the experimental results show that existing VQA methods are limited in assessing the perceptual loss of DDHs.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2403.15285.pdf' target='_blank'>https://arxiv.org/pdf/2403.15285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou, Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15285">Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driven by the great advances in metaverse and edge computing technologies, vehicular edge metaverses are expected to disrupt the current paradigm of intelligent transportation systems. As highly computerized avatars of Vehicular Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can provide valuable metaverse services to improve driving safety and on-board satisfaction for their VMUs throughout journeys. To maintain uninterrupted metaverse experiences, VTs must be migrated among edge servers following the movements of vehicles. This can raise concerns about privacy breaches during the dynamic communications among vehicular edge metaverses. To address these concerns and safeguard location privacy, pseudonyms as temporary identifiers can be leveraged by both VMUs and VTs to realize anonymous communications in the physical space and virtual spaces. However, existing pseudonym management methods fall short in meeting the extensive pseudonym demands in vehicular edge metaverses, thus dramatically diminishing the performance of privacy preservation. To this end, we present a cross-metaverse empowered dual pseudonym management framework. We utilize cross-chain technology to enhance management efficiency and data security for pseudonyms. Furthermore, we propose a metric to assess the privacy level and employ a Multi-Agent Deep Reinforcement Learning (MADRL) approach to obtain an optimal pseudonym generating strategy. Numerical results demonstrate that our proposed schemes are high-efficiency and cost-effective, showcasing their promising applications in vehicular edge metaverses.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2309.04703.pdf' target='_blank'>https://arxiv.org/pdf/2309.04703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinbo Wen, Jiawen Kang, Zehui Xiong, Yang Zhang, Hongyang Du, Yutao Jiao, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04703">Task Freshness-aware Incentive Mechanism for Vehicle Twin Migration in Vehicular Metaverses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicular metaverse, which is treated as the future continuum between automotive industry and metaverse, is envisioned as a blended immersive domain as the digital twins of intelligent transportation systems. Vehicles access the vehicular metaverses by their own Vehicle Twins (VTs) (e.g., avatars) that resource-limited vehicles offload the tasks of building VTs to their nearby RoadSide Units (RSUs). However, due to the limited coverage of RSUs and the mobility of vehicles, VTs have to be migrated from one RSU to other RSUs to ensure uninterrupted metaverse services for users within vehicles. This process requires the next RSUs to contribute sufficient bandwidth resources for VT migrations under asymmetric information. To this end, in this paper, we design an efficient incentive mechanism framework for VT migrations. We first propose a novel metric named Age of Migration Task (AoMT) to quantify the task freshness of the VT migration. AoMT measures the time elapsed from the first collected sensing data of the freshest avatar migration task to the last successfully processed data at the next RSU. To incentivize the contribution of bandwidth resources among the next RSUs, we propose an AoMT-based contract model, where the optimal contract is derived to maximize the expected utility of the RSU that provides metaverse services. Numerical results demonstrate the efficiency of the proposed incentive mechanism for VT migrations.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2308.07618.pdf' target='_blank'>https://arxiv.org/pdf/2308.07618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Boon Hee Soong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07618">Vision-based Semantic Communications for Metaverse Services: A Contest Theoretic Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The popularity of Metaverse as an entertainment, social, and work platform has led to a great need for seamless avatar integration in the virtual world. In Metaverse, avatars must be updated and rendered to reflect users' behaviour. Achieving real-time synchronization between the virtual bilocation and the user is complex, placing high demands on the Metaverse Service Provider (MSP)'s rendering resource allocation scheme. To tackle this issue, we propose a semantic communication framework that leverages contest theory to model the interactions between users and MSPs and determine optimal resource allocation for each user. To reduce the consumption of network resources in wireless transmission, we use the semantic communication technique to reduce the amount of data to be transmitted. Under our simulation settings, the encoded semantic data only contains 51 bytes of skeleton coordinates instead of the image size of 8.243 megabytes. Moreover, we implement Deep Q-Network to optimize reward settings for maximum performance and efficient resource allocation. With the optimal reward setting, users are incentivized to select their respective suitable uploading frequency, reducing down-sampling loss due to rendering resource constraints by 66.076\% compared with the traditional average distribution method. The framework provides a novel solution to resource allocation for avatar association in VR environments, ensuring a smooth and immersive experience for all users.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2306.14683.pdf' target='_blank'>https://arxiv.org/pdf/2306.14683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlong Chen, Jiawen Kang, Minrui Xu, Zehui Xiong, Dusit Niyato, Chuan Chen, Abbas Jamalipour, Shengli Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14683">Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in AIoT-enabled Vehicular Metaverses with Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars, as promising digital assistants in Vehicular Metaverses, can enable drivers and passengers to immerse in 3D virtual spaces, serving as a practical emerging example of Artificial Intelligence of Things (AIoT) in intelligent vehicular environments. The immersive experience is achieved through seamless human-avatar interaction, e.g., augmented reality navigation, which requires intensive resources that are inefficient and impractical to process on intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide Units (RSUs) or cloud servers for remote execution can effectively reduce resource consumption. However, the high mobility of vehicles, the dynamic workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making avatar migration decisions. To address these challenges, in this paper, we propose a dynamic migration framework for avatar tasks based on real-time trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL). Specifically, we propose a model to predict the future trajectories of intelligent vehicles based on their historical data, indicating the future workloads of RSUs.Based on the expected workloads of RSUs, we formulate the avatar task migration problem as a long-term mixed integer programming problem. To tackle this problem efficiently, the problem is transformed into a Partially Observable Markov Decision Process (POMDP) and solved by multiple DRL agents with hybrid continuous and discrete actions in decentralized. Numerical results demonstrate that our proposed algorithm can effectively reduce the latency of executing avatar tasks by around 25% without prediction and 30% with prediction and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse (AeVeM).
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2411.18197.pdf' target='_blank'>https://arxiv.org/pdf/2411.18197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, Ran Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18197">Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D characters are essential to modern creative industries, but making them animatable often demands extensive manual work in tasks like rigging and skinning. Existing automatic rigging tools face several limitations, including the necessity for manual annotations, rigid skeleton topologies, and limited generalization across diverse shapes and poses. An alternative approach is to generate animatable avatars pre-bound to a rigged template mesh. However, this method often lacks flexibility and is typically limited to realistic human shapes. To address these issues, we present Make-It-Animatable, a novel data-driven method to make any 3D humanoid model ready for character animation in less than one second, regardless of its shapes and poses. Our unified framework generates high-quality blend weights, bones, and pose transformations. By incorporating a particle-based shape autoencoder, our approach supports various 3D representations, including meshes and 3D Gaussian splats. Additionally, we employ a coarse-to-fine representation and a structure-aware modeling strategy to ensure both accuracy and robustness, even for characters with non-standard skeleton structures. We conducted extensive experiments to validate our framework's effectiveness. Compared to existing methods, our approach demonstrates significant improvements in both quality and speed. More demos and code are available at https://jasongzy.github.io/Make-It-Animatable/.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2306.10900.pdf' target='_blank'>https://arxiv.org/pdf/2306.10900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10900">MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Visit our webpage at https://qiqiapink.github.io/MotionGPT/.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2310.08529.pdf' target='_blank'>https://arxiv.org/pdf/2310.08529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08529">GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2508.08248.pdf' target='_blank'>https://arxiv.org/pdf/2508.08248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Chong Luo, Zuxuan Wu, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08248">StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current diffusion models for audio-driven avatar video generation struggle to synthesize long videos with natural audio synchronization and identity consistency. This paper presents StableAvatar, the first end-to-end video diffusion transformer that synthesizes infinite-length high-quality videos without post-processing. Conditioned on a reference image and audio, StableAvatar integrates tailored training and inference modules to enable infinite-length video generation. We observe that the main reason preventing existing models from generating long videos lies in their audio modeling. They typically rely on third-party off-the-shelf extractors to obtain audio embeddings, which are then directly injected into the diffusion model via cross-attention. Since current diffusion backbones lack any audio-related priors, this approach causes severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to drift away from the optimal distribution gradually. To address this, StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents error accumulation via time-step-aware modulation. During inference, we propose a novel Audio Native Guidance Mechanism to further enhance the audio synchronization by leveraging the diffusion's own evolving joint audio-latent prediction as a dynamic guidance signal. To enhance the smoothness of the infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy that fuses latent over time. Experiments on benchmarks show the effectiveness of StableAvatar both qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2303.12965.pdf' target='_blank'>https://arxiv.org/pdf/2303.12965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoke Huang, Yiji Cheng, Yansong Tang, Xiu Li, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12965">Efficient Meshy Neural Fields for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently digitizing high-fidelity animatable human avatars from videos is a challenging and active research topic. Recent volume rendering-based neural representations open a new way for human digitization with their friendly usability and photo-realistic reconstruction quality. However, they are inefficient for long optimization times and slow inference speed; their implicit nature results in entangled geometry, materials, and dynamics of humans, which are hard to edit afterward. Such drawbacks prevent their direct applicability to downstream applications, especially the prominent rasterization-based graphic ones. We present EMA, a method that Efficiently learns Meshy neural fields to reconstruct animatable human Avatars. It jointly optimizes explicit triangular canonical mesh, spatial-varying material, and motion dynamics, via inverse rendering in an end-to-end fashion. Each above component is derived from separate neural fields, relaxing the requirement of a template, or rigging. The mesh representation is highly compatible with the efficient rasterization-based renderer, thus our method only takes about an hour of training and can render in real-time. Moreover, only minutes of optimization is enough for plausible reconstruction results. The disentanglement of meshes enables direct downstream applications. Extensive experiments illustrate the very competitive performance and significant speed boost against previous methods. We also showcase applications including novel pose synthesis, material editing, and relighting. The project page: https://xk-huang.github.io/ema/.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2407.19976.pdf' target='_blank'>https://arxiv.org/pdf/2407.19976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chencan Fu, Yabiao Wang, Jiangning Zhang, Zhengkai Jiang, Xiaofeng Mao, Jiafu Wu, Weijian Cao, Chengjie Wang, Yanhao Ge, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19976">MambaGesture: Enhancing Co-Speech Gesture Generation with Mamba and Disentangled Multi-Modality Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gesture generation is crucial for producing synchronized and realistic human gestures that accompany speech, enhancing the animation of lifelike avatars in virtual environments. While diffusion models have shown impressive capabilities, current approaches often overlook a wide range of modalities and their interactions, resulting in less dynamic and contextually varied gestures. To address these challenges, we present MambaGesture, a novel framework integrating a Mamba-based attention block, MambaAttn, with a multi-modality feature fusion module, SEAD. The MambaAttn block combines the sequential data processing strengths of the Mamba model with the contextual richness of attention mechanisms, enhancing the temporal coherence of generated gestures. SEAD adeptly fuses audio, text, style, and emotion modalities, employing disentanglement to deepen the fusion process and yield gestures with greater realism and diversity. Our approach, rigorously evaluated on the multi-modal BEAT dataset, demonstrates significant improvements in FrÃ©chet Gesture Distance (FGD), diversity scores, and beat alignment, achieving state-of-the-art performance in co-speech gesture generation. Project website: $\href{https://fcchit.github.io/mambagesture/}{\textit{https://fcchit.github.io/mambagesture/}}$.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2406.11208.pdf' target='_blank'>https://arxiv.org/pdf/2406.11208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Su, Xiaofeng Luo, Zhenmou Liu, Jiawen Kang, Min Hao, Zehui Xiong, Zhaohui Yang, Chongwen Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11208">Privacy-preserving Pseudonym Schemes for Personalized 3D Avatars in Mobile Social Metaverses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of mobile social metaverses, a novel paradigm bridging physical and virtual realms, has led to the widespread adoption of avatars as digital representations for Social Metaverse Users (SMUs) within virtual spaces. Equipped with immersive devices, SMUs leverage Edge Servers (ESs) to deploy their avatars and engage with other SMUs in virtual spaces. To enhance immersion, SMUs incline to opt for 3D avatars for social interactions. However, existing 3D avatars are typically generated through scanning the real faces of SMUs, which can raise concerns regarding information privacy and security, such as profile identity leakages. To tackle this, we introduce a new framework for personalized 3D avatar construction, leveraging a two-layer network model that provides SMUs with the option to customize their personal avatars for privacy preservation. Specifically, our approach introduces avatar pseudonyms to jointly safeguard the profile and digital identity privacy of the generated avatars. Then, we design a novel metric named Privacy of Personalized Avatars (PoPA), to evaluate effectiveness of the avatar pseudonyms. To optimize pseudonym resource, we model the pseudonym distribution process as a Stackelberg game and employ Deep Reinforcement Learning (DRL) to learn equilibrium strategies under incomplete information. Simulation results validate the efficacy and feasibility of our proposed schemes for mobile social metaverses.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2403.02905.pdf' target='_blank'>https://arxiv.org/pdf/2403.02905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Wang, Jiangning Zhang, Xin Tan, Zhifeng Xie, Chengjie Wang, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02905">MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The body movements accompanying speech aid speakers in expressing their ideas. Co-speech motion generation is one of the important approaches for synthesizing realistic avatars. Due to the intricate correspondence between speech and motion, generating realistic and diverse motion is a challenging task. In this paper, we propose MMoFusion, a Multi-modal co-speech Motion generation framework based on the diffusion model to ensure both the authenticity and diversity of generated motion. We propose a progressive fusion strategy to enhance the interaction of inter-modal and intra-modal, efficiently integrating multi-modal information. Specifically, we employ a masked style matrix based on emotion and identity information to control the generation of different motion styles. Temporal modeling of speech and motion is partitioned into style-guided specific feature encoding and shared feature encoding, aiming to learn both inter-modal and intra-modal features. Besides, we propose a geometric loss to enforce the joints' velocity and acceleration coherence among frames. Our framework generates vivid, diverse, and style-controllable motion of arbitrary length through inputting speech and editing identity and emotion. Extensive experiments demonstrate that our method outperforms current co-speech motion generation methods including upper body and challenging full body.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2405.00954.pdf' target='_blank'>https://arxiv.org/pdf/2405.00954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00954">X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry->Texture->Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: https://xmu-xiaoma666.github.io/Projects/X-Oscar/.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2305.10345.pdf' target='_blank'>https://arxiv.org/pdf/2305.10345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfei Yang, He Huang, Yunjiao Zhou, Xinyan Chen, Yuecong Xu, Shenghai Yuan, Han Zou, Chris Xiaoxuan Lu, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10345">MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation action categories, to bridge the gap between wireless sensing and high-level human perception tasks. MM-Fi consists of over 320k synchronized frames of five modalities from 40 human subjects. Various annotations are provided to support potential sensing tasks, e.g., human pose estimation and action recognition. Extensive experiments have been conducted to compare the sensing capacity of each or several modalities in terms of multiple tasks. We envision that MM-Fi can contribute to wireless sensing research with respect to action recognition, human pose estimation, multi-modal learning, cross-modal supervision, and interdisciplinary healthcare research.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2507.02419.pdf' target='_blank'>https://arxiv.org/pdf/2507.02419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhong, Xiaolin Zhang, Ligang Liu, Yao Zhao, Yunchao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02419">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2508.00748.pdf' target='_blank'>https://arxiv.org/pdf/2508.00748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00748">Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. These avatars allow for more immersive communication, but they also introduce serious security risks. One emerging threat is impersonation: an attacker can steal a user's avatar, preserving his appearance and voice, making it nearly impossible to detect its fraudulent usage by sight or sound alone. In this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. Our main question is whether an individual's facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatar's visual appearance is a facsimile of its owner. To answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and impostor avatar videos. We also propose a lightweight, explainable spatio-temporal Graph Convolutional Network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. Experimental results demonstrate that facial motion cues enable meaningful identity verification with AUC values approaching 80%. The proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2304.06969.pdf' target='_blank'>https://arxiv.org/pdf/2304.06969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Fan, Jing Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06969">UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose rendering, Geometry and Texture Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural radiance field (NeRF) has become a popular 3D representation method for human avatar reconstruction due to its high-quality rendering capabilities, e.g., regarding novel views and poses. However, previous methods for editing the geometry and appearance of the avatar only allow for global editing through body shape parameters and 2D texture maps. In this paper, we propose a new approach named \textbf{U}nified \textbf{V}olumetric \textbf{A}vatar (\textbf{UVA}) that enables local and independent editing of both geometry and texture, while retaining the ability to render novel views and poses. UVA transforms each observation point to a canonical space using a skinning motion field and represents geometry and texture in separate neural fields. Each field is composed of a set of structured latent codes that are attached to anchor nodes on a deformable mesh in canonical space and diffused into the entire space via interpolation, allowing for local editing. To address spatial ambiguity in code interpolation, we use a local signed height indicator. We also replace the view-dependent radiance color with a pose-dependent shading factor to better represent surface illumination in different poses. Experiments on multiple human avatars demonstrate that our UVA achieves competitive results in novel view synthesis and novel pose rendering while enabling local and independent editing of geometry and appearance. The source code will be released.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2503.06499.pdf' target='_blank'>https://arxiv.org/pdf/2503.06499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xukun Zhou, Fengxin Li, Ming Chen, Yan Zhou, Pengfei Wan, Di Zhang, Yeying Jin, Zhaoxin Fan, Hongyan Liu, Jun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06499">ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven human gesture synthesis is a crucial task with broad applications in virtual avatars, human-computer interaction, and creative content generation. Despite notable progress, existing methods often produce gestures that are coarse, lack expressiveness, and fail to fully align with audio semantics. To address these challenges, we propose ExGes, a novel retrieval-enhanced diffusion framework with three key designs: (1) a Motion Base Construction, which builds a gesture library using training dataset; (2) a Motion Retrieval Module, employing constrative learning and momentum distillation for fine-grained reference poses retreiving; and (3) a Precision Control Module, integrating partial masking and stochastic masking to enable flexible and fine-grained control. Experimental evaluations on BEAT2 demonstrate that ExGes reduces FrÃ©chet Gesture Distance by 6.2\% and improves motion diversity by 5.3\% over EMAGE, with user studies revealing a 71.3\% preference for its naturalness and semantic relevance. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2503.16942.pdf' target='_blank'>https://arxiv.org/pdf/2503.16942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingying Fan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Errui Ding, Yu Wu, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16942">Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current digital human studies focusing on lip-syncing and body movement are no longer sufficient to meet the growing industrial demand, while human video generation techniques that support interacting with real-world environments (e.g., objects) have not been well investigated. Despite human hand synthesis already being an intricate problem, generating objects in contact with hands and their interactions presents an even more challenging task, especially when the objects exhibit obvious variations in size and shape. To tackle these issues, we present a novel video Reenactment framework focusing on Human-Object Interaction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD). Our key insight is to employ specialized layout representation for hands and objects, respectively. Such representations enable effective disentanglement of hand modeling and object adaptation to diverse motion sequences. To further improve the generation quality of HOI, we design an interactive textural enhancement module for both hands and objects by introducing two independent memory banks. We also propose a layout adjustment strategy for the cross-object reenactment scenario to adaptively adjust unreasonable layouts caused by diverse object sizes during inference. Comprehensive qualitative and quantitative evaluations demonstrate that our proposed framework significantly outperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2410.10696.pdf' target='_blank'>https://arxiv.org/pdf/2410.10696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhi Guan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Shengyi He, Zhiliang Xu, Haocheng Feng, Errui Ding, Jingdong Wang, Hongtao Xie, Youjian Zhao, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10696">TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, 2D speaking avatars have increasingly participated in everyday scenarios due to the fast development of facial animation techniques. However, most existing works neglect the explicit control of human bodies. In this paper, we propose to drive not only the faces but also the torso and gesture movements of a speaking figure. Inspired by recent advances in diffusion models, we propose the Motion-Enhanced Textural-Aware ModeLing for SpeaKing Avatar Reenactment (TALK-Act) framework, which enables high-fidelity avatar reenactment from only short footage of monocular video. Our key idea is to enhance the textural awareness with explicit motion guidance in diffusion modeling. Specifically, we carefully construct 2D and 3D structural information as intermediate guidance. While recent diffusion models adopt a side network for control information injection, they fail to synthesize temporally stable results even with person-specific fine-tuning. We propose a Motion-Enhanced Textural Alignment module to enhance the bond between driving and target signals. Moreover, we build a Memory-based Hand-Recovering module to help with the difficulties in hand-shape preserving. After pre-training, our model can achieve high-fidelity 2D avatar reenactment with only 30 seconds of person-specific data. Extensive experiments demonstrate the effectiveness and superiority of our proposed framework. Resources can be found at https://guanjz20.github.io/projects/TALK-Act.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2402.16607.pdf' target='_blank'>https://arxiv.org/pdf/2402.16607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16607">GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation lies in addressing the intricate challenges of delivering high-fidelity human body reconstructions and aligning 3D Gaussians with human skin surfaces accurately. The key contributions of this paper are twofold. Firstly, we introduce a pose refinement technique to improve hand and foot pose accuracy by aligning normal maps and silhouettes. Precise pose is crucial for correct shape and appearance reconstruction. Secondly, we address the problems of unbalanced aggregation and initialization bias that previously diminished the quality of 3D Gaussian avatars, through a novel surface-guided re-initialization method that ensures accurate alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive experimental analyses validate the performance qualitatively and quantitatively, demonstrating that it achieves state-of-the-art performance in photo-realistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GVA/.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2308.07749.pdf' target='_blank'>https://arxiv.org/pdf/2308.07749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07749">Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT. For preserving background continuity, we put forth a background alignment pipeline, amalgamating insights from segment anything and image inpainting techniques. Furthermore, we propose an inter-frame alignment module that draws inspiration from an auto-regressive pipeline to augment temporal consistency between adjacent frames, where the preceding frame guides the synthesis process of the current frame. Comparisons with state-of-the-art methods demonstrate that Dancing Avatar exhibits the capacity to generate human videos with markedly superior quality, both in terms of human and background fidelity, as well as temporal coherence compared to existing state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2412.09892.pdf' target='_blank'>https://arxiv.org/pdf/2412.09892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liu, Ziyang Ma, Qi Chen, Feilong Chen, Shuai Fan, Xie Chen, Kai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09892">VQTalker: Towards Multilingual Talking Avatars through Facial Motion Tokenization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VQTalker, a Vector Quantization-based framework for multilingual talking head generation that addresses the challenges of lip synchronization and natural motion across diverse languages. Our approach is grounded in the phonetic principle that human speech comprises a finite set of distinct sound units (phonemes) and corresponding visual articulations (visemes), which often share commonalities across languages. We introduce a facial motion tokenizer based on Group Residual Finite Scalar Quantization (GRFSQ), which creates a discretized representation of facial features. This method enables comprehensive capture of facial movements while improving generalization to multiple languages, even with limited training data. Building on this quantized representation, we implement a coarse-to-fine motion generation process that progressively refines facial animations. Extensive experiments demonstrate that VQTalker achieves state-of-the-art performance in both video-driven and speech-driven scenarios, particularly in multilingual settings. Notably, our method achieves high-quality results at a resolution of 512*512 pixels while maintaining a lower bitrate of approximately 11 kbps. Our work opens new possibilities for cross-lingual talking face generation. Synthetic results can be viewed at https://x-lance.github.io/VQTalker.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2407.17438.pdf' target='_blank'>https://arxiv.org/pdf/2407.17438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17438">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2508.02106.pdf' target='_blank'>https://arxiv.org/pdf/2508.02106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02106">Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2502.06392.pdf' target='_blank'>https://arxiv.org/pdf/2502.06392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Long, Zijun Zhao, Min Ouyang, Qingcheng Zhao, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06392">TANGLED: Generating 3D Hair Strands from Images with Arbitrary Styles and Viewpoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hairstyles are intricate and culturally significant with various geometries, textures, and structures. Existing text or image-guided generation methods fail to handle the richness and complexity of diverse styles. We present TANGLED, a novel approach for 3D hair strand generation that accommodates diverse image inputs across styles, viewpoints, and quantities of input views. TANGLED employs a three-step pipeline. First, our MultiHair Dataset provides 457 diverse hairstyles annotated with 74 attributes, emphasizing complex and culturally significant styles to improve model generalization. Second, we propose a diffusion framework conditioned on multi-view linearts that can capture topological cues (e.g., strand density and parting lines) while filtering out noise. By leveraging a latent diffusion model with cross-attention on lineart features, our method achieves flexible and robust 3D hair generation across diverse input conditions. Third, a parametric post-processing module enforces braid-specific constraints to maintain coherence in complex structures. This framework not only advances hairstyle realism and diversity but also enables culturally inclusive digital avatars and novel applications like sketch-based 3D strand editing for animation and augmented reality.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2402.10483.pdf' target='_blank'>https://arxiv.org/pdf/2402.10483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10483">GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hairstyle reflects culture and ethnicity at first glance. In the digital era, various realistic human hairstyles are also critical to high-fidelity digital human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time rendering for animation is a formidable challenge due to its sheer number of strands, complicated structures of geometry, and sophisticated interaction with light. This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities. At the heart of GaussianHair is the novel concept of representing each hair strand as a sequence of connected cylindrical 3D Gaussian primitives. This approach not only retains the hair's geometric structure and appearance but also allows for efficient rasterization onto a 2D image plane, facilitating differentiable volumetric rendering. We further enhance this model with the "GaussianHair Scattering Model", adept at recreating the slender structure of hair strands and accurately capturing their local diffuse color in uniform lighting. Through extensive experiments, we substantiate that GaussianHair achieves breakthroughs in both geometric and appearance fidelity, transcending the limitations encountered in state-of-the-art methods for hair reconstruction. Beyond representation, GaussianHair extends to support editing, relighting, and dynamic rendering of hair, offering seamless integration with conventional CG pipeline workflows. Complementing these advancements, we have compiled an extensive dataset of real human hair, each with meticulously detailed strand geometry, to propel further research in this field.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2401.16465.pdf' target='_blank'>https://arxiv.org/pdf/2401.16465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai He, Kaixin Yao, Qixuan Zhang, Jingyi Yu, Lingjie Liu, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16465">DressCode: Autoregressively Sewing and Generating Garments from Text Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Apparel's significant role in human appearance underscores the importance of garment digitalization for digital human creation. Recent advances in 3D content creation are pivotal for digital human creation. Nonetheless, garment generation from text guidance is still nascent. We introduce a text-driven 3D garment generation framework, DressCode, which aims to democratize design for novices and offer immense potential in fashion design, virtual try-on, and digital human creation. We first introduce SewingGPT, a GPT-based architecture integrating cross-attention with text-conditioned embedding to generate sewing patterns with text guidance. We then tailor a pre-trained Stable Diffusion to generate tile-based Physically-based Rendering (PBR) textures for the garments. By leveraging a large language model, our framework generates CG-friendly garments through natural language interaction. It also facilitates pattern completion and texture editing, streamlining the design process through user-friendly interaction. This framework fosters innovation by allowing creators to freely experiment with designs and incorporate unique elements into their work. With comprehensive evaluations and comparisons with other state-of-the-art methods, our method showcases superior quality and alignment with input prompts. User studies further validate our high-quality rendering results, highlighting its practical utility and potential in production settings. Our project page is https://IHe-KaiI.github.io/DressCode/.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2304.03117.pdf' target='_blank'>https://arxiv.org/pdf/2304.03117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03117">DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging Metaverse applications demand accessible, accurate, and easy-to-use tools for 3D digital human creations in order to depict different cultures and societies as if in the physical world. Recent large-scale vision-language advances pave the way to for novices to conveniently customize 3D content. However, the generated CG-friendly assets still cannot represent the desired facial traits for human characteristics. In this paper, we present DreamFace, a progressive scheme to generate personalized 3D faces under text guidance. It enables layman users to naturally customize 3D facial assets that are compatible with CG pipelines, with desired shapes, textures, and fine-grained animation capabilities. From a text input to describe the facial traits, we first introduce a coarse-to-fine scheme to generate the neutral facial geometry with a unified topology. We employ a selection strategy in the CLIP embedding space, and subsequently optimize both the details displacements and normals using Score Distillation Sampling from generic Latent Diffusion Model. Then, for neutral appearance generation, we introduce a dual-path mechanism, which combines the generic LDM with a novel texture LDM to ensure both the diversity and textural specification in the UV space. We also employ a two-stage optimization to perform SDS in both the latent and image spaces to significantly provides compact priors for fine-grained synthesis. Our generated neutral assets naturally support blendshapes-based facial animations. We further improve the animation ability with personalized deformation characteristics by learning the universal expression prior using the cross-identity hypernetwork. Notably, DreamFace can generate of realistic 3D facial assets with physically-based rendering quality and rich animation ability from video footage, even for fashion icons or exotic characters in cartoons and fiction movies.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2505.01746.pdf' target='_blank'>https://arxiv.org/pdf/2505.01746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Yatian Wang, Hengyuan Zhang, Jiahao Pan, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01746">Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter. Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. Considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. Specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a Temporal Interaction Module (TIM). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. The dataset and source code are publicly available at \href{https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2405.16874.pdf' target='_blank'>https://arxiv.org/pdf/2405.16874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Hengyuan Zhang, Yatian Wang, Jiahao Pan, Chen Liu, Peng Li, Xiaowei Chi, Mengfei Li, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16874">CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2311.17532.pdf' target='_blank'>https://arxiv.org/pdf/2311.17532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17532">Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating vivid and emotional 3D co-speech gestures is crucial for virtual avatar animation in human-machine interaction applications. While the existing methods enable generating the gestures to follow a single emotion label, they overlook that long gesture sequence modeling with emotion transition is more practical in real scenes. In addition, the lack of large-scale available datasets with emotional transition speech and corresponding 3D human gestures also limits the addressing of this task. To fulfill this goal, we first incorporate the ChatGPT-4 and an audio inpainting approach to construct the high-fidelity emotion transition human speeches. Considering obtaining the realistic 3D pose annotations corresponding to the dynamically inpainted emotion transition audio is extremely difficult, we propose a novel weakly supervised training strategy to encourage authority gesture transitions. Specifically, to enhance the coordination of transition gestures w.r.t different emotional ones, we model the temporal association representation between two different emotional gesture sequences as style guidance and infuse it into the transition generation. We further devise an emotion mixture mechanism that provides weak supervision based on a learnable mixed emotion label for transition gestures. Last, we present a keyframe sampler to supply effective initial posture cues in long sequences, enabling us to generate diverse gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models constructed by adapting single emotion-conditioned counterparts on our newly defined emotion transition task and datasets. Our code and dataset will be released on the project page: https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2410.07164.pdf' target='_blank'>https://arxiv.org/pdf/2410.07164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Liang Pan, Kai Han, Kwan-Yee K. Wong, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07164">AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in diffusion models have led to significant improvements in the generation and animation of 4D full-body human-object interactions (HOI). Nevertheless, existing methods primarily focus on SMPL-based motion generation, which is limited by the scarcity of realistic large-scale interaction data. This constraint affects their ability to create everyday HOI scenes. This paper addresses this challenge using a zero-shot approach with a pre-trained diffusion model. Despite this potential, achieving our goals is difficult due to the diffusion model's lack of understanding of ''where'' and ''how'' objects interact with the human body. To tackle these issues, we introduce AvatarGO, a novel framework designed to generate animatable 4D HOI scenes directly from textual inputs. Specifically, 1) for the ''where'' challenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to identify the contact body part from text prompts, ensuring precise representation of human-object spatial relations. 2) For the ''how'' challenge, we introduce correspondence-aware motion optimization that constructs motion fields for both human and object models using the linear blend skinning function from SMPL-X. Our framework not only generates coherent compositional motions, but also exhibits greater robustness in handling penetration issues. Extensive experiments with existing methods validate AvatarGO's superior generation and animation capabilities on a variety of human-object pairs and diverse poses. As the first attempt to synthesize 4D avatars with object interactions, we hope AvatarGO could open new doors for human-centric 4D content creation.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2401.08503.pdf' target='_blank'>https://arxiv.org/pdf/2401.08503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08503">Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples and source code are available at https://real3dportrait.github.io .
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2305.00787.pdf' target='_blank'>https://arxiv.org/pdf/2305.00787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhui Ye, Jinzheng He, Ziyue Jiang, Rongjie Huang, Jiawei Huang, Jinglin Liu, Yi Ren, Xiang Yin, Zejun Ma, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00787">GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating talking person portraits with arbitrary speech audio is a crucial problem in the field of digital human and metaverse. A modern talking face generation method is expected to achieve the goals of generalized audio-lip synchronization, good video quality, and high system efficiency. Recently, neural radiance field (NeRF) has become a popular rendering technique in this field since it could achieve high-fidelity and 3D-consistent talking face generation with a few-minute-long training video. However, there still exist several challenges for NeRF-based methods: 1) as for the lip synchronization, it is hard to generate a long facial motion sequence of high temporal consistency and audio-lip accuracy; 2) as for the video quality, due to the limited data used to train the renderer, it is vulnerable to out-of-domain input condition and produce bad rendering results occasionally; 3) as for the system efficiency, the slow training and inference speed of the vanilla NeRF severely obstruct its usage in real-world applications. In this paper, we propose GeneFace++ to handle these challenges by 1) utilizing the pitch contour as an auxiliary feature and introducing a temporal loss in the facial motion prediction process; 2) proposing a landmark locally linear embedding method to regulate the outliers in the predicted motion sequence to avoid robustness issues; 3) designing a computationally efficient NeRF-based motion-to-video renderer to achieves fast training and real-time inference. With these settings, GeneFace++ becomes the first NeRF-based method that achieves stable and real-time talking face generation with generalized audio-lip synchronization. Extensive experiments show that our method outperforms state-of-the-art baselines in terms of subjective and objective evaluation. Video samples are available at https://genefaceplusplus.github.io .
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2502.01045.pdf' target='_blank'>https://arxiv.org/pdf/2502.01045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01045">WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at https://wyiguanw.github.io/WonderHuman/.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2312.05295.pdf' target='_blank'>https://arxiv.org/pdf/2312.05295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Cheng Lin, Xin Li, Wenping Wang, Rong Xie, Li Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05295">Disentangled Clothed Avatar Generation from Text Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel text-to-avatar generation method that separately generates the human body and the clothes and allows high-quality animation on the generated avatar. While recent advancements in text-to-avatar generation have yielded diverse human avatars from text prompts, these methods typically combine all elements-clothes, hair, and body-into a single 3D representation. Such an entangled approach poses challenges for downstream tasks like editing or animation. To overcome these limitations, we propose a novel disentangled 3D avatar representation named Sequentially Offset-SMPL (SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body and clothes with two separate meshes but associates them with offsets to ensure the physical alignment between the body and the clothes. Then, we design a Score Distillation Sampling (SDS)-based distillation framework to generate the proposed SO-SMPL representation from text prompts. Our approach not only achieves higher texture and geometry quality and better semantic alignment with text prompts, but also significantly improves the visual quality of character animation, virtual try-on, and avatar editing. Project page: https://shanemankiw.github.io/SO-SMPL/.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2211.03703.pdf' target='_blank'>https://arxiv.org/pdf/2211.03703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Latif U. Khan, Ibrar Yaqoob, Khaled Salah, Choong Seon Hong, Dusit Niyato, Zhu Han, Mohsen Guizani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.03703">Machine Learning for Wireless Metaverse: Fundamentals, Use Case, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's wireless systems are posing key challenges in terms of quality of service and quality of physical experience. Metaverse has the potential to reshape, transform, and add innovations to the existing wireless systems. A metaverse is a collective virtual open space that can enable wireless systems using digital twins, digital avatars, and interactive experience technologies. Machine learning (ML) is indispensable for modeling twins, avatars, and deploying interactive experience technologies. In this paper, we present the role of ML in enabling metaverse-based wireless systems. We discuss key fundamental concepts for advancing ML in the metaverse-based wireless systems. Moreover, we present a case study of deep reinforcement learning for metaverse sensing. Finally, we discuss the future directions along with potential solutions.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2207.00413.pdf' target='_blank'>https://arxiv.org/pdf/2207.00413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Latif U. Khan, Zhu Han, Dusit Niyato, Mohsen Guizani, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.00413">Metaverse for Wireless Systems: Vision, Enablers, Architecture, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, significant research efforts have been initiated to enable the next-generation, namely, the sixth-generation (6G) wireless systems. In this article, we present a vision of metaverse towards effectively enabling the development of 6G wireless systems. A metaverse will use virtual representation (e.g., digital twin), digital avatars, and interactive experience technologies (e.g., extended reality) to assist analyses, optimizations, and operations of various wireless applications. Specifically, the metaverse can offer virtual wireless system operations through the digital twin that allows network designers, mobile developers, and telecommunications engineers to monitor, observe, analyze, and simulations their solutions collaboratively and virtually. We first introduce a general architecture for metaverse-based wireless systems. We discuss key driving applications, design trends, and key enablers of metaverse-based wireless systems. Finally, we present several open challenges and their potential solutions.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2508.09137.pdf' target='_blank'>https://arxiv.org/pdf/2508.09137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timo Teufel, Pulkit Gera, Xilong Zhou, Umar Iqbal, Pramod Rao, Jan Kautz, Vladislav Golyanik, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09137">HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous relighting and novel-view rendering of digital human representations is an important yet challenging task with numerous applications. Progress in this area has been significantly limited due to the lack of publicly available, high-quality datasets, especially for full-body human captures. To address this critical gap, we introduce the HumanOLAT dataset, the first publicly accessible large-scale dataset of multi-view One-Light-at-a-Time (OLAT) captures of full-body humans. The dataset includes HDR RGB frames under various illuminations, such as white light, environment maps, color gradients and fine-grained OLAT illuminations. Our evaluations of state-of-the-art relighting and novel-view synthesis methods underscore both the dataset's value and the significant challenges still present in modeling complex human-centric appearance and lighting interactions. We believe HumanOLAT will significantly facilitate future research, enabling rigorous benchmarking and advancements in both general and human-specific relighting and rendering techniques.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2312.11587.pdf' target='_blank'>https://arxiv.org/pdf/2312.11587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diogo Luvizon, Vladislav Golyanik, Adam Kortylewski, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11587">Relightable Neural Actor with Intrinsic Decomposition and Pose Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a controllable and relightable digital avatar from multi-view video with fixed illumination is a very challenging problem since humans are highly articulated, creating pose-dependent appearance effects, and skin as well as clothing require space-varying BRDF modeling. Existing works on creating animatible avatars either do not focus on relighting at all, require controlled illumination setups, or try to recover a relightable avatar from very low cost setups, i.e. a single RGB video, at the cost of severely limited result quality, e.g. shadows not even being modeled. To address this, we propose Relightable Neural Actor, a new video-based method for learning a pose-driven neural human model that can be relighted, allows appearance editing, and models pose-dependent effects such as wrinkles and self-shadows. Importantly, for training, our method solely requires a multi-view recording of the human under a known, but static lighting condition. To tackle this challenging problem, we leverage an implicit geometry representation of the actor with a drivable density field that models pose-dependent deformations and derive a dynamic mapping between 3D and UV spaces, where normal, visibility, and materials are effectively encoded. To evaluate our approach in real-world scenarios, we collect a new dataset with four identities recorded under different light conditions, indoors and outdoors, providing the first benchmark of its kind for human relighting, and demonstrating state-of-the-art relighting results for novel human poses.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2309.16670.pdf' target='_blank'>https://arxiv.org/pdf/2309.16670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soshi Shimada, Vladislav Golyanik, Patrick PÃ©rez, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16670">Decaf: Monocular Deformation Capture for Face and Hand Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for 3D tracking from monocular RGB videos predominantly consider articulated and rigid objects. Modelling dense non-rigid object deformations in this setting remained largely unaddressed so far, although such effects can improve the realism of the downstream applications such as AR/VR and avatar communications. This is due to the severe ill-posedness of the monocular view setting and the associated challenges. While it is possible to naively track multiple non-rigid objects independently using 3D templates or parametric 3D models, such an approach would suffer from multiple artefacts in the resulting 3D estimates such as depth ambiguity, unnatural intra-object collisions and missing or implausible deformations. Hence, this paper introduces the first method that addresses the fundamental challenges depicted above and that allows tracking human hands interacting with human faces in 3D from single monocular RGB videos. We model hands as articulated objects inducing non-rigid face deformations during an active interaction. Our method relies on a new hand-face motion and interaction capture dataset with realistic face deformations acquired with a markerless multi-view camera system. As a pivotal step in its creation, we process the reconstructed raw 3D shapes with position-based dynamics and an approach for non-uniform stiffness estimation of the head tissues, which results in plausible annotations of the surface deformations, hand-face contact regions and head-hand positions. At the core of our neural approach are a variational auto-encoder supplying the hand-face depth prior and modules that guide the 3D tracking by estimating the contacts and the deformations. Our final 3D hand and face reconstructions are realistic and more plausible compared to several baselines applicable in our setting, both quantitatively and qualitatively. https://vcai.mpi-inf.mpg.de/projects/Decaf
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2309.01848.pdf' target='_blank'>https://arxiv.org/pdf/2309.01848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Howe Yuan Zhu, Nguyen Quang Hieu, Dinh Thai Hoang, Diep N. Nguyen, Chin-Teng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01848">A Human-Centric Metaverse Enabled by Brain-Computer Interface: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing interest in the Metaverse has generated momentum for members of academia and industry to innovate toward realizing the Metaverse world. The Metaverse is a unique, continuous, and shared virtual world where humans embody a digital form within an online platform. Through a digital avatar, Metaverse users should have a perceptual presence within the environment and can interact and control the virtual world around them. Thus, a human-centric design is a crucial element of the Metaverse. The human users are not only the central entity but also the source of multi-sensory data that can be used to enrich the Metaverse ecosystem. In this survey, we study the potential applications of Brain-Computer Interface (BCI) technologies that can enhance the experience of Metaverse users. By directly communicating with the human brain, the most complex organ in the human body, BCI technologies hold the potential for the most intuitive human-machine system operating at the speed of thought. BCI technologies can enable various innovative applications for the Metaverse through this neural pathway, such as user cognitive state monitoring, digital avatar control, virtual interactions, and imagined speech communications. This survey first outlines the fundamental background of the Metaverse and BCI technologies. We then discuss the current challenges of the Metaverse that can potentially be addressed by BCI, such as motion sickness when users experience virtual environments or the negative emotional states of users in immersive virtual applications. After that, we propose and discuss a new research direction called Human Digital Twin, in which digital twins can create an intelligent and interactable avatar from the user's brain signals. We also present the challenges and potential solutions in synchronizing and communicating between virtual and physical entities in the Metaverse.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2308.07903.pdf' target='_blank'>https://arxiv.org/pdf/2308.07903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07903">Relightable and Animatable Neural Avatar from Sparse-View Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse distances based on a parametric human model and compute fine distances by exploiting the local deformation invariance of SDF. Based on the HDQ algorithm, we leverage sphere tracing to efficiently estimate the surface intersection and light visibility. This allows us to develop the first system to recover animatable and relightable neural avatars from sparse view (or monocular) inputs. Experiments demonstrate that our approach is able to produce superior results compared to state-of-the-art methods. Our code will be released for reproducibility.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2307.00842.pdf' target='_blank'>https://arxiv.org/pdf/2307.00842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyingcheng Liao, Vladislav Golyanik, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00842">VINECS: Video-based Neural Character Skinning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rigging and skinning clothed human avatars is a challenging task and traditionally requires a lot of manual work and expertise. Recent methods addressing it either generalize across different characters or focus on capturing the dynamics of a single character observed under different pose configurations. However, the former methods typically predict solely static skinning weights, which perform poorly for highly articulated poses, and the latter ones either require dense 3D character scans in different poses or cannot generate an explicit mesh with vertex correspondence over time. To address these challenges, we propose a fully automated approach for creating a fully rigged character with pose-dependent skinning weights, which can be solely learned from multi-view video. Therefore, we first acquire a rigged template, which is then statically skinned. Next, a coordinate-based MLP learns a skinning weights field parameterized over the position in a canonical pose space and the respective pose. Moreover, we introduce our pose- and view-dependent appearance field allowing us to differentiably render and supervise the posed mesh using multi-view imagery. We show that our approach outperforms state-of-the-art while not relying on dense 4D scans.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2306.00547.pdf' target='_blank'>https://arxiv.org/pdf/2306.00547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00547">AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Capturing and editing full head performances enables the creation of virtual characters with various applications such as extended reality and media production. The past few years witnessed a steep rise in the photorealism of human head avatars. Such avatars can be controlled through different input data modalities, including RGB, audio, depth, IMUs and others. While these data modalities provide effective means of control, they mostly focus on editing the head movements such as the facial expressions, head pose and/or camera viewpoint. In this paper, we propose AvatarStudio, a text-based method for editing the appearance of a dynamic full head avatar. Our approach builds on existing work to capture dynamic performances of human heads using neural radiance field (NeRF) and edits this representation with a text-to-image diffusion model. Specifically, we introduce an optimization strategy for incorporating multiple keyframes representing different camera viewpoints and time stamps of a video performance into a single diffusion model. Using this personalized diffusion model, we edit the dynamic NeRF by introducing view-and-time-aware Score Distillation Sampling (VT-SDS) following a model-based guidance approach. Our method edits the full head in a canonical space, and then propagates these edits to remaining time steps via a pretrained deformation network. We evaluate our method visually and numerically via a user study, and results show that our method outperforms existing approaches. Our experiments validate the design choices of our method and highlight that our edits are genuine, personalized, as well as 3D- and time-consistent.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2212.08811.pdf' target='_blank'>https://arxiv.org/pdf/2212.08811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Quang Hieu, Dinh Thai Hoang, Diep N. Nguyen, Eryk Dutkiewicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08811">Toward BCI-enabled Metaverse: A Joint Learning and Resource Allocation Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Toward user-driven Metaverse applications with fast wireless connectivity and tremendous computing demand through future 6G infrastructures, we propose a Brain-Computer Interface (BCI) enabled framework that paves the way for the creation of intelligent human-like avatars. Our approach takes a first step toward the Metaverse systems in which the digital avatars are envisioned to be more intelligent by collecting and analyzing brain signals through cellular networks. In our proposed system, Metaverse users experience Metaverse applications while sending their brain signals via uplink wireless channels in order to create intelligent human-like avatars at the base station. As such, the digital avatars can not only give useful recommendations for the users but also enable the system to create user-driven applications. Our proposed framework involves a mixed decision-making and classification problem in which the base station has to allocate its computing and radio resources to the users and classify the brain signals of users in an efficient manner. To this end, we propose a hybrid training algorithm that utilizes recent advances in deep reinforcement learning to address the problem. Specifically, our hybrid training algorithm contains three deep neural networks cooperating with each other to enable better realization of the mixed decision-making and classification problem. Simulation results show that our proposed framework can jointly address resource allocation for the system and classify brain signals of the users with highly accurate predictions.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2212.07555.pdf' target='_blank'>https://arxiv.org/pdf/2212.07555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.07555">IMos: Intent-Driven Full-Body Motion Synthesis for Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we make virtual characters in a scene interact with their surrounding objects through simple instructions? Is it possible to synthesize such motion plausibly with a diverse set of objects and instructions? Inspired by these questions, we present the first framework to synthesize the full-body motion of virtual human characters performing specified actions with 3D objects placed within their reach. Our system takes textual instructions specifying the objects and the associated intentions of the virtual characters as input and outputs diverse sequences of full-body motions. This contrasts existing works, where full-body action synthesis methods generally do not consider object interactions, and human-object interaction methods focus mainly on synthesizing hand or finger movements for grasping objects. We accomplish our objective by designing an intent-driven fullbody motion generator, which uses a pair of decoupled conditional variational auto-regressors to learn the motion of the body parts in an autoregressive manner. We also optimize the 6-DoF pose of the objects such that they plausibly fit within the hands of the synthesized characters. We compare our proposed method with the existing methods of motion synthesis and establish a new and stronger state-of-the-art for the task of intent-driven motion synthesis.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2203.08133.pdf' target='_blank'>https://arxiv.org/pdf/2203.08133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.08133">Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce a pose-driven deformation field based on the linear blend skinning algorithm, which combines the blend weight field and the 3D human skeleton to produce observation-to-canonical correspondences. Since 3D human skeletons are more observable, they can regularize the learning of the deformation field. Moreover, the pose-driven deformation field can be controlled by input skeletal motions to generate new deformation fields to animate the canonical human model. Experiments show that our approach significantly outperforms recent human modeling methods. The code is available at https://zju3dv.github.io/animatable_nerf/.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2506.04606.pdf' target='_blank'>https://arxiv.org/pdf/2506.04606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Huang-Menders, Xinhang Liu, Andy Xu, Yuyao Zhang, Chi-Keung Tang, Yu-Wing Tai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04606">SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SmartAvatar is a vision-language-agent-driven framework for generating fully rigged, animation-ready 3D human avatars from a single photo or textual prompt. While diffusion-based methods have made progress in general 3D object generation, they continue to struggle with precise control over human identity, body shape, and animation readiness. In contrast, SmartAvatar leverages the commonsense reasoning capabilities of large vision-language models (VLMs) in combination with off-the-shelf parametric human generators to deliver high-quality, customizable avatars. A key innovation is an autonomous verification loop, where the agent renders draft avatars, evaluates facial similarity, anatomical plausibility, and prompt alignment, and iteratively adjusts generation parameters for convergence. This interactive, AI-guided refinement process promotes fine-grained control over both facial and body features, enabling users to iteratively refine their avatars via natural-language conversations. Unlike diffusion models that rely on static pre-trained datasets and offer limited flexibility, SmartAvatar brings users into the modeling loop and ensures continuous improvement through an LLM-driven procedural generation and verification system. The generated avatars are fully rigged and support pose manipulation with consistent identity and appearance, making them suitable for downstream animation and interactive applications. Quantitative benchmarks and user studies demonstrate that SmartAvatar outperforms recent text- and image-driven avatar generation systems in terms of reconstructed mesh quality, identity fidelity, attribute accuracy, and animation readiness, making it a versatile tool for realistic, customizable avatar creation on consumer-grade hardware.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2503.08165.pdf' target='_blank'>https://arxiv.org/pdf/2503.08165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08165">Multimodal Generation of Animatable 3D Human Models with AvatarForge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AvatarForge, a framework for generating animatable 3D human avatars from text or image inputs using AI-driven procedural generation. While diffusion-based methods have made strides in general 3D object generation, they struggle with high-quality, customizable human avatars due to the complexity and diversity of human body shapes, poses, exacerbated by the scarcity of high-quality data. Additionally, animating these avatars remains a significant challenge for existing methods. AvatarForge overcomes these limitations by combining LLM-based commonsense reasoning with off-the-shelf 3D human generators, enabling fine-grained control over body and facial details. Unlike diffusion models which often rely on pre-trained datasets lacking precise control over individual human features, AvatarForge offers a more flexible approach, bringing humans into the iterative design and modeling loop, with its auto-verification system allowing for continuous refinement of the generated avatars, and thus promoting high accuracy and customization. Our evaluations show that AvatarForge outperforms state-of-the-art methods in both text- and image-to-avatar generation, making it a versatile tool for artistic creation and animation.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2312.15059.pdf' target='_blank'>https://arxiv.org/pdf/2312.15059.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15059">Deformable 3D Gaussian Splatting for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2312.07385.pdf' target='_blank'>https://arxiv.org/pdf/2312.07385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiming Zhang, Zhihao Yuan, Chaoda Zheng, Xu Yan, Baoyuan Wang, Guanbin Li, Song Wu, Shuguang Cui, Zhen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07385">GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although existing speech-driven talking face generation methods achieve significant progress, they are far from real-world application due to the avatar-specific training demand and unstable lip movements. To address the above issues, we propose the GSmoothFace, a novel two-stage generalized talking face generation model guided by a fine-grained 3d face model, which can synthesize smooth lip dynamics while preserving the speaker's identity. Our proposed GSmoothFace model mainly consists of the Audio to Expression Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT) module. Specifically, we first develop the A2EP module to predict expression parameters synchronized with the driven speech. It uses a transformer to capture the long-term audio context and learns the parameters from the fine-grained 3D facial vertices, resulting in accurate and smooth lip-synchronization performance. Afterward, the well-designed TAFT module, empowered by Morphology Augmented Face Blending (MAFB), takes the predicted expression parameters and target video as inputs to modify the facial region of the target video without distorting the background content. The TAFT effectively exploits the identity appearance and background context in the target video, which makes it possible to generalize to different speakers without retraining. Both quantitative and qualitative experiments confirm the superiority of our method in terms of realism, lip synchronization, and visual quality. See the project page for code, data, and request pre-trained models: https://zhanghm1995.github.io/GSmoothFace.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2301.02700.pdf' target='_blank'>https://arxiv.org/pdf/2301.02700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, Sergey Tulyakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02700">3DAvatarGAN: Bridging Domains for Personalized Editable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We then distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling -- as a byproduct -- personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions -- for the first time -- allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2509.18924.pdf' target='_blank'>https://arxiv.org/pdf/2509.18924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Teotia, Helge Rhodin, Mohit Mendiratta, Hyeongwoo Kim, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18924">Audio-Driven Universal Gaussian Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the first method for audio-driven universal photorealistic avatar synthesis, combining a person-agnostic speech model with our novel Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity multi-view videos. In particular, our UHAP is supervised with neutral scan data, enabling it to capture the identity-specific details at high fidelity. In contrast to previous approaches, which predominantly map audio features to geometric deformations only while ignoring audio-dependent appearance variations, our universal speech model directly maps raw audio inputs into the UHAP latent expression space. This expression space inherently encodes, both, geometric and appearance variations. For efficient personalization to new subjects, we employ a monocular encoder, which enables lightweight regression of dynamic expression variations across video frames. By accounting for these expression-dependent changes, it enables the subsequent model fine-tuning stage to focus exclusively on capturing the subject's global appearance and geometry. Decoding these audio-driven expression codes via UHAP generates highly realistic avatars with precise lip synchronization and nuanced expressive details, such as eyebrow movement, gaze shifts, and realistic mouth interior appearance as well as motion. Extensive evaluations demonstrate that our method is not only the first generalizable audio-driven avatar model that can account for detailed appearance modeling and rendering, but it also outperforms competing (geometry-only) methods across metrics measuring lip-sync accuracy, quantitative image quality, and perceptual realism.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2506.19833.pdf' target='_blank'>https://arxiv.org/pdf/2506.19833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yubo Huang, Weiqiang Wang, Sirui Zhao, Tong Xu, Lin Liu, Enhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19833">Bind-Your-Avatar: Multi-Talking-Character Video Generation with Dynamic 3D-mask-based Embedding Router</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed remarkable advances in audio-driven talking head generation. However, existing approaches predominantly focus on single-character scenarios. While some methods can create separate conversation videos between two individuals, the critical challenge of generating unified conversation videos with multiple physically co-present characters sharing the same spatial environment remains largely unaddressed. This setting presents two key challenges: audio-to-character correspondence control and the lack of suitable datasets featuring multi-character talking videos within the same scene. To address these challenges, we introduce Bind-Your-Avatar, an MM-DiT-based model specifically designed for multi-talking-character video generation in the same scene. Specifically, we propose (1) A novel framework incorporating a fine-grained Embedding Router that binds `who' and `speak what' together to address the audio-to-character correspondence control. (2) Two methods for implementing a 3D-mask embedding router that enables frame-wise, fine-grained control of individual characters, with distinct loss functions based on observed geometric priors and a mask refinement strategy to enhance the accuracy and temporal smoothness of the predicted masks. (3) The first dataset, to the best of our knowledge, specifically constructed for multi-talking-character video generation, and accompanied by an open-source data processing pipeline, and (4) A benchmark for the dual-talking-characters video generation, with extensive experiments demonstrating superior performance over multiple state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2506.01802.pdf' target='_blank'>https://arxiv.org/pdf/2506.01802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Zhu, Guoxing Sun, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01802">UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning an animatable and clothed human avatar model with vivid dynamics and photorealistic appearance from multi-view videos is an important foundational research problem in computer graphics and vision. Fueled by recent advances in implicit representations, the quality of the animatable avatars has achieved an unprecedented level by attaching the implicit representation to drivable human template meshes. However, they usually fail to preserve the highest level of detail, particularly apparent when the virtual camera is zoomed in and when rendering at 4K resolution and higher. We argue that this limitation stems from inaccurate surface tracking, specifically, depth misalignment and surface drift between character geometry and the ground truth surface, which forces the detailed appearance model to compensate for geometric errors. To address this, we propose a latent deformation model and supervising the 3D deformation of the animatable character using guidance from foundational 2D video point trackers, which offer improved robustness to shading and surface variations, and are less prone to local minima than differentiable rendering. To mitigate the drift over time and lack of 3D awareness of 2D point trackers, we introduce a cascaded training strategy that generates consistent 3D point tracks by anchoring point tracks to the rendered avatar, which ultimately supervises our avatar at the vertex and texel level. To validate the effectiveness of our approach, we introduce a novel dataset comprising five multi-view video sequences, each over 10 minutes in duration, captured using 40 calibrated 6K-resolution cameras, featuring subjects dressed in clothing with challenging texture patterns and wrinkle deformations. Our approach demonstrates significantly improved performance in rendering quality and geometric accuracy over the prior state of the art.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2505.15385.pdf' target='_blank'>https://arxiv.org/pdf/2505.15385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendrik Junkawitsch, Guoxing Sun, Heming Zhu, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15385">EVA: Expressive Virtual Avatars from Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an expressive template geometry layer and a 3D Gaussian appearance layer. First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2503.23094.pdf' target='_blank'>https://arxiv.org/pdf/2503.23094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Boscolo Camiletto, Jian Wang, Eduardo Alvarado, Rishabh Dabral, Thabo Beeler, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23094">FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric motion capture with a head-mounted body-facing stereo camera is crucial for VR and AR applications but presents significant challenges such as heavy occlusions and limited annotated real-world data. Existing methods rely on synthetic pretraining and struggle to generate smooth and accurate predictions in real-world settings, particularly for lower limbs. Our work addresses these limitations by introducing a lightweight VR-based data collection setup with on-board, real-time 6D pose tracking. Using this setup, we collected the most extensive real-world dataset for ego-facing ego-mounted cameras to date in size and motion variability. Effectively integrating this multimodal input -- device pose and camera feeds -- is challenging due to the differing characteristics of each data source. To address this, we propose FRAME, a simple yet effective architecture that combines device pose and camera feeds for state-of-the-art body pose prediction through geometrically sound multimodal integration and can run at 300 FPS on modern hardware. Lastly, we showcase a novel training strategy to enhance the model's generalization capabilities. Our approach exploits the problem's geometric properties, yielding high-quality motion capture free from common artifacts in prior works. Qualitative and quantitative evaluations, along with extensive comparisons, demonstrate the effectiveness of our method. Data, code, and CAD designs will be available at https://vcai.mpi-inf.mpg.de/projects/FRAME/
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2410.01835.pdf' target='_blank'>https://arxiv.org/pdf/2410.01835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchun Chen, Jian Wang, Yinda Zhang, Rohit Pandey, Thabo Beeler, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01835">EgoAvatar: Egocentric View-Driven and Photorealistic Full-body Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive VR telepresence ideally means being able to interact and communicate with digital avatars that are indistinguishable from and precisely reflect the behaviour of their real counterparts. The core technical challenge is two fold: Creating a digital double that faithfully reflects the real human and tracking the real human solely from egocentric sensing devices that are lightweight and have a low energy consumption, e.g. a single RGB camera. Up to date, no unified solution to this problem exists as recent works solely focus on egocentric motion capture, only model the head, or build avatars from multi-view captures. In this work, we, for the first time in literature, propose a person-specific egocentric telepresence approach, which jointly models the photoreal digital avatar while also driving it from a single egocentric video. We first present a character model that is animatible, i.e. can be solely driven by skeletal motion, while being capable of modeling geometry and appearance. Then, we introduce a personalized egocentric motion capture component, which recovers full-body motion from an egocentric video. Finally, we apply the recovered pose to our character model and perform a test-time mesh refinement such that the geometry faithfully projects onto the egocentric view. To validate our design choices, we propose a new and challenging benchmark, which provides paired egocentric and dense multi-view videos of real humans performing various motions. Our experiments demonstrate a clear step towards egocentric and photoreal telepresence as our method outperforms baselines as well as competing methods. For more details, code, and data, we refer to our project page.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2409.11951.pdf' target='_blank'>https://arxiv.org/pdf/2409.11951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Teotia, Hyeongwoo Kim, Pablo Garrido, Marc Habermann, Mohamed Elgharib, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11951">GaussianHeads: End-to-End Learning of Drivable Gaussian Head Avatars from Coarse-to-fine Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time rendering of human head avatars is a cornerstone of many computer graphics applications, such as augmented reality, video games, and films, to name a few. Recent approaches address this challenge with computationally efficient geometry primitives in a carefully calibrated multi-view setup. Albeit producing photorealistic head renderings, it often fails to represent complex motion changes such as the mouth interior and strongly varying head poses. We propose a new method to generate highly dynamic and deformable human head avatars from multi-view imagery in real-time. At the core of our method is a hierarchical representation of head models that allows to capture the complex dynamics of facial expressions and head movements. First, with rich facial features extracted from raw input frames, we learn to deform the coarse facial geometry of the template mesh. We then initialize 3D Gaussians on the deformed surface and refine their positions in a fine step. We train this coarse-to-fine facial avatar model along with the head pose as a learnable parameter in an end-to-end framework. This enables not only controllable facial animation via video inputs, but also high-fidelity novel view synthesis of challenging facial expressions, such as tongue deformations and fine-grained teeth structure under large motion changes. Moreover, it encourages the learned head avatar to generalize towards new facial expressions and head poses at inference time. We demonstrate the performance of our method with comparisons against the related methods on different datasets, spanning challenging facial expression sequences across multiple identities. We also show the potential application of our approach by demonstrating a cross-identity facial performance transfer application.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2408.15995.pdf' target='_blank'>https://arxiv.org/pdf/2408.15995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basavaraj Sunagad, Heming Zhu, Mohit Mendiratta, Adam Kortylewski, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15995">TEDRA: Text-based Editing of Dynamic and Photoreal Actors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past years, significant progress has been made in creating photorealistic and drivable 3D avatars solely from videos of real humans. However, a core remaining challenge is the fine-grained and user-friendly editing of clothing styles by means of textual descriptions. To this end, we present TEDRA, the first method allowing text-based edits of an avatar, which maintains the avatar's high fidelity, space-time coherency, as well as dynamics, and enables skeletal pose and view control. We begin by training a model to create a controllable and high-fidelity digital replica of the real actor. Next, we personalize a pretrained generative diffusion model by fine-tuning it on various frames of the real character captured from different camera angles, ensuring the digital representation faithfully captures the dynamics and movements of the real person. This two-stage process lays the foundation for our approach to dynamic human avatar editing. Utilizing this personalized diffusion model, we modify the dynamic avatar based on a provided text prompt using our Personalized Normal Aligned Score Distillation Sampling (PNA-SDS) within a model-based guidance framework. Additionally, we propose a time step annealing strategy to ensure high-quality edits. Our results demonstrate a clear improvement over prior work in functionality and visual quality.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2312.05941.pdf' target='_blank'>https://arxiv.org/pdf/2312.05941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05941">ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2310.11449.pdf' target='_blank'>https://arxiv.org/pdf/2310.11449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11449">DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance. This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime. Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time. The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2210.12003.pdf' target='_blank'>https://arxiv.org/pdf/2210.12003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-Moll, Michael Zollhoefer, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12003">HDHumans: A Hybrid Approach for High-fidelity Digital Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photo-real digital human avatars are of enormous importance in graphics, as they enable immersive communication over the globe, improve gaming and entertainment experiences, and can be particularly beneficial for AR and VR settings. However, current avatar generation approaches either fall short in high-fidelity novel view synthesis, generalization to novel motions, reproduction of loose clothing, or they cannot render characters at the high resolution offered by modern displays. To this end, we propose HDHumans, which is the first method for HD human character synthesis that jointly produces an accurate and temporally coherent 3D deforming surface and highly photo-realistic images of arbitrary novel views and of motions not seen at training time. At the technical core, our method tightly integrates a classical deforming character template with neural radiance fields (NeRF). Our method is carefully designed to achieve a synergy between classical surface deformation and NeRF. First, the template guides the NeRF, which allows synthesizing novel views of a highly dynamic and articulated character and even enables the synthesis of novel motions. Second, we also leverage the dense pointclouds resulting from NeRF to further improve the deforming surface via 3D-to-3D supervision. We outperform the state of the art quantitatively and qualitatively in terms of synthesis quality and resolution, as well as the quality of 3D surface reconstruction.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2306.03576.pdf' target='_blank'>https://arxiv.org/pdf/2306.03576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Sun, Dingkang Yang, Dongliang Kou, Yang Jiang, Weihua Shan, Zhe Yan, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03576">Human 3D Avatar Modeling with Implicit Neural Representation: A Brief Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A human 3D avatar is one of the important elements in the metaverse, and the modeling effect directly affects people's visual experience. However, the human body has a complex topology and diverse details, so it is often expensive, time-consuming, and laborious to build a satisfactory model. Recent studies have proposed a novel method, implicit neural representation, which is a continuous representation method and can describe objects with arbitrary topology at arbitrary resolution. Researchers have applied implicit neural representation to human 3D avatar modeling and obtained more excellent results than traditional methods. This paper comprehensively reviews the application of implicit neural representation in human body modeling. First, we introduce three implicit representations of occupancy field, SDF, and NeRF, and make a classification of the literature investigated in this paper. Then the application of implicit modeling methods in the body, hand, and head are compared and analyzed respectively. Finally, we point out the shortcomings of current work and provide available suggestions for researchers.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2407.12371.pdf' target='_blank'>https://arxiv.org/pdf/2407.12371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xintao Lv, Liang Xu, Yichao Yan, Xin Jin, Congsheng Xu, Shuwen Wu, Yifan Liu, Lincheng Li, Mengxiao Bi, Wenjun Zeng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12371">HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating human-object interactions (HOIs) is critical with the tremendous advances of digital avatars. Existing datasets are typically limited to humans interacting with a single object while neglecting the ubiquitous manipulation of multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of full-body human interacting with multiple objects, containing 3.3K 4D HOI sequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual descriptions and temporal segments, benchmarking two novel tasks of HOI synthesis conditioned on either the whole text prompt or the segmented text prompts as fine-grained timeline control. To address these novel tasks, we propose a dual-branch conditional diffusion model with a mutual interaction module for HOI synthesis. Besides, an auto-regressive generation pipeline is also designed to obtain smooth transitions between HOI segments. Experimental results demonstrate the generalization ability to unseen object geometries and temporal compositions.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2304.03903.pdf' target='_blank'>https://arxiv.org/pdf/2304.03903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi, Xudong Liu, Guo-Jun Qi, Yong Zhang, Xuan Wang, Xiangyu Zhu, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03903">High-Fidelity Clothed Avatar Reconstruction from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence o f the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2204.11184.pdf' target='_blank'>https://arxiv.org/pdf/2204.11184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Zhu, Tingting Liao, Jiangjing Lyu, Xiang Yan, Yunfeng Wang, Kan Guo, Qiong Cao, Stan Z. Li, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.11184">MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider a novel problem of reconstructing a 3D human avatar from multiple unconstrained frames, independent of assumptions on camera calibration, capture space, and constrained actions. The problem should be addressed by a framework that takes multiple unconstrained images as inputs, and generates a shape-with-skinning avatar in the canonical space, finished in one feed-forward pass. To this end, we present 3D Avatar Reconstruction in the wild (ARwild), which first reconstructs the implicit skinning fields in a multi-level manner, by which the image features from multiple images are aligned and integrated to estimate a pixel-aligned implicit function that represents the clothed shape. To enable the training and testing of the new framework, we contribute a large-scale dataset, MVP-Human (Multi-View and multi-Pose 3D Human), which contains 400 subjects, each of which has 15 scans in different poses and 8-view images for each pose, providing 6,000 3D scans and 48,000 images in total. Overall, benefits from the specific network architecture and the diverse data, the trained model enables 3D avatar reconstruction from unconstrained frames and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2509.00403.pdf' target='_blank'>https://arxiv.org/pdf/2509.00403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushuo Chen, Ruizhi Shao, Youxin Pang, Hongwen Zhang, Xinyi Wu, Rihui Wu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00403">DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2411.15205.pdf' target='_blank'>https://arxiv.org/pdf/2411.15205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Zhuang, Di Kang, Linchao Bao, Liang Lin, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15205">DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven avatar generation has gained significant attention owing to its convenience. However, existing methods typically model the human body with all garments as a single 3D model, limiting its usability, such as clothing replacement, and reducing user control over the generation process. To overcome the limitations above, we propose DAGSM, a novel pipeline that generates disentangled human bodies and garments from the given text prompts. Specifically, we model each part (e.g., body, upper/lower clothes) of the clothed human as one GS-enhanced mesh (GSM), which is a traditional mesh attached with 2D Gaussians to better handle complicated textures (e.g., woolen, translucent clothes) and produce realistic cloth animations. During the generation, we first create the unclothed body, followed by a sequence of individual cloth generation based on the body, where we introduce a semantic-based algorithm to achieve better human-cloth and garment-garment separation. To improve texture quality, we propose a view-consistent texture refinement module, including a cross-view attention mechanism for texture style consistency and an incident-angle-weighted denoising (IAW-DE) strategy to update the appearance. Extensive experiments have demonstrated that DAGSM generates high-quality disentangled avatars, supports clothing replacement and realistic animation, and outperforms the baselines in visual quality.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2405.07319.pdf' target='_blank'>https://arxiv.org/pdf/2405.07319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07319">LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2312.03029.pdf' target='_blank'>https://arxiv.org/pdf/2312.03029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanfeng Liao, Yuelang Xu, Zhe Li, Qijing Li, Boyao Zhou, Ruifeng Bai, Di Xu, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03029">HHAvatar: Gaussian Head Avatar with Dynamic Hairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity 3D head avatars has always been a research hotspot, but it remains a great challenge under lightweight sparse view setups. In this paper, we propose HHAvatar represented by controllable 3D Gaussians for high-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians to represent the appearance of the head, and then jointly optimize neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. To address the problem of dynamic hair modeling, we introduce a hybrid head model into our avatar representation based Gaussian Head Avatar and a training method that considers timing information and an occlusion perception module to model the non-rigid motion of hair. Experiments show that our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions and driving hairs reasonably with the motion of the head
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2309.17128.pdf' target='_blank'>https://arxiv.org/pdf/2309.17128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen Zhang, Jinli Suo, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17128">HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of modeling an animatable 3D human head avatar under light-weight setups is of significant importance but has not been well solved. Existing 3D representations either perform well in the realism of portrait images synthesis or the accuracy of expression control, but not both. To address the problem, we introduce a novel hybrid explicit-implicit 3D representation, Facial Model Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF and the prior information from the parametric template. At the core of our representation, a synthetic-renderings-based condition method is proposed to fuse the prior information from the parametric model into the implicit field without constraining its topological flexibility. Besides, based on the hybrid representation, we properly overcome the inconsistent shape issue presented in existing methods and improve the animation stability. Moreover, by adopting an overall GAN-based architecture using an image-to-image translation network, we achieve high-resolution, realistic and view-consistent synthesis of dynamic head appearance. Experiments demonstrate that our method can achieve state-of-the-art performance for 3D head avatar animation compared with previous methods.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2308.05925.pdf' target='_blank'>https://arxiv.org/pdf/2308.05925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqi Su, Liangxiao Hu, Siyou Lin, Hongwen Zhang, Shengping Zhang, Justus Thies, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05925">CaPhy: Capturing Physical Properties for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and physical properties of the clothing from real observations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing. To this end, we combine unsupervised training with physics-based losses and 3D-supervised training using scanned data to reconstruct a dynamic model of clothing that is physically realistic and conforms to the human scans. We also optimize the physical parameters of the underlying physical model from the scans by introducing gradient constraints of the physics-based losses. In contrast to previous work on 3D avatar reconstruction, our method is able to generalize to novel poses with realistic dynamic cloth deformations. Experiments on several subjects demonstrate that our method can estimate the physical properties of the garments, resulting in superior quantitative and qualitative results compared with previous methods.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2308.04914.pdf' target='_blank'>https://arxiv.org/pdf/2308.04914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xumin Huang, Yuan Wu, Jiawen Kang, Jiangtian Nie, Weifeng Zhong, Dong In Kim, Shengli Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04914">Service Reservation and Pricing for Green Metaverses: A Stackelberg Game Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse enables users to communicate, collaborate and socialize with each other through their digital avatars. Due to the spatio-temporal characteristics, co-located users are served well by performing their software components in a collaborative manner such that a Metaverse service provider (MSP) eliminates redundant data transmission and processing, ultimately reducing the total energy consumption. The energyefficient service provision is crucial for enabling the green and sustainable Metaverse. In this article, we take an augmented reality (AR) application as an example to achieve this goal. Moreover, we study an economic issue on how the users reserve offloading services from the MSP and how the MSP determines an optimal charging price since each user is rational to decide whether to accept the offloading service by taking into account the monetary cost. A single-leader multi-follower Stackelberg game is formulated between the MSP and users while each user optimizes an offloading probability to minimize the weighted sum of time, energy consumption and monetary cost. Numerical results show that our scheme achieves energy savings and satisfies individual rationality simultaneously compared with the conventional schemes. Finally, we identify and discuss open directions on how several emerging technologies are combined with the sustainable green Metaverse.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2305.04789.pdf' target='_blank'>https://arxiv.org/pdf/2305.04789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04789">AvatarReX: Real-time Expressive Full-body Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present AvatarReX, a new method for learning NeRF-based full-body avatars from video data. The learnt avatar not only provides expressive control of the body, hands and the face together, but also supports real-time animation and rendering. To this end, we propose a compositional avatar representation, where the body, hands and the face are separately modeled in a way that the structural prior from parametric mesh templates is properly utilized without compromising representation flexibility. Furthermore, we disentangle the geometry and appearance for each part. With these technical designs, we propose a dedicated deferred rendering pipeline, which can be executed in real-time framerate to synthesize high-quality free-view images. The disentanglement of geometry and appearance also allows us to design a two-pass training strategy that combines volume rendering and surface rendering for network training. In this way, patch-level supervision can be applied to force the network to learn sharp appearance details on the basis of geometry estimation. Overall, our method enables automatic construction of expressive full-body avatars with real-time rendering capability, and can generate photo-realistic images with dynamic details for novel body motions and facial expressions.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2305.01190.pdf' target='_blank'>https://arxiv.org/pdf/2305.01190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen Zhao, Han Huang, Guojun Qi, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01190">LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing approaches to animatable NeRF-based head avatars are either built upon face templates or use the expression coefficients of templates as the driving signal. Despite the promising progress, their performances are heavily bound by the expression power and the tracking accuracy of the templates. In this work, we present LatentAvatar, an expressive neural head avatar driven by latent expression codes. Such latent expression codes are learned in an end-to-end and self-supervised manner without templates, enabling our method to get rid of expression and tracking issues. To achieve this, we leverage a latent head NeRF to learn the person-specific latent expression codes from a monocular portrait video, and further design a Y-shaped network to learn the shared latent expression codes of different subjects for cross-identity reenactment. By optimizing the photometric reconstruction objectives in NeRF, the latent expression codes are learned to be 3D-aware while faithfully capturing the high-frequency detailed expressions. Moreover, by learning a mapping between the latent expression code learned in shared and person-specific settings, LatentAvatar is able to perform expressive reenactment between different subjects. Experimental results show that our LatentAvatar is able to capture challenging expressions and the subtle movement of teeth and even eyeballs, which outperforms previous state-of-the-art solutions in both quantitative and qualitative comparisons. Project page: https://www.liuyebin.com/latentavatar.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2304.03167.pdf' target='_blank'>https://arxiv.org/pdf/2304.03167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwen Zhang, Siyou Lin, Ruizhi Shao, Yuxiang Zhang, Zerong Zheng, Han Huang, Yandong Guo, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03167">CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating animatable avatars from static scans requires the modeling of clothing deformations in different poses. Existing learning-based methods typically add pose-dependent deformations upon a minimally-clothed mesh template or a learned implicit template, which have limitations in capturing details or hinder end-to-end learning. In this paper, we revisit point-based solutions and propose to decompose explicit garment-related templates and then add pose-dependent wrinkles to them. In this way, the clothing deformations are disentangled such that the pose-dependent wrinkles can be better learned and applied to unseen poses. Additionally, to tackle the seam artifact issues in recent state-of-the-art point-based methods, we propose to learn point features on a body surface, which establishes a continuous and compact feature space to capture the fine-grained and pose-dependent clothing geometry. To facilitate the research in this field, we also introduce a high-quality scan dataset of humans in real-world clothing. Our approach is validated on two existing datasets and our newly introduced dataset, showing better clothing deformation results in unseen poses. The project page with code and dataset can be found at https://www.liuyebin.com/closet.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2211.13206.pdf' target='_blank'>https://arxiv.org/pdf/2211.13206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13206">AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural Voxels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With NeRF widely used for facial reenactment, recent methods can recover photo-realistic 3D head avatar from just a monocular video. Unfortunately, the training process of the NeRF-based methods is quite time-consuming, as MLP used in the NeRF-based methods is inefficient and requires too many iterations to converge. To overcome this problem, we propose AvatarMAV, a fast 3D head avatar reconstruction method using Motion-Aware Neural Voxels. AvatarMAV is the first to model both the canonical appearance and the decoupled expression motion by neural voxels for head avatar. In particular, the motion-aware neural voxels is generated from the weighted concatenation of multiple 4D tensors. The 4D tensors semantically correspond one-to-one with 3DMM expression basis and share the same weights as 3DMM expression coefficients. Benefiting from our novel representation, the proposed AvatarMAV can recover photo-realistic head avatars in just 5 minutes (implemented with pure PyTorch), which is significantly faster than the state-of-the-art facial reenactment methods. Project page: https://www.liuyebin.com/avatarmav.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2211.11208.pdf' target='_blank'>https://arxiv.org/pdf/2211.11208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11208">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D-aware generative adversarial networks (GANs) synthesize high-fidelity and multi-view-consistent facial images using only collections of single-view 2D imagery. Towards fine-grained control over facial attributes, recent efforts incorporate 3D Morphable Face Model (3DMM) to describe deformation in generative radiance fields either explicitly or implicitly. Explicit methods provide fine-grained expression control but cannot handle topological changes caused by hair and accessories, while implicit ones can model varied topologies but have limited generalization caused by the unconstrained deformation fields. We propose a novel 3D GAN framework for unsupervised learning of generative, high-quality and 3D-consistent facial avatars from unstructured 2D images. To achieve both deformation accuracy and topological flexibility, we propose a 3D representation called Generative Texture-Rasterized Tri-planes. The proposed representation learns Generative Neural Textures on top of parametric mesh templates and then projects them into three orthogonal-viewed feature planes through rasterization, forming a tri-plane feature representation for volume rendering. In this way, we combine both fine-grained expression control of mesh-guided explicit deformation and the flexibility of implicit volumetric representation. We further propose specific modules for modeling mouth interior which is not taken into account by 3DMM. Our method demonstrates state-of-the-art 3D-aware synthesis quality and animation ability through extensive experiments. Furthermore, serving as 3D prior, our animatable 3D representation boosts multiple applications including one-shot facial avatars and 3D-aware stylization.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2312.08889.pdf' target='_blank'>https://arxiv.org/pdf/2312.08889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyou Xu, Zongxin Yang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08889">SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: https://yoxu515.github.io/SEEAvatar/.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2309.04946.pdf' target='_blank'>https://arxiv.org/pdf/2309.04946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04946">Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking-head synthesis is a popular research topic for virtual human-related applications. However, the inflexibility and inefficiency of existing methods, which necessitate expensive end-to-end training to transfer emotions from guidance videos to talking-head predictions, are significant limitations. In this work, we propose the Emotional Adaptation for Audio-driven Talking-head (EAT) method, which transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adaptations. Our approach utilizes a pretrained emotion-agnostic talking-head transformer and introduces three lightweight adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and Emotional Adaptation Module) from different perspectives to enable precise and realistic emotion controls. Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including LRW and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable generalization ability, even in scenarios where emotional training videos are scarce or nonexistent. Project website: https://yuangan.github.io/eat/
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2309.04076.pdf' target='_blank'>https://arxiv.org/pdf/2309.04076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, David Lo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04076">Greening Large Language Models of Code</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models of code have shown remarkable effectiveness across various software engineering tasks. Despite the availability of many cloud services built upon these powerful models, there remain several scenarios where developers cannot take full advantage of them, stemming from factors such as restricted or unreliable internet access, institutional privacy policies that prohibit external transmission of code to third-party vendors, and more. Therefore, developing a compact, efficient, and yet energy-saving model for deployment on developers' devices becomes essential.
  To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code by optimizing it in terms of model size, inference latency, energy consumption, and carbon footprint while maintaining a comparable level of effectiveness. The key idea of Avatar is to formulate the optimization of language models as a multi-objective configuration tuning problem and solve it with the help of a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The SMT solver is used to form an appropriate configuration space, while the optimization algorithm identifies the Pareto-optimal set of configurations for training the optimized models using knowledge distillation. We evaluate Avatar with two popular language models of code, i.e., CodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulnerability prediction and clone detection. We use Avatar to produce optimized models with a small size (3 MB), which is 160$\times$ smaller than the original large models. On the two tasks, the optimized models significantly reduce the energy consumption (up to 184$\times$ less), carbon footprint (up to 157$\times$ less), and inference latency (up to 76$\times$ faster), with only a negligible loss in effectiveness (1.67\% on average).
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2307.06526.pdf' target='_blank'>https://arxiv.org/pdf/2307.06526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Huang, Zongxin Yang, Liangting Li, Yi Yang, Jia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06526">AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale pre-trained vision-language models allow for the zero-shot text-based generation of 3D avatars. The previous state-of-the-art method utilized CLIP to supervise neural implicit models that reconstructed a human body mesh. However, this approach has two limitations. Firstly, the lack of avatar-specific models can cause facial distortion and unrealistic clothing in the generated avatars. Secondly, CLIP only provides optimization direction for the overall appearance, resulting in less impressive results. To address these limitations, we propose AvatarFusion, the first framework to use a latent diffusion model to provide pixel-level guidance for generating human-realistic avatars while simultaneously segmenting clothing from the avatar's body. AvatarFusion includes the first clothing-decoupled neural implicit avatar model that employs a novel Dual Volume Rendering strategy to render the decoupled skin and clothing sub-models in one space. We also introduce a novel optimization method, called Pixel-Semantics Difference-Sampling (PS-DS), which semantically separates the generation of body and clothes, and generates a variety of clothing styles. Moreover, we establish the first benchmark for zero-shot text-to-avatar generation. Our experimental results demonstrate that our framework outperforms previous approaches, with significant improvements observed in all metrics. Additionally, since our model is clothing-decoupled, we can exchange the clothes of avatars. Code are available on our project page https://hansenhuang0823.github.io/AvatarFusion.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2504.14967.pdf' target='_blank'>https://arxiv.org/pdf/2504.14967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Wang, Xuan Wang, Ran Yi, Yanbo Fan, Jichen Hu, Jingcheng Zhu, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14967">3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to construct high-quality 3D head avatars. In this line of research, existing methods either fail to capture the dynamic textures or incur significant overhead in terms of runtime speed or storage space. To this end, we propose a novel method that addresses all the aforementioned demands. In specific, we introduce an expressive and compact representation that encodes texture-related attributes of the 3D Gaussians in the tensorial format. We store appearance of neutral expression in static tri-planes, and represents dynamic texture details for different expressions using lightweight 1D feature lines, which are then decoded into opacity offset relative to the neutral face. We further propose adaptive truncated opacity penalty and class-balanced sampling to improve generalization across different expressions. Experiments show this design enables accurate face dynamic details capturing while maintains real-time rendering and significantly reduces storage costs, thus broadening the applicability to more scenarios.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2502.08556.pdf' target='_blank'>https://arxiv.org/pdf/2502.08556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08556">Human-Centric Foundation Models: Perception, Generation and Agentic Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs) inspired by the success of generalist models, such as large language and vision models, have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding. (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content. (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis. (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2404.02152.pdf' target='_blank'>https://arxiv.org/pdf/2404.02152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02152">GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2306.03038.pdf' target='_blank'>https://arxiv.org/pdf/2306.03038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03038">HeadSculpt: Crafting 3D Head Avatars with Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text-guided 3D generative methods have made remarkable advancements in producing high-quality textures and geometry, capitalizing on the proliferation of large vision-language and image diffusion models. However, existing methods still struggle to create high-fidelity 3D head avatars in two aspects: (1) They rely mostly on a pre-trained text-to-image diffusion model whilst missing the necessary 3D awareness and head priors. This makes them prone to inconsistency and geometric distortions in the generated avatars. (2) They fall short in fine-grained editing. This is primarily due to the inherited limitations from the pre-trained 2D image diffusion models, which become more pronounced when it comes to 3D head avatars. In this work, we address these challenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculpt for crafting (i.e., generating and editing) 3D head avatars from textual prompts. Specifically, we first equip the diffusion model with 3D awareness by leveraging landmark-based control and a learned textual embedding representing the back view appearance of heads, enabling 3D-consistent head avatar generations. We further propose a novel identity-aware editing score distillation strategy to optimize a textured mesh with a high-resolution differentiable rendering technique. This enables identity preservation while following the editing instruction. We showcase HeadSculpt's superior fidelity and editing capabilities through comprehensive experiments and comparisons with existing methods.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2304.11835.pdf' target='_blank'>https://arxiv.org/pdf/2304.11835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonggan Fu, Yuecheng Li, Chenghui Li, Jason Saragih, Peizhao Zhang, Xiaoliang Dai, Yingyan Celine Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11835">Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-time Mobile Telepresence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time and robust photorealistic avatars for telepresence in AR/VR have been highly desired for enabling immersive photorealistic telepresence. However, there still exists one key bottleneck: the considerable computational expense needed to accurately infer facial expressions captured from headset-mounted cameras with a quality level that can match the realism of the avatar's human appearance. To this end, we propose a framework called Auto-CARD, which for the first time enables real-time and robust driving of Codec Avatars when exclusively using merely on-device computing resources. This is achieved by minimizing two sources of redundancy. First, we develop a dedicated neural architecture search technique called AVE-NAS for avatar encoding in AR/VR, which explicitly boosts both the searched architectures' robustness in the presence of extreme facial expressions and hardware friendliness on fast evolving AR/VR headsets. Second, we leverage the temporal redundancy in consecutively captured images during continuous rendering and develop a mechanism dubbed LATEX to skip the computation of redundant frames. Specifically, we first identify an opportunity from the linearity of the latent space derived by the avatar decoder and then propose to perform adaptive latent extrapolation for redundant frames. For evaluation, we demonstrate the efficacy of our Auto-CARD framework in real-time Codec Avatar driving settings, where we achieve a 5.05x speed-up on Meta Quest 2 while maintaining a comparable or even better animation quality than state-of-the-art avatar encoder designs.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2405.08527.pdf' target='_blank'>https://arxiv.org/pdf/2405.08527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arian Beckmann, Tilman Stephani, Felix Klotzsche, Yonghao Chen, Simon M. Hofmann, Arno Villringer, Michael Gaebler, Vadim Nikulin, Sebastian Bosse, Peter Eisert, Anna Hilsmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08527">EEG-Features for Generalized Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the advent of Deepfakes in digital media, the development of robust and reliable detection mechanism is urgently called for. In this study, we explore a novel approach to Deepfake detection by utilizing electroencephalography (EEG) measured from the neural processing of a human participant who viewed and categorized Deepfake stimuli from the FaceForensics++ datset. These measurements serve as input features to a binary support vector classifier, trained to discriminate between real and manipulated facial images. We examine whether EEG data can inform Deepfake detection and also if it can provide a generalized representation capable of identifying Deepfakes beyond the training domain. Our preliminary results indicate that human neural processing signals can be successfully integrated into Deepfake detection frameworks and hint at the potential for a generalized neural representation of artifacts in computer generated faces. Moreover, our study provides next steps towards the understanding of how digital realism is embedded in the human cognitive system, possibly enabling the development of more realistic digital avatars in the future.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2403.04380.pdf' target='_blank'>https://arxiv.org/pdf/2403.04380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wolfgang Paier, Paul Hinzer, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04380">Video-Driven Animation of Neural Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new approach for video-driven animation of high-quality neural 3D head models, addressing the challenge of person-independent animation from video input. Typically, high-quality generative models are learned for specific individuals from multi-view video footage, resulting in person-specific latent representations that drive the generation process. In order to achieve person-independent animation from video input, we introduce an LSTM-based animation network capable of translating person-independent expression features into personalized animation parameters of person-specific 3D head models. Our approach combines the advantages of personalized head models (high quality and realism) with the convenience of video-driven animation employing multi-person facial performance capture. We demonstrate the effectiveness of our approach on synthesized animations with high quality based on different source videos as well as an ablation study.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2401.00711.pdf' target='_blank'>https://arxiv.org/pdf/2401.00711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00711">Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human models directly from text helps reduce the cost and time of character modeling. However, achieving multi-attribute controllable and realistic 3D human avatar generation is still challenging due to feature coupling and the scarcity of realistic 3D human avatar datasets. To address these issues, we propose Text2Avatar, which can generate realistic-style 3D avatars based on the coupled text prompts. Text2Avatar leverages a discrete codebook as an intermediate feature to establish a connection between text and avatars, enabling the disentanglement of features. Furthermore, to alleviate the scarcity of realistic style 3D human avatar data, we utilize a pre-trained unconditional 3D human avatar generation model to obtain a large amount of 3D avatar pseudo data, which allows Text2Avatar to achieve realistic style generation. Experimental results demonstrate that our method can generate realistic 3D avatars from coupled textual data, which is challenging for other existing methods in this field.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2307.03441.pdf' target='_blank'>https://arxiv.org/pdf/2307.03441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, Baoyuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03441">NOFA: NeRF-based One-shot Facial Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D facial avatar reconstruction has been a significant research topic in computer graphics and computer vision, where photo-realistic rendering and flexible controls over poses and expressions are necessary for many related applications. Recently, its performance has been greatly improved with the development of neural radiance fields (NeRF). However, most existing NeRF-based facial avatars focus on subject-specific reconstruction and reenactment, requiring multi-shot images containing different views of the specific subject for training, and the learned model cannot generalize to new identities, limiting its further applications. In this work, we propose a one-shot 3D facial avatar reconstruction framework that only requires a single source image to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking generalization ability and missing multi-view information, we leverage the generative prior of 3D GAN and develop an efficient encoder-decoder network to reconstruct the canonical neural volume of the source image, and further propose a compensation network to complement facial details. To enable fine-grained control over facial dynamics, we propose a deformation field to warp the canonical volume into driven expressions. Through extensive experimental comparisons, we achieve superior synthesis results compared to several state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2306.03504.pdf' target='_blank'>https://arxiv.org/pdf/2306.03504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03504">Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We are interested in a novel task, namely low-resource text-to-talking avatar. Given only a few-minute-long talking person video with the audio track as the training data and arbitrary texts as the driving input, we aim to synthesize high-quality talking portrait videos corresponding to the input text. This task has broad application prospects in the digital human industry but has not been technically achieved yet due to two challenges: (1) It is challenging to mimic the timbre from out-of-domain audio for a traditional multi-speaker Text-to-Speech system. (2) It is hard to render high-fidelity and lip-synchronized talking avatars with limited training data. In this paper, we introduce Adaptive Text-to-Talking Avatar (Ada-TTA), which (1) designs a generic zero-shot multi-speaker TTS model that well disentangles the text content, timbre, and prosody; and (2) embraces recent advances in neural rendering to achieve realistic audio-driven talking face video generation. With these designs, our method overcomes the aforementioned two challenges and achieves to generate identity-preserving speech and realistic talking person video. Experiments demonstrate that our method could synthesize realistic, identity-preserving, and audio-visual synchronized talking avatar videos.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2211.15064.pdf' target='_blank'>https://arxiv.org/pdf/2211.15064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunpeng Bai, Yanbo Fan, Xuan Wang, Yong Zhang, Jingxiang Sun, Chun Yuan, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15064">High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity facial avatar reconstruction from a monocular video is a significant research problem in computer graphics and computer vision. Recently, Neural Radiance Field (NeRF) has shown impressive novel view rendering results and has been considered for facial avatar reconstruction. However, the complex facial dynamics and missing 3D information in monocular videos raise significant challenges for faithful facial reconstruction. In this work, we propose a new method for NeRF-based facial avatar reconstruction that utilizes 3D-aware generative prior. Different from existing works that depend on a conditional deformation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN. We propose an efficient method to construct the personalized generative prior based on a small set of facial images of a given individual. After learning, it allows for photo-realistic rendering with novel views and the face reenactment can be realized by performing navigation in the latent space. Our proposed method is applicable for different driven signals, including RGB images, 3DMM coefficients, and audios. Compared with existing works, we obtain superior novel view synthesis results and faithfully face reenactment performance.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2503.00038.pdf' target='_blank'>https://arxiv.org/pdf/2503.00038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00038">from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2502.12080.pdf' target='_blank'>https://arxiv.org/pdf/2502.12080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12080">HumanGif: Single-View Human Diffusion with Generative Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous 3D human creation methods have made significant progress in synthesizing view-consistent and temporally aligned results from sparse-view images or monocular videos. However, it remains challenging to produce perpetually realistic, view-consistent, and temporally coherent human avatars from a single image, as limited information is available in the single-view input setting. Motivated by the success of 2D character animation, we propose HumanGif, a single-view human diffusion model with generative prior. Specifically, we formulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models to complement the missing information. To ensure fine-grained and consistent novel view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn spatially aligned features from the input image, implicitly capturing the relative camera and human pose transformation. Furthermore, we introduce an image-level loss during optimization to bridge the gap between latent and image spaces in diffusion models. Extensive experiments on RenderPeople, DNA-Rendering, THuman 2.1, and TikTok datasets demonstrate that HumanGif achieves the best perceptual performance, with better generalizability for novel view and pose synthesis.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2502.10124.pdf' target='_blank'>https://arxiv.org/pdf/2502.10124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Li, Yishu Ji, Ruijia Chen, Tianqi Liu, Yuntao Wang, Yuanchun Shi, Yukang Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10124">Modeling the Impact of Visual Stimuli on Redirection Noticeability with Gaze Behavior in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While users could embody virtual avatars that mirror their physical movements in Virtual Reality, these avatars' motions can be redirected to enable novel interactions. Excessive redirection, however, could break the user's sense of embodiment due to perceptual conflicts between vision and proprioception. While prior work focused on avatar-related factors influencing the noticeability of redirection, we investigate how the visual stimuli in the surrounding virtual environment affect user behavior and, in turn, the noticeability of redirection. Given the wide variety of different types of visual stimuli and their tendency to elicit varying individual reactions, we propose to use users' gaze behavior as an indicator of their response to the stimuli and model the noticeability of redirection. We conducted two user studies to collect users' gaze behavior and noticeability, investigating the relationship between them and identifying the most effective gaze behavior features for predicting noticeability. Based on the data, we developed a regression model that takes users' gaze behavior as input and outputs the noticeability of redirection. We then conducted an evaluation study to test our model on unseen visual stimuli, achieving an accuracy of 0.012 MSE. We further implemented an adaptive redirection technique and conducted a proof-of-concept study to evaluate its effectiveness with complex visual stimuli in two applications. The results indicated that participants experienced less physical demanding and a stronger sense of body ownership when using our adaptive technique, demonstrating the potential of our model to support real-world use cases.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2412.12145.pdf' target='_blank'>https://arxiv.org/pdf/2412.12145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Yan, Sheng Sun, Junqi Tong, Min Liu, Qi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12145">Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaphor serves as an implicit approach to convey information, while enabling the generalized comprehension of complex subjects. However, metaphor can potentially be exploited to bypass the safety alignment mechanisms of Large Language Models (LLMs), leading to the theft of harmful knowledge. In our study, we introduce a novel attack framework that exploits the imaginative capacity of LLMs to achieve jailbreaking, the J\underline{\textbf{A}}ilbreak \underline{\textbf{V}}ia \underline{\textbf{A}}dversarial Me\underline{\textbf{TA}} -pho\underline{\textbf{R}} (\textit{AVATAR}). Specifically, to elicit the harmful response, AVATAR extracts harmful entities from a given harmful target and maps them to innocuous adversarial entities based on LLM's imagination. Then, according to these metaphors, the harmful target is nested within human-like interaction for jailbreaking adaptively. Experimental results demonstrate that AVATAR can effectively and transferablly jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs. Our study exposes a security risk in LLMs from their endogenous imaginative capabilities. Furthermore, the analytical study reveals the vulnerability of LLM to adversarial metaphors and the necessity of developing defense methods against jailbreaking caused by the adversarial metaphor. \textcolor{orange}{ \textbf{Warning: This paper contains potentially harmful content from LLMs.}}
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2407.03204.pdf' target='_blank'>https://arxiv.org/pdf/2407.03204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, Zhangyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03204">Expressive Gaussian Human Avatars from Monocular RGB Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuanced expressiveness, particularly through fine-grained hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations. In this work, we focus on investigating the expressiveness of human avatars when learned from monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details. To this end, we introduce EVA, a drivable human model that meticulously sculpts fine details based on 3D Gaussians and SMPL-X, an expressive parametric human model. Focused on enhancing expressiveness, our work makes three key contributions. First, we highlight the critical importance of aligning the SMPL-X model with RGB frames for effective avatar learning. Recognizing the limitations of current SMPL-X prediction methods for in-the-wild videos, we introduce a plug-and-play module that significantly ameliorates misalignment issues. Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts. Last but not least, we develop a feedback mechanism that predicts per-pixel confidence to better guide the learning of 3D Gaussians. Extensive experiments on two benchmarks demonstrate the superiority of our framework both quantitatively and qualitatively, especially on the fine-grained hand and facial details. See the project website at \url{https://evahuman.github.io}
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2311.05126.pdf' target='_blank'>https://arxiv.org/pdf/2311.05126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianqi Liu, Joshua Rafael Sanchez, Yuntao Wang, Xin Yi, Yuanchun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05126">Exploring and Analyzing the Effect of Avatar's Realism on Anxiety of English as Second Language (ESL) Speakers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of virtual avatars provides innovative opportunities for remote conferencing, education, and more. Our study investigates how the realism of avatars, used by native English speakers, impacts the anxiety levels of English as a Second Language (ESL) speakers during interactions. ESL participants engaged in conversations with native English speakers represented through cartoonish avatars, realistic-like avatars, or actual video streams. We measured both the ESL speakers' self-reported anxiety and their physiological indicators of anxiety. Our findings show that interactions with native speakers using cartoonish avatars or direct video lead to reduced anxiety levels among ESL participants. However, interactions with avatars that closely resemble humans heightened these anxieties. These insights are critically important for the design and application of virtual avatars, especially in addressing cross-cultural communication barriers and enhancing user experience.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2312.07077.pdf' target='_blank'>https://arxiv.org/pdf/2312.07077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Theofanis P. Raptis, Chiara Boldrini, Marco Conti, Andrea Passarella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07077">On the Potential of an Independent Avatar to Augment Metaverse Social Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a computational modelling approach which targets capturing the specifics on how to virtually augment a Metaverse user's available social time capacity via using an independent and autonomous version of her digital representation in the Metaverse. We motivate why this is a fundamental building block to model large-scale social networks in the Metaverse, and emerging properties herein. We envision a Metaverse-focused extension of the traditional avatar concept: An avatar can be as well programmed to operate independently when its user is not controlling it directly, thus turning it into an agent-based digital human representation. This way, we highlight how such an independent avatar could help its user to better navigate their social relationships and optimize their socializing time in the Metaverse by (partly) offloading some interactions to the avatar. We model the setting and identify the characteristic variables by using selected concepts from social sciences: ego networks, social presence, and social cues. Then, we formulate the problem of maximizing the user's non-avatar-mediated spare time as a linear optimization. Finally, we analyze the feasible region of the problem and we present some initial insights on the spare time that can be achieved for different parameter values of the avatar-mediated interactions.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2509.02466.pdf' target='_blank'>https://arxiv.org/pdf/2509.02466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwen Wang, Yiyu Zhuang, Jiawei Zhang, Li Wang, Yifei Zeng, Xun Cao, Xinxin Zuo, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02466">TeRA: Rethinking Text-guided Realistic 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative models. Our approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human representation. Experiments have proven our approach's superiority over previous text-to-avatar generative models in subjective and objective evaluation.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2501.13335.pdf' target='_blank'>https://arxiv.org/pdf/2501.13335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianrui Luo, Juewen Peng, Zhongang Cai, Lei Yang, Fan Yang, Zhiguo Cao, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13335">Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for modeling high-fidelity, animatable 3D human avatars from motion-blurred monocular video inputs. Motion blur is prevalent in real-world dynamic video capture, especially due to human movements in 3D human avatar modeling. Existing methods either (1) assume sharp image inputs, failing to address the detail loss introduced by motion blur, or (2) mainly consider blur by camera movements, neglecting the human motion blur which is more common in animatable avatars. Our proposed approach integrates a human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By explicitly modeling human motion trajectories during exposure time, we jointly optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality human avatars. We employ a pose-dependent fusion mechanism to distinguish moving body regions, optimizing both blurred and sharp areas effectively. Extensive experiments on synthetic and real-world datasets demonstrate that our method significantly outperforms existing methods in rendering quality and quantitative metrics, producing sharp avatar reconstructions and enabling real-time rendering under challenging motion blur conditions.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2501.04631.pdf' target='_blank'>https://arxiv.org/pdf/2501.04631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitian Zhang, Yichao Yan, Sijing Wu, Manwen Liao, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04631">Disentangled Clothed Avatar Generation with Layered Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. Previous methods have achieved success in generating diverse digital avatars, however, generating avatars with disentangled components (\eg, body, hair, and clothes) has long been a challenge. In this paper, we propose LayerAvatar, the first feed-forward diffusion-based method for generating component-disentangled clothed avatars. To achieve this, we first propose a layered UV feature plane representation, where components are distributed in different layers of the Gaussian-based UV feature plane with corresponding semantic labels. This representation supports high-resolution and real-time rendering, as well as expressive animation including controllable gestures and facial expressions. Based on the well-designed representation, we train a single-stage diffusion model and introduce constrain terms to address the severe occlusion problem of the innermost human body layer. Extensive experiments demonstrate the impressive performances of our method in generating disentangled clothed avatars, and we further explore its applications in component transfer. The project page is available at: https://olivia23333.github.io/LayerAvatar/
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2412.14963.pdf' target='_blank'>https://arxiv.org/pdf/2412.14963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14963">IDOL: Instant Photorealistic 3D Human Creation from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks. Project page: https://yiyuzhuang.github.io/IDOL/.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2411.15604.pdf' target='_blank'>https://arxiv.org/pdf/2411.15604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Zhang, Zijian Wu, Zhiyang Liang, Yicheng Gong, Dongfang Hu, Yao Yao, Xun Cao, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15604">FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing high-fidelity, animatable 3D head avatars from effortlessly captured monocular videos is a pivotal yet formidable challenge. Although significant progress has been made in rendering performance and manipulation capabilities, notable challenges remain, including incomplete reconstruction and inefficient Gaussian representation. To address these challenges, we introduce FATE, a novel method for reconstructing an editable full-head avatar from a single monocular video. FATE integrates a sampling-based densification strategy to ensure optimal positional distribution of points, improving rendering efficiency. A neural baking technique is introduced to convert discrete Gaussian representations into continuous attribute maps, facilitating intuitive appearance editing. Furthermore, we propose a universal completion framework to recover non-frontal appearance, culminating in a 360$^\circ$-renderable 3D head avatar. FATE outperforms previous approaches in both qualitative and quantitative evaluations, achieving state-of-the-art performance. To the best of our knowledge, FATE is the first animatable and 360$^\circ$ full-head monocular reconstruction method for a 3D head avatar.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2410.01226.pdf' target='_blank'>https://arxiv.org/pdf/2410.01226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyu Zhuang, Yuxiao He, Jiawei Zhang, Yanwen Wang, Jiahe Zhu, Yao Yao, Siyu Zhu, Xun Cao, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01226">Towards Native Generative Model for 3D Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating 3D head avatars is a significant yet challenging task for many applicated scenarios. Previous studies have set out to learn 3D human head generative models using massive 2D image data. Although these models are highly generalizable for human appearance, their result models are not 360$^\circ$-renderable, and the predicted 3D geometry is unreliable. Therefore, such results cannot be used in VR, game modeling, and other scenarios that require 360$^\circ$-renderable 3D head models. An intuitive idea is that 3D head models with limited amount but high 3D accuracy are more reliable training data for a high-quality 3D generative model. In this vein, we delve into how to learn a native generative model for 360$^\circ$ full head from a limited 3D head dataset. Specifically, three major problems are studied: 1) how to effectively utilize various representations for generating the 360$^\circ$-renderable human head; 2) how to disentangle the appearance, shape, and motion of human faces to generate a 3D head model that can be edited by appearance and driven by motion; 3) and how to extend the generalization capability of the generative model to support downstream tasks. Comprehensive experiments are conducted to verify the effectiveness of the proposed model. We hope the proposed models and artist-designed dataset can inspire future research on learning native generative 3D head models from limited 3D datasets.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2405.19203.pdf' target='_blank'>https://arxiv.org/pdf/2405.19203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19203">$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing. Our project page is https://olivia23333.github.io/E3Gen.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2312.02209.pdf' target='_blank'>https://arxiv.org/pdf/2312.02209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yang, Tianyi Chen, Xiaosheng He, Zhongang Cai, Lei Yang, Si Wu, Guosheng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02209">AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Editable 3D-aware generation, which supports user-interacted editing, has witnessed rapid development recently. However, existing editable 3D GANs either fail to achieve high-accuracy local editing or suffer from huge computational costs. We propose AttriHuman-3D, an editable 3D human generation model, which address the aforementioned problems with attribute decomposition and indexing. The core idea of the proposed model is to generate all attributes (e.g. human body, hair, clothes and so on) in an overall attribute space with six feature planes, which are then decomposed and manipulated with different attribute indexes. To precisely extract features of different attributes from the generated feature planes, we propose a novel attribute indexing method as well as an orthogonal projection regularization to enhance the disentanglement. We also introduce a hyper-latent training strategy and an attribute-specific sampling strategy to avoid style entanglement and misleading punishment from the discriminator. Our method allows users to interactively edit selected attributes in the generated 3D human avatars while keeping others fixed. Both qualitative and quantitative experiments demonstrate that our model provides a strong disentanglement between different attributes, allows fine-grained image editing and generates high-quality 3D human avatars.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2306.09864.pdf' target='_blank'>https://arxiv.org/pdf/2306.09864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09864">AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AvatarBooth, a novel method for generating high-quality 3D avatars using text prompts or specific images. Unlike previous approaches that can only synthesize avatars based on simple text descriptions, our method enables the creation of personalized avatars from casually captured face or body images, while still supporting text-based model generation and editing. Our key contribution is the precise avatar generation control by using dual fine-tuned diffusion models separately for the human face and body. This enables us to capture intricate details of facial appearance, clothing, and accessories, resulting in highly realistic avatar generations. Furthermore, we introduce pose-consistent constraint to the optimization process to enhance the multi-view consistency of synthesized head images from the diffusion model and thus eliminate interference from uncontrolled human poses. In addition, we present a multi-resolution rendering strategy that facilitates coarse-to-fine supervision of 3D avatar generation, thereby enhancing the performance of the proposed system. The resulting avatar model can be further edited using additional text descriptions and driven by motion sequences. Experiments show that AvatarBooth outperforms previous text-to-3D methods in terms of rendering and geometric quality from either text prompts or specific images. Please check our project website at https://zeng-yifei.github.io/avatarbooth_page/.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2203.07931.pdf' target='_blank'>https://arxiv.org/pdf/2203.07931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.07931">DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversation is an essential component of virtual avatar activities in the metaverse. With the development of natural language processing, textual and vocal conversation generation has achieved a significant breakthrough. However, face-to-face conversations account for the vast majority of daily conversations, while most existing methods focused on single-person talking head generation. In this work, we take a step further and consider generating realistic face-to-face conversation videos. Conversation generation is more challenging than single-person talking head generation, since it not only requires generating photo-realistic individual talking heads but also demands the listener to respond to the speaker. In this paper, we propose a novel unified framework based on neural radiance field (NeRF) to address this task. Specifically, we model both the speaker and listener with a NeRF framework, with different conditions to control individual expressions. The speaker is driven by the audio signal, while the response of the listener depends on both visual and acoustic information. In this way, face-to-face conversation videos are generated between human avatars, with all the interlocutors modeled within the same network. Moreover, to facilitate future research on this task, we collect a new human conversation dataset containing 34 clips of videos. Quantitative and qualitative experiments evaluate our method in different aspects, e.g., image quality, pose sequence trend, and naturalness of the rendering videos. Experimental results demonstrate that the avatars in the resulting videos are able to perform a realistic conversation, and maintain individual styles. All the code, data, and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2404.01053.pdf' target='_blank'>https://arxiv.org/pdf/2404.01053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01053">HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2402.04101.pdf' target='_blank'>https://arxiv.org/pdf/2402.04101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04101">VRMM: A Volumetric Relightable Morphable Head Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables high-quality 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2309.04247.pdf' target='_blank'>https://arxiv.org/pdf/2309.04247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Yang, Mingwu Zheng, Wanquan Feng, Haibin Huang, Yu-Kun Lai, Pengfei Wan, Zhongyuan Wang, Chongyang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04247">Towards Practical Capture of High-Fidelity Relightable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel framework, Tracking-free Relightable Avatar (TRAvatar), for capturing and reconstructing high-fidelity 3D avatars. Compared to previous methods, TRAvatar works in a more practical and efficient setting. Specifically, TRAvatar is trained with dynamic image sequences captured in a Light Stage under varying lighting conditions, enabling realistic relighting and real-time animation for avatars in diverse scenes. Additionally, TRAvatar allows for tracking-free avatar capture and obviates the need for accurate surface tracking under varying illumination conditions. Our contributions are two-fold: First, we propose a novel network architecture that explicitly builds on and ensures the satisfaction of the linear nature of lighting. Trained on simple group light captures, TRAvatar can predict the appearance in real-time with a single forward pass, achieving high-quality relighting effects under illuminations of arbitrary environment maps. Second, we jointly optimize the facial geometry and relightable appearance from scratch based on image sequences, where the tracking is implicitly learned. This tracking-free approach brings robustness for establishing temporal correspondences between frames under different lighting conditions. Extensive qualitative and quantitative experiments demonstrate that our framework achieves superior performance for photorealistic avatar animation and relighting.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2308.09705.pdf' target='_blank'>https://arxiv.org/pdf/2308.09705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09705">Guide3D: Create 3D Avatars from Text and Image Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text-to-image generation has exhibited remarkable advancements, with the ability to produce visually impressive results. In contrast, text-to-3D generation has not yet reached a comparable level of quality. Existing methods primarily rely on text-guided score distillation sampling (SDS), and they encounter difficulties in transferring 2D attributes of the generated images to 3D content. In this work, we aim to develop an effective 3D generative model capable of synthesizing high-resolution textured meshes by leveraging both textual and image information. To this end, we introduce Guide3D, a zero-shot text-and-image-guided generative model for 3D avatar generation based on diffusion models. Our model involves (1) generating sparse-view images of a text-consistent character using diffusion models, and (2) jointly optimizing multi-resolution differentiable marching tetrahedral grids with pixel-aligned image features. We further propose a similarity-aware feature fusion strategy for efficiently integrating features from different views. Moreover, we introduce two novel training objectives as an alternative to calculating SDS, significantly enhancing the optimization process. We thoroughly evaluate the performance and components of our framework, which outperforms the current state-of-the-art in producing topologically and structurally correct geometry and high-resolution textures. Guide3D enables the direct transfer of 2D-generated images to the 3D space. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2307.05000.pdf' target='_blank'>https://arxiv.org/pdf/2307.05000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Wang, Di Kang, Yan-Pei Cao, Linchao Bao, Ying Shan, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05000">Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rendering photorealistic and dynamically moving human heads is crucial for ensuring a pleasant and immersive experience in AR/VR and video conferencing applications. However, existing methods often struggle to model challenging facial regions (e.g., mouth interior, eyes, hair/beard), resulting in unrealistic and blurry results. In this paper, we propose {\fullname} ({\name}), a method that adopts the neural point representation as well as the neural volume rendering process and discards the predefined connectivity and hard correspondence imposed by mesh-based approaches. Specifically, the neural points are strategically constrained around the surface of the target expression via a high-resolution UV displacement map, achieving increased modeling capacity and more accurate control. We introduce three technical innovations to improve the rendering and training efficiency: a patch-wise depth-guided (shading point) sampling strategy, a lightweight radiance decoding process, and a Grid-Error-Patch (GEP) ray sampling strategy during training. By design, our {\name} is better equipped to handle topologically changing regions and thin structures while also ensuring accurate expression control when animating avatars. Experiments conducted on three subjects from the Multiface dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods, especially in handling challenging facial regions.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2304.00916.pdf' target='_blank'>https://arxiv.org/pdf/2304.00916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00916">DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body's shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face ''Janus'' problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing methods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2509.07552.pdf' target='_blank'>https://arxiv.org/pdf/2509.07552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Li, Yisheng He, Yingdong Hu, Yuan Dong, Weihao Yuan, Yuan Liu, Zilong Dong, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07552">PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2506.13766.pdf' target='_blank'>https://arxiv.org/pdf/2506.13766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingteng Qiu, Peihao Li, Qi Zuo, Xiaodong Gu, Yuan Dong, Weihao Yuan, Siyu Zhu, Xiaoguang Han, Guanying Chen, Zilong Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13766">PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations. Code and models will be released to the public.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2503.10625.pdf' target='_blank'>https://arxiv.org/pdf/2503.10625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10625">LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2412.11586.pdf' target='_blank'>https://arxiv.org/pdf/2412.11586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaokun Sun, Zeyu Cai, Ying Tai, Jian Yang, Zhenyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11586">StrandHead: Text to Hair-Disentangled 3D Head Avatars Using Human-Centric Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the data limitation or entangled representation. We propose StrandHead, a novel text-driven method capable of generating 3D hair strands and disentangled head avatars with strand-level attributes. Instead of using large-scale hair-text paired data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative models pre-trained on human mesh data. To this end, we propose a meshing approach guided by strand geometry to guarantee the gradient flow from the distillation objective to the neural strand representation. The optimization is then regularized by statistically significant haircut features, leading to stable updating of strands against unreasonable drifting. These employed 2D/3D human-centric priors contribute to text-aligned and realistic 3D strand generation. Extensive experiments show that StrandHead achieves the state-of-the-art performance on text to strand generation and disentangled 3D head avatar modeling. The generated 3D hair can be applied on avatars for strand-level editing, as well as implemented in the graphics engine for physical simulation or other applications. Project page: https://xiaokunsun.github.io/StrandHead.github.io/.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2412.02684.pdf' target='_blank'>https://arxiv.org/pdf/2412.02684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02684">AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2408.09126.pdf' target='_blank'>https://arxiv.org/pdf/2408.09126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaokun Sun, Zhenyu Zhang, Ying Tai, Hao Tang, Zili Yi, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09126">Barbie: Text to Barbie-Style 3D Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To integrate digital humans into everyday life, there is a strong demand for generating high-quality, fine-grained disentangled 3D avatars that support expressive animation and simulation capabilities, ideally from low-cost textual inputs. Although text-driven 3D avatar generation has made significant progress by leveraging 2D generative priors, existing methods still struggle to fulfill all these requirements simultaneously. To address this challenge, we propose Barbie, a novel text-driven framework for generating animatable 3D avatars with separable shoes, accessories, and simulation-ready garments, truly capturing the iconic ``Barbie doll'' aesthetic. The core of our framework lies in an expressive 3D representation combined with appropriate modeling constraints. Unlike previous methods, we innovatively employ G-Shell to uniformly model both watertight components (e.g., bodies, shoes, and accessories) and non-watertight garments compatible with simulation. Furthermore, we introduce a well-designed initialization and a hole regularization loss to ensure clean open surface modeling. These disentangled 3D representations are then optimized by specialized expert diffusion models tailored to each domain, ensuring high-fidelity outputs. To mitigate geometric artifacts and texture conflicts when combining different expert models, we further propose several effective geometric losses and strategies. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation. Our framework further enables diverse applications, including apparel combination, editing, expressive animation, and physical simulation. Our project page is: https://xiaokunsun.github.io/Barbie.github.io
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2507.08627.pdf' target='_blank'>https://arxiv.org/pdf/2507.08627.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-en Amy Tai, Pengyu Nie, Lukasz Golab, Alexander Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08627">NL in the Middle: Code Translation with LLMs and Intermediate Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Studies show that large language models (LLMs) produce buggy code translations. One promising avenue to improve translation accuracy is through intermediate representations, which provide structured guidance for the translation process. We investigate whether LLM-based code translation can benefit from intermediate representations, specifically in the form of natural language (NL) summaries and abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM performance, we consider several ways to integrate these representations, from one-shot to chain-of-thought (CoT) prompting. Using Open GPT4 8X7B and specialized StarCoder and CodeGen models on popular code translation benchmarks (CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs best, with an increase of 13.8% and 6.7%, respectively, in successful translations for the best-performing model (Open GPT4 8X7B) compared to the zero-shot prompt.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2409.17145.pdf' target='_blank'>https://arxiv.org/pdf/2409.17145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17145">DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2405.15758.pdf' target='_blank'>https://arxiv.org/pdf/2405.15758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15758">InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is https://wangyuchi369.github.io/InstructAvatar/.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2404.19110.pdf' target='_blank'>https://arxiv.org/pdf/2404.19110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19110">EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:
  Enhance the model's capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.
  Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.
  We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2402.17364.pdf' target='_blank'>https://arxiv.org/pdf/2402.17364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17364">Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2311.15230.pdf' target='_blank'>https://arxiv.org/pdf/2311.15230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, Sheng Zhao, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15230">GAIA: Zero-shot Talking Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot talking avatar generation aims at synthesizing natural talking videos from speech and a single portrait image. Previous methods have relied on domain-specific heuristics such as warping-based motion representation and 3D Morphable Models, which limit the naturalness and diversity of the generated avatars. In this work, we introduce GAIA (Generative AI for Avatar), which eliminates the domain priors in talking avatar generation. In light of the observation that the speech only drives the motion of the avatar while the appearance of the avatar and the background typically remain the same throughout the entire video, we divide our approach into two stages: 1) disentangling each frame into motion and appearance representations; 2) generating motion sequences conditioned on the speech and reference portrait image. We collect a large-scale high-quality talking avatar dataset and train the model on it with different scales (up to 2B parameters). Experimental results verify the superiority, scalability, and flexibility of GAIA as 1) the resulting model beats previous baseline models in terms of naturalness, diversity, lip-sync quality, and visual quality; 2) the framework is scalable since larger models yield better results; 3) it is general and enables different applications like controllable talking avatar generation and text-instructed avatar generation.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2308.15016.pdf' target='_blank'>https://arxiv.org/pdf/2308.15016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longbin Ji, Pengfei Wei, Yi Ren, Jinglin Liu, Chen Zhang, Xiang Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15016">C2G2: Controllable Co-speech Gesture Generation with Latent Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gesture generation is crucial for automatic digital avatar animation. However, existing methods suffer from issues such as unstable training and temporal inconsistency, particularly in generating high-fidelity and comprehensive gestures. Additionally, these methods lack effective control over speaker identity and temporal editing of the generated gestures. Focusing on capturing temporal latent information and applying practical controlling, we propose a Controllable Co-speech Gesture Generation framework, named C2G2. Specifically, we propose a two-stage temporal dependency enhancement strategy motivated by latent diffusion models. We further introduce two key features to C2G2, namely a speaker-specific decoder to generate speaker-related real-length skeletons and a repainting strategy for flexible gesture generation/editing. Extensive experiments on benchmark gesture datasets verify the effectiveness of our proposed C2G2 compared with several state-of-the-art baselines. The link of the project demo page can be found at https://c2g2-gesture.github.io/c2_gesture
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2305.12529.pdf' target='_blank'>https://arxiv.org/pdf/2305.12529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12529">DreamWaltz: Make a Scene with Complex 3D Animatable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable 3D avatar representation from abundant image priors of diffusion model conditioned on various poses, which could animate complex non-rigged avatars given arbitrary poses without retraining. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The proposed framework further enables the creation of complex scenes with diverse compositions, including avatar-avatar, avatar-object and avatar-scene interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and animation results.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2506.13477.pdf' target='_blank'>https://arxiv.org/pdf/2506.13477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pegah Salehi, Sajad Amouei Sheshkal, Vajira Thambawita, Michael A. Riegler, PÃ¥l Halvorsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13477">Multimodal Integration Challenges in Emotionally Expressive Child Avatars for Training Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic facial emotion is essential for believable AI-generated avatars, yet most systems remain visually static, limiting their use in simulations like virtual training for investigative interviews with abused children. We present a real-time architecture combining Unreal Engine 5 MetaHuman rendering with NVIDIA Omniverse Audio2Face to generate facial expressions from vocal prosody in photorealistic child avatars. Due to limited TTS options, both avatars were voiced using young adult female models from two systems to better fit character profiles, introducing a voice-age mismatch. This confound may affect audiovisual alignment. We used a two-PC setup to decouple speech generation from GPU-intensive rendering, enabling low-latency interaction in desktop and VR. A between-subjects study (N=70) compared audio+visual vs. visual-only conditions as participants rated emotional clarity, facial realism, and empathy for avatars expressing joy, sadness, and anger. While emotions were generally recognized - especially sadness and joy - anger was harder to detect without audio, highlighting the role of voice in high-arousal expressions. Interestingly, silencing clips improved perceived realism by removing mismatches between voice and animation, especially when tone or age felt incongruent. These results emphasize the importance of audiovisual congruence: mismatched voice undermines expression, while a good match can enhance weaker visuals - posing challenges for emotionally coherent avatars in sensitive contexts.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2506.12847.pdf' target='_blank'>https://arxiv.org/pdf/2506.12847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhelun Shen, Chenming Wu, Junsheng Zhou, Chen Zhao, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Wei He, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12847">iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital human video generation is gaining traction in fields like education and e-commerce, driven by advancements in head-body animation and lip-syncing technologies. However, realistic Hand-Object Interaction (HOI) - the complex dynamics between human hands and objects - continues to pose challenges. Generating natural and believable HOI reenactments is difficult due to issues such as occlusion between hands and objects, variations in object shapes and orientations, and the necessity for precise physical interactions, and importantly, the ability to generalize to unseen humans and objects. This paper presents a novel framework iDiT-HOI that enables in-the-wild HOI reenactment generation. Specifically, we propose a unified inpainting-based token process method, called Inp-TPU, with a two-stage video diffusion transformer (DiT) model. The first stage generates a key frame by inserting the designated object into the hand region, providing a reference for subsequent frames. The second stage ensures temporal coherence and fluidity in hand-object interactions. The key contribution of our method is to reuse the pretrained model's context perception capabilities without introducing additional parameters, enabling strong generalization to unseen objects and scenarios, and our proposed paradigm naturally supports long video generation. Comprehensive evaluations demonstrate that our approach outperforms existing methods, particularly in challenging real-world scenes, offering enhanced realism and more seamless hand-object interactions.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2411.13209.pdf' target='_blank'>https://arxiv.org/pdf/2411.13209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pegah Salehi, Sajad Amouei Sheshkal, Vajira Thambawita, Sushant Gautam, Saeed S. Sabet, Dag Johansen, Michael A. Riegler, PÃ¥l Halvorsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13209">Comparative Analysis of Audio Feature Extraction for Real-Time Talking Portrait Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper examines the integration of real-time talking-head generation for interviewer training, focusing on overcoming challenges in Audio Feature Extraction (AFE), which often introduces latency and limits responsiveness in real-time applications. To address these issues, we propose and implement a fully integrated system that replaces conventional AFE models with Open AI's Whisper, leveraging its encoder to optimize processing and improve overall system efficiency. Our evaluation of two open-source real-time models across three different datasets shows that Whisper not only accelerates processing but also improves specific aspects of rendering quality, resulting in more realistic and responsive talking-head interactions. These advancements make the system a more effective tool for immersive, interactive training applications, expanding the potential of AI-driven avatars in interviewer training.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2405.01112.pdf' target='_blank'>https://arxiv.org/pdf/2405.01112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Guo, Zhiyu Pan, Ziheng Xi, Alapati Tuerxun, Jianjiang Feng, Jie Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01112">Sports Analysis and VR Viewing System Based on Player Tracking and Pose Estimation with Multimodal and Multiview Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sports analysis and viewing play a pivotal role in the current sports domain, offering significant value not only to coaches and athletes but also to fans and the media. In recent years, the rapid development of virtual reality (VR) and augmented reality (AR) technologies have introduced a new platform for watching games. Visualization of sports competitions in VR/AR represents a revolutionary technology, providing audiences with a novel immersive viewing experience. However, there is still a lack of related research in this area. In this work, we present for the first time a comprehensive system for sports competition analysis and real-time visualization on VR/AR platforms. First, we utilize multiview LiDARs and cameras to collect multimodal game data. Subsequently, we propose a framework for multi-player tracking and pose estimation based on a limited amount of supervised data, which extracts precise player positions and movements from point clouds and images. Moreover, we perform avatar modeling of players to obtain their 3D models. Ultimately, using these 3D player data, we conduct competition analysis and real-time visualization on VR/AR. Extensive quantitative experiments demonstrate the accuracy and robustness of our multi-player tracking and pose estimation framework. The visualization results showcase the immense potential of our sports visualization system on the domain of watching games on VR/AR devices. The multimodal competition dataset we collected and all related code will be released soon.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2402.06390.pdf' target='_blank'>https://arxiv.org/pdf/2402.06390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, SÅawomir Tadeja, Jacek Tabor, PrzemysÅaw Spurek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06390">Deepfake for the Good: Generating Avatars through Face-Swapping with Implicit Deepfake Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the object's shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the object's characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Deepfakes refers to artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such simple strategies can produce plausible 3D deepfake-based avatars.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2311.05371.pdf' target='_blank'>https://arxiv.org/pdf/2311.05371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Ou, Yuzhe Zhang, Yuntang Wang, Shwetak Patel, Daniel McDuf, Yuzhe Yang, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05371">Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in supervised deep learning techniques have demonstrated the possibility to remotely measure human physiological vital signs (e.g., photoplethysmograph, heart rate) just from facial videos. However, the performance of these methods heavily relies on the availability and diversity of real labeled data. Yet, collecting large-scale real-world data with high-quality labels is typically challenging and resource intensive, which also raises privacy concerns when storing personal bio-metric data. Synthetic video-based datasets (e.g., SCAMPS \cite{mcduff2022scamps}) with photo-realistic synthesized avatars are introduced to alleviate the issues while providing high-quality synthetic data. However, there exists a significant gap between synthetic and real-world data, which hinders the generalization of neural models trained on these synthetic datasets. In this paper, we proposed several measures to add real-world noise to synthetic physiological signals and corresponding facial videos. We experimented with individual and combined augmentation methods and evaluated our framework on three public real-world datasets. Our results show that we were able to reduce the average MAE from 6.9 to 2.0.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2507.12741.pdf' target='_blank'>https://arxiv.org/pdf/2507.12741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lotfi El Hafi, Kazuma Onishi, Shoichi Hasegawa, Akira Oyama, Tomochika Ishikawa, Masashi Osada, Carl Tornberg, Ryoma Kado, Kento Murata, Saki Hashimoto, Sebastian Carrera Villalobos, Akira Taniguchi, Gustavo Alfonso Garcia Ricardez, Yoshinobu Hagiwara, Tatsuya Aoki, Kensuke Iwata, Takato Horii, Yukiko Horikawa, Takahiro Miyashita, Tadahiro Taniguchi, Hiroshi Ishiguro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12741">Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybernetic avatars (CAs) are key components of an avatar-symbiotic society, enabling individuals to overcome physical limitations through virtual agents and robotic assistants. While semi-autonomous CAs intermittently require human teleoperation and supervision, the deployment of fully autonomous CAs remains a challenge. This study evaluates public perception and potential social impacts of fully autonomous CAs for physical support in daily life. To this end, we conducted a large-scale demonstration and survey during Avatar Land, a 19-day public event in Osaka, Japan, where fully autonomous robotic CAs, alongside semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we analyzed responses from 2,285 visitors who engaged with various CAs, including a subset of 333 participants who interacted with fully autonomous CAs and shared their perceptions and concerns through a survey questionnaire. The survey results indicate interest in CAs for physical support in daily life and at work. However, concerns were raised regarding task execution reliability. In contrast, cost and human-like interaction were not dominant concerns. Project page: https://lotfielhafi.github.io/FACA-Survey/.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2410.18974.pdf' target='_blank'>https://arxiv.org/pdf/2410.18974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hansheng Chen, Bokui Shen, Yulin Liu, Ruoxi Shi, Linqi Zhou, Connor Z. Lin, Jiayuan Gu, Hao Su, Gordon Wetzstein, Leonidas Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18974">3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view image diffusion models have significantly advanced open-domain 3D object generation. However, most existing models rely on 2D network architectures that lack inherent 3D biases, resulting in compromised geometric consistency. To address this challenge, we introduce 3D-Adapter, a plug-in module designed to infuse 3D geometry awareness into pretrained image diffusion models. Central to our approach is the idea of 3D feedback augmentation: for each denoising step in the sampling loop, 3D-Adapter decodes intermediate multi-view features into a coherent 3D representation, then re-encodes the rendered RGBD views to augment the pretrained base model through feature addition. We study two variants of 3D-Adapter: a fast feed-forward version based on Gaussian splatting and a versatile training-free version utilizing neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter not only greatly enhances the geometry quality of text-to-multi-view models such as Instant3D and Zero123++, but also enables high-quality 3D generation using the plain text-to-image Stable Diffusion. Furthermore, we showcase the broad application potential of 3D-Adapter by presenting high quality results in text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2405.11286.pdf' target='_blank'>https://arxiv.org/pdf/2405.11286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11286">Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant interest in creating 3D avatars and motions, driven by their diverse applications in areas like film-making, video games, AR/VR, and human-robot interaction. However, current efforts primarily concentrate on either generating the 3D avatar mesh alone or producing motion sequences, with integrating these two aspects proving to be a persistent challenge. Additionally, while avatar and motion generation predominantly target humans, extending these techniques to animals remains a significant challenge due to inadequate training data and methods. To bridge these gaps, our paper presents three key contributions. Firstly, we proposed a novel agent-based approach named Motion Avatar, which allows for the automatic generation of high-quality customizable human and animal avatars with motions through text queries. The method significantly advanced the progress in dynamic 3D character generation. Secondly, we introduced a LLM planner that coordinates both motion and avatar generation, which transforms a discriminative planning into a customizable Q&A fashion. Lastly, we presented an animal motion dataset named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65 animal categories and its building pipeline ZooGen, which serves as a valuable resource for the community. See project website https://steve-zeyu-zhang.github.io/MotionAvatar/
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2404.04421.pdf' target='_blank'>https://arxiv.org/pdf/2404.04421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zheng, Qingqing Zhao, Guandao Yang, Wang Yifan, Donglai Xiang, Florian Dubost, Dmitry Lagun, Thabo Beeler, Federico Tombari, Leonidas Guibas, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04421">PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling and rendering photorealistic avatars is of crucial importance in many applications. Existing methods that build a 3D avatar from visual observations, however, struggle to reconstruct clothed humans. We introduce PhysAvatar, a novel framework that combines inverse rendering with inverse physics to automatically estimate the shape and appearance of a human from multi-view video data along with the physical parameters of the fabric of their clothes. For this purpose, we adopt a mesh-aligned 4D Gaussian technique for spatio-temporal mesh tracking as well as a physically based inverse renderer to estimate the intrinsic material properties. PhysAvatar integrates a physics simulator to estimate the physical parameters of the garments using gradient-based optimization in a principled manner. These novel capabilities enable PhysAvatar to create high-quality novel-view renderings of avatars dressed in loose-fitting clothes under motions and lighting conditions not seen in the training data. This marks a significant advancement towards modeling photorealistic digital humans using physically based inverse rendering with physics in the loop. Our project website is at: https://qingqing-zhao.github.io/PhysAvatar
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2403.11589.pdf' target='_blank'>https://arxiv.org/pdf/2403.11589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11589">UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the paper is accepted.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2312.10921.pdf' target='_blank'>https://arxiv.org/pdf/2312.10921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongze Li, Kang Zhao, Wei Wang, Bo Peng, Yingya Zhang, Jing Dong, Tieniu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10921">AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head synthesis is a promising topic with wide applications in digital human, film making and virtual reality. Recent NeRF-based approaches have shown superiority in quality and fidelity compared to previous studies. However, when it comes to few-shot talking head generation, a practical scenario where only few seconds of talking video is available for one identity, two limitations emerge: 1) they either have no base model, which serves as a facial prior for fast convergence, or ignore the importance of audio when building the prior; 2) most of them overlook the degree of correlation between different face regions and audio, e.g., mouth is audio related, while ear is audio independent. In this paper, we present Audio Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can generate realistic portraits of a new speaker with fewshot dataset. Specifically, we introduce an Audio Aware Aggregation module into the feature fusion stage of the reference scheme, where the weight is determined by the similarity of audio between reference and target image. Then, an Audio-Aligned Face Generation strategy is proposed to model the audio related and audio independent regions respectively, with a dual-NeRF framework. Extensive experiments have shown AE-NeRF surpasses the state-of-the-art on image fidelity, audio-lip synchronization, and generalization ability, even in limited training set or training iterations.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2311.17917.pdf' target='_blank'>https://arxiv.org/pdf/2311.17917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfeng Zhang, Xuanmeng Zhang, Huichao Zhang, Jun Hao Liew, Chenxu Zhang, Yi Yang, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17917">AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of creating high-fidelity and animatable 3D avatars from only textual descriptions. Existing text-to-avatar methods are either limited to static avatars which cannot be animated or struggle to generate animatable avatars with promising quality and precise pose control. To address these limitations, we propose AvatarStudio, a coarse-to-fine generative model that generates explicit textured 3D meshes for animatable human avatars. Specifically, AvatarStudio begins with a low-resolution NeRF-based representation for coarse generation, followed by incorporating SMPL-guided articulation into the explicit mesh representation to support avatar animation and high resolution rendering. To ensure view consistency and pose controllability of the resulting avatars, we introduce a 2D diffusion model conditioned on DensePose for Score Distillation Sampling supervision. By effectively leveraging the synergy between the articulated mesh representation and the DensePose-conditional diffusion model, AvatarStudio can create high-quality avatars from text that are ready for animation, significantly outperforming previous methods. Moreover, it is competent for many applications, e.g., multimodal avatar animations and style-guided avatar creation. For more results, please refer to our project page: http://jeff95.me/projects/avatarstudio.html
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2311.13574.pdf' target='_blank'>https://arxiv.org/pdf/2311.13574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Jiashi Feng, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13574">XAGen: 3D Expressive Human Avatars Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D-aware GAN models have enabled the generation of realistic and controllable human body images. However, existing methods focus on the control of major body joints, neglecting the manipulation of expressive attributes, such as facial expressions, jaw poses, hand poses, and so on. In this work, we present XAGen, the first 3D generative model for human avatars capable of expressive control over body, face, and hands. To enhance the fidelity of small-scale regions like face and hands, we devise a multi-scale and multi-part 3D representation that models fine details. Based on this representation, we propose a multi-part rendering technique that disentangles the synthesis of body, face, and hands to ease model training and enhance geometric quality. Furthermore, we design multi-part discriminators that evaluate the quality of the generated avatars with respect to their appearance and fine-grained control capabilities. Experiments show that XAGen surpasses state-of-the-art methods in terms of realism, diversity, and expressive control abilities. Code and data will be made available at https://showlab.github.io/xagen.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2309.16162.pdf' target='_blank'>https://arxiv.org/pdf/2309.16162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hitoshi Teshima, Naoki Wake, Diego Thomas, Yuta Nakashima, Hiroshi Kawasaki, Katsushi Ikeuchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16162">ACT2G: Attention-based Contrastive Learning for Text-to-Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent increase of remote-work, online meeting and tele-operation task makes people find that gesture for avatars and communication robots is more important than we have thought. It is one of the key factors to achieve smooth and natural communication between humans and AI systems and has been intensively researched. Current gesture generation methods are mostly based on deep neural network using text, audio and other information as the input, however, they generate gestures mainly based on audio, which is called a beat gesture. Although the ratio of the beat gesture is more than 70% of actual human gestures, content based gestures sometimes play an important role to make avatars more realistic and human-like. In this paper, we propose a attention-based contrastive learning for text-to-gesture (ACT2G), where generated gestures represent content of the text by estimating attention weight for each word from the input text. In the method, since text and gesture features calculated by the attention weight are mapped to the same latent space by contrastive learning, once text is given as input, the network outputs a feature vector which can be used to generate gestures related to the content. User study confirmed that the gestures generated by ACT2G were better than existing methods. In addition, it was demonstrated that wide variation of gestures were generated from the same text by changing attention weights by creators.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2308.14748.pdf' target='_blank'>https://arxiv.org/pdf/2308.14748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng, Jun Hao Liew
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14748">MagicAvatar: Multimodal Avatar Generation and Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents MagicAvatar, a framework for multimodal video generation and animation of human avatars. Unlike most existing methods that generate avatar-centric videos directly from multimodal inputs (e.g., text prompts), MagicAvatar explicitly disentangles avatar video generation into two stages: (1) multimodal-to-motion and (2) motion-to-video generation. The first stage translates the multimodal inputs into motion/ control signals (e.g., human pose, depth, DensePose); while the second stage generates avatar-centric video guided by these motion signals. Additionally, MagicAvatar supports avatar animation by simply providing a few images of the target person. This capability enables the animation of the provided human identity according to the specific motion derived from the first stage. We demonstrate the flexibility of MagicAvatar through various applications, including text-guided and video-guided avatar generation, as well as multimodal avatar animation.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2305.03043.pdf' target='_blank'>https://arxiv.org/pdf/2305.03043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03043">Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing demand for the accessible creation of high-quality 3D avatars that are animatable and customizable. Although 3D morphable models provide intuitive control for editing and animation, and robustness for single-view face reconstruction, they cannot easily capture geometric and appearance details. Methods based on neural implicit representations, such as signed distance functions (SDF) or neural radiance fields, approach photo-realism, but are difficult to animate and do not generalize well to unseen data. To tackle this problem, we propose a novel method for constructing implicit 3D morphable face models that are both generalizable and intuitive for editing. Trained from a collection of high-quality 3D scans, our face model is parameterized by geometry, expression, and texture latent codes with a learned SDF and explicit UV texture parameterization. Once trained, we can reconstruct an avatar from a single in-the-wild image by leveraging the learned prior to project the image into the latent space of our model. Our implicit morphable face models can be used to render an avatar from novel views, animate facial expressions by modifying expression codes, and edit textures by directly painting on the learned UV-texture maps. We demonstrate quantitatively and qualitatively that our method improves upon photo-realism, geometry, and expression accuracy compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2504.03536.pdf' target='_blank'>https://arxiv.org/pdf/2504.03536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03536">HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce \textbf{HumanDreamer-X}, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, \textbf{HumanFixer} is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2411.18438.pdf' target='_blank'>https://arxiv.org/pdf/2411.18438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ka Hei Carrie Lau, Sema Sen, Philipp Stark, Efe Bozkir, Enkelejda Kasneci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18438">Adaptive Gen-AI Guidance in Virtual Reality: A Multimodal Exploration of Engagement in Neapolitan Pizza-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual reality (VR) offers promising opportunities for procedural learning, particularly in preserving intangible cultural heritage. Advances in generative artificial intelligence (Gen-AI) further enrich these experiences by enabling adaptive learning pathways. However, evaluating such adaptive systems using traditional temporal metrics remains challenging due to the inherent variability in Gen-AI response times. To address this, our study employs multimodal behavioural metrics, including visual attention, physical exploratory behaviour, and verbal interaction, to assess user engagement in an adaptive VR environment. In a controlled experiment with 54 participants, we compared three levels of adaptivity (high, moderate, and non-adaptive baseline) within a Neapolitan pizza-making VR experience. Results show that moderate adaptivity optimally enhances user engagement, significantly reducing unnecessary exploratory behaviour and increasing focused visual attention on the AI avatar. Our findings suggest that a balanced level of adaptive AI provides the most effective user support, offering practical design recommendations for future adaptive educational technologies.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2408.13995.pdf' target='_blank'>https://arxiv.org/pdf/2408.13995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Geng Foo, Yixuan He, Ajmal Saeed Mian, Hossein Rahmani, Jun Liu, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13995">Avatar Concept Slider: Controllable Editing of Concepts in 3D Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based editing of 3D human avatars to precisely match user requirements is challenging due to the inherent ambiguity and limited expressiveness of natural language. To overcome this, we propose the Avatar Concept Slider (ACS), a 3D avatar editing method that allows precise editing of semantic concepts in human avatars towards a specified intermediate point between two extremes of concepts, akin to moving a knob along a slider track. To achieve this, our ACS has three designs: Firstly, a Concept Sliding Loss based on linear discriminant analysis to pinpoint the concept-specific axes for precise editing. Secondly, an Attribute Preserving Loss based on principal component analysis for improved preservation of avatar identity during editing. We further propose a 3D Gaussian Splatting primitive selection mechanism based on concept-sensitivity, which updates only the primitives that are the most sensitive to our target concept, to improve efficiency. Results demonstrate that our ACS enables controllable 3D avatar editing, without compromising the avatar quality or its identifying attributes.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2406.07867.pdf' target='_blank'>https://arxiv.org/pdf/2406.07867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, Yong Man Ro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07867">Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation. Demo and data are available at https://multidialog.github.io and https://huggingface.co/datasets/IVLLab/MultiDialog, respectively.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2405.12663.pdf' target='_blank'>https://arxiv.org/pdf/2405.12663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Gong, Shenyu Ji, Lin Geng Foo, Kang Chen, Hossein Rahmani, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12663">LAGA: Layered 3D Avatar Generation and Customization via Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating and customizing a 3D clothed avatar from textual descriptions is a critical and challenging task. Traditional methods often treat the human body and clothing as inseparable, limiting users' ability to freely mix and match garments. In response to this limitation, we present LAyered Gaussian Avatar (LAGA), a carefully designed framework enabling the creation of high-fidelity decomposable avatars with diverse garments. By decoupling garments from avatar, our framework empowers users to conviniently edit avatars at the garment level. Our approach begins by modeling the avatar using a set of Gaussian points organized in a layered structure, where each layer corresponds to a specific garment or the human body itself. To generate high-quality garments for each layer, we introduce a coarse-to-fine strategy for diverse garment generation and a novel dual-SDS loss function to maintain coherence between the generated garments and avatar components, including the human body and other garments. Moreover, we introduce three regularization losses to guide the movement of Gaussians for garment transfer, allowing garments to be freely transferred to various avatars. Extensive experimentation demonstrates that our approach surpasses existing methods in the generation of 3D clothed humans.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2402.03907.pdf' target='_blank'>https://arxiv.org/pdf/2402.03907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Efe Bozkir, SÃ¼leyman Ãzdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03907">Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence and human-computer interaction will likely lead to extended reality (XR) becoming pervasive. While XR can provide users with interactive, engaging, and immersive experiences, non-player characters are often utilized in pre-scripted and conventional ways. This paper argues for using large language models (LLMs) in XR by embedding them in avatars or as narratives to facilitate inclusion through prompt engineering and fine-tuning the LLMs. We argue that this inclusion will promote diversity for XR use. Furthermore, the versatile conversational capabilities of LLMs will likely increase engagement in XR, helping XR become ubiquitous. Lastly, we speculate that combining the information provided to LLM-powered spaces by users and the biometric data obtained might lead to novel privacy invasions. While exploring potential privacy breaches, examining user privacy concerns and preferences is also essential. Therefore, despite challenges, LLM-powered XR is a promising area with several opportunities.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2402.00827.pdf' target='_blank'>https://arxiv.org/pdf/2402.00827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, Yunlong Tang, Huaijin Tu, Jiebo Luo, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00827">GaussianStyle: Gaussian Head Avatar via StyleGAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods like Neural Radiation Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant strides in facial attribute control such as facial animation and components editing, yet they struggle with fine-grained representation and scalability in dynamic head modeling. To address these limitations, we propose GaussianStyle, a novel framework that integrates the volumetric strengths of 3DGS with the powerful implicit representation of StyleGAN. The GaussianStyle preserves structural information, such as expressions and poses, using Gaussian points, while projecting the implicit volumetric representation into StyleGAN to capture high-frequency details and mitigate the over-smoothing commonly observed in neural texture rendering. Experimental outcomes indicate that our method achieves state-of-the-art performance in reenactment, novel view synthesis, and animation.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2309.16964.pdf' target='_blank'>https://arxiv.org/pdf/2309.16964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunjiao Zhou, Jianfei Yang, He Huang, Lihua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16964">AdaPose: Towards Cross-Site Device-Free Human Pose Estimation with Commodity WiFi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>WiFi-based pose estimation is a technology with great potential for the development of smart homes and metaverse avatar generation. However, current WiFi-based pose estimation methods are predominantly evaluated under controlled laboratory conditions with sophisticated vision models to acquire accurately labeled data. Furthermore, WiFi CSI is highly sensitive to environmental variables, and direct application of a pre-trained model to a new environment may yield suboptimal results due to domain shift. In this paper, we proposes a domain adaptation algorithm, AdaPose, designed specifically for weakly-supervised WiFi-based pose estimation. The proposed method aims to identify consistent human poses that are highly resistant to environmental dynamics. To achieve this goal, we introduce a Mapping Consistency Loss that aligns the domain discrepancy of source and target domains based on inner consistency between input and output at the mapping level. We conduct extensive experiments on domain adaptation in two different scenes using our self-collected pose estimation dataset containing WiFi CSI frames. The results demonstrate the effectiveness and robustness of AdaPose in eliminating domain shift, thereby facilitating the widespread application of WiFi-based pose estimation in smart cities.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2308.14177.pdf' target='_blank'>https://arxiv.org/pdf/2308.14177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Geng Foo, Hossein Rahmani, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14177">AI-Generated Content (AIGC) for Various Data Modalities: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated content (AIGC) methods aim to produce text, images, videos, 3D assets, and other media using AI algorithms. Due to its wide range of applications and the potential of recent works, AIGC developments -- especially in Machine Learning (ML) and Deep Learning (DL) -- have been attracting significant attention, and this survey focuses on comprehensively reviewing such advancements in ML/DL. AIGC methods have been developed for various data modalities, such as image, video, text, 3D shape, 3D scene, 3D human avatar, 3D motion, and audio -- each presenting unique characteristics and challenges. Furthermore, there have been significant developments in cross-modality AIGC methods, where generative methods receive conditioning input in one modality and produce outputs in another. Examples include going from various modalities to image, video, 3D, and audio. This paper provides a comprehensive review of AIGC methods across different data modalities, including both single-modality and cross-modality methods, highlighting the various challenges, representative works, and recent technical directions in each setting. We also survey the representative datasets throughout the modalities, and present comparative results for various modalities. Moreover, we discuss the typical applications of AIGC methods in various domains, challenges, and future research directions.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2305.06131.pdf' target='_blank'>https://arxiv.org/pdf/2305.06131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Li, Chaoning Zhang, Joseph Cho, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06131">Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI has made significant progress in recent years, with text-guided content generation being the most practical as it facilitates interaction between human instructions and AI-generated content (AIGC). Thanks to advancements in text-to-image and 3D modeling technologies, like neural radiance field (NeRF), text-to-3D has emerged as a nascent yet highly active research field. Our work conducts a comprehensive survey on this topic and follows up on subsequent research progress in the overall field, aiming to help readers interested in this direction quickly catch up with its rapid development. First, we introduce 3D data representations, including both Structured and non-Structured data. Building on this pre-requisite, we introduce various core technologies to achieve satisfactory text-to-3D results. Additionally, we present mainstream baselines and research directions in recent text-to-3D technology, including fidelity, efficiency, consistency, controllability, diversity, and applicability. Furthermore, we summarize the usage of text-to-3D technology in various applications, including avatar generation, texture generation, scene generation and 3D editing. Finally, we discuss the agenda for the future development of text-to-3D.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2502.10397.pdf' target='_blank'>https://arxiv.org/pdf/2502.10397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Wang, Qinnan Hu, Zhou Su, Linkang Du, Qichao Xu, Weiwei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10397">Large Model Empowered Metaverse: State-of-the-Art, Challenges and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Metaverse represents a transformative shift beyond traditional mobile Internet, creating an immersive, persistent digital ecosystem where users can interact, socialize, and work within 3D virtual environments. Powered by large models such as ChatGPT and Sora, the Metaverse benefits from precise large-scale real-world modeling, automated multimodal content generation, realistic avatars, and seamless natural language understanding, which enhance user engagement and enable more personalized, intuitive interactions. However, challenges remain, including limited scalability, constrained responsiveness, and low adaptability in dynamic environments. This paper investigates the integration of large models within the Metaverse, examining their roles in enhancing user interaction, perception, content creation, and service quality. To address existing challenges, we propose a generative AI-based framework for optimizing Metaverse rendering. This framework includes a cloud-edge-end collaborative model to allocate rendering tasks with minimal latency, a mobility-aware pre-rendering mechanism that dynamically adjusts to user movement, and a diffusion model-based adaptive rendering strategy to fine-tune visual details. Experimental results demonstrate the effectiveness of our approach in enhancing rendering efficiency and reducing rendering overheads, advancing large model deployment for a more responsive and immersive Metaverse.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2405.15372.pdf' target='_blank'>https://arxiv.org/pdf/2405.15372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sushmita Gupta, Tanmay Inamdar, Pallavi Jain, Daniel Lokshtanov, Fahad Panolan, Saket Saurabh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15372">When far is better: The Chamberlin-Courant approach to obnoxious committee selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classical work on metric space based committee selection problem interprets distance as ``near is better''. In this work, motivated by real-life situations, we interpret distance as ``far is better''. Formally stated, we initiate the study of ``obnoxious'' committee scoring rules when the voters' preferences are expressed via a metric space. To this end, we propose a model where large distances imply high satisfaction and study the egalitarian avatar of the well-known Chamberlin-Courant voting rule and some of its generalizations. For a given integer value $1 \le Î»\le k$, the committee size k, a voter derives satisfaction from only the $Î»$-th favorite committee member; the goal is to maximize the satisfaction of the least satisfied voter. For the special case of $Î»= 1$, this yields the egalitarian Chamberlin-Courant rule. In this paper, we consider general metric space and the special case of a $d$-dimensional Euclidean space.
  We show that when $Î»$ is $1$ and $k$, the problem is polynomial-time solvable in $\mathbb{R}^2$ and general metric space, respectively. However, for $Î»= k-1$, it is NP-hard even in $\mathbb{R}^2$. Thus, we have ``double-dichotomy'' in $\mathbb{R}^2$ with respect to the value of Î», where the extreme cases are solvable in polynomial time but an intermediate case is NP-hard. Furthermore, this phenomenon appears to be ``tight'' for $\mathbb{R}^2$ because the problem is NP-hard for general metric space, even for $Î»=1$. Consequently, we are motivated to explore the problem in the realm of (parameterized) approximation algorithms and obtain positive results. Interestingly, we note that this generalization of Chamberlin-Courant rules encodes practical constraints that are relevant to solutions for certain facility locations.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2404.19398.pdf' target='_blank'>https://arxiv.org/pdf/2404.19398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjie Ma, Yanlin Weng, Tianjia Shao, Kun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19398">3D Gaussian Blendshapes for Head Avatar Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars. Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models. Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance. The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients. High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting. Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2301.10221.pdf' target='_blank'>https://arxiv.org/pdf/2301.10221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Wang, Zhou Su, Miao Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10221">Social Metaverse: Challenges and Solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social metaverse is a shared digital space combining a series of interconnected virtual worlds for users to play, shop, work, and socialize. In parallel with the advances of artificial intelligence (AI) and growing awareness of data privacy concerns, federated learning (FL) is promoted as a paradigm shift towards privacy-preserving AI-empowered social metaverse. However, challenges including privacy-utility tradeoff, learning reliability, and AI model thefts hinder the deployment of FL in real metaverse applications. In this paper, we exploit the pervasive social ties among users/avatars to advance a social-aware hierarchical FL framework, i.e., SocialFL for a better privacy-utility tradeoff in the social metaverse. Then, an aggregator-free robust FL mechanism based on blockchain is devised with a new block structure and an improved consensus protocol featured with on/off-chain collaboration. Furthermore, based on smart contracts and digital watermarks, an automatic federated AI (FedAI) model ownership provenance mechanism is designed to prevent AI model thefts and collusive avatars in social metaverse. Experimental findings validate the feasibility and effectiveness of proposed framework. Finally, we envision promising future research directions in this emerging area.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2503.19906.pdf' target='_blank'>https://arxiv.org/pdf/2503.19906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Liu, Xuan Wang, Ziyu Wan, Yue Ma, Jingye Chen, Yanbo Fan, Yujun Shen, Yibing Song, Qifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19906">AvatarArtist: Open-Domain 4D Avatarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2503.01158.pdf' target='_blank'>https://arxiv.org/pdf/2503.01158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suzhen Wang, Weijie Chen, Wei Zhang, Minda Zhao, Lincheng Li, Rongsheng Zhang, Zhipeng Hu, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01158">EasyCraft: A Robust and Efficient Framework for Automatic Avatar Crafting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Character customization, or 'face crafting,' is a vital feature in role-playing games (RPGs), enhancing player engagement by enabling the creation of personalized avatars. Existing automated methods often struggle with generalizability across diverse game engines due to their reliance on the intermediate constraints of specific image domain and typically support only one type of input, either text or image. To overcome these challenges, we introduce EasyCraft, an innovative end-to-end feedforward framework that automates character crafting by uniquely supporting both text and image inputs. Our approach employs a translator capable of converting facial images of any style into crafting parameters. We first establish a unified feature distribution in the translator's image encoder through self-supervised learning on a large-scale dataset, enabling photos of any style to be embedded into a unified feature representation. Subsequently, we map this unified feature distribution to crafting parameters specific to a game engine, a process that can be easily adapted to most game engines and thus enhances EasyCraft's generalizability. By integrating text-to-image techniques with our translator, EasyCraft also facilitates precise, text-based character crafting. EasyCraft's ability to integrate diverse inputs significantly enhances the versatility and accuracy of avatar creation. Extensive experiments on two RPG games demonstrate the effectiveness of our method, achieving state-of-the-art results and facilitating adaptability across various avatar engines.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2409.13180.pdf' target='_blank'>https://arxiv.org/pdf/2409.13180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Qiu, Wei Zhang, Chen Liu, Rudong An, Lincheng Li, Yu Ding, Changjie Fan, Zhipeng Hu, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13180">FreeAvatar: Robust 3D Facial Animation Transfer by Learning an Expression Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-driven 3D facial animation transfer aims to drive avatars to reproduce the expressions of actors. Existing methods have achieved remarkable results by constraining both geometric and perceptual consistency. However, geometric constraints (like those designed on facial landmarks) are insufficient to capture subtle emotions, while expression features trained on classification tasks lack fine granularity for complex emotions. To address this, we propose \textbf{FreeAvatar}, a robust facial animation transfer method that relies solely on our learned expression representation. Specifically, FreeAvatar consists of two main components: the expression foundation model and the facial animation transfer model. In the first component, we initially construct a facial feature space through a face reconstruction task and then optimize the expression feature space by exploring the similarities among different expressions. Benefiting from training on the amounts of unlabeled facial images and re-collected expression comparison dataset, our model adapts freely and effectively to any in-the-wild input facial images. In the facial animation transfer component, we propose a novel Expression-driven Multi-avatar Animator, which first maps expressive semantics to the facial control parameters of 3D avatars and then imposes perceptual constraints between the input and output images to maintain expression consistency. To make the entire process differentiable, we employ a trained neural renderer to translate rig parameters into corresponding images. Furthermore, unlike previous methods that require separate decoders for each avatar, we propose a dynamic identity injection module that allows for the joint training of multiple avatars within a single network.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2305.18891.pdf' target='_blank'>https://arxiv.org/pdf/2305.18891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Chen Liu, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18891">EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating vivid and diverse 3D co-speech gestures is crucial for various applications in animating virtual avatars. While most existing methods can generate gestures from audio directly, they usually overlook that emotion is one of the key factors of authentic co-speech gesture generation. In this work, we propose EmotionGesture, a novel framework for synthesizing vivid and diverse emotional co-speech 3D gestures from audio. Considering emotion is often entangled with the rhythmic beat in speech audio, we first develop an Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features as well as model their correlation via a transcript-based visual-rhythm alignment. Then, we propose an initial pose based Spatial-Temporal Prompter (STP) to generate future gestures from the given initial poses. STP effectively models the spatial-temporal correlations between the initial poses and the future gestures, thus producing the spatial-temporal coherent pose prompt. Once we obtain pose prompts, emotion, and audio beat features, we will generate 3D co-speech gestures through a transformer architecture. However, considering the poses of existing datasets often contain jittering effects, this would lead to generating unstable gestures. To address this issue, we propose an effective objective function, dubbed Motion-Smooth Loss. Specifically, we model motion offset to compensate for jittering ground-truth by forcing gestures to be smooth. Last, we present an emotion-conditioned VAE to sample emotion features, enabling us to generate diverse emotional results. Extensive experiments demonstrate that our framework outperforms the state-of-the-art, achieving vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be released at the project page: https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2212.02469.pdf' target='_blank'>https://arxiv.org/pdf/2212.02469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, Deng Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02469">One-shot Implicit Animatable Avatars with Model-based Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pretrained models. Both priors are used to jointly guide the optimization for creating plausible content in the invisible areas. Taking advantage of the CLIP models, ELICIT can use text descriptions to generate text-conditioned unseen regions. In order to further improve visual details, we propose a segmentation-based sampling strategy that locally refines different parts of the avatar. Comprehensive evaluations on multiple popular benchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT has outperformed strong baseline methods of avatar creation when only a single image is available. The code is public for research purposes at https://huangyangyi.github.io/ELICIT/.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2411.18675.pdf' target='_blank'>https://arxiv.org/pdf/2411.18675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivangi Aneja, Artem Sevastopolsky, Tobias Kirschstein, Justus Thies, Angela Dai, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18675">GaussianSpeech: Audio-Driven Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GaussianSpeech, a novel approach that synthesizes high-fidelity animation sequences of photo-realistic, personalized 3D human head avatars from spoken audio. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple speech signal with 3D Gaussian splatting to create realistic, temporally coherent motion sequences. We propose a compact and efficient 3DGS-based avatar representation that generates expression-dependent color and leverages wrinkle- and perceptually-based losses to synthesize facial details, including wrinkles that occur with different expressions. To enable sequence modeling of 3D Gaussian splats with audio, we devise an audio-conditioned transformer model capable of extracting lip and expression features directly from audio input. Due to the absence of high-quality datasets of talking humans in correspondence with audio, we captured a new large-scale multi-view dataset of audio-visual sequences of talking humans with native English accents and diverse facial geometry. GaussianSpeech consistently achieves state-of-the-art performance with visually natural motion at real time rendering rates, while encompassing diverse facial expressions and styles.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2411.12981.pdf' target='_blank'>https://arxiv.org/pdf/2411.12981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobao Wei, Peng Chen, Guangyu Li, Ming Lu, Hui Chen, Feng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12981">GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, the first high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. Leveraging the unstructured nature of 3DGS, we develop a novel representation of the eye for rigid eye rotation based on the target gaze direction. To enable synthesis generalization across various subjects, we integrate an expression-guided module to inject subject-specific information into the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. The code is available at: https://ucwxb.github.io/GazeGaussian.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2311.17857.pdf' target='_blank'>https://arxiv.org/pdf/2311.17857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rameen Abdal, Wang Yifan, Zifan Shi, Yinghao Xu, Ryan Po, Zhengfei Kuang, Qifeng Chen, Dit-Yan Yeung, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17857">Gaussian Shell Maps for Efficient 3D Human Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \times 512$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2308.04830.pdf' target='_blank'>https://arxiv.org/pdf/2308.04830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyang Chen, Zhiyong Wu, Runnan Li, Weihong Bao, Jun Ling, Xu Tan, Sheng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04830">VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current talking face generation methods mainly focus on speech-lip synchronization. However, insufficient investigation on the facial talking style leads to a lifeless and monotonous avatar. Most previous works fail to imitate expressive styles from arbitrary video prompts and ensure the authenticity of the generated video. This paper proposes an unsupervised variational style transfer model (VAST) to vivify the neutral photo-realistic avatars. Our model consists of three key components: a style encoder that extracts facial style representations from the given video prompts; a hybrid facial expression decoder to model accurate speech-related movements; a variational style enhancer that enhances the style space to be highly expressive and meaningful. With our essential designs on facial style learning, our model is able to flexibly capture the expressive facial style from arbitrary video prompts and transfer it onto a personalized image renderer in a zero-shot manner. Experimental results demonstrate the proposed approach contributes to a more vivid talking avatar with higher authenticity and richer expressiveness.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2307.05462.pdf' target='_blank'>https://arxiv.org/pdf/2307.05462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinghao Xu, Wang Yifan, Alexander W. Bergman, Menglei Chai, Bolei Zhou, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05462">Efficient 3D Articulated Human Generation with Layered Surface Volumes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Access to high-quality and diverse 3D articulated digital human assets is crucial in various applications, ranging from virtual reality to social platforms. Generative approaches, such as 3D generative adversarial networks (GANs), are rapidly replacing laborious manual content creation tools. However, existing 3D GAN frameworks typically rely on scene representations that leverage either template meshes, which are fast but offer limited quality, or volumes, which offer high capacity but are slow to render, thereby limiting the 3D fidelity in GAN settings. In this work, we introduce layered surface volumes (LSVs) as a new 3D object representation for articulated digital humans. LSVs represent a human body using multiple textured mesh layers around a conventional template. These layers are rendered using alpha compositing with fast differentiable rasterization, and they can be interpreted as a volumetric representation that allocates its capacity to a manifold of finite thickness around the template. Unlike conventional single-layer templates that struggle with representing fine off-surface details like hair or accessories, our surface volumes naturally capture such details. LSVs can be articulated, and they exhibit exceptional efficiency in GAN settings, where a 2D generator learns to synthesize the RGBA textures for the individual layers. Trained on unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality and view-consistent 3D articulated digital humans without the need for view-inconsistent 2D upsampling networks.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2508.18337.pdf' target='_blank'>https://arxiv.org/pdf/2508.18337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18337">EAI-Avatar: Emotion-Aware Interactive Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have advanced rapidly, enabling impressive talking head generation that brings AI to life. However, most existing methods focus solely on one-way portrait animation. Even the few that support bidirectional conversational interactions lack precise emotion-adaptive capabilities, significantly limiting their practical applicability. In this paper, we propose EAI-Avatar, a novel emotion-aware talking head generation framework for dyadic interactions. Leveraging the dialogue generation capability of large language models (LLMs, e.g., GPT-4), our method produces temporally consistent virtual avatars with rich emotional variations that seamlessly transition between speaking and listening states. Specifically, we design a Transformer-based head mask generator that learns temporally consistent motion features in a latent mask space, capable of generating arbitrary-length, temporally consistent mask sequences to constrain head motions. Furthermore, we introduce an interactive talking tree structure to represent dialogue state transitions, where each tree node contains information such as child/parent/sibling nodes and the current character's emotional state. By performing reverse-level traversal, we extract rich historical emotional cues from the current node to guide expression synthesis. Extensive experiments demonstrate the superior performance and effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2503.22225.pdf' target='_blank'>https://arxiv.org/pdf/2503.22225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22225">Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained conditional diffusion models have demonstrated remarkable potential in image editing. However, they often face challenges with temporal consistency, particularly in the talking head domain, where continuous changes in facial expressions intensify the level of difficulty. These issues stem from the independent editing of individual images and the inherent loss of temporal continuity during the editing process. In this paper, we introduce Follow Your Motion (FYM), a generic framework for maintaining temporal consistency in portrait editing. Specifically, given portrait images rendered by a pre-trained 3D Gaussian Splatting model, we first develop a diffusion model that intuitively and inherently learns motion trajectory changes at different scales and pixel coordinates, from the first frame to each subsequent frame. This approach ensures that temporally inconsistent edited avatars inherit the motion information from the rendered avatars. Secondly, to maintain fine-grained expression temporal consistency in talking head editing, we propose a dynamic re-weighted attention mechanism. This mechanism assigns higher weight coefficients to landmark points in space and dynamically updates these weights based on landmark loss, achieving more consistent and refined facial expressions. Extensive experiments demonstrate that our method outperforms existing approaches in terms of temporal consistency and can be used to optimize and compensate for temporally inconsistent outputs in a range of applications, such as text-driven editing, relighting, and various other applications.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2502.10841.pdf' target='_blank'>https://arxiv.org/pdf/2502.10841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, Xiang Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10841">SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SkyReels-A1, a simple yet effective framework built upon video diffusion Transformer to facilitate portrait image animation. Existing methodologies still encounter issues, including identity distortion, background instability, and unrealistic facial dynamics, particularly in head-only animation scenarios. Besides, extending to accommodate diverse body proportions usually leads to visual inconsistencies or unnatural articulations. To address these challenges, SkyReels-A1 capitalizes on the strong generative capabilities of video DiT, enhancing facial motion transfer precision, identity retention, and temporal coherence. The system incorporates an expression-aware conditioning module that enables seamless video synthesis driven by expression-guided landmark inputs. Integrating the facial image-text alignment module strengthens the fusion of facial attributes with motion trajectories, reinforcing identity preservation. Additionally, SkyReels-A1 incorporates a multi-stage training paradigm to incrementally refine the correlation between expressions and motion while ensuring stable identity reproduction. Extensive empirical evaluations highlight the model's ability to produce visually coherent and compositionally diverse results, making it highly applicable to domains such as virtual avatars, remote communication, and digital media generation.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2502.00229.pdf' target='_blank'>https://arxiv.org/pdf/2502.00229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyao Wang, Youyu Sheng, Qihang He, Haolong Hu, Shuwen Liu, Feiqi Gu, Yumei Jing, Dengbo He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00229">Enhancing Psychotherapeutic Alliance in College: When and How to Integrate Multimodal Large Language Models in Psychotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As mental health issues rise among college students, there is an increasing interest and demand in leveraging Multimodal Language Models (MLLM) to enhance mental support services, yet integrating them into psychotherapy remains theoretical or non-user-centered. This study investigated the opportunities and challenges of using MLLMs within the campus psychotherapy alliance in China. Through three studies involving both therapists and student clients, we argue that the ideal role for MLLMs at this stage is as an auxiliary tool to human therapists. Users widely expect features such as triage matching and real-time emotion recognition. At the same time, for independent therapy by MLLM, concerns about capabilities and privacy ethics remain prominent, despite high demands for personalized avatars and non-verbal communication. Our findings further indicate that users' sense of social identity and perceived relative status of MLLMs significantly influence their acceptance. This study provides insights for future intelligent campus mental healthcare.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2501.10687.pdf' target='_blank'>https://arxiv.org/pdf/2501.10687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10687">EMO2: End-Effector Guided Audio-Driven Avatar Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel audio-driven talking head method capable of simultaneously generating highly expressive facial expressions and hand gestures. Unlike existing methods that focus on generating full-body or half-body poses, we investigate the challenges of co-speech gesture generation and identify the weak correspondence between audio features and full-body gestures as a key limitation. To address this, we redefine the task as a two-stage process. In the first stage, we generate hand poses directly from audio input, leveraging the strong correlation between audio signals and hand movements. In the second stage, we employ a diffusion model to synthesize video frames, incorporating the hand poses generated in the first stage to produce realistic facial expressions and body movements. Our experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, such as CyberHost and Vlogger, in terms of both visual quality and synchronization accuracy. This work provides a new perspective on audio-driven gesture generation and a robust framework for creating expressive and natural talking head animations.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2411.15436.pdf' target='_blank'>https://arxiv.org/pdf/2411.15436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15436">ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head Avatar with Temporal Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have shown impressive potential on talking head generation. While plausible appearance and talking effect are achieved, these methods still suffer from temporal, 3D or expression inconsistency due to the error accumulation and inherent limitation of single-image generation ability. In this paper, we propose ConsistentAvatar, a novel framework for fully consistent and high-fidelity talking avatar generation. Instead of directly employing multi-modal conditions to the diffusion process, our method learns to first model the temporal representation for stability between adjacent frames. Specifically, we propose a Temporally-Sensitive Detail (TSD) map containing high-frequency feature and contours that vary significantly along the time axis. Using a temporal consistent diffusion module, we learn to align TSD of the initial result to that of the video frame ground truth. The final avatar is generated by a fully consistent diffusion module, conditioned on the aligned TSD, rough head normal, and emotion prompt embedding. We find that the aligned TSD, which represents the temporal patterns, constrains the diffusion process to generate temporally stable talking head. Further, its reliable guidance complements the inaccuracy of other conditions, suppressing the accumulated error while improving the consistency on various aspects. Extensive experiments demonstrate that ConsistentAvatar outperforms the state-of-the-art methods on the generated appearance, 3D, expression and temporal consistency. Project page: https://njust-yang.github.io/ConsistentAvatar.github.io/
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2409.16382.pdf' target='_blank'>https://arxiv.org/pdf/2409.16382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Nasimzada, Jens Kleesiek, Ken Herrmann, Alina Roitberg, Constantin Seibold
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16382">Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing pain in video is crucial for improving patient-computer interaction systems, yet traditional data collection in this domain raises significant ethical and logistical challenges. This study introduces a novel approach that leverages synthetic data to enhance video-based pain recognition models, providing an ethical and scalable alternative. We present a pipeline that synthesizes realistic 3D facial models by capturing nuanced facial movements from a small participant pool, and mapping these onto diverse synthetic avatars. This process generates 8,600 synthetic faces, accurately reflecting genuine pain expressions from varied angles and perspectives.
  Utilizing advanced facial capture techniques, and leveraging public datasets like CelebV-HQ and FFHQ-UV for demographic diversity, our new synthetic dataset significantly enhances model training while ensuring privacy by anonymizing identities through facial replacements.
  Experimental results demonstrate that models trained on combinations of synthetic data paired with a small amount of real participants achieve superior performance in pain recognition, effectively bridging the gap between synthetic simulations and real-world applications. Our approach addresses data scarcity and ethical concerns, offering a new solution for pain detection and opening new avenues for research in privacy-preserving dataset generation. All resources are publicly available to encourage further innovation in this field.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2408.02110.pdf' target='_blank'>https://arxiv.org/pdf/2408.02110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feichi Lu, Zijian Dong, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02110">AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite progress in human motion capture, existing multi-view methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2404.18630.pdf' target='_blank'>https://arxiv.org/pdf/2404.18630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18630">4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2401.10242.pdf' target='_blank'>https://arxiv.org/pdf/2401.10242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Gao, Li Hu, Peng Zhang, Bang Zhang, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10242">DanceMeld: Unraveling Dance Phrases with Hierarchical Latent Codes for Music-to-Dance Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of 3D digital human applications, music-to-dance presents a challenging task. Given the one-to-many relationship between music and dance, previous methods have been limited in their approach, relying solely on matching and generating corresponding dance movements based on music rhythm. In the professional field of choreography, a dance phrase consists of several dance poses and dance movements. Dance poses composed of a series of basic meaningful body postures, while dance movements can reflect dynamic changes such as the rhythm, melody, and style of dance. Taking inspiration from these concepts, we introduce an innovative dance generation pipeline called DanceMeld, which comprising two stages, i.e., the dance decouple stage and the dance generation stage. In the decouple stage, a hierarchical VQ-VAE is used to disentangle dance poses and dance movements in different feature space levels, where the bottom code represents dance poses, and the top code represents dance movements. In the generation stage, we utilize a diffusion model as a prior to model the distribution and generate latent codes conditioned on music features. We have experimentally demonstrated the representational capabilities of top code and bottom code, enabling the explicit decoupling expression of dance poses and dance movements. This disentanglement not only provides control over motion details, styles, and rhythm but also facilitates applications such as dance style transfer and dance unit editing. Our approach has undergone qualitative and quantitative experiments on the AIST++ dataset, demonstrating its superiority over other methods.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2308.16894.pdf' target='_blank'>https://arxiv.org/pdf/2308.16894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Zarate, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16894">EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present EMDB, the Electromagnetic Database of Global 3D Human Pose and Shape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPL pose and shape parameters with global body and camera trajectories for in-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors and a hand-held iPhone to record a total of 58 minutes of motion data, distributed over 81 indoor and outdoor sequences and 10 participants. Together with accurate body poses and shapes, we also provide global camera poses and body root trajectories. To construct EMDB, we propose a multi-stage optimization procedure, which first fits SMPL to the 6-DoF EM measurements and then refines the poses via image observations. To achieve high-quality results, we leverage a neural implicit avatar model to reconstruct detailed human surface geometry and appearance, which allows for improved alignment and smoothness via a dense pixel-level objective. Our evaluations, conducted with a multi-view volumetric capture system, indicate that EMDB has an expected accuracy of 2.3 cm positional and 10.6 degrees angular error, surpassing the accuracy of previous in-the-wild datasets. We evaluate existing state-of-the-art monocular RGB methods for camera-relative and global pose estimation on EMDB. EMDB is publicly available under https://ait.ethz.ch/emdb
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2305.00121.pdf' target='_blank'>https://arxiv.org/pdf/2305.00121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsuan-I Ho, Lixin Xue, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00121">Learning Locally Editable Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of our work lies a representation that combines the modeling power of neural fields with the ease of use and inherent 3D consistency of skinned meshes. To this end, we construct a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model, thus exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that admits fitting to unseen scans and sampling of realistic avatars with varied appearances and geometries. Furthermore, our representation allows local editing by swapping local features between 3D assets. To verify our method for avatar creation and editing, we contribute a new high-quality dataset, dubbed CustomHumans, for training and evaluation. Our experiments quantitatively and qualitatively show that our method generates diverse detailed avatars and achieves better model fitting performance compared to state-of-the-art methods. Our code and dataset are available at https://custom-humans.github.io/.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2303.15380.pdf' target='_blank'>https://arxiv.org/pdf/2303.15380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15380">Hi4D: 4D Instance Segmentation of Close Human Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Hi4D, a method and dataset for the automatic analysis of physically close human-human interaction under prolonged contact. Robustly disentangling several in-contact subjects is a challenging task due to occlusions and complex shapes. Hence, existing multi-view systems typically fuse 3D surfaces of close subjects into a single, connected mesh. To address this issue we leverage i) individually fitted neural implicit avatars; ii) an alternating optimization scheme that refines pose and surface through periods of close proximity; and iii) thus segment the fused raw scans into individual instances. From these instances we compile Hi4D dataset of 4D textured scans of 20 subject pairs, 100 sequences, and a total of more than 11K frames. Hi4D contains rich interaction-centric annotations in 2D and 3D alongside accurately registered parametric body models. We define varied human pose and shape estimation tasks on this dataset and provide results from state-of-the-art methods on these benchmarks.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2303.13129.pdf' target='_blank'>https://arxiv.org/pdf/2303.13129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanzhou Li, Jingbo Wang, Chen Change Loy, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13129">Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital human motion synthesis is a vibrant research field with applications in movies, AR/VR, and video games. Whereas methods were proposed to generate natural and realistic human motions, most only focus on modeling humans and largely ignore object movements. Generating task-oriented human-object interaction motions in simulation is challenging. For different intents of using the objects, humans conduct various motions, which requires the human first to approach the objects and then make them move consistently with the human instead of staying still. Also, to deploy in downstream applications, the synthesized motions are desired to be flexible in length, providing options to personalize the predicted motions for various purposes. To this end, we propose TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations, which generates full human-object interaction motions to conduct specific tasks, given only the task type, the object, and a starting human status. TOHO generates human-object motions in three steps: 1) it first estimates the keyframe poses of conducting a task given the task type and object information; 2) then, it infills the keyframes and generates continuous motions; 3) finally, it applies a compact closed-form object motion estimation to generate the object motion. Our method generates continuous motions that are parameterized only by the temporal coordinate, which allows for upsampling or downsampling of the sequence to arbitrary frames and adjusting the motion speeds by designing the temporal coordinate vector. We demonstrate the effectiveness of our method, both qualitatively and quantitatively. This work takes a step further toward general human-scene interaction simulation.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2303.04805.pdf' target='_blank'>https://arxiv.org/pdf/2303.04805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Julien Valentin, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04805">X-Avatar: Expressive Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present X-Avatar, a novel avatar model that captures the full expressiveness of digital humans to bring about life-like experiences in telepresence, AR/VR and beyond. Our method models bodies, hands, facial expressions and appearance in a holistic fashion and can be learned from either full 3D scans or RGB-D data. To achieve this, we propose a part-aware learned forward skinning module that can be driven by the parameter space of SMPL-X, allowing for expressive animation of X-Avatars. To efficiently learn the neural shape and deformation fields, we propose novel part-aware sampling and initialization strategies. This leads to higher fidelity results, especially for smaller body parts while maintaining efficient training despite increased number of articulated bones. To capture the appearance of the avatar with high-frequency details, we extend the geometry and deformation fields with a texture network that is conditioned on pose, facial expression, geometry and the normals of the deformed surface. We show experimentally that our method outperforms strong baselines in both data domains both quantitatively and qualitatively on the animation task. To facilitate future research on expressive avatars we contribute a new dataset, called X-Humans, containing 233 sequences of high-quality textured scans from 20 participants, totalling 35,500 data frames.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2302.11566.pdf' target='_blank'>https://arxiv.org/pdf/2302.11566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11566">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human geometry reconstructions. We evaluate our methods on publicly available datasets and show improvements over prior art.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2507.17029.pdf' target='_blank'>https://arxiv.org/pdf/2507.17029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17029">StreamME: Simplify 3D Gaussian Avatar within Live Stream</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2505.16152.pdf' target='_blank'>https://arxiv.org/pdf/2505.16152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bolin Chen, Shanzhi Yin, Hanwei Zhu, Lingyu Zhu, Zihan Zhang, Jie Chen, Ru-Ling Liao, Shiqi Wang, Yan Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16152">Compressing Human Body Video with Interactive Semantics: A Generative Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose to compress human body video with interactive semantics, which can facilitate video coding to be interactive and controllable by manipulating semantic-level representations embedded in the coded bitstream. In particular, the proposed encoder employs a 3D human model to disentangle nonlinear dynamics and complex motion of human body signal into a series of configurable embeddings, which are controllably edited, compactly compressed, and efficiently transmitted. Moreover, the proposed decoder can evolve the mesh-based motion fields from these decoded semantics to realize the high-quality human body video reconstruction. Experimental results illustrate that the proposed framework can achieve promising compression performance for human body videos at ultra-low bitrate ranges compared with the state-of-the-art video coding standard Versatile Video Coding (VVC) and the latest generative compression schemes. Furthermore, the proposed framework enables interactive human body video coding without any additional pre-/post-manipulation processes, which is expected to shed light on metaverse-related digital human communication in the future.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2505.14135.pdf' target='_blank'>https://arxiv.org/pdf/2505.14135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihuang Li, Caijin Zhou, Shoujian Zheng, Jianxiang Lu, Jiabin Huang, Comi Chen, Junshu Tang, Guangzheng Xu, Jiale Tao, Hongmei Wang, Donghao Li, Wenqing Yu, Senbo Wang, Zhimin Li, Yetshuan Shi, Haoyu Yang, Yukun Wang, Wenxun Dai, Jiaqi Li, Linqing Wang, Qixun Wang, Zhiyong Xu, Yingfang Zhang, Jiangfeng Xiong, Weijie Kong, Chao Zhang, Hongxin Zhang, Qiaoling Zheng, Weiting Guo, Xinchi Deng, Yixuan Li, Renjia Wei, Yulin Jian, Duojun Huang, Xuhua Ren, Junkun Yuan, Zhengguang Zhou, Jiaxiang Cheng, Bing Ma, Shirui Huang, Jiawang Bai, Chao Li, Sihuan Lin, Yifu Sun, Yuan Zhou, Joey Wang, Qin Lin, Tianxiang Zheng, Jingmiao Yu, Jihong Zhang, Caesar Zhong, Di Wang, Yuhong Liu, Linus, Jie Jiang, Longhuang Wu, Shuai Shao, Qinglin Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14135">Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent game creation represents a transformative advancement in game development, utilizing generative artificial intelligence to dynamically generate and enhance game content. Despite notable progress in generative models, the comprehensive synthesis of high-quality game assets, including both images and videos, remains a challenging frontier. To create high-fidelity game content that simultaneously aligns with player preferences and significantly boosts designer efficiency, we present Hunyuan-Game, an innovative project designed to revolutionize intelligent game production. Hunyuan-Game encompasses two primary branches: image generation and video generation. The image generation component is built upon a vast dataset comprising billions of game images, leading to the development of a group of customized image generation models tailored for game scenarios: (1) General Text-to-Image Generation. (2) Game Visual Effects Generation, involving text-to-effect and reference image-based game visual effect generation. (3) Transparent Image Generation for characters, scenes, and game visual effects. (4) Game Character Generation based on sketches, black-and-white images, and white models. The video generation component is built upon a comprehensive dataset of millions of game and anime videos, leading to the development of five core algorithmic models, each targeting critical pain points in game development and having robust adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2) 360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4) Generative Video Super-Resolution. (5) Interactive Game Video Generation. These image and video generation models not only exhibit high-level aesthetic expression but also deeply integrate domain-specific knowledge, establishing a systematic understanding of diverse game and anime art styles.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2504.20629.pdf' target='_blank'>https://arxiv.org/pdf/2504.20629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongsoo Choi, Ji-Hoon Kim, Kim Sung-Bin, Tae-Hyun Oh, Joon Son Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20629">AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT .
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2504.02542.pdf' target='_blank'>https://arxiv.org/pdf/2504.02542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fa-Ting Hong, Zunnan Xu, Zixiang Zhou, Jun Zhou, Xiu Li, Qin Lin, Qinglin Lu, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02542">Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce \textbf{ACTalker}, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict. The project website can be found at https://harlanhong.github.io/publications/actalker/index.html.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2411.17799.pdf' target='_blank'>https://arxiv.org/pdf/2411.17799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronglai Zuo, Rolandos Alexandros Potamias, Evangelos Ververas, Jiankang Deng, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17799">Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language is a visual language that encompasses all linguistic features of natural languages and serves as the primary communication method for the deaf and hard-of-hearing communities. Although many studies have successfully adapted pretrained language models (LMs) for sign language translation (sign-to-text), the reverse task-sign language generation (text-to-sign)-remains largely unexplored. In this work, we introduce a multilingual sign language model, Signs as Tokens (SOKE), which can generate 3D sign avatars autoregressively from text inputs using a pretrained LM. To align sign language with the LM, we leverage a decoupled tokenizer that discretizes continuous signs into token sequences representing various body parts. During decoding, unlike existing approaches that flatten all part-wise tokens into a single sequence and predict one token at a time, we propose a multi-head decoding method capable of predicting multiple tokens simultaneously. This approach improves inference efficiency while maintaining effective information fusion across different body parts. To further ease the generation process, we propose a retrieval-enhanced SLG approach, which incorporates external sign dictionaries to provide accurate word-level signs as auxiliary conditions, significantly improving the precision of generated signs. Extensive qualitative and quantitative evaluations demonstrate the effectiveness of SOKE.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2410.07160.pdf' target='_blank'>https://arxiv.org/pdf/2410.07160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song, Lele Chen, Celong Liu, Pinxin Liu, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07160">TextToon: Real-Time Text Toonify Head Avatar from Single Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2409.15095.pdf' target='_blank'>https://arxiv.org/pdf/2409.15095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Honerkamp, Harsh Mahesheka, Jan Ole von Hartz, Tim Welschehold, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15095">Whole-Body Teleoperation for Mobile Manipulation at Zero Added Cost</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Demonstration data plays a key role in learning complex behaviors and training robotic foundation models. While effective control interfaces exist for static manipulators, data collection remains cumbersome and time intensive for mobile manipulators due to their large number of degrees of freedom. While specialized hardware, avatars, or motion tracking can enable whole-body control, these approaches are either expensive, robot-specific, or suffer from the embodiment mismatch between robot and human demonstrator. In this work, we present MoMa-Teleop, a novel teleoperation method that infers end-effector motions from existing interfaces and delegates the base motions to a previously developed reinforcement learning agent, leaving the operator to focus fully on the task-relevant end-effector motions. This enables whole-body teleoperation of mobile manipulators with no additional hardware or setup costs via standard interfaces such as joysticks or hand guidance. Moreover, the operator is not bound to a tracked workspace and can move freely with the robot over spatially extended tasks. We demonstrate that our approach results in a significant reduction in task completion time across a variety of robots and tasks. As the generated data covers diverse whole-body motions without embodiment mismatch, it enables efficient imitation learning. By focusing on task-specific end-effector motions, our approach learns skills that transfer to unseen settings, such as new obstacles or changed object positions, from as little as five demonstrations. We make code and videos available at https://moma-teleop.cs.uni-freiburg.de.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2408.13674.pdf' target='_blank'>https://arxiv.org/pdf/2408.13674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keqiang Sun, Amin Jourabloo, Riddhish Bhalodia, Moustafa Meshry, Yu Rong, Zhengyu Yang, Thu Nguyen-Phuoc, Christian Haene, Jiu Xu, Sam Johnson, Hongsheng Li, Sofien Bouaziz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13674">GenCA: A Text-conditioned Generative Model for Realistic and Drivable Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photo-realistic and controllable 3D avatars are crucial for various applications such as virtual and mixed reality (VR/MR), telepresence, gaming, and film production. Traditional methods for avatar creation often involve time-consuming scanning and reconstruction processes for each avatar, which limits their scalability. Furthermore, these methods do not offer the flexibility to sample new identities or modify existing ones. On the other hand, by learning a strong prior from data, generative models provide a promising alternative to traditional reconstruction methods, easing the time constraints for both data capture and processing. Additionally, generative methods enable downstream applications beyond reconstruction, such as editing and stylization. Nonetheless, the research on generative 3D avatars is still in its infancy, and therefore current methods still have limitations such as creating static avatars, lacking photo-realism, having incomplete facial details, or having limited drivability. To address this, we propose a text-conditioned generative model that can generate photo-realistic facial avatars of diverse identities, with more complete details like hair, eyes and mouth interior, and which can be driven through a powerful non-parametric latent expression space. Specifically, we integrate the generative and editing capabilities of latent diffusion models with a strong prior model for avatar expression driving.
  Our model can generate and control high-fidelity avatars, even those out-of-distribution. We also highlight its potential for downstream applications, including avatar editing and single-shot avatar reconstruction.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2403.13535.pdf' target='_blank'>https://arxiv.org/pdf/2403.13535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siying Cui, Jia Guo, Xiang An, Jiankang Deng, Yongle Zhao, Xinyu Wei, Ziyong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13535">IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging Stable Diffusion for the generation of personalized portraits has emerged as a powerful and noteworthy tool, enabling users to create high-fidelity, custom character avatars based on their specific prompts. However, existing personalization methods face challenges, including test-time fine-tuning, the requirement of multiple input images, low preservation of identity, and limited diversity in generated outcomes. To overcome these challenges, we introduce IDAdapter, a tuning-free approach that enhances the diversity and identity preservation in personalized image generation from a single face image. IDAdapter integrates a personalized concept into the generation process through a combination of textual and visual injections and a face identity loss. During the training phase, we incorporate mixed features from multiple reference images of a specific identity to enrich identity-related content details, guiding the model to generate images with more diverse styles, expressions, and angles compared to previous works. Extensive evaluations demonstrate the effectiveness of our method, achieving both diversity and identity fidelity in generated images.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2401.09386.pdf' target='_blank'>https://arxiv.org/pdf/2401.09386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song, Pinxin Liu, Lele Chen, Guojun Yin, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09386">Tri$^{2}$-plane: Thinking Head Avatar via Feature Pyramid</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed considerable achievements in facial avatar reconstruction with neural volume rendering. Despite notable advancements, the reconstruction of complex and dynamic head movements from monocular videos still suffers from capturing and restoring fine-grained details. In this work, we propose a novel approach, named Tri$^2$-plane, for monocular photo-realistic volumetric head avatar reconstructions. Distinct from the existing works that rely on a single tri-plane deformation field for dynamic facial modeling, the proposed Tri$^2$-plane leverages the principle of feature pyramids and three top-to-down lateral connections tri-planes for details improvement. It samples and renders facial details at multiple scales, transitioning from the entire face to specific local regions and then to even more refined sub-regions. Moreover, we incorporate a camera-based geometry-aware sliding window method as an augmentation in training, which improves the robustness beyond the canonical space, with a particular improvement in cross-identity generation capabilities. Experimental outcomes indicate that the Tri$^2$-plane not only surpasses existing methodologies but also achieves superior performance across quantitative and qualitative assessments. The project website is: \url{https://songluchuan.github.io/Tri2Plane.github.io/}.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2312.13578.pdf' target='_blank'>https://arxiv.org/pdf/2312.13578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13578">DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of emotional talking faces from a single portrait image remains a significant challenge. The simultaneous achievement of expressive emotional talking and accurate lip-sync is particularly difficult, as expressiveness is often compromised for the accuracy of lip-sync. As widely adopted by many prior works, the LSTM network often fails to capture the subtleties and variations of emotional expressions. To address these challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven framework, tailored for generating diverse expressions and accurate lip-sync concurrently. In the first stage, we propose EmoDiff, a novel diffusion module that generates diverse highly dynamic emotional expressions and head poses in accordance with the audio and the referenced emotion style. Given the strong correlation between lip motion and audio, we then refine the dynamics with enhanced lip-sync accuracy using audio features and emotion style. To this end, we deploy a video-to-video rendering module to transfer the expressions and lip motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of expressiveness, lip-sync accuracy and perceptual quality.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2312.02963.pdf' target='_blank'>https://arxiv.org/pdf/2312.02963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02963">MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset. Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data. To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation. Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2312.02702.pdf' target='_blank'>https://arxiv.org/pdf/2312.02702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Baltatzis, Rolandos Alexandros Potamias, Evangelos Ververas, Guanxiong Sun, Jiankang Deng, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02702">Neural Sign Actors: A diffusion model for 3D sign language production from text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign Languages (SL) serve as the primary mode of communication for the Deaf and Hard of Hearing communities. Deep learning methods for SL recognition and translation have achieved promising results. However, Sign Language Production (SLP) poses a challenge as the generated motions must be realistic and have precise semantic meaning. Most SLP methods rely on 2D data, which hinders their realism. In this work, a diffusion-based SLP model is trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an unconstrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neural network defined on the SMPL-X body skeleton. Through quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous methods of SLP. This work makes an important step towards realistic neural sign avatars, bridging the communication gap between Deaf and hearing communities.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2311.00994.pdf' target='_blank'>https://arxiv.org/pdf/2311.00994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kim Sung-Bin, Lee Hyun, Da Hye Hong, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00994">LaughTalk: Expressive 3D Talking Head Generation with Laughter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Laughter is a unique expression, essential to affirmative social interactions of humans. Although current 3D talking head generation methods produce convincing verbal articulations, they often fail to capture the vitality and subtleties of laughter and smiles despite their importance in social context. In this paper, we introduce a novel task to generate 3D talking heads capable of both articulate speech and authentic laughter. Our newly curated dataset comprises 2D laughing videos paired with pseudo-annotated and human-validated 3D FLAME parameters and vertices. Given our proposed dataset, we present a strong baseline with a two-stage training scheme: the model first learns to talk and then acquires the ability to express laughter. Extensive experiments demonstrate that our method performs favorably compared to existing approaches in both talking head generation and expressing laughter signals. We further explore potential applications on top of our proposed method for rigging realistic avatars.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2310.02714.pdf' target='_blank'>https://arxiv.org/pdf/2310.02714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanmeng Zhang, Jianfeng Zhang, Rohan Chacko, Hongyi Xu, Guoxian Song, Yi Yang, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02714">GETAvatar: Generative Textured Meshes for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of 3D-aware full-body human generation, aiming at creating animatable human avatars with high-quality textures and geometries. Generally, two challenges remain in this field: i) existing methods struggle to generate geometries with rich realistic details such as the wrinkles of garments; ii) they typically utilize volumetric radiance fields and neural renderers in the synthesis process, making high-resolution rendering non-trivial. To overcome these problems, we propose GETAvatar, a Generative model that directly generates Explicit Textured 3D meshes for animatable human Avatar, with photo-realistic appearance and fine geometric details. Specifically, we first design an articulated 3D human representation with explicit surface modeling, and enrich the generated humans with realistic surface details by learning from the 2D normal maps of 3D scan data. Second, with the explicit mesh representation, we can use a rasterization-based renderer to perform surface rendering, allowing us to achieve high-resolution image generation efficiently. Extensive experiments demonstrate that GETAvatar achieves state-of-the-art performance on 3D-aware human generation both in appearance and geometry quality. Notably, GETAvatar can generate images at 512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving upon previous methods by 2x. Our code and models will be available.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2305.09641.pdf' target='_blank'>https://arxiv.org/pdf/2305.09641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09641">FitMe: Deep Photorealistic 3D Morphable Model Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable human avatars from single or multiple images. The model consists of a multi-modal style-based generator, that captures facial appearance in terms of diffuse and specular reflectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an optimization pipeline, while also achieving photorealistic facial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by exploiting the expressivity of the style-based latent representation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on single "in-the-wild" facial images, while it produces impressive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent implicit avatar reconstructions, FitMe requires only one minute and produces relightable mesh and texture-based avatars, that can be used by end-user applications.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2509.12027.pdf' target='_blank'>https://arxiv.org/pdf/2509.12027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wei, Jia Sun, Junxiang Liao, Lik-Hang Lee, Pan Hui, Huamin Qu, Wai Tong, Xian Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12027">Exploring Gaze Dynamics in VR Film Education: Gender, Avatar, and the Shift Between Male and Female Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In virtual reality (VR) education, especially in creative fields like film production, avatar design and narrative style extend beyond appearance and aesthetics. This study explores how the interaction between avatar gender, the dominant narrative actor's gender, and the learner's gender influences film production learning in VR, focusing on gaze dynamics and gender perspectives. Using a 2*2*2 experimental design, 48 participants operated avatars of different genders and interacted with male or female-dominant narratives. The results show that the consistency between the avatar and gender affects presence, and learners' control over the avatar is also influenced by gender matching. Learners using avatars of the opposite gender reported stronger control, suggesting gender incongruity prompted more focus on the avatar. Additionally, female participants with female avatars were more likely to adopt a "female gaze," favoring soft lighting and emotional shots, while male participants with male avatars were more likely to adopt a "male gaze," choosing dynamic shots and high contrast. When male participants used female avatars, they favored "female gaze," while female participants with male avatars focused on "male gaze". These findings advance our understanding of how avatar design and narrative style in VR-based education influence creativity and the cultivation of gender perspectives, and they offer insights for developing more inclusive and diverse VR teaching tools going forward.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2508.14357.pdf' target='_blank'>https://arxiv.org/pdf/2508.14357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rihao Chang, He Jiao, Weizhi Nie, Honglin Guo, Keliang Xie, Zhenhua Wu, Lina Zhao, Yunpeng Bai, Yongtao Ma, Lanjun Wang, Yuting Su, Xi Gao, Weijie Wang, Nicu Sebe, Bruno Lepri, Bingwei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14357">Organ-Agents: Virtual Human Physiology Simulator via LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have enabled new possibilities in simulating complex physiological systems. We introduce Organ-Agents, a multi-agent framework that simulates human physiology via LLM-driven agents. Each Simulator models a specific system (e.g., cardiovascular, renal, immune). Training consists of supervised fine-tuning on system-specific time-series data, followed by reinforcement-guided coordination using dynamic reference selection and error correction. We curated data from 7,134 sepsis patients and 7,895 controls, generating high-resolution trajectories across 9 systems and 125 variables. Organ-Agents achieved high simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and robustness across SOFA-based severity strata. External validation on 22,689 ICU patients from two hospitals showed moderate degradation under distribution shifts with stable simulation. Organ-Agents faithfully reproduces critical multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with coherent timing and phase progression. Evaluation by 15 critical care physicians confirmed realism and physiological plausibility (mean Likert ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations under alternative sepsis treatment strategies, generating trajectories and APACHE II scores aligned with matched real-world patients. In downstream early warning tasks, classifiers trained on synthetic data showed minimal AUROC drops (<0.04), indicating preserved decision-relevant patterns. These results position Organ-Agents as a credible, interpretable, and generalizable digital twin for precision diagnosis, treatment simulation, and hypothesis testing in critical care.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2507.04258.pdf' target='_blank'>https://arxiv.org/pdf/2507.04258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhong, Jingxiang Sun, Liang An, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04258">MoReMouse: Monocular Reconstruction of Laboratory Mouse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Laboratory mice play a crucial role in biomedical research, yet accurate 3D mouse surface motion reconstruction remains challenging due to their complex non-rigid geometric deformations and textureless appearance. Moreover, the absence of structured 3D datasets severely hinders the progress beyond sparse keypoint tracking. To narrow the gap, we present MoReMouse, the first monocular dense 3D reconstruction network tailored for laboratory mice. To achieve this goal, we highlight three key designs. First, we construct the first high-fidelity dense-view synthetic dataset for mice, by rendering our self-designed realistic Gaussian mouse avatar. Second, MoReMouse adopts a transformer-based feedforward architecture with triplane representation, achieving high-quality 3D surface generation from a single image. Third, we create geodesic-based continuous correspondence embeddings on mouse surface, which serve as strong semantic priors to improve reconstruction stability and surface consistency. Extensive quantitative and qualitative experiments demonstrate that MoReMouse significantly outperforms existing open-source methods in accuracy and robustness. Video results are available at https://zyyw-eric.github.io/MoreMouse-webpage/.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2506.06645.pdf' target='_blank'>https://arxiv.org/pdf/2506.06645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Peng, Jingxiang Sun, Yushuo Chen, Zhaoqi Su, Zhuo Su, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06645">Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2505.15058.pdf' target='_blank'>https://arxiv.org/pdf/2505.15058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, Zhaoxin Fan, Wenjun Wu, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15058">AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2503.22249.pdf' target='_blank'>https://arxiv.org/pdf/2503.22249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22249">FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2503.17544.pdf' target='_blank'>https://arxiv.org/pdf/2503.17544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Yao Feng, AlpÃ¡r Cseke, Nitin Saini, Nathan Bajandas, Nicolas Heron, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17544">PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formulate the motor system of an interactive avatar as a generative motion model that can drive the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although human motion generation has been extensively studied, many existing methods lack the responsiveness and realism of real human movements. Inspired by recent advances in foundation models, we propose PRIMAL, which is learned with a two-stage paradigm. In the pretraining stage, the model learns body movements from a large number of sub-second motion segments, providing a generative foundation from which more complex motions are built. This training is fully unsupervised without annotations. Given a single-frame initial state during inference, the pretrained model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In the adaptation phase, we employ a novel ControlNet-like adaptor to fine-tune the base model efficiently, adapting it to new tasks such as few-shot personalized action generation and spatial target reaching. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that feels highly responsive and natural. Code, models, and more results are available at: https://yz-cnsdqz.github.io/eigenmotion/PRIMAL
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2503.14501.pdf' target='_blank'>https://arxiv.org/pdf/2503.14501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaowei Miao, Kehan Li, Jinsheng Quan, Zhiyuan Min, Shaojie Ma, Yichao Xu, Yi Yang, Ping Liu, Yawei Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14501">Advances in 4D Generation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative artificial intelligence has recently progressed from static image and video synthesis to 3D content generation, culminating in the emergence of 4D generation-the task of synthesizing temporally coherent dynamic 3D assets guided by user input. As a burgeoning research frontier, 4D generation enables richer interactive and immersive experiences, with applications ranging from digital humans to autonomous driving. Despite rapid progress, the field lacks a unified understanding of 4D representations, generative frameworks, basic paradigms, and the core technical challenges it faces. This survey provides a systematic and in-depth review of the 4D generation landscape. To comprehensively characterize 4D generation, we first categorize fundamental 4D representations and outline associated techniques for 4D generation. We then present an in-depth analysis of representative generative pipelines based on conditions and representation methods. Subsequently, we discuss how motion and geometry priors are integrated into 4D outputs to ensure spatio-temporal consistency under various control schemes. From an application perspective, this paper summarizes 4D generation tasks in areas such as dynamic object/scene generation, digital human synthesis, editable 4D content, and embodied AI. Furthermore, we summarize and multi-dimensionally compare four basic paradigms for 4D generation: End-to-End, Generated-Data-Based, Implicit-Distillation-Based, and Explicit-Supervision-Based. Concluding our analysis, we highlight five key challenges-consistency, controllability, diversity, efficiency, and fidelity-and contextualize these with current approaches.By distilling recent advances and outlining open problems, this work offers a comprehensive and forward-looking perspective to guide future research in 4D generation.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2503.12751.pdf' target='_blank'>https://arxiv.org/pdf/2503.12751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhan, Wangze Xu, Qingtian Zhu, Muyao Niu, Mingze Ma, Yifei Liu, Zhihang Zhong, Xiao Sun, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12751">R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing Photorealistic Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present R3-Avatar, incorporating a temporal codebook, to overcome the inability of human avatars to be both animatable and of high-fidelity rendering quality. Existing video-based reconstruction of 3D human avatars either focuses solely on rendering, lacking animation support, or learns a pose-appearance mapping for animating, which degrades under limited training poses or complex clothing. In this paper, we adopt a "record-retrieve-reconstruct" strategy that ensures high-quality rendering from novel views while mitigating degradation in novel poses. Specifically, disambiguating timestamps record temporal appearance variations in a codebook, ensuring high-fidelity novel-view rendering, while novel poses retrieve corresponding timestamps by matching the most similar training poses for augmented appearance. Our R3-Avatar outperforms cutting-edge video-based human avatar reconstruction, particularly in overcoming visual quality degradation in extreme scenarios with limited training human poses and complex clothing.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2411.16768.pdf' target='_blank'>https://arxiv.org/pdf/2411.16768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wangze Xu, Yifan Zhan, Zhihang Zhong, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16768">Sequential Gaussian Avatars with Hierarchical Motion Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of neural rendering has significantly advanced the rendering quality of 3D human avatars, with the recently popular 3DGS technique enabling real-time performance. However, SMPL-driven 3DGS human avatars still struggle to capture fine appearance details due to the complex mapping from pose to appearance during fitting. In this paper, we propose SeqAvatar, which excavates the explicit 3DGS representation to better model human avatars based on a hierarchical motion context. Specifically, we utilize a coarse-to-fine motion conditions that incorporate both the overall human skeleton and fine-grained vertex motions for non-rigid deformation. To enhance the robustness of the proposed motion conditions, we adopt a spatio-temporal multi-scale sampling strategy to hierarchically integrate more motion clues to model human avatars. Extensive experiments demonstrate that our method significantly outperforms 3DGS-based approaches and renders human avatars orders of magnitude faster than the latest NeRF-based models that incorporate temporal context, all while delivering performance that is at least comparable or even superior. Project page: https://zezeaaa.github.io/projects/SeqAvatar/
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2410.08082.pdf' target='_blank'>https://arxiv.org/pdf/2410.08082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhan, Qingtian Zhu, Muyao Niu, Mingze Ma, Jiancheng Zhao, Zhihang Zhong, Xiao Sun, Yu Qiao, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08082">ToMiE: Towards Explicit Exoskeleton for the Reconstruction of Complicated 3D Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we highlight a critical yet often overlooked factor in most 3D human tasks, namely modeling complicated 3D human with with hand-held objects or loose-fitting clothing. It is known that the parameterized formulation of SMPL is able to fit human skin; while hand-held objects and loose-fitting clothing, are difficult to get modeled within the unified framework, since their movements are usually decoupled with the human body. To enhance the capability of SMPL skeleton in response to this situation, we propose a growth strategy that enables the joint tree of the skeleton to expand adaptively. Specifically, our method, called ToMiE, consists of parent joints localization and external joints optimization. For parent joints localization, we employ a gradient-based approach guided by both LBS blending weights and motion kernels. Once the external joints are obtained, we proceed to optimize their transformations in SE(3) across different frames, enabling rendering and explicit animation. ToMiE manages to outperform other methods across various cases with hand-held objects and loose-fitting clothing, not only in rendering quality but also by offering free animation of grown joints, thereby enhancing the expressive ability of SMPL skeleton for a broader range of applications.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2408.09665.pdf' target='_blank'>https://arxiv.org/pdf/2408.09665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Chen Yang, Hao Wang, Xingyue Zhao, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09665">Topology-aware Human Avatars with Semantically-guided Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photo-realistic and topology-aware animatable human avatars from monocular videos remains challenging in computer vision and graphics. Recently, methods using 3D Gaussians to represent the human body have emerged, offering faster optimization and real-time rendering. However, due to ignoring the crucial role of human body semantic information which represents the explicit topological and intrinsic structure within human body, they fail to achieve fine-detail reconstruction of human avatars. To address this issue, we propose SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid deformation, and non-rigid cloth dynamics deformation to create photo-realistic human avatars. We then design a Semantic Human-Body Annotator (SHA) which utilizes SMPL's semantic prior for efficient body part semantic labeling. The generated labels are used to guide the optimization of semantic attributes of Gaussian. To capture the explicit topological structure of the human body, we employ a 3D network that integrates both topological and geometric associations for human avatar deformation. We further implement three key strategies to enhance the semantic accuracy of 3D Gaussians and rendering quality: semantic projection with 2D regularization, semantic-guided density regularization and semantic-aware regularization with neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves state-of-the-art geometry and appearance reconstruction performance.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2408.09663.pdf' target='_blank'>https://arxiv.org/pdf/2408.09663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09663">3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing approaches for human avatar generation--both NeRF-based and 3D Gaussian Splatting (3DGS) based--struggle with maintaining 3D consistency and exhibit degraded detail reconstruction, particularly when training with sparse inputs. To address this challenge, we propose CHASE, a novel framework that achieves dense-input-level performance using only sparse inputs through two key innovations: cross-pose intrinsic 3D consistency supervision and 3D geometry contrastive learning. Building upon prior skeleton-driven approaches that combine rigid deformation with non-rigid cloth dynamics, we first establish baseline avatars with fundamental 3D consistency. To enhance 3D consistency under sparse inputs, we introduce a Dynamic Avatar Adjustment (DAA) module, which refines deformed Gaussians by leveraging similar poses from the training set. By minimizing the rendering discrepancy between adjusted Gaussians and reference poses, DAA provides additional supervision for avatar reconstruction. We further maintain global 3D consistency through a novel geometry-aware contrastive learning strategy. While designed for sparse inputs, CHASE surpasses state-of-the-art methods across both full and sparse settings on ZJU-MoCap and H36M datasets, demonstrating that our enhanced 3D consistency leads to superior rendering quality.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2407.20156.pdf' target='_blank'>https://arxiv.org/pdf/2407.20156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyun Chen, Abdeldjallil Naceri, Abdalla Swikir, Sandra Hirche, Sami Haddadin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20156">Autonomous and Teleoperation Control of a Drawing Robot Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A drawing robot avatar is a robotic system that allows for telepresence-based drawing, enabling users to remotely control a robotic arm and create drawings in real-time from a remote location. The proposed control framework aims to improve bimanual robot telepresence quality by reducing the user workload and required prior knowledge through the automation of secondary or auxiliary tasks. The introduced novel method calculates the near-optimal Cartesian end-effector pose in terms of visual feedback quality for the attached eye-to-hand camera with motion constraints in consideration. The effectiveness is demonstrated by conducting user studies of drawing reference shapes using the implemented robot avatar compared to stationary and teleoperated camera pose conditions. Our results demonstrate that the proposed control framework offers improved visual feedback quality and drawing performance.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2407.06938.pdf' target='_blank'>https://arxiv.org/pdf/2407.06938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Yiji Cheng, Chunyu Wang, Ting Zhang, Jiaolong Yang, Yansong Tang, Feng Zhao, Dong Chen, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06938">RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RodinHD, which can generate high-fidelity 3D avatars from a portrait image. Existing methods fail to capture intricate details such as hairstyles which we tackle in this paper. We first identify an overlooked problem of catastrophic forgetting that arises when fitting triplanes sequentially on many avatars, caused by the MLP decoder sharing scheme. To overcome this issue, we raise a novel data scheduling strategy and a weight consolidation regularization term, which improves the decoder's capability of rendering sharper details. Additionally, we optimize the guiding effect of the portrait image by computing a finer-grained hierarchical representation that captures rich 2D texture cues, and injecting them to the 3D diffusion model at multiple layers via cross-attention. When trained on 46K avatars with a noise schedule optimized for triplanes, the resulting model can generate 3D avatars with notably better details than previous methods and can generalize to in-the-wild portrait input.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2403.19655.pdf' target='_blank'>https://arxiv.org/pdf/2403.19655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19655">GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling. Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods. We derive GaussianCube by first using a novel densification-constrained Gaussian fitting algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians, and then rearranging these Gaussians into a predefined voxel grid via Optimal Transport. Since GaussianCube is a structured grid representation, it allows us to use standard 3D U-Net as our backbone in diffusion modeling without elaborate designs. More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude. The compactness of GaussianCube greatly eases the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling. Project page: https://gaussiancube.github.io/.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2311.12817.pdf' target='_blank'>https://arxiv.org/pdf/2311.12817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binzhe Li, Bolin Chen, Zhao Wang, Shiqi Wang, Yan Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12817">Semantic Face Compression for Metaverse: A Compact 3D Descriptor Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this letter, we envision a new metaverse communication paradigm for virtual avatar faces, and develop the semantic face compression with compact 3D facial descriptors. The fundamental principle is that the communication of virtual avatar faces primarily emphasizes the conveyance of semantic information. In light of this, the proposed scheme offers the advantages of being highly flexible, efficient and semantically meaningful. The semantic face compression, which allows the communication of the descriptors for artificial intelligence based understanding, could facilitate numerous applications without the involvement of humans in metaverse. The promise of the proposed paradigm is also demonstrated by performance comparisons with the state-of-the-art video coding standard, Versatile Video Coding. A significant improvement in terms of rate-accuracy performance has been achieved. The proposed scheme is expected to enable numerous applications, such as digital human communication based on machine analysis, and to form the cornerstone of interaction and communication in the metaverse.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2309.07125.pdf' target='_blank'>https://arxiv.org/pdf/2309.07125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07125">Text-Guided Generation and Editing of Compositional 3D Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to create a realistic 3D facial avatar with hair and accessories using only a text description. While this challenge has attracted significant recent interest, existing methods either lack realism, produce unrealistic shapes, or do not support editing, such as modifications to the hairstyle. We argue that existing methods are limited because they employ a monolithic modeling approach, using a single representation for the head, face, hair, and accessories. Our observation is that the hair and face, for example, have very different structural qualities that benefit from different representations. Building on this insight, we generate avatars with a compositional model, in which the head, face, and upper body are represented with traditional 3D meshes, and the hair, clothing, and accessories with neural radiance fields (NeRF). The model-based mesh representation provides a strong geometric prior for the face region, improving realism while enabling editing of the person's appearance. By using NeRFs to represent the remaining components, our method is able to model and synthesize parts with complex geometry and appearance, such as curly hair and fluffy scarves. Our novel system synthesizes these high-quality compositional avatars from text descriptions. The experimental results demonstrate that our method, Text-guided generation and Editing of Compositional Avatars (TECA), produces avatars that are more realistic than those of recent methods while being editable because of their compositional nature. For example, our TECA enables the seamless transfer of compositional features like hairstyles, scarves, and other accessories between avatars. This capability supports applications such as virtual try-on.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2309.06441.pdf' target='_blank'>https://arxiv.org/pdf/2309.06441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06441">Learning Disentangled Avatars with Hybrid 3D Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tremendous efforts have been made to learn animatable and photorealistic human avatars. Towards this end, both explicit and implicit 3D representations are heavily studied for a holistic modeling and capture of the whole human (e.g., body, clothing, face and hair), but neither representation is an optimal choice in terms of representation efficacy since different parts of the human avatar have different modeling desiderata. For example, meshes are generally not suitable for modeling clothing and hair. Motivated by this, we present Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit 3D representations. DELTA takes a monocular RGB video as input, and produces a human avatar with separate body and clothing/hair layers. Specifically, we demonstrate two important applications for DELTA. For the first one, we consider the disentanglement of the human body and clothing and in the second, we disentangle the face and hair. To do so, DELTA represents the body or face with an explicit mesh-based parametric 3D model and the clothing or hair with an implicit neural radiance field. To make this possible, we design an end-to-end differentiable renderer that integrates meshes into volumetric rendering, enabling DELTA to learn directly from monocular videos without any 3D supervision. Finally, we show that how these two applications can be easily combined to model full-body avatars, such that the hair, face, body and clothing can be fully disentangled yet jointly rendered. Such a disentanglement enables hair and clothing transfer to arbitrary body shapes. We empirically validate the effectiveness of DELTA's disentanglement by demonstrating its promising performance on disentangled reconstruction, virtual clothing try-on and hairstyle transfer. To facilitate future research, we also release an open-sourced pipeline for the study of hybrid human avatar modeling.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2308.14461.pdf' target='_blank'>https://arxiv.org/pdf/2308.14461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Fillioux, Emilie Gontran, JÃ©rÃ´me Cartry, Jacques RR Mathieu, Sabrina Bedja, Alice BoilÃ¨ve, Paul-Henry CournÃ¨de, Fanny Jaulin, Stergios Christodoulidis, Maria Vakalopoulou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14461">Spatio-Temporal Analysis of Patient-Derived Organoid Videos Using Deep Learning for the Prediction of Drug Efficacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last ten years, Patient-Derived Organoids (PDOs) emerged as the most reliable technology to generate ex-vivo tumor avatars. PDOs retain the main characteristics of their original tumor, making them a system of choice for pre-clinical and clinical studies. In particular, PDOs are attracting interest in the field of Functional Precision Medicine (FPM), which is based upon an ex-vivo drug test in which living tumor cells (such as PDOs) from a specific patient are exposed to a panel of anti-cancer drugs. Currently, the Adenosine Triphosphate (ATP) based cell viability assay is the gold standard test to assess the sensitivity of PDOs to drugs. The readout is measured at the end of the assay from a global PDO population and therefore does not capture single PDO responses and does not provide time resolution of drug effect. To this end, in this study, we explore for the first time the use of powerful large foundation models for the automatic processing of PDO data. In particular, we propose a novel imaging-based high-throughput screening method to assess real-time drug efficacy from a time-lapse microscopy video of PDOs. The recently proposed SAM algorithm for segmentation and DINOv2 model are adapted in a comprehensive pipeline for processing PDO microscopy frames. Moreover, an attention mechanism is proposed for fusing temporal and spatial features in a multiple instance learning setting to predict ATP. We report better results than other non-time-resolved methods, indicating that the temporality of data is an important factor for the prediction of ATP. Extensive ablations shed light on optimizing the experimental setting and automating the prediction both in real-time and for forecasting.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2306.00200.pdf' target='_blank'>https://arxiv.org/pdf/2306.00200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiashun Wang, Xueting Li, Sifei Liu, Shalini De Mello, Orazio Gallo, Xiaolong Wang, Jan Kautz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00200">Zero-shot Pose Transfer for Unrigged Stylized 3D Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transferring the pose of a reference avatar to stylized 3D characters of various shapes is a fundamental task in computer graphics. Existing methods either require the stylized characters to be rigged, or they use the stylized character in the desired pose as ground truth at training. We present a zero-shot approach that requires only the widely available deformed non-stylized avatars in training, and deforms stylized characters of significantly different shapes at inference. Classical methods achieve strong generalization by deforming the mesh at the triangle level, but this requires labelled correspondences. We leverage the power of local deformation, but without requiring explicit correspondence labels. We introduce a semi-supervised shape-understanding module to bypass the need for explicit correspondences at test time, and an implicit pose deformation module that deforms individual surface points to match the target pose. Furthermore, to encourage realistic and accurate deformation of stylized characters, we introduce an efficient volume-based test-time training procedure. Because it does not need rigging, nor the deformed stylized character at training time, our model generalizes to categories with scarce annotation, such as stylized quadrupeds. Extensive experiments demonstrate the effectiveness of the proposed method compared to the state-of-the-art approaches trained with comparable or more supervision. Our project page is available at https://jiashunwang.github.io/ZPT
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2304.08577.pdf' target='_blank'>https://arxiv.org/pdf/2304.08577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, Artsiom Sanakoyeu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08577">Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent surge in popularity of AR/VR applications, realistic and accurate control of 3D full-body avatars has become a highly demanded feature. A particular challenge is that only a sparse tracking signal is available from standalone HMDs (Head Mounted Devices), often limited to tracking the user's head and wrists. While this signal is resourceful for reconstructing the upper body motion, the lower body is not tracked and must be synthesized from the limited information provided by the upper body joints. In this paper, we present AGRoL, a novel conditional diffusion model specifically designed to track full bodies given sparse upper-body tracking signals. Our model is based on a simple multi-layer perceptron (MLP) architecture and a novel conditioning scheme for motion data. It can predict accurate and smooth full-body motion, particularly the challenging lower body movement. Unlike common diffusion architectures, our compact architecture can run in real-time, making it suitable for online body-tracking applications. We train and evaluate our model on AMASS motion capture dataset, and demonstrate that our approach outperforms state-of-the-art methods in generated motion accuracy and smoothness. We further justify our design choices through extensive experiments and ablation studies.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2303.13071.pdf' target='_blank'>https://arxiv.org/pdf/2303.13071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, Linjie Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13071">PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in $360^\circ$ with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2302.09919.pdf' target='_blank'>https://arxiv.org/pdf/2302.09919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bolin Chen, Zhao Wang, Binzhe Li, Shurun Wang, Shiqi Wang, Yan Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09919">Interactive Face Video Coding: A Generative Compression Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel framework for Interactive Face Video Coding (IFVC), which allows humans to interact with the intrinsic visual representations instead of the signals. The proposed solution enjoys several distinct advantages, including ultra-compact representation, low delay interaction, and vivid expression/headpose animation. In particular, we propose the Internal Dimension Increase (IDI) based representation, greatly enhancing the fidelity and flexibility in rendering the appearance while maintaining reasonable representation cost. By leveraging strong statistical regularities, the visual signals can be effectively projected into controllable semantics in the three dimensional space (e.g., mouth motion, eye blinking, head rotation, head translation and head location), which are compressed and transmitted. The editable bitstream, which naturally supports the interactivity at the semantic level, can synthesize the face frames via the strong inference ability of the deep generative model. Experimental results have demonstrated the performance superiority and application prospects of our proposed IFVC scheme. In particular, the proposed scheme not only outperforms the state-of-the-art video coding standard Versatile Video Coding (VVC) and the latest generative compression schemes in terms of rate-distortion performance for face videos, but also enables the interactive coding without introducing additional manipulation processes. Furthermore, the proposed framework is expected to shed lights on the future design of the digital human communication in the metaverse.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2109.02081.pdf' target='_blank'>https://arxiv.org/pdf/2109.02081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Sha, Wei Zhang, Tong Shen, Zhoujun Li, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.02081">Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep person generation has attracted extensive research attention due to its wide applications in virtual agents, video conferencing, online shopping and art/movie production. With the advancement of deep learning, visual appearances (face, pose, cloth) of a person image can be easily generated or manipulated on demand. In this survey, we first summarize the scope of person generation, and then systematically review recent progress and technical trends in deep person generation, covering three major tasks: talking-head generation (face), pose-guided person generation (pose) and garment-oriented person generation (cloth). More than two hundred papers are covered for a thorough overview, and the milestone works are highlighted to witness the major technical breakthrough. Based on these fundamental tasks, a number of applications are investigated, e.g., virtual fitting, digital human, generative data augmentation. We hope this survey could shed some light on the future prospects of deep person generation, and provide a helpful foundation for full applications towards digital human.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2509.02278.pdf' target='_blank'>https://arxiv.org/pdf/2509.02278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikai Huang, Yihan Zhou, Xuemiao Xu, Cheng Xu, Xiaofen Xing, Jing Qin, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02278">Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Singing-driven 3D head animation is a challenging yet promising task with applications in virtual avatars, entertainment, and education. Unlike speech, singing involves richer emotional nuance, dynamic prosody, and lyric-based semantics, requiring the synthesis of fine-grained, temporally coherent facial motion. Existing speech-driven approaches often produce oversimplified, emotionally flat, and semantically inconsistent results, which are insufficient for singing animation. To address this, we propose Think2Sing, a diffusion-based framework that leverages pretrained large language models to generate semantically coherent and temporally consistent 3D head animations, conditioned on both lyrics and acoustics. A key innovation is the introduction of motion subtitles, an auxiliary semantic representation derived through a novel Singing Chain-of-Thought reasoning process combined with acoustic-guided retrieval. These subtitles contain precise timestamps and region-specific motion descriptions, serving as interpretable motion priors. We frame the task as a motion intensity prediction problem, enabling finer control over facial regions and improving the modeling of expressive motion. To support this, we create a multimodal singing dataset with synchronized video, acoustic descriptors, and motion subtitles, enabling diverse and expressive motion learning. Extensive experiments show that Think2Sing outperforms state-of-the-art methods in realism, expressiveness, and emotional fidelity, while also offering flexible, user-controllable animation editing.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2508.18621.pdf' target='_blank'>https://arxiv.org/pdf/2508.18621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18621">Wan-S2V: Audio-Driven Cinematic Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2507.19124.pdf' target='_blank'>https://arxiv.org/pdf/2507.19124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Ahmed Mohsin, Sagnik Bhattacharya, Abhiram Gorle, Muhammad Ali Jamshed, John M. Cioffi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19124">AI Enabled 6G for Semantic Metaverse: Prospects, Challenges and Solutions for Future Wireless VR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wireless support of virtual reality (VR) has challenges when a network has multiple users, particularly for 3D VR gaming, digital AI avatars, and remote team collaboration. This work addresses these challenges through investigation of the low-rank channels that inevitably occur when there are more active users than there are degrees of spatial freedom, effectively often the number of antennas. The presented approach uses optimal nonlinear transceivers, equivalently generalized decision-feedback or successive cancellation for uplink and superposition or dirty-paper precoders for downlink. Additionally, a powerful optimization approach for the users' energy allocation and decoding order appears to provide large improvements over existing methods, effectively nearing theoretical optima. As the latter optimization methods pose real-time challenges, approximations using deep reinforcement learning (DRL) are used to approximate best performance with much lower (5x at least) complexity. Experimental results show significantly larger sum rates and very large power savings to attain the data rates found necessary to support VR. Experimental results show the proposed algorithm outperforms current industry standards like orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA), as well as the highly researched methods in multi-carrier NOMA (MC-NOMA), enhancing sum data rate by 39%, 28%, and 16%, respectively, at a given power level. For the same data rate, it achieves power savings of 75%, 45%, and 40%, making it ideal for VR applications. Additionally, a near-optimal deep reinforcement learning (DRL)-based resource allocation framework for real-time use by being 5x faster and reaching 83% of the global optimum is introduced.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2505.03351.pdf' target='_blank'>https://arxiv.org/pdf/2505.03351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Yang Li, Minghan Qin, Yu Li, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03351">GUAVA: Generalizable Upper Body 3D Gaussian Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing a high-quality, animatable 3D human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3D human avatar reconstruction typically requires multi-view or monocular videos and training on individual IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's expressiveness, these methods often focus on body motion but struggle with facial expressions. To address these challenges, we first introduce an expressive human model (EHM) to enhance facial expression capabilities and develop an accurate tracking method. Based on this template model, we propose GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction. We leverage inverse texture mapping and projection sampling techniques to infer Ubody (upper-body) Gaussians from a single image. The rendered images are refined through a neural refiner. Experimental results demonstrate that GUAVA significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2504.14852.pdf' target='_blank'>https://arxiv.org/pdf/2504.14852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaofan Wang, Guanjie Qiu, Xiaodong Gu, Beijun Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14852">APIRAT: Integrating Multi-source API Knowledge for Enhanced Code Translation with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code translation is an essential task in software migration, multilingual development, and system refactoring. Recent advancements in large language models (LLMs) have demonstrated significant potential in this task. However, prior studies have highlighted that LLMs often struggle with domain-specific code, particularly in resolving cross-lingual API mappings. To tackle this challenge, we propose APIRAT, a novel code translation method that integrates multi-source API knowledge. APIRAT employs three API knowledge augmentation techniques, including API sequence retrieval, API sequence back-translation, and API mapping, to guide LLMs to translating code, ensuring both the correct structure of API sequences and the accurate usage of individual APIs. Extensive experiments on two public datasets, CodeNet and AVATAR, indicate that APIRAT significantly surpasses existing LLM-based methods, achieving improvements in computational accuracy ranging from 4% to 15.1%. Additionally, our evaluation across different LLMs showcases the generalizability of APIRAT. An ablation study further confirms the individual contributions of each API knowledge component, underscoring the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2503.11978.pdf' target='_blank'>https://arxiv.org/pdf/2503.11978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric M. Chen, Di Liu, Sizhuo Ma, Michael Vasilkovsky, Bing Zhou, Qiang Gao, Wenzhou Wang, Jiahao Luo, Dimitris N. Metaxas, Vincent Sitzmann, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11978">Snapmoji: Instant Generation of Animatable Dual-Stylized Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing popularity of personalized avatar systems, such as Snapchat Bitmojis and Apple Memojis, highlights the growing demand for digital self-representation. Despite their widespread use, existing avatar platforms face significant limitations, including restricted expressivity due to predefined assets, tedious customization processes, or inefficient rendering requirements. Addressing these shortcomings, we introduce Snapmoji, an avatar generation system that instantly creates animatable, dual-stylized avatars from a selfie. We propose Gaussian Domain Adaptation (GDA), which is pre-trained on large-scale Gaussian models using 3D data from sources such as Objaverse and fine-tuned with 2D style transfer tasks, endowing it with a rich 3D prior. This enables Snapmoji to transform a selfie into a primary stylized avatar, like the Bitmoji style, and apply a secondary style, such as Plastic Toy or Alien, all while preserving the user's identity and the primary style's integrity. Our system is capable of producing 3D Gaussian avatars that support dynamic animation, including accurate facial expression transfer. Designed for efficiency, Snapmoji achieves selfie-to-avatar conversion in just 0.9 seconds and supports real-time interactions on mobile devices at 30 to 40 frames per second. Extensive testing confirms that Snapmoji outperforms existing methods in versatility and speed, making it a convenient tool for automatic avatar creation in various styles.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2503.08224.pdf' target='_blank'>https://arxiv.org/pdf/2503.08224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Kangjie Chen, Minghan Qin, Yu Li, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08224">HRAvatar: High-Quality and Relightable Gaussian Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing animatable and high-quality 3D head avatars from monocular videos, especially with realistic relighting, is a valuable task. However, the limited information from single-view input, combined with the complex head poses and facial movements, makes this challenging. Previous methods achieve real-time performance by combining 3D Gaussian Splatting with a parametric head model, but the resulting head quality suffers from inaccurate face tracking and limited expressiveness of the deformation model. These methods also fail to produce realistic effects under novel lighting conditions. To address these issues, we propose HRAvatar, a 3DGS-based method that reconstructs high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors through end-to-end optimization and better captures individual facial deformations using learnable blendshapes and learnable linear blend skinning. Additionally, it decomposes head appearance into several physical properties and incorporates physically-based shading to account for environmental lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs superior-quality heads but also achieves realistic visual effects under varying lighting conditions.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2502.19739.pdf' target='_blank'>https://arxiv.org/pdf/2502.19739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Liu, Teng Deng, Giljoo Nam, Yu Rong, Stanislav Pidhorskyi, Junxuan Li, Jason Saragih, Dimitris N. Metaxas, Chen Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19739">LUCAS: Layered Universal Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic 3D head avatar reconstruction faces critical challenges in modeling dynamic face-hair interactions and achieving cross-identity generalization, particularly during expressions and head movements. We present LUCAS, a novel Universal Prior Model (UPM) for codec avatar modeling that disentangles face and hair through a layered representation. Unlike previous UPMs that treat hair as an integral part of the head, our approach separates the modeling of the hairless head and hair into distinct branches. LUCAS is the first to introduce a mesh-based UPM, facilitating real-time rendering on devices. Our layered representation also improves the anchor geometry for precise and visually appealing Gaussian renderings. Experimental results indicate that LUCAS outperforms existing single-mesh and Gaussian-based avatar models in both quantitative and qualitative assessments, including evaluations on held-out subjects in zero-shot driving scenarios. LUCAS demonstrates superior dynamic performance in managing head pose changes, expression transfer, and hairstyle variations, thereby advancing the state-of-the-art in 3D head avatar reconstruction.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2501.10020.pdf' target='_blank'>https://arxiv.org/pdf/2501.10020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao He, Jianqiang Ren, Yuan Dong, Jianjing Xiang, Xiejie Shen, Weihao Yuan, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10020">Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 2D cartoon style is a prominent art form in digital character creation, particularly popular among younger audiences. While advancements in digital human technology have spurred extensive research into photorealistic digital humans and 3D characters, interactive 2D cartoon characters have received comparatively less attention. Unlike 3D counterparts, which require sophisticated construction and resource-intensive rendering, Live2D, a widely-used format for 2D cartoon characters, offers a more efficient alternative, which allows to animate 2D characters in a manner that simulates 3D movement without the necessity of building a complete 3D model. Furthermore, Live2D employs lightweight HTML5 (H5) rendering, improving both accessibility and efficiency. In this technical report, we introduce Textoon, an innovative method for generating diverse 2D cartoon characters in the Live2D format based on text descriptions. The Textoon leverages cutting-edge language and vision models to comprehend textual intentions and generate 2D appearance, capable of creating a wide variety of stunning and interactive 2D characters within one minute. The project homepage is https://human3daigc.github.io/Textoon_webpage/.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2412.14762.pdf' target='_blank'>https://arxiv.org/pdf/2412.14762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maddalena Feder, Giorgio Grioli, Manuel G. Catalano, Antonio Bicchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14762">A General Control Method for Human-Robot Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new generalized control method designed for multi-degrees-of-freedom devices to help people with limited motion capabilities in their daily activities. The challenge lies in finding the most adapted strategy for the control interface to effectively map user's motions in a low-dimensional space to complex robotic assistive devices, such as prostheses, supernumerary limbs, up to remote robotic avatars. The goal is a system which integrates the human and the robotic parts into a unique system, moving so as to reach the targets decided by the human while autonomously reducing the user's effort and discomfort. We present a framework to control general multi DoFs assistive systems, which translates user-performed compensatory motions into the necessary robot commands for reaching targets while canceling or reducing compensation. The framework extends to prostheses of any number of DoF up to full robotic avatars, regarded here as a sort of whole-body prosthesis of the person who sees the robot as an artificial extension of their own body without a physical link but with a sensory-motor integration. We have validated and applied this control strategy through tests encompassing simulated scenarios and real-world trials involving a virtual twin of the robotic parts (prosthesis and robot) and a physical humanoid avatar.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2411.19942.pdf' target='_blank'>https://arxiv.org/pdf/2411.19942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Ye, Xiaoxuan Ma, Hai Ci, Wentao Zhu, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19942">FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving realistic animated human avatars requires accurate modeling of pose-dependent clothing deformations. Existing learning-based methods heavily rely on the Linear Blend Skinning (LBS) of minimally-clothed human models like SMPL to model deformation. However, they struggle to handle loose clothing, such as long dresses, where the canonicalization process becomes ill-defined when the clothing is far from the body, leading to disjointed and fragmented results. To overcome this limitation, we propose FreeCloth, a novel hybrid framework to model challenging clothed humans. Our core idea is to use dedicated strategies to model different regions, depending on whether they are close to or distant from the body. Specifically, we segment the human body into three categories: unclothed, deformed, and generated. We simply replicate unclothed regions that require no deformation. For deformed regions close to the body, we leverage LBS to handle the deformation. As for the generated regions, which correspond to loose clothing areas, we introduce a novel free-form, part-aware generator to model them, as they are less affected by movements. This free-form generation paradigm brings enhanced flexibility and expressiveness to our hybrid framework, enabling it to capture the intricate geometric details of challenging loose clothing, such as skirts and dresses. Experimental results on the benchmark dataset featuring loose clothing demonstrate that FreeCloth achieves state-of-the-art performance with superior visual fidelity and realism, particularly in the most challenging cases.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2407.15212.pdf' target='_blank'>https://arxiv.org/pdf/2407.15212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Zhao, Chenming Wu, Binbin Huang, Yihao Zhi, Chen Zhao, Jingdong Wang, Shenghua Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15212">Surfel-based Gaussian Inverse Rendering for Fast and Relightable Dynamic Human Reconstruction from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate reconstruction of a relightable, dynamic clothed human avatar from a monocular video is crucial for the entertainment industry. This paper introduces the Surfel-based Gaussian Inverse Avatar (SGIA) method, which introduces efficient training and rendering for relightable dynamic human reconstruction. SGIA advances previous Gaussian Avatar methods by comprehensively modeling Physically-Based Rendering (PBR) properties for clothed human avatars, allowing for the manipulation of avatars into novel poses under diverse lighting conditions. Specifically, our approach integrates pre-integration and image-based lighting for fast light calculations that surpass the performance of existing implicit-based techniques. To address challenges related to material lighting disentanglement and accurate geometry reconstruction, we propose an innovative occlusion approximation strategy and a progressive training approach. Extensive experiments demonstrate that SGIA not only achieves highly accurate physical properties but also significantly enhances the realistic relighting of dynamic human avatars, providing a substantial speed advantage. We exhibit more results in our project page: https://GS-IA.github.io.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2405.19712.pdf' target='_blank'>https://arxiv.org/pdf/2405.19712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Sanvito, Andrea Ramazzina, Stefanie Walz, Mario Bijelic, Felix Heide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19712">HINT: Learning Complete Human Neural Representations from Limited Viewpoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No augmented application is possible without animated humanoid avatars. At the same time, generating human replicas from real-world monocular hand-held or robotic sensor setups is challenging due to the limited availability of views. Previous work showed the feasibility of virtual avatars but required the presence of 360 degree views of the targeted subject. To address this issue, we propose HINT, a NeRF-based algorithm able to learn a detailed and complete human model from limited viewing angles. We achieve this by introducing a symmetry prior, regularization constraints, and training cues from large human datasets. In particular, we introduce a sagittal plane symmetry prior to the appearance of the human, directly supervise the density function of the human model using explicit 3D body modeling, and leverage a co-learned human digitization network as additional supervision for the unseen angles. As a result, our method can reconstruct complete humans even from a few viewing angles, increasing performance by more than 15% PSNR compared to previous state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2405.02336.pdf' target='_blank'>https://arxiv.org/pdf/2405.02336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Walid Saad, Omar Hashash, Christo Kurisummoottil Thomas, Christina Chaccour, Merouane Debbah, Narayan Mandayam, Zhu Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02336">Artificial General Intelligence (AGI)-Native Wireless Systems: A Journey Beyond 6G</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building future wireless systems that support services like digital twins (DTs) is challenging to achieve through advances to conventional technologies like meta-surfaces. While artificial intelligence (AI)-native networks promise to overcome some limitations of wireless technologies, developments still rely on AI tools like neural networks. Such tools struggle to cope with the non-trivial challenges of the network environment and the growing demands of emerging use cases. In this paper, we revisit the concept of AI-native wireless systems, equipping them with the common sense necessary to transform them into artificial general intelligence (AGI)-native systems. These systems acquire common sense by exploiting different cognitive abilities such as perception, analogy, and reasoning, that enable them to generalize and deal with unforeseen scenarios. Towards developing the components of such a system, we start by showing how the perception module can be built through abstracting real-world elements into generalizable representations. These representations are then used to create a world model, founded on principles of causality and hyper-dimensional (HD) computing, that aligns with intuitive physics and enables analogical reasoning, that define common sense. Then, we explain how methods such as integrated information theory play a role in the proposed intent-driven and objective-driven planning methods that maneuver the AGI-native network to take actions. Next, we discuss how an AGI-native network can enable use cases related to human and autonomous agents: a) analogical reasoning for next-generation DTs, b) synchronized and resilient experiences for cognitive avatars, and c) brain-level metaverse experiences like holographic teleportation. Finally, we conclude with a set of recommendations to build AGI-native systems. Ultimately, we envision this paper as a roadmap for the beyond 6G era.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2404.19026.pdf' target='_blank'>https://arxiv.org/pdf/2404.19026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19026">MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2404.04876.pdf' target='_blank'>https://arxiv.org/pdf/2404.04876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Yang, Dong Liu, Shuhai Zhang, Zeshuai Deng, Zixiong Huang, Mingkui Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04876">HiLo: Detailed and Robust 3D Clothed Human Reconstruction with High-and Low-Frequency Information of Parametric Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D clothed human involves creating a detailed geometry of individuals in clothing, with applications ranging from virtual try-on, movies, to games. To enable practical and widespread applications, recent advances propose to generate a clothed human from an RGB image. However, they struggle to reconstruct detailed and robust avatars simultaneously. We empirically find that the high-frequency (HF) and low-frequency (LF) information from a parametric model has the potential to enhance geometry details and improve robustness to noise, respectively. Based on this, we propose HiLo, namely clothed human reconstruction with high- and low-frequency information, which contains two components. 1) To recover detailed geometry using HF information, we propose a progressive HF Signed Distance Function to enhance the detailed 3D geometry of a clothed human. We analyze that our progressive learning manner alleviates large gradients that hinder model convergence. 2) To achieve robust reconstruction against inaccurate estimation of the parametric model by using LF information, we propose a spatial interaction implicit function. This function effectively exploits the complementary spatial information from a low-resolution voxel grid of the parametric model. Experimental results demonstrate that HiLo outperforms the state-of-the-art methods by 10.43% and 9.54% in terms of Chamfer distance on the Thuman2.0 and CAPE datasets, respectively. Additionally, HiLo demonstrates robustness to noise from the parametric model, challenging poses, and various clothing styles.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2403.16210.pdf' target='_blank'>https://arxiv.org/pdf/2403.16210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16210">Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar cloth re-targeting. Our project page is available at: https://wolfball.github.io/frankenstein/.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2312.05430.pdf' target='_blank'>https://arxiv.org/pdf/2312.05430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Diao, Ming Cheng, Wayner Barrios, SouYoung Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05430">FT2TF: First-Person Statement Text-To-Talking Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking face generation has gained immense popularity in the computer vision community, with various applications including AR, VR, teleconferencing, digital assistants, and avatars. Traditional methods are mainly audio-driven, which have to deal with the inevitable resource-intensive nature of audio storage and processing. To address such a challenge, we propose FT2TF - First-Person Statement Text-To-Talking Face Generation, a novel one-stage end-to-end pipeline for talking face generation driven by first-person statement text. Different from previous work, our model only leverages visual and textual information without any other sources (e.g., audio/landmark/pose) during inference. Extensive experiments are conducted on LRS2 and LRS3 datasets, and results on multi-dimensional evaluation metrics are reported. Both quantitative and qualitative results showcase that FT2TF outperforms existing relevant methods and reaches the state-of-the-art. This achievement highlights our model's capability to bridge first-person statements and dynamic face generation, providing insightful guidance for future work.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2311.06443.pdf' target='_blank'>https://arxiv.org/pdf/2311.06443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Ma, Tong Zhang, Shanlin Sun, Xiangyi Yan, Kun Han, Xiaohui Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06443">CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing personalized animatable head avatars has significant implications in the fields of AR/VR. Existing methods for achieving explicit face control of 3D Morphable Models (3DMM) typically rely on multi-view images or videos of a single subject, making the reconstruction process complex. Additionally, the traditional rendering pipeline is time-consuming, limiting real-time animation possibilities. In this paper, we introduce CVTHead, a novel approach that generates controllable neural head avatars from a single reference image using point-based neural rendering. CVTHead considers the sparse vertices of mesh as the point set and employs the proposed Vertex-feature Transformer to learn local feature descriptors for each vertex. This enables the modeling of long-range dependencies among all the vertices. Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves comparable performance to state-of-the-art graphics-based methods. Moreover, it enables efficient rendering of novel human heads with various expressions, head poses, and camera views. These attributes can be explicitly controlled using the coefficients of 3DMMs, facilitating versatile and realistic animation in real-time scenarios.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2310.16191.pdf' target='_blank'>https://arxiv.org/pdf/2310.16191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuolin Yang, Zain Sarwar, Iris Hwang, Ronik Bhaskar, Ben Y. Zhao, Haitao Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16191">Can Virtual Reality Protect Users from Keystroke Inference Attacks?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) has gained popularity by providing immersive and interactive experiences without geographical limitations. It also provides a sense of personal privacy through physical separation. In this paper, we show that despite assumptions of enhanced privacy, VR is unable to shield its users from side-channel attacks that steal private information. Ironically, this vulnerability arises from VR's greatest strength, its immersive and interactive nature. We demonstrate this by designing and implementing a new set of keystroke inference attacks in shared virtual environments, where an attacker (VR user) can recover the content typed by another VR user by observing their avatar. While the avatar displays noisy telemetry of the user's hand motion, an intelligent attacker can use that data to recognize typed keys and reconstruct typed content, without knowing the keyboard layout or gathering labeled data. We evaluate the proposed attacks using IRB-approved user studies across multiple VR scenarios. For 13 out of 15 tested users, our attacks accurately recognize 86%-98% of typed keys, and the recovered content retains up to 98% of the meaning of the original typed content. We also discuss potential defenses.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2310.03586.pdf' target='_blank'>https://arxiv.org/pdf/2310.03586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanyi Kong, Grazia Zambella, Simone Monteleone, Giorgio Grioli, Manuel G. Catalano, Antonio Bicchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03586">A Suspended Aerial Manipulation Avatar for Physical Interaction in Unstructured Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an aerial platform capable of performing physically interactive tasks in unstructured environments with human-like dexterity under human supervision. This aerial platform consists of a humanoid torso attached to a hexacopter. A two-degree-of-freedom head and two five-degree-of-freedom arms equipped with softhands provide the requisite dexterity to allow human operators to carry out various tasks. A robust tendon-driven structure is purposefully designed for the arms, considerably reducing the impact of arm inertia on the floating base in motion. In addition, tendons provide flexibility to the joints, which enhances the robustness of the arm preventing damage in interaction with the environment. To increase the payload of the aerial system and the battery life, we use the concept of Suspended Aerial Manipulation, i.e., the flying humanoid can be connected with a tether to a structure, e.g., a larger airborne carrier or a supporting crane. Importantly, to maximize portability and applicability, we adopt a modular approach exploiting commercial components for the aerial base hardware and autopilot, while developing an outer stabilizing control loop to maintain the attitude, compensating for the tether force and for the humanoid head and arm motions. The humanoid can be controlled by a remote operator, thus effectively realizing a Suspended Aerial Manipulation Avatar. The proposed system is validated through experiments in indoor scenarios reproducing post-disaster tasks.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2305.01309.pdf' target='_blank'>https://arxiv.org/pdf/2305.01309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinju Wu, Pingping Zhang, Meng Wang, Peilin Chen, Shiqi Wang, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01309">Geometric Prior Based Deep Human Point Cloud Geometry Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of digital avatars has raised an exponential increase in the demand for human point clouds with realistic and intricate details. The compression of such data becomes challenging with overwhelming data amounts comprising millions of points. Herein, we leverage the human geometric prior in geometry redundancy removal of point clouds, greatly promoting the compression performance. More specifically, the prior provides topological constraints as geometry initialization, allowing adaptive adjustments with a compact parameter set that could be represented with only a few bits. Therefore, we can envisage high-resolution human point clouds as a combination of geometric priors and structural deviations. The priors could first be derived with an aligned point cloud, and subsequently the difference of features is compressed into a compact latent code. The proposed framework can operate in a play-and-plug fashion with existing learning based point cloud compression methods. Extensive experimental results show that our approach significantly improves the compression performance without deteriorating the quality, demonstrating its promise in a variety of applications.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2304.10482.pdf' target='_blank'>https://arxiv.org/pdf/2304.10482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria-Paola Forte, Peter Kulits, Chun-Hao Huang, Vasileios Choutas, Dimitrios Tzionas, Katherine J. Kuchenbecker, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10482">Reconstructing Signing Avatars From Video Using Linguistic Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify's 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at $\href{http://sgnify.is.tue.mpg.de}{\text{sgnify.is.tue.mpg.de}}$.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2304.10282.pdf' target='_blank'>https://arxiv.org/pdf/2304.10282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Hashash, Christina Chaccour, Walid Saad, Tao Yu, Kei Sakaguchi, Merouane Debbah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10282">The Seven Worlds and Experiences of the Wireless Metaverse: Challenges and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The wireless metaverse will create diverse user experiences at the intersection of the physical, digital, and virtual worlds. These experiences will enable novel interactions between the constituents (e.g., extended reality (XR) users and avatars) of the three worlds. However, remarkably, to date, there is no holistic vision that identifies the full set of metaverse worlds, constituents, and experiences, and the implications of their associated interactions on next-generation communication and computing systems. In this paper, we present a holistic vision of a limitless, wireless metaverse that distills the metaverse into an intersection of seven worlds and experiences that include the: i) physical, digital, and virtual worlds, along with the ii) cyber, extended, live, and parallel experiences. We then articulate how these experiences bring forth interactions between diverse metaverse constituents, namely, a) humans and avatars and b) connected intelligence systems and their digital twins (DTs). Then, we explore the wireless, computing, and artificial intelligence (AI) challenges that must be addressed to establish metaverse-ready networks that support these experiences and interactions. We particularly highlight the need for end-to-end synchronization of DTs, and the role of human-level AI and reasoning abilities for cognitive avatars. Moreover, we articulate a sequel of open questions that should ignite the quest for the future metaverse. We conclude with a set of recommendations to deploy the limitless metaverse over future wireless systems.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2210.13289.pdf' target='_blank'>https://arxiv.org/pdf/2210.13289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adnan Qayyum, Muhammad Atif Butt, Hassan Ali, Muhammad Usman, Osama Halabi, Ala Al-Fuqaha, Qammer H. Abbasi, Muhammad Ali Imran, Junaid Qadir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13289">Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR) for Metaverses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalised experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies like augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization, etc.), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users' privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2112.11454.pdf' target='_blank'>https://arxiv.org/pdf/2112.11454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omid Taheri, Vasileios Choutas, Michael J. Black, Dimitrios Tzionas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.11454">GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied, but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this, the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. GOAL takes a step towards synthesizing realistic full-body object grasping.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2111.01906.pdf' target='_blank'>https://arxiv.org/pdf/2111.01906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Fu, Fares Abawi, Hugo Carneiro, Matthias Kerzel, Ziwei Chen, Erik Strahl, Xun Liu, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.01906">A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enhance human-robot social interaction, it is essential for robots to process multiple social cues in a complex real-world environment. However, incongruency of input information across modalities is inevitable and could be challenging for robots to process. To tackle this challenge, our study adopted the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention. A behavioural experiment was conducted on 37 participants for the human study. We designed a round-table meeting scenario with three animated avatars to improve ecological validity. Each avatar wore a medical mask to obscure the facial cues of the nose, mouth, and jaw. The central avatar shifted its eye gaze while the peripheral avatars generated sound. Gaze direction and sound locations were either spatially congruent or incongruent. We observed that the central avatar's dynamic gaze could trigger crossmodal social attention responses. In particular, human performances are better under the congruent audio-visual condition than the incongruent condition. Our saliency prediction model was trained to detect social cues, predict audio-visual saliency, and attend selectively for the robot study. After mounting the trained model on the iCub, the robot was exposed to laboratory conditions similar to the human experiment. While the human performances were overall superior, our trained model demonstrated that it could replicate attention responses similar to humans.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2505.20156.pdf' target='_blank'>https://arxiv.org/pdf/2505.20156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, Qinglin Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20156">HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed significant progress in audio-driven human animation. However, critical challenges remain in (i) generating highly dynamic videos while preserving character consistency, (ii) achieving precise emotion alignment between characters and audio, and (iii) enabling multi-character audio-driven animation. To address these challenges, we propose HunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model capable of simultaneously generating dynamic, emotion-controllable, and multi-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces three key innovations: (i) A character image injection module is designed to replace the conventional addition-based character conditioning scheme, eliminating the inherent condition mismatch between training and inference. This ensures the dynamic motion and strong character consistency; (ii) An Audio Emotion Module (AEM) is introduced to extract and transfer the emotional cues from an emotion reference image to the target generated video, enabling fine-grained and accurate emotion style control; (iii) A Face-Aware Audio Adapter (FAA) is proposed to isolate the audio-driven character with latent-level face mask, enabling independent audio injection via cross-attention for multi-character scenarios. These innovations empower HunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets and a newly proposed wild dataset, generating realistic avatars in dynamic, immersive scenarios.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2505.07736.pdf' target='_blank'>https://arxiv.org/pdf/2505.07736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eason Chen, Xinyi Tang, Aprille Xi, Chenyu Lin, Conrad Borchers, Shivang Gupta, Jionghao Lin, Kenneth R Koedinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07736">VTutor for High-Impact Tutoring at Scale: Managing Engagement and Real-Time Multi-Screen Monitoring with P2P Connections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid tutoring, where a human tutor supports multiple students in learning with educational technology, is an increasingly common application to deliver high-impact tutoring at scale. However, past hybrid tutoring applications are limited in guiding tutor attention to students that require support. Specifically, existing conferencing tools, commonly used in hybrid tutoring, do not allow tutors to monitor multiple students' screens while directly communicating and attending to multiple students simultaneously. To address this issue, this paper introduces VTutor, a web-based platform leveraging peer-to-peer screen sharing and virtual avatars to deliver real-time, context-aware tutoring feedback at scale. By integrating a multi-student monitoring dashboard with AI-powered avatar prompts, VTutor empowers a single educator or tutor to rapidly detect off-task or struggling students and intervene proactively, thus enhancing the benefits of one-on-one interactions in classroom contexts with several students. Drawing on insight from the learning sciences and past research on animated pedagogical agents, we demonstrate how stylized avatars can potentially sustain student engagement while accommodating varying infrastructure constraints. Finally, we address open questions on refining large-scale, AI-driven tutoring solutions for improved learner outcomes, and how VTutor could help interpret real-time learner interactions to support remote tutors at scale. The VTutor platform can be accessed at https://ls2025.vtutor.ai. The system demo video is at https://ls2025.vtutor.ai/video.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2503.08516.pdf' target='_blank'>https://arxiv.org/pdf/2503.08516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfu Zhang, Yujie Gao, Jiahui Zhan, Wentao Wang, Yiyi Zhang, Haohua Zhao, Liqing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08516">High-Quality 3D Head Reconstruction from Any Single Portrait Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce a novel high-fidelity 3D head reconstruction method from a single portrait image, regardless of perspective, expression, or accessories. Despite significant efforts in adapting 2D generative models for novel view synthesis and 3D optimization, most methods struggle to produce high-quality 3D portraits. The lack of crucial information, such as identity, expression, hair, and accessories, limits these approaches in generating realistic 3D head models. To address these challenges, we construct a new high-quality dataset containing 227 sequences of digital human portraits captured from 96 different perspectives, totalling 21,792 frames, featuring diverse expressions and accessories. To further improve performance, we integrate identity and expression information into the multi-view diffusion process to enhance facial consistency across views. Specifically, we apply identity- and expression-aware guidance and supervision to extract accurate facial representations, which guide the model and enforce objective functions to ensure high identity and expression consistency during generation. Finally, we generate an orbital video around the portrait consisting of 96 multi-view frames, which can be used for 3D portrait model reconstruction. Our method demonstrates robust performance across challenging scenarios, including side-face angles and complex accessories
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2501.01649.pdf' target='_blank'>https://arxiv.org/pdf/2501.01649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01649">AVATAR: Adversarial Autoencoders with Autoregressive Refinement for Time Series Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation can significantly enhance the performance of machine learning tasks by addressing data scarcity and improving generalization. However, generating time series data presents unique challenges. A model must not only learn a probability distribution that reflects the real data distribution but also capture the conditional distribution at each time step to preserve the inherent temporal dependencies. To address these challenges, we introduce AVATAR, a framework that combines Adversarial Autoencoders (AAE) with Autoregressive Learning to achieve both objectives. Specifically, our technique integrates the autoencoder with a supervisor and introduces a novel supervised loss to assist the decoder in learning the temporal dynamics of time series data. Additionally, we propose another innovative loss function, termed distribution loss, to guide the encoder in more efficiently aligning the aggregated posterior of the autoencoder's latent representation with a prior Gaussian distribution. Furthermore, our framework employs a joint training mechanism to simultaneously train all networks using a combined loss, thereby fulfilling the dual objectives of time series generation. We evaluate our technique across a variety of time series datasets with diverse characteristics. Our experiments demonstrate significant improvements in both the quality and practical utility of the generated data, as assessed by various qualitative and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2410.05131.pdf' target='_blank'>https://arxiv.org/pdf/2410.05131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ashrafi, Francesco Vona, Carina Ringsdorf, Christian Hertel, Luca Toni, Sarina Kailer, Alice Bartels, Tanja Kojic, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05131">Enhancing Job Interview Preparation Through Immersive Experiences Using Photorealistic, AI-powered Metahuman Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study will investigate the user experience while interacting with highly photorealistic virtual job interviewer avatars in Virtual Reality (VR), Augmented Reality (AR), and on a 2D screen. Having a precise speech recognition mechanism, our virtual character performs a mock-up software engineering job interview to adequately immerse the user in a life-like scenario. To evaluate the efficiency of our system, we measure factors such as the provoked level of anxiety, social presence, self-esteem, and intrinsic motivation. This research is a work in progress with a prospective within-subject user study including approximately 40 participants. All users will engage with three job interview conditions (VR, AR, and desktop) and provide their feedback. Additionally, users' bio-physical responses will be collected using a biosensor to measure the level of anxiety during the job interview.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2410.04852.pdf' target='_blank'>https://arxiv.org/pdf/2410.04852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ashrafi, Francesco Vona, Philipp Graf, Philipp Harnisch, Sina Hinzmann, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04852">Single Vs Dual: Influence of the Number of Displays on User Experience within Virtually Embodied Conversational Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The current research evaluates user experience and preference when interacting with a patient-reported outcome measure (PROM) healthcare application displayed on a single tablet in comparison to interaction with the same application distributed across two tablets. We conducted a within-subject user study with 43 participants who engaged with and rated the usability of our system and participated in a post-experiment interview to collect subjective data. Our findings showed significantly higher usability and higher pragmatic quality ratings for the single tablet condition. However, some users attribute a higher level of presence to the avatar and prefer it to be placed on a second tablet.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2408.04068.pdf' target='_blank'>https://arxiv.org/pdf/2408.04068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timothy Rupprecht, Sung-En Chang, Yushu Wu, Lei Lu, Enfu Nan, Chih-hsiang Li, Caiyue Lai, Zhimin Li, Zhijun Hu, Yumei He, David Kaeli, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04068">Digital Avatars: Framework Development and Their Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel prompting strategy for artificial intelligence driven digital avatars. To better quantify how our prompting strategy affects anthropomorphic features like humor, authenticity, and favorability we present Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a large language model (LLM) candidate over competitors answering the same or similar prompts. To visualize the responses of our LLM, and the effectiveness of our prompting strategy we propose an end-to-end framework for creating high-fidelity artificial intelligence (AI) driven digital avatars. This pipeline effectively captures an individual's essence for interaction and our streaming algorithm delivers a high-quality digital avatar with real-time audio-video streaming from server to mobile device. Both our visualization tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have state-of-the-art humor, authenticity, and favorability outperforming all competitors and baselines. In the case of our Donald Trump and Joe Biden avatars, their authenticity and favorability are rated higher than even their real-world equivalents.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2408.02319.pdf' target='_blank'>https://arxiv.org/pdf/2408.02319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raphael Memmesheimer, Christian Lenz, Max Schwarz, Michael Schreiber, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02319">Self-Centering 3-DoF Feet Controller for Hands-Free Locomotion Control in Telepresence and Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel seated feet controller for handling 3-DoF aimed to control locomotion for telepresence robotics and virtual reality environments. Tilting the feet on two axes yields in forward, backward and sideways motion. In addition, a separate rotary joint allows for rotation around the vertical axis. Attached springs on all joints self-center the controller. The HTC Vive tracker is used to translate the trackers' orientation into locomotion commands. The proposed self-centering feet controller was used successfully for the ANA Avatar XPRIZE competition, where a naive operator traversed the robot through a longer distance, surpassing obstacles while solving various interaction and manipulation tasks in between. We publicly provide the models of the mostly 3D-printed feet controller for reproduction.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2407.00925.pdf' target='_blank'>https://arxiv.org/pdf/2407.00925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuling Zhang, Ziru Zhang, Yuyang Wang, Lik-hang Lee, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00925">SIDQL: An Efficient Keyframe Extraction and Motion Reconstruction Framework in Motion Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse, which integrates the virtual and physical worlds, has emerged as an innovative paradigm for changing people's lifestyles. Motion capture has become a reliable approach to achieve seamless synchronization of the movements between avatars and human beings, which plays an important role in diverse Metaverse applications. However, due to the continuous growth of data, current communication systems face a significant challenge of meeting the demand of ultra-low latency during application. In addition, current methods also have shortcomings when selecting keyframes, e.g., relying on recognizing motion types and artificially selected keyframes. Therefore, the utilization of keyframe extraction and motion reconstruction techniques could be considered a feasible and promising solution. In this work, a new motion reconstruction algorithm is designed in a spherical coordinate system involving location and velocity information. Then, we formalize the keyframe extraction problem into an optimization problem to reduce the reconstruction error. Using Deep Q-Learning (DQL), the Spherical Interpolation based Deep Q-Learning (SIDQL) framework is proposed to generate proper keyframes for reconstructing the motion sequences. We use the CMU database to train and evaluate the framework. Our scheme can significantly reduce the data volume and transmission latency compared to various baselines while maintaining a reconstruction error of less than 0.09 when extracting five keyframes.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2406.04253.pdf' target='_blank'>https://arxiv.org/pdf/2406.04253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihe Wang, Yukang Cao, Kai Han, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04253">A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2404.15383.pdf' target='_blank'>https://arxiv.org/pdf/2404.15383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Nikos Athanasiou, Omid Taheri, Xi Wang, Otmar Hilliges, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15383">WANDR: Intention-guided Human Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching. To address this, we introduce WANDR, a data-driven model that takes an avatar's initial pose and a goal's 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement. Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations. Our models and code are available for research purposes at wandr.is.tue.mpg.de.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2404.09174.pdf' target='_blank'>https://arxiv.org/pdf/2404.09174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Vona, Sina Hinzmann, Michael Stern, Tanja KojiÄ, Navid Ashrafi, David Grieshammer, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09174">Investigating the impact of virtual element misalignment in collaborative Augmented Reality experiences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The collaboration in co-located shared environments has sparked an increased interest in immersive technologies, including Augmented Reality (AR). Since research in this field has primarily focused on individual user experiences in AR, the collaborative aspects within shared AR spaces remain less explored, and fewer studies can provide guidelines for designing this type of experience. This article investigates how the user experience in a collaborative shared AR space is affected by divergent perceptions of virtual objects and the effects of positional synchrony and avatars. For this purpose, we developed an AR app and used two distinct experimental conditions to study the influencing factors. Forty-eight participants, organized into 24 pairs, participated in the experiment and jointly interacted with shared virtual objects. Results indicate that divergent perceptions of virtual objects did not directly influence communication and collaboration dynamics. Conversely, positional synchrony emerged as a critical factor, significantly enhancing the quality of the collaborative experience. On the contrary, while not negligible, avatars played a relatively less pronounced role in influencing these dynamics. The findings can potentially offer valuable practical insights, guiding the development of future collaborative AR/VR environments.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2404.01700.pdf' target='_blank'>https://arxiv.org/pdf/2404.01700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang YU, Jiayuan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01700">MotionChain: Conversational Motion Controllers via Multimodal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2403.09879.pdf' target='_blank'>https://arxiv.org/pdf/2403.09879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanja KojiÄ, Maurizio Vergari, Marco Podratz, Sebastian MÃ¶ller, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09879">The Influence of Extended Reality and Virtual Characters' Embodiment Levels on User Experience in Well-Being Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Millions of people have seen their daily habits transform, reducing physical activity and leading to mental health issues. This study explores how virtual characters impact motivation for well-being. Three prototypes with cartoon, robotic, and human-like avatars were tested by 22 participants. Results show that animated virtual avatars, especially with extended reality, boost motivation, enhance comprehension of activities, and heighten presence. Multiple output modalities, like audio and text, with character animations, improve the user experience. Notably, the cartoon-like character evoked positive responses. This research highlights virtual characters' potential to engage individuals in daily well-being activities.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2403.09544.pdf' target='_blank'>https://arxiv.org/pdf/2403.09544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ashrafi, Vanessa Neuhaus, Francesco Vona, Nicolina Laura Peperkorn, Youssef Shiban, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09544">Effect of external characteristics of a virtual human being during the use of a computer-assisted therapy tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identification within media, whether with real or fictional characters, significantly impacts users, shaping their behavior and enriching their social and emotional experiences. Immersive media, like video games, utilize virtual entities such as agents, avatars, or NPCs to connect users with virtual worlds, fostering a heightened sense of immersion and identification. However, challenges arise in visually representing these entities, with design decisions crucial for enhancing user interaction. Recent research highlights the potential of user-defined design, or customization, which goes beyond mere visual resemblance to the user. Understanding how identification with virtual avatars influences user experiences, especially in psychological interventions, is pivotal. In a study exploring this, 22 participants created virtual agents either similar or dissimilar to themselves, which then addressed their dysfunctional thoughts. Results indicate that similarity between users and virtual agents not only boosts identification but also positively impacts emotions and motivation, enhancing interest and enjoyment. This study sheds light on the significance of customization and identification, particularly in computer-assisted therapy tools, underscoring the importance of visual design for optimizing user experiences.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2403.06862.pdf' target='_blank'>https://arxiv.org/pdf/2403.06862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Luo, Jinkun Cao, Rawal Khirodkar, Alexander Winkler, Jing Huang, Kris Kitani, Weipeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06862">Real-Time Simulated Avatar from Head-Mounted Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SimXR, a method for controlling a simulated avatar from information (headset pose and cameras) obtained from AR / VR headsets. Due to the challenging viewpoint of head-mounted cameras, the human body is often clipped out of view, making traditional image-based egocentric pose estimation challenging. On the other hand, headset poses provide valuable information about overall body motion, but lack fine-grained details about the hands and feet. To synergize headset poses with cameras, we control a humanoid to track headset movement while analyzing input images to decide body movement. When body parts are seen, the movements of hands and feet will be guided by the images; when unseen, the laws of physics guide the controller to generate plausible motion. We design an end-to-end method that does not rely on any intermediate representations and learns to directly map from images and headset poses to humanoid control signals. To train our method, we also propose a large-scale synthetic dataset created using camera configurations compatible with a commercially available VR headset (Quest 2) and show promising results on real-world captures. To demonstrate the applicability of our framework, we also test it on an AR headset with a forward-facing camera.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2402.13724.pdf' target='_blank'>https://arxiv.org/pdf/2402.13724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Bai, Peng Chen, Xiaolan Peng, Lu Liu, Hui Chen, Mike Zheng Shou, Feng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13724">Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2312.12090.pdf' target='_blank'>https://arxiv.org/pdf/2312.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Yan, Zhiming Hu, Syn Schmitt, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12090">GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human motion prediction is important for many virtual and augmented reality (VR/AR) applications such as collision avoidance and realistic avatar generation. Existing methods have synthesised body motion only from observed past motion, despite the fact that human eye gaze is known to correlate strongly with body movements and is readily available in recent VR/AR headsets. We present GazeMoDiff - a novel gaze-guided denoising diffusion model to generate stochastic human motions. Our method first uses a gaze encoder and a motion encoder to extract the gaze and motion features respectively, then employs a graph attention network to fuse these features, and finally injects the gaze-motion features into a noise prediction network via a cross-attention mechanism to progressively generate multiple reasonable human motions in the future. Extensive experiments on the MoGaze and GIMO datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin in terms of multi-modal final displacement error (17.3% on MoGaze and 13.3% on GIMO). We further conducted a human study (N=21) and validated that the motions generated by our method were perceived as both more precise and more realistic than those of prior methods. Taken together, these results reveal the significant information content available in eye gaze for stochastic human motion prediction as well as the effectiveness of our method in exploiting this information.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2310.17519.pdf' target='_blank'>https://arxiv.org/pdf/2310.17519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrisha Bharadwaj, Yufeng Zheng, Otmar Hilliges, Michael J. Black, Victoria Fernandez-Abrevaya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17519">FLARE: Fast Learning of Animatable and Relightable Mesh Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2308.12238.pdf' target='_blank'>https://arxiv.org/pdf/2308.12238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Lenz, Max Schwarz, Andre Rochow, Bastian PÃ¤tzold, Raphael Memmesheimer, Michael Schreiber, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12238">NimbRo wins ANA Avatar XPRIZE Immersive Telepresence Competition: Human-Centric Evaluation and Lessons Learned</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic avatar systems can enable immersive telepresence with locomotion, manipulation, and communication capabilities. We present such an avatar system, based on the key components of immersive 3D visualization and transparent force-feedback telemanipulation. Our avatar robot features an anthropomorphic upper body with dexterous hands. The remote human operator drives the arms and fingers through an exoskeleton-based operator station, which provides force feedback both at the wrist and for each finger. The robot torso is mounted on a holonomic base, providing omnidirectional locomotion on flat floors, controlled using a 3D rudder device. Finally, the robot features a 6D movable head with stereo cameras, which stream images to a VR display worn by the operator. Movement latency is hidden using spherical rendering. The head also carries a telepresence screen displaying an animated image of the operator's face, enabling direct interaction with remote persons. Our system won the \$10M ANA Avatar XPRIZE competition, which challenged teams to develop intuitive and immersive avatar systems that could be operated by briefly trained judges. We analyze our successful participation in the semifinals and finals and provide insight into our operator training and lessons learned. In addition, we evaluate our system in a user study that demonstrates its intuitive and easy usability.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2306.08768.pdf' target='_blank'>https://arxiv.org/pdf/2306.08768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano, Umar Iqbal, Jan Kautz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08768">Generalizable One-shot Neural Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method that reconstructs and animates a 3D head avatar from a single-view portrait image. Existing methods either involve time-consuming optimization for a specific person with multiple images, or they struggle to synthesize intricate appearance details beyond the facial region. To address these limitations, we propose a framework that not only generalizes to unseen identities based on a single-view image without requiring person-specific optimization, but also captures characteristic details within and beyond the face area (e.g. hairstyle, accessories, etc.). At the core of our method are three branches that produce three tri-planes representing the coarse 3D geometry, detailed appearance of a source image, as well as the expression of a target image. By applying volumetric rendering to the combination of the three tri-planes followed by a super-resolution module, our method yields a high fidelity image of the desired identity, expression and pose. Once trained, our model enables efficient 3D head avatar reconstruction and animation via a single forward pass through a network. Experiments show that the proposed approach generalizes well to unseen validation datasets, surpassing SOTA baseline methods by a large margin on head avatar reconstruction and animation.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2305.14784.pdf' target='_blank'>https://arxiv.org/pdf/2305.14784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, Ashwin Kalyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14784">Anthropomorphization of AI: Opportunities and Risks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anthropomorphization is the tendency to attribute human-like traits to non-human entities. It is prevalent in many social contexts -- children anthropomorphize toys, adults do so with brands, and it is a literary device. It is also a versatile tool in science, with behavioral psychology and evolutionary biology meticulously documenting its consequences. With widespread adoption of AI systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly. We take a dyadic approach to understanding this phenomenon with large language models (LLMs) by studying (1) the objective legal implications, as analyzed through the lens of the recent blueprint of AI bill of rights and the (2) subtle psychological aspects customization and anthropomorphization. We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction, with potential for manipulation and negative influence. With LLMs being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution. We propose a conservative strategy for the cautious use of anthropomorphization to improve trustworthiness of AI systems.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2305.06456.pdf' target='_blank'>https://arxiv.org/pdf/2305.06456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Luo, Jinkun Cao, Alexander Winkler, Kris Kitani, Weipeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06456">Perpetual Humanoid Control for Real-time Simulated Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand motion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At its core, we propose the progressive multiplicative control policy (PMCP), which dynamically allocates new network capacity to learn harder and harder motion sequences. PMCP allows efficient scaling for learning from large-scale motion databases and adding new tasks, such as fail-state recovery, without catastrophic forgetting. We demonstrate the effectiveness of our controller by using it to imitate noisy poses from video-based pose estimators and language-based motion generators in a live and real-time multi-person avatar use case.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2305.02312.pdf' target='_blank'>https://arxiv.org/pdf/2305.02312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Dong, Xu Chen, Jinlong Yang, Michael J. Black, Otmar Hilliges, Andreas Geiger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02312">AG3D: Learning to Generate 3D Avatars from 2D Image Collections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses. In this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps. We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2303.07186.pdf' target='_blank'>https://arxiv.org/pdf/2303.07186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bastian PÃ¤tzold, Andre Rochow, Michael Schreiber, Raphael Memmesheimer, Christian Lenz, Max Schwarz, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07186">Audio-based Roughness Sensing and Tactile Feedback for Haptic Perception in Telepresence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Haptic perception is highly important for immersive teleoperation of robots, especially for accomplishing manipulation tasks. We propose a low-cost haptic sensing and rendering system, which is capable of detecting and displaying surface roughness. As the robot fingertip moves across a surface of interest, two microphones capture sound coupled directly through the fingertip and through the air, respectively. A learning-based detector system analyzes the data in real time and gives roughness estimates with both high temporal resolution and low latency. Finally, an audio-based vibrational actuator displays the result to the human operator. We demonstrate the effectiveness of our system through lab experiments and our winning entry in the ANA Avatar XPRIZE competition finals, where briefly trained judges solved a roughness-based selection task even without additional vision feedback. We publish our dataset used for training and evaluation together with our trained models to enable reproducibility of results.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2303.03297.pdf' target='_blank'>https://arxiv.org/pdf/2303.03297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Schwarz, Christian Lenz, Raphael Memmesheimer, Bastian PÃ¤tzold, Andre Rochow, Michael Schreiber, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03297">Robust Immersive Telepresence and Mobile Telemanipulation: NimbRo wins ANA Avatar XPRIZE Finals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic avatar systems promise to bridge distances and reduce the need for travel. We present the updated NimbRo avatar system, winner of the $5M grand prize at the international ANA Avatar XPRIZE competition, which required participants to build intuitive and immersive robotic telepresence systems that could be operated by briefly trained operators. We describe key improvements for the finals, compared to the system used in the semifinals: To operate without a power- and communications tether, we integrated a battery and a robust redundant wireless communication system. Video and audio data are compressed using low-latency HEVC and Opus codecs. We propose a new locomotion control device with tunable resistance force. To increase flexibility, the robot's upper-body height can be adjusted by the operator. We describe essential monitoring and robustness tools which enabled the success at the competition. Finally, we analyze our performance at the competition finals and discuss lessons learned.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2509.16922.pdf' target='_blank'>https://arxiv.org/pdf/2509.16922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16922">PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2509.04145.pdf' target='_blank'>https://arxiv.org/pdf/2509.04145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongliang Cao, Guoxing Sun, Marc Habermann, Florian Bernard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04145">Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2508.19209.pdf' target='_blank'>https://arxiv.org/pdf/2508.19209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, Mingyuan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19209">OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, \textbf{we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive.} Our model, \textbf{OmniHuman-1.5}, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: \href{https://omnihuman-lab.github.io/v1_5/}
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2508.18389.pdf' target='_blank'>https://arxiv.org/pdf/2508.18389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Liang, Zhixuan Ge, Ashish Tiwari, Soumendu Majee, G. M. Dilshan Godaliyadda, Ashok Veeraraghavan, Guha Balakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18389">FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present FastAvatar, a pose-invariant, feed-forward framework that can generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel encoder-decoder neural network design to achieve both fast fitting and identity preservation regardless of input pose. First, FastAvatar constructs a 3DGS face ``template'' model from a training dataset of faces with multi-view captures. Second, FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model. By only inferring residuals in a feed-forward fashion, model inference is fast and robust. FastAvatar significantly outperforms existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction quality, and runs 1000x faster than per-face optimization methods (e.g., FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework. FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2508.10576.pdf' target='_blank'>https://arxiv.org/pdf/2508.10576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10576">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2507.05620.pdf' target='_blank'>https://arxiv.org/pdf/2507.05620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaojie Bai, Seunghyeon Seo, Yida Wang, Chenghui Li, Owen Wang, Te-Li Wang, Tianyang Ma, Jason Saragih, Shih-En Wei, Nojun Kwak, Hyung Jun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05620">Generative Head-Mounted Camera Captures for Photorealistic Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling photorealistic avatar animations in virtual and augmented reality (VR/AR) has been challenging because of the difficulty of obtaining ground truth state of faces. It is physically impossible to obtain synchronized images from head-mounted cameras (HMC) sensing input, which has partial observations in infrared (IR), and an array of outside-in dome cameras, which have full observations that match avatars' appearance. Prior works relying on analysis-by-synthesis methods could generate accurate ground truth, but suffer from imperfect disentanglement between expression and style in their personalized training. The reliance of extensive paired captures (HMC and dome) for the same subject makes it operationally expensive to collect large-scale datasets, which cannot be reused for different HMC viewpoints and lighting. In this work, we propose a novel generative approach, Generative HMC (GenHMC), that leverages large unpaired HMC captures, which are much easier to collect, to directly generate high-quality synthetic HMC images given any conditioning avatar state from dome captures. We show that our method is able to properly disentangle the input conditioning signal that specifies facial expression and viewpoint, from facial appearance, leading to more accurate ground truth. Furthermore, our method can generalize to unseen identities, removing the reliance on the paired captures. We demonstrate these breakthroughs by both evaluating synthetic HMC images and universal face encoders trained from these new HMC-avatar correspondences, which achieve better data efficiency and state-of-the-art accuracy.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2505.24877.pdf' target='_blank'>https://arxiv.org/pdf/2505.24877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24877">AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2504.19165.pdf' target='_blank'>https://arxiv.org/pdf/2504.19165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Li, Ziqian Bai, Feitong Tan, Zhaopeng Cui, Sean Fanello, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19165">IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2504.18810.pdf' target='_blank'>https://arxiv.org/pdf/2504.18810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Xie, Fei Ma, Yi Bin, Ying He, Fei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18810">Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Talking face video generation with arbitrary speech audio is a significant challenge within the realm of digital human technology. The previous studies have emphasized the significance of audio-lip synchronization and visual quality. Currently, limited attention has been given to the learning of visual uncertainty, which creates several issues in existing systems, including inconsistent visual quality and unreliable performance across different input conditions. To address the problem, we propose a Joint Uncertainty Learning Network (JULNet) for high-quality talking face video generation, which incorporates a representation of uncertainty that is directly related to visual error. Specifically, we first design an uncertainty module to individually predict the error map and uncertainty map after obtaining the generated image. The error map represents the difference between the generated image and the ground truth image, while the uncertainty map is used to predict the probability of incorrect estimates. Furthermore, to match the uncertainty distribution with the error distribution through a KL divergence term, we introduce a histogram technique to approximate the distributions. By jointly optimizing error and uncertainty, the performance and robustness of our model can be enhanced. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking face video generation compared to previous methods.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2504.09671.pdf' target='_blank'>https://arxiv.org/pdf/2504.09671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Manu, Astitva Srivastava, Amit Raj, Varun Jampani, Avinash Sharma, P. J. Narayanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09671">LightHeadEd: Relightable & Editable Head Avatars from a Smartphone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic, animatable, and relightable 3D head avatars traditionally requires expensive Lightstage with multiple calibrated cameras, making it inaccessible for widespread adoption. To bridge this gap, we present a novel, cost-effective approach for creating high-quality relightable head avatars using only a smartphone equipped with polaroid filters. Our approach involves simultaneously capturing cross-polarized and parallel-polarized video streams in a dark room with a single point-light source, separating the skin's diffuse and specular components during dynamic facial performances. We introduce a hybrid representation that embeds 2D Gaussians in the UV space of a parametric head model, facilitating efficient real-time rendering while preserving high-fidelity geometric details. Our learning-based neural analysis-by-synthesis pipeline decouples pose and expression-dependent geometrical offsets from appearance, decomposing the surface into albedo, normal, and specular UV texture maps, along with the environment maps. We collect a unique dataset of various subjects performing diverse facial expressions and head movements.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2504.05046.pdf' target='_blank'>https://arxiv.org/pdf/2504.05046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghao Ren, Yi Lu, Jiayi Huang, Jiayi Zhao, He Zhang, Tao Yu, Qiu Shen, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05046">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human Motion Capture (MoCap) methods mostly focus on the visual similarity while neglecting the physical plausibility. As a result, downstream tasks such as driving virtual human in 3D scene or humanoid robots in real world suffer from issues such as timing drift and jitter, spatial problems like sliding and penetration, and poor global trajectory accuracy. In this paper, we revisit human MoCap from the perspective of interaction between human body and physical world by exploring the role of pressure. Firstly, we construct a large-scale human Motion capture dataset with Pressure, RGB and Optical sensors (named MotionPRO), which comprises 70 volunteers performing 400 types of motion, encompassing a total of 12.4M pose frames. Secondly, we examine both the necessity and effectiveness of the pressure signal through two challenging tasks: (1) pose and trajectory estimation based solely on pressure: We propose a network that incorporates a small kernel decoder and a long-short-term attention module, and proof that pressure could provide accurate global trajectory and plausible lower body pose. (2) pose and trajectory estimation by fusing pressure and RGB: We impose constraints on orthographic similarity along the camera axis and whole-body contact along the vertical axis to enhance the cross-attention strategy to fuse pressure and RGB feature maps. Experiments demonstrate that fusing pressure with RGB features not only significantly improves performance in terms of objective metrics, but also plausibly drives virtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that incorporating physical perception enables humanoid robots to perform more precise and stable actions, which is highly beneficial for the development of embodied artificial intelligence. Project page is available at: https://nju-cite-mocaphumanoid.github.io/MotionPRO/
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2503.12242.pdf' target='_blank'>https://arxiv.org/pdf/2503.12242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Jiang, Zhehao Shen, Chengcheng Guo, Yu Hong, Zhuo Su, Yingliang Zhang, Marc Habermann, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12242">RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric volumetric videos offer immersive free-viewpoint experiences, yet existing methods focus either on replaying general dynamic scenes or animating human avatars, limiting their ability to re-perform general dynamic scenes. In this paper, we present RePerformer, a novel Gaussian-based representation that unifies playback and re-performance for high-fidelity human-centric volumetric videos. Specifically, we hierarchically disentangle the dynamic scenes into motion Gaussians and appearance Gaussians which are associated in the canonical space. We further employ a Morton-based parameterization to efficiently encode the appearance Gaussians into 2D position and attribute maps. For enhanced generalization, we adopt 2D CNNs to map position maps to attribute maps, which can be assembled into appearance Gaussians for high-fidelity rendering of the dynamic scenes. For re-performance, we develop a semantic-aware alignment module and apply deformation transfer on motion Gaussians, enabling photo-real rendering under novel motions. Extensive experiments validate the robustness and effectiveness of RePerformer, setting a new benchmark for playback-then-reperformance paradigm in human-centric volumetric videos.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2503.12052.pdf' target='_blank'>https://arxiv.org/pdf/2503.12052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyao Sun, Yu-Hui Wen, Matthieu Lin, Ho-Jui Fang, Sheng Ye, Tian Lv, Yong-Jin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12052">Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating detailed 3D human avatars with garments typically requires specialized expertise and labor-intensive processes. Although recent advances in generative AI have enabled text-to-3D human/clothing generation, current methods fall short in offering accessible, integrated pipelines for producing ready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated text-to-avatar system that generates high-fidelity, customizable 3D humans with simulation-ready garments. Our system includes a three-stage pipeline. We first employ a large language model to interpret textual descriptions into parameterized body shapes and semantically matched garment templates. Next, we develop topology-preserving deformation with novel geometric losses to adapt garments precisely to body geometries. Furthermore, an enhanced texture diffusion module with a symmetric local attention mechanism ensures both view consistency and photorealistic details. Quantitative and qualitative evaluations demonstrate that Tailor outperforms existing SoTA methods in terms of fidelity, usability, and diversity. Code will be available for academic use.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2412.16915.pdf' target='_blank'>https://arxiv.org/pdf/2412.16915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16915">FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage http://fadavatar.github.io.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2412.15609.pdf' target='_blank'>https://arxiv.org/pdf/2412.15609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jotaro Sakamiya, I-Chao Shen, Jinsong Zhang, Mustafa Doga Dogan, Takeo Igarashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15609">AvatarPerfect: User-Assisted 3D Gaussian Splatting Avatar Refinement with Automatic Pose Suggestion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality 3D avatars using 3D Gaussian Splatting (3DGS) from a monocular video benefits virtual reality and telecommunication applications. However, existing automatic methods exhibit artifacts under novel poses due to limited information in the input video. We propose AvatarPerfect, a novel system that allows users to iteratively refine 3DGS avatars by manually editing the rendered avatar images. In each iteration, our system suggests a new body and camera pose to help users identify and correct artifacts. The edited images are then used to update the current avatar, and our system suggests the next body and camera pose for further refinement. To investigate the effectiveness of AvatarPerfect, we conducted a user study comparing our method to an existing 3DGS editor SuperSplat, which allows direct manipulation of Gaussians without automatic pose suggestions. The results indicate that our system enables users to obtain higher quality refined 3DGS avatars than the existing 3DGS editor.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2412.09545.pdf' target='_blank'>https://arxiv.org/pdf/2412.09545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09545">SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2411.10489.pdf' target='_blank'>https://arxiv.org/pdf/2411.10489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Agarwal, Raghavendra Ramachandra, Sushma Venkatesh, S. R. Mahadeva Prasanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10489">Biometrics in Extended Reality: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of Extended Reality (XR), particularly Virtual Reality (VR), extensive research has been devoted to harnessing this transformative technology in various real-world applications. However, a critical challenge that must be addressed before unleashing the full potential of XR in practical scenarios is to ensure robust security and safeguard user privacy. This paper presents a systematic survey of the utility of biometric characteristics applied in the XR environment. To this end, we present a comprehensive overview of the different types of biometric modalities used for authentication and representation of users in a virtual environment. We discuss different biometric vulnerability gateways in general XR systems for the first time in the literature along with taxonomy. A comprehensive discussion on generating and authenticating biometric-based photorealistic avatars in XR environments is presented with a stringent taxonomy. We also discuss the availability of different datasets that are widely employed in evaluating biometric authentication in XR environments together with performance evaluation metrics. Finally, we discuss the open challenges and potential future work that need to be addressed in the field of biometrics in XR.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2410.11682.pdf' target='_blank'>https://arxiv.org/pdf/2410.11682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeseong Lee, Taewoong Kang, Marcel C. BÃ¼hler, Min-Jung Kim, Sungwon Hwang, Junha Hyung, Hyojin Jang, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11682">SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2409.02634.pdf' target='_blank'>https://arxiv.org/pdf/2409.02634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02634">Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2409.01876.pdf' target='_blank'>https://arxiv.org/pdf/2409.01876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, Yanbo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01876">CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based video generation technology has advanced significantly, catalyzing a proliferation of research in human animation. However, the majority of these studies are confined to same-modality driving settings, with cross-modality human body animation remaining relatively underexplored. In this paper, we introduce, an end-to-end audio-driven human animation framework that ensures hand integrity, identity consistency, and natural motion. The key design of CyberHost is the Region Codebook Attention mechanism, which improves the generation quality of facial and hand animations by integrating fine-grained local features with learned motion pattern priors. Furthermore, we have developed a suite of human-prior-guided training strategies, including body movement map, hand clarity score, pose-aligned reference feature, and local enhancement supervision, to improve synthesis results. To our knowledge, CyberHost is the first end-to-end audio-driven human diffusion model capable of facilitating zero-shot video generation within the scope of human body. Extensive experiments demonstrate that CyberHost surpasses previous works in both quantitative and qualitative aspects.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2409.01159.pdf' target='_blank'>https://arxiv.org/pdf/2409.01159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Elobaid, Stefano Dafarra, Ehsan Ranjbari, Giulio Romualdi, Tomohiro Chaki, Tomohiro Kawakami, Takahide Yoshiike, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01159">Remote telepresence over large distances via robot avatars: case studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper discusses the necessary considerations and adjustments that allow a recently proposed avatar system architecture to be used with different robotic avatar morphologies (both wheeled and legged robots with various types of hands and kinematic structures) for the purpose of enabling remote (intercontinental) telepresence under communication bandwidth restrictions. The case studies reported involve robots using both position and torque control modes, independently of their software middleware.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2407.10865.pdf' target='_blank'>https://arxiv.org/pdf/2407.10865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexey Kotcov, Maria Dronova, Vladislav Cheremnykh, Sausar Karaf, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10865">AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future Communication Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving landscape of digital content creation, the demand for fast, convenient, and autonomous methods of crafting detailed 3D reconstructions of humans has grown significantly. Addressing this pressing need, our AirNeRF system presents an innovative pathway to the creation of a realistic 3D human avatar. Our approach leverages Neural Radiance Fields (NeRF) with an automated drone-based video capturing method. The acquired data provides a swift and precise way to create high-quality human body reconstructions following several stages of our system. The rigged mesh derived from our system proves to be an excellent foundation for free-view synthesis of dynamic humans, particularly well-suited for the immersive experiences within gaming and virtual reality.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2407.05712.pdf' target='_blank'>https://arxiv.org/pdf/2407.05712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang, Yongming Zhu, Jiaqi Yang, Tianyun Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05712">MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, we introduce a mixed representation of explicit and implicit keypoints for precise motion modeling and precomputed visual features for enhanced foreground and background synthesis. With these two key designs and using simple U-Nets as backbones, our method achieves state-of-the-art performance with less than one-tenth the computational demand. It has been validated to reach speeds of over 100 FPS on mobile devices and support both video and audio-driven inputs.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2406.03625.pdf' target='_blank'>https://arxiv.org/pdf/2406.03625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Sergey Prokudin, Marko Mihajlovic, Qianli Ma, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03625">Degrees of Freedom Matter: Inferring Dynamics from Point Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the dynamics of generic 3D scenes is fundamentally challenging in computer vision, essential in enhancing applications related to scene reconstruction, motion tracking, and avatar creation. In this work, we address the task as the problem of inferring dense, long-range motion of 3D points. By observing a set of point trajectories, we aim to learn an implicit motion field parameterized by a neural network to predict the movement of novel points within the same domain, without relying on any data-driven or scene-specific priors. To achieve this, our approach builds upon the recently introduced dynamic point field model that learns smooth deformation fields between the canonical frame and individual observation frames. However, temporal consistency between consecutive frames is neglected, and the number of required parameters increases linearly with the sequence length due to per-frame modeling. To address these shortcomings, we exploit the intrinsic regularization provided by SIREN, and modify the input layer to produce a spatiotemporally smooth motion field. Additionally, we analyze the motion field Jacobian matrix, and discover that the motion degrees of freedom (DOFs) in an infinitesimal area around a point and the network hidden variables have different behaviors to affect the model's representational power. This enables us to improve the model representation capability while retaining the model compactness. Furthermore, to reduce the risk of overfitting, we introduce a regularization term based on the assumption of piece-wise motion smoothness. Our experiments assess the model's performance in predicting unseen point trajectories and its application in temporal mesh alignment with guidance. The results demonstrate its superiority and effectiveness. The code and data for the project are publicly available: \url{https://yz-cnsdqz.github.io/eigenmotion/DOMA/}
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2404.01543.pdf' target='_blank'>https://arxiv.org/pdf/2404.01543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01543">Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2402.11909.pdf' target='_blank'>https://arxiv.org/pdf/2402.11909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11909">One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2401.04728.pdf' target='_blank'>https://arxiv.org/pdf/2401.04728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04728">Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multi-view-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks. The code for our project is publicly available.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2401.03108.pdf' target='_blank'>https://arxiv.org/pdf/2401.03108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanthika Naik, Kunwar Singh, Astitva Srivastava, Dhawal Sirikonda, Amit Raj, Varun Jampani, Avinash Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03108">Dress-Me-Up: A Dataset & Method for Self-Supervised 3D Garment Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel self-supervised framework for retargeting non-parameterized 3D garments onto 3D human avatars of arbitrary shapes and poses, enabling 3D virtual try-on (VTON). Existing self-supervised 3D retargeting methods only support parametric and canonical garments, which can only be draped over parametric body, e.g. SMPL. To facilitate the non-parametric garments and body, we propose a novel method that introduces Isomap Embedding based correspondences matching between the garment and the human body to get a coarse alignment between the two meshes. We perform neural refinement of the coarse alignment in a self-supervised setting. Further, we leverage a Laplacian detail integration method for preserving the inherent details of the input garment. For evaluating our 3D non-parametric garment retargeting framework, we propose a dataset of 255 real-world garments with realistic noise and topological deformations. The dataset contains $44$ unique garments worn by 15 different subjects in 5 distinctive poses, captured using a multi-view RGBD capture setup. We show superior retargeting quality on non-parametric garments and human avatars over existing state-of-the-art methods, acting as the first-ever baseline on the proposed dataset for non-parametric 3D garment retargeting.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2312.11461.pdf' target='_blank'>https://arxiv.org/pdf/2312.11461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11461">GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2312.04558.pdf' target='_blank'>https://arxiv.org/pdf/2312.04558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04558">MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2312.03763.pdf' target='_blank'>https://arxiv.org/pdf/2312.03763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushi Lan, Feitong Tan, Di Qiu, Qiangeng Xu, Kyle Genova, Zeng Huang, Sean Fanello, Rohit Pandey, Thomas Funkhouser, Chen Change Loy, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03763">Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for generating photorealistic 3D human head and subsequently manipulating and reposing them with remarkable flexibility. The proposed approach leverages an implicit function representation of 3D human heads, employing 3D Gaussians anchored on a parametric face model. To enhance representational capabilities and encode spatial information, we embed a lightweight tri-plane payload within each Gaussian rather than directly storing color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space via a 3DMM, enabling effective utilization of the diffusion model for 3D head avatar generation. Our method facilitates the creation of diverse and realistic 3D human heads with fine-grained editing over facial features and expressions. Extensive experiments demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2311.16482.pdf' target='_blank'>https://arxiv.org/pdf/2311.16482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Xiang Huang, Minghan Qin, Qinwei Lin, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16482">Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural radiance fields are capable of reconstructing high-quality drivable human avatars but are expensive to train and render and not suitable for multi-human scenes with complex shadows. To reduce consumption, we propose Animatable 3D Gaussian, which learns human avatars from input images and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of skinned 3D Gaussians and a corresponding skeleton in canonical space and deforming 3D Gaussians to posed space according to the input poses. We introduce a multi-head hash encoder for pose-dependent shape and appearance and a time-dependent ambient occlusion module to achieve high-quality reconstructions in scenes containing complex motions and dynamic shadows. On both novel view synthesis and novel pose synthesis tasks, our method achieves higher reconstruction quality than InstantAvatar with less training time (1/60), less GPU memory (1/4), and faster rendering speed (7x). Our method can be easily extended to multi-human scenes and achieve comparable novel view synthesis results on a scene with ten people in only 25 seconds of training.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2311.16096.pdf' target='_blank'>https://arxiv.org/pdf/2311.16096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Yipengjing Sun, Zerong Zheng, Lizhen Wang, Shengping Zhang, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16096">Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front & back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. To tackle the realistic relighting of animatable avatars, we introduce physically-based rendering into the avatar representation for decomposing avatar materials and environment illumination. Overall, our method can create lifelike avatars with dynamic, realistic, generalized and relightable appearances. Experiments show that our method outperforms other state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2310.13724.pdf' target='_blank'>https://arxiv.org/pdf/2310.13724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, VladimÃ­r VondruÅ¡, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, Roozbeh Mottaghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13724">Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2310.06275.pdf' target='_blank'>https://arxiv.org/pdf/2310.06275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghan Qin, Yifan Liu, Yuelang Xu, Xiaochen Zhao, Yebin Liu, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06275">High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One crucial aspect of 3D head avatar reconstruction lies in the details of facial expressions. Although recent NeRF-based photo-realistic 3D head avatar methods achieve high-quality avatar rendering, they still encounter challenges retaining intricate facial expression details because they overlook the potential of specific expression variations at different spatial positions when conditioning the radiance field. Motivated by this observation, we introduce a novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained by a simple MLP-based generation network, encompassing both spatial positional features and global expression information. Benefiting from rich and diverse information of the SVE at different positions, the proposed SVE-conditioned neural radiance field can deal with intricate facial expressions and achieve realistic rendering and geometry details of high-fidelity 3D head avatars. Additionally, to further elevate the geometric and rendering quality, we introduce a new coarse-to-fine training strategy, including a geometry initialization strategy at the coarse stage and an adaptive importance sampling strategy at the fine stage. Extensive experiments indicate that our method outperforms other state-of-the-art (SOTA) methods in rendering and geometry quality on mobile phone-collected and public datasets.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2309.03550.pdf' target='_blank'>https://arxiv.org/pdf/2309.03550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungwon Hwang, Junha Hyung, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03550">Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models such as ControlNet have enabled geometrically controllable, high-fidelity text-to-image generation. However, none of them addresses the question of adding such controllability to text-to-3D generation. In response, we propose Text2Control3D, a controllable text-to-3D avatar generation method whose facial expression is controllable given a monocular video casually captured with hand-held camera. Our main strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF) optimized with a set of controlled viewpoint-aware images that we generate from ControlNet, whose condition input is the depth map extracted from the input video. When generating the viewpoint-aware images, we utilize cross-reference attention to inject well-controlled, referential facial expression and appearance via cross attention. We also conduct low-pass filtering of Gaussian latent of the diffusion model in order to ameliorate the viewpoint-agnostic texture problem we observed from our empirical analysis, where the viewpoint-aware images contain identical textures on identical pixel positions that are incomprehensible in 3D. Finally, to train NeRF with the images that are viewpoint-aware yet are not strictly consistent in geometry, our approach considers per-image geometric variation as a view of deformation from a shared 3D canonical space. Consequently, we construct the 3D avatar in a canonical space of deformable NeRF by learning a set of per-image deformation via deformation field table. We demonstrate the empirical results and discuss the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2308.06493.pdf' target='_blank'>https://arxiv.org/pdf/2308.06493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Jiang, Paul Streli, Manuel Meier, Christian Holz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06493">EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Full-body egocentric pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representations on headset-based platforms. However, existing methods over-rely on the indoor motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous joint motion capture and uniform body dimensions. We propose EgoPoser to overcome these limitations with four main contributions. 1) EgoPoser robustly models body pose from intermittent hand position and orientation tracking only when inside a headset's field of view. 2) We rethink input representations for headset-based ego-pose estimation and introduce a novel global motion decomposition method that predicts full-body pose independent of global positions. 3) We enhance pose estimation by capturing longer motion time series through an efficient SlowFast module design that maintains computational efficiency. 4) EgoPoser generalizes across various body shapes for different users. We experimentally evaluate our method and show that it outperforms state-of-the-art methods both qualitatively and quantitatively while maintaining a high inference speed of over 600fps. EgoPoser establishes a robust baseline for future work where full-body pose estimation no longer needs to rely on outside-in capture and can scale to large-scale and unseen environments.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2307.04859.pdf' target='_blank'>https://arxiv.org/pdf/2307.04859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander W. Bergman, Wang Yifan, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04859">Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar that is consistent with the text description and can be readily articulated using the deformation model of the 3DMM. We show that our diffusion-based articulated head avatars outperform state-of-the-art approaches for this task. The latter are typically based on CLIP, which is known to provide limited diversity of generation and accuracy for 3D object generation.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2306.12570.pdf' target='_blank'>https://arxiv.org/pdf/2306.12570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12570">Local 3D Editing via 3D Distillation of CLIP Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D content manipulation is an important computer vision task with many real-world applications (e.g., product design, cartoon generation, and 3D Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic 3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of NeRF still remains a challenging problem since the visual quality tends to degrade after manipulation and suboptimal control handles such as 2D semantic maps are used for manipulations. While text-guided manipulations have shown potential in 3D editing, such approaches often lack locality. To overcome these problems, we propose Local Editing NeRF (LENeRF), which only requires text inputs for fine-grained and localized manipulation. Specifically, we present three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field Network, and the Deformation Network, which are jointly used for local manipulations of 3D features by estimating a 3D attention field. The 3D attention field is learned in an unsupervised way, by distilling the zero-shot mask generation capability of CLIP to the 3D space with multi-view guidance. We conduct diverse experiments and thorough evaluations both quantitatively and qualitatively.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2304.03950.pdf' target='_blank'>https://arxiv.org/pdf/2304.03950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijing Wu, Yichao Yan, Yunhao Li, Yuhao Cheng, Wenhan Zhu, Ke Gao, Xiaobo Li, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03950">GANHead: Towards Generative Animatable Neural Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To bring digital avatars into people's lives, it is highly demanded to efficiently generate complete, realistic, and animatable head avatars. This task is challenging, and it is difficult for existing methods to satisfy all the requirements at once. To achieve these goals, we propose GANHead (Generative Animatable Neural Head Avatar), a novel generative head model that takes advantages of both the fine-grained control over the explicit expression parameters and the realistic rendering results of implicit representations. Specifically, GANHead represents coarse geometry, fine-gained details and texture via three networks in canonical space to obtain the ability to generate complete and realistic head avatars. To achieve flexible animation, we define the deformation filed by standard linear blend skinning (LBS), with the learned continuous pose and expression bases and LBS weights. This allows the avatars to be directly animated by FLAME parameters and generalize well to unseen poses and expressions. Compared to state-of-the-art (SOTA) methods, GANHead achieves superior performance on head avatar generation and raw scan fitting.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2304.02626.pdf' target='_blank'>https://arxiv.org/pdf/2304.02626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergey Prokudin, Qianli Ma, Maxime Raafat, Julien Valentin, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02626">Dynamic Point Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed significant progress in the field of neural surface reconstruction. While the extensive focus was put on volumetric and implicit approaches, a number of works have shown that explicit graphics primitives such as point clouds can significantly reduce computational complexity, without sacrificing the reconstructed surface quality. However, less emphasis has been put on modeling dynamic surfaces with point primitives. In this work, we present a dynamic point field model that combines the representational benefits of explicit point-based graphics with implicit deformation networks to allow efficient modeling of non-rigid 3D surfaces. Using explicit surface primitives also allows us to easily incorporate well-established constraints such as-isometric-as-possible regularisation. While learning this deformation model is prone to local optima when trained in a fully unsupervised manner, we propose to additionally leverage semantic information such as keypoint dynamics to guide the deformation learning. We demonstrate our model with an example application of creating an expressive animatable human avatar from a collection of 3D scans. Here, previous methods mostly rely on variants of the linear blend skinning paradigm, which fundamentally limits the expressivity of such models when dealing with complex cloth appearances such as long skirts. We show the advantages of our dynamic point field framework in terms of its representational power, learning efficiency, and robustness to out-of-distribution novel poses.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2304.01436.pdf' target='_blank'>https://arxiv.org/pdf/2304.01436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar, Danhang Tang, Di Qiu, Abhimitra Meka, Ruofei Du, Mingsong Dou, Sergio Orts-Escolano, Rohit Pandey, Ping Tan, Thabo Beeler, Sean Fanello, Yinda Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01436">Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method to learn a high-quality implicit 3D head avatar from a monocular RGB video captured in the wild. The learnt avatar is driven by a parametric face model to achieve user-controlled facial expressions and head poses. Our hybrid pipeline combines the geometry prior and dynamic tracking of a 3DMM with a neural radiance field to achieve fine-grained control and photorealism. To reduce over-smoothing and improve out-of-model expressions synthesis, we propose to predict local features anchored on the 3DMM geometry. These learnt features are driven by 3DMM deformation and interpolated in 3D space to yield the volumetric radiance at a designated query point. We further show that using a Convolutional Neural Network in the UV space is critical in incorporating spatial context and producing representative local features. Extensive experiments show that we are able to reconstruct high-quality avatars, with more accurate expression-dependent details, good generalization to out-of-training expressions, and quantitatively superior renderings compared to other state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2302.10531.pdf' target='_blank'>https://arxiv.org/pdf/2302.10531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Jansen, Julian Britten, Alexander HÃ¤usele, Thilo Segschneider, Mark Colley, Enrico Rukzio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10531">AutoVis: Enabling Mixed-Immersive Analysis of Automotive User Interface Interaction Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automotive user interface (AUI) evaluation becomes increasingly complex due to novel interaction modalities, driving automation, heterogeneous data, and dynamic environmental contexts. Immersive analytics may enable efficient explorations of the resulting multilayered interplay between humans, vehicles, and the environment. However, no such tool exists for the automotive domain. With AutoVis, we address this gap by combining a non-immersive desktop with a virtual reality view enabling mixed-immersive analysis of AUIs. We identify design requirements based on an analysis of AUI research and domain expert interviews (N=5). AutoVis supports analyzing passenger behavior, physiology, spatial interaction, and events in a replicated study environment using avatars, trajectories, and heatmaps. We apply context portals and driving-path events as automotive-specific visualizations. To validate AutoVis against real-world analysis tasks, we implemented a prototype, conducted heuristic walkthroughs using authentic data from a case study and public datasets, and leveraged a real vehicle in the analysis process.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2212.09530.pdf' target='_blank'>https://arxiv.org/pdf/2212.09530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Korrawe Karunratanakul, Sergey Prokudin, Otmar Hilliges, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.09530">HARP: Personalized Hand Reconstruction from a Monocular RGB Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HARP (HAnd Reconstruction and Personalization), a personalized hand avatar creation approach that takes a short monocular RGB video of a human hand as input and reconstructs a faithful hand avatar exhibiting a high-fidelity appearance and geometry. In contrast to the major trend of neural implicit representations, HARP models a hand with a mesh-based parametric hand model, a vertex displacement map, a normal map, and an albedo without any neural components. As validated by our experiments, the explicit nature of our representation enables a truly scalable, robust, and efficient approach to hand avatar creation. HARP is optimized via gradient descent from a short sequence captured by a hand-held mobile phone and can be directly used in AR/VR applications with real-time rendering capability. To enable this, we carefully design and implement a shadow-aware differentiable rendering scheme that is robust to high degree articulations and self-shadowing regularly present in hand motion sequences, as well as challenging lighting conditions. It also generalizes to unseen poses and novel viewpoints, producing photo-realistic renderings of hand animations performing highly-articulated motions. Furthermore, the learned HARP representation can be used for improving 3D hand pose estimation quality in challenging viewpoints. The key advantages of HARP are validated by the in-depth analyses on appearance reconstruction, novel-view and novel pose synthesis, and 3D hand pose refinement. It is an AR/VR-ready personalized hand representation that shows superior fidelity and scalability.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2203.06972.pdf' target='_blank'>https://arxiv.org/pdf/2203.06972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Dafarra, Ugo Pattacini, Giulio Romualdi, Lorenzo Rapetti, Riccardo Grieco, Kourosh Darvish, Gianluca Milani, Enrico Valli, Ines Sorrentino, Paolo Maria Viceconte, Alessandro Scalzo, Silvio Traversaro, Carlotta Sartore, Mohamed Elobaid, Nuno Guedelha, Connor Herron, Alexander Leonessa, Francesco Draicchio, Giorgio Metta, Marco Maggiali, Daniele Pucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.06972">iCub3 Avatar System: Enabling Remote Fully-Immersive Embodiment of Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an avatar system designed to facilitate the embodiment of humanoid robots by human operators, validated through iCub3, a humanoid developed at the Istituto Italiano di Tecnologia (IIT). More precisely, the contribution of the paper is twofold: first, we present the humanoid iCub3 as a robotic avatar which integrates the latest significant improvements after about fifteen years of development of the iCub series; second, we present a versatile avatar system enabling humans to embody humanoid robots encompassing aspects such as locomotion, manipulation, voice, and face expressions with comprehensive sensory feedback including visual, auditory, haptic, weight, and touch modalities. We validate the system by implementing several avatar architecture instances, each tailored to specific requirements. First, we evaluated the optimized architecture for verbal, non-verbal, and physical interactions with a remote recipient. This testing involved the operator in Genoa and the avatar in the Biennale di Venezia, Venice - about 290 Km away - thus allowing the operator to visit remotely the Italian art exhibition. Second, we evaluated the optimised architecture for recipient physical collaboration and public engagement on-stage, live, at the We Make Future show, a prominent world digital innovation festival. In this instance, the operator was situated in Genoa while the avatar operates in Rimini - about 300 Km away - interacting with a recipient who entrusted the avatar a payload to carry on stage before an audience of approximately 2000 spectators. Third, we present the architecture implemented by the iCub Team for the ANA Avatar XPrize competition.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2509.19259.pdf' target='_blank'>https://arxiv.org/pdf/2509.19259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markos Diomataris, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh, Omid Taheri, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19259">Moving by Looking: Towards Vision-Driven Avatar Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The way we perceive the world fundamentally shapes how we move, whether it is how we navigate in a room or how we interact with other humans. Current human motion generation methods, neglect this interdependency and use task-specific ``perception'' that differs radically from that of humans. We argue that the generation of human-like avatar behavior requires human-like perception. Consequently, in this work we present CLOPS, the first human avatar that solely uses egocentric vision to perceive its surroundings and navigate. Using vision as the primary driver of motion however, gives rise to a significant challenge for training avatars: existing datasets have either isolated human motion, without the context of a scene, or lack scale. We overcome this challenge by decoupling the learning of low-level motion skills from learning of high-level control that maps visual input to motion. First, we train a motion prior model on a large motion capture dataset. Then, a policy is trained using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior. Our experiments empirically demonstrate that egocentric vision can give rise to human-like motion characteristics in our avatars. For example, the avatars walk such that they avoid obstacles present in their visual field. These findings suggest that equipping avatars with human-like sensors, particularly egocentric vision, holds promise for training avatars that behave like humans.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2507.10542.pdf' target='_blank'>https://arxiv.org/pdf/2507.10542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias NieÃner, Derek Bradley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10542">ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2506.22355.pdf' target='_blank'>https://arxiv.org/pdf/2506.22355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Louis-Philippe Morency, ThÃ©o Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Paden Tomasello, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22355">Embodied AI Agents: Modeling the World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2506.05397.pdf' target='_blank'>https://arxiv.org/pdf/2506.05397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jerrin Bright, Zhibo Wang, Yuhao Chen, Sirisha Rambhatla, John Zelek, David Clausi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05397">Gen4D: Synthesizing Humans and Scenes in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lack of input data for in-the-wild activities often results in low performance across various computer vision tasks. This challenge is particularly pronounced in uncommon human-centric domains like sports, where real-world data collection is complex and impractical. While synthetic datasets offer a promising alternative, existing approaches typically suffer from limited diversity in human appearance, motion, and scene composition due to their reliance on rigid asset libraries and hand-crafted rendering pipelines. To address this, we introduce Gen4D, a fully automated pipeline for generating diverse and photorealistic 4D human animations. Gen4D integrates expert-driven motion encoding, prompt-guided avatar generation using diffusion-based Gaussian splatting, and human-aware background synthesis to produce highly varied and lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale synthetic dataset spanning three sports: baseball, icehockey, and soccer. Together, Gen4D and SportPAL provide a scalable foundation for constructing synthetic datasets tailored to in-the-wild human-centric vision tasks, with no need for manual 3D modeling or scene design.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2503.11085.pdf' target='_blank'>https://arxiv.org/pdf/2503.11085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sixiang Ye, Zeyu Sun, Guoqing Wang, Liwei Guo, Qingyuan Liang, Zheng Li, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11085">Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code generation has emerged as a key task to automate software development by converting high-level descriptions into executable code. Large language models (LLMs) excel at this but depend heavily on input prompt quality.Manual prompt engineering can be time-consuming and inconsistent, limiting LLM effectiveness. This paper introduces Prochemy, an innovative method for automatically refining prompts to boost code generation. Prochemy overcomes manual prompt limitations by automating optimization, ensuring consistency during inference, and supporting multi-agent systems.It iteratively refines prompts based on model performance, using an optimized final prompt for improved consistency across tasks. We tested Prochemy on natural language-based code generation and translation tasks using three LLM series. Results indicate Prochemy enhances existing methods, improving performance by 5.0% for GPT-3.5-Turbo and 1.9% for GPT-4o over zero-shot baselines on HumanEval. In state-of-the-art LDB, Prochemy + LDB surpasses standalone methods by 1.2-1.8%. For code translation, Prochemy boosts GPT-4o's Java-to-Python (AVATAR) performance from 74.5 to 84.1 (+12.9%) and Python-to-Java from 66.8 to 78.2 (+17.1%). Moreover, Prochemy maintains strong performance when integrated with the o1-mini model, validating its efficacy in code tasks. Designed as plug-and-play, Prochemy optimizes prompts with minimal human input, bridging the gap between simple prompts and complex frameworks.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2503.00825.pdf' target='_blank'>https://arxiv.org/pdf/2503.00825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijing Zhao, Brian Diep, Jiaxin Pei, Dongwook Yoon, David Jurgens, Jian Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00825">Who Reaps All the Superchats? A Large-Scale Analysis of Income Inequality in Virtual YouTuber Livestreaming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosive growth of Virtual YouTubers (VTubers)-streamers who perform behind virtual anime avatars-has created a unique digital economy with profound implications for content creators, platforms, and viewers. Understanding the economic landscape of VTubers is crucial for designing equitable platforms, supporting content creator livelihoods, and fostering sustainable digital communities. To this end, we conducted a large-scale study of over 1 million hours of publicly available streaming records from 1,923 VTubers on YouTube, covering tens of millions of dollars in actual profits. Our analysis reveals stark inequality within the VTuber community and characterizes the sources of income for VTubers from multiple perspectives. Furthermore, we also found that the VTuber community is increasingly monopolized by two agencies, driving the financial disparity. This research illuminates the financial dynamics of VTuber communities, informing the design of equitable platforms and sustainable support systems for digital content creators.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2502.02372.pdf' target='_blank'>https://arxiv.org/pdf/2502.02372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02372">MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of a virtual digital avatar is a crucial research topic in the field of computer vision. Many existing works utilize Neural Radiance Fields (NeRF) to address this issue and have achieved impressive results. However, previous works assume the images of the training person are available and fixed while the appearances and poses of a subject could constantly change and increase in real-world scenarios. How to update the human avatar but also maintain the ability to render the old appearance of the person is a practical challenge. One trivial solution is to combine the existing virtual avatar models based on NeRF with continual learning methods. However, there are some critical issues in this approach: learning new appearances and poses can cause the model to forget past information, which in turn leads to a degradation in the rendering quality of past appearances, especially color bleeding issues, and incorrect human body poses. In this work, we propose a maintainable avatar (MaintaAvatar) based on neural radiance fields by continual learning, which resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose Distillation Module. Overall, our model requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting, thus achieving a maintainable virtual avatar. The experimental results validate the effectiveness of our MaintaAvatar model.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2501.05379.pdf' target='_blank'>https://arxiv.org/pdf/2501.05379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05379">Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail. Please visit https://arc2avatar.github.io for more resources.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2412.08684.pdf' target='_blank'>https://arxiv.org/pdf/2412.08684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengze Wang, Xueting Li, Chao Liu, Matthew Chan, Michael Stengel, Henry Fuchs, Shalini De Mello, Koki Nagano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08684">Coherent3D: Coherent 3D Portrait Video Reconstruction via Triplane Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in single-image 3D portrait reconstruction have enabled telepresence systems to stream 3D portrait videos from a single camera in real-time, democratizing telepresence. However, per-frame 3D reconstruction exhibits temporal inconsistency and forgets the user's appearance. On the other hand, self-reenactment methods can render coherent 3D portraits by driving a 3D avatar built from a single reference image, but fail to faithfully preserve the user's per-frame appearance (e.g., instantaneous facial expression and lighting). As a result, none of these two frameworks is an ideal solution for democratized 3D telepresence. In this work, we address this dilemma and propose a novel solution that maintains both coherent identity and dynamic per-frame appearance to enable the best possible realism. To this end, we propose a new fusion-based method that takes the best of both worlds by fusing a canonical 3D prior from a reference view with dynamic appearance from per-frame input views, producing temporally stable 3D videos with faithful reconstruction of the user's per-frame appearance. Trained only using synthetic data produced by an expression-conditioned 3D GAN, our encoder-based method achieves both state-of-the-art 3D reconstruction and temporal consistency on in-studio and in-the-wild datasets. https://research.nvidia.com/labs/amri/projects/coherent3d
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2412.06698.pdf' target='_blank'>https://arxiv.org/pdf/2412.06698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06698">Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic 3D objects and clothed avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot guarantee the generated multi-view images are 3D consistent. In this paper, we propose Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy. We leverage a pre-trained 2D diffusion model and a 3D diffusion model via our elegantly designed process that synchronizes two diffusion models at both training and sampling time. The synergy between the 2D and 3D diffusion models brings two major advantages: 1) 2D helps 3D in generalization: the pretrained 2D model has strong generalization ability to unseen images, providing strong shape priors for the 3D diffusion model; 2) 3D helps 2D in multi-view consistency: the 3D diffusion model enhances the 3D consistency of 2D multi-view sampling process, resulting in more accurate multi-view generation. We validate our idea through extensive experiments in image-based objects and clothed avatar generation tasks. Results show that our method generates realistic 3D objects and avatars with high-fidelity geometry and texture. Extensive ablations also validate our design choices and demonstrate the strong generalization ability to diverse clothing and compositional shapes. Our code and pretrained models will be publicly released on https://yuxuan-xue.com/gen-3diffusion.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2410.17098.pdf' target='_blank'>https://arxiv.org/pdf/2410.17098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Schneider, Sina Sajadmanesh, Vikash Sehwag, Saquib Sarfraz, Rainer Stiefelhagen, Lingjuan Lyu, Vivek Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17098">Activity Recognition on Avatar-Anonymized Datasets with Masked Differential Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Privacy-preserving computer vision is an important emerging problem in machine learning and artificial intelligence. Prevalent methods tackling this problem use differential privacy (DP) or obfuscation techniques to protect the privacy of individuals. In both cases, the utility of the trained model is sacrificed heavily in this process. In this work, we present an anonymization pipeline that replaces sensitive human subjects in video datasets with synthetic avatars within context, employing a combined rendering and stable diffusion-based strategy. Additionally we propose masked differential privacy ({MaskDP}) to protect non-anonymized but privacy sensitive background information. MaskDP allows for controlling sensitive regions where differential privacy is applied, in contrast to applying DP on the entire input. This combined methodology provides strong privacy protection while minimizing the usual performance penalty of privacy preserving methods. Experiments on multiple challenging action recognition datasets demonstrate that our proposed techniques result in better utility-privacy trade-offs compared to standard differentially private training in the especially demanding $Îµ<1$ regime.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2407.08414.pdf' target='_blank'>https://arxiv.org/pdf/2407.08414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushuo Chen, Zerong Zheng, Zhe Li, Chao Xu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08414">MeshAvatar: Learning High-quality Triangular Human Avatars from Multi-view Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel pipeline for learning high-quality triangular human avatars from multi-view videos. Recent methods for avatar learning are typically based on neural radiance fields (NeRF), which is not compatible with traditional graphics pipeline and poses great challenges for operations like editing or synthesizing under different environments. To overcome these limitations, our method represents the avatar with an explicit triangular mesh extracted from an implicit SDF field, complemented by an implicit material field conditioned on given poses. Leveraging this triangular avatar representation, we incorporate physics-based rendering to accurately decompose geometry and texture. To enhance both the geometric and appearance details, we further employ a 2D UNet as the network backbone and introduce pseudo normal ground-truth as additional supervision. Experiments show that our method can learn triangular avatars with high-quality geometry reconstruction and plausible material decomposition, inherently supporting editing, manipulation or relighting operations.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2407.02165.pdf' target='_blank'>https://arxiv.org/pdf/2407.02165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Huang, Shoukang Hu, Guangcong Wang, Tianqi Liu, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02165">WildAvatar: Learning In-the-wild 3D Avatars from the Web</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing research on avatar creation is typically limited to laboratory datasets, which require high costs against scalability and exhibit insufficient representation of the real world. On the other hand, the web abounds with off-the-shelf real-world human videos, but these videos vary in quality and require accurate annotations for avatar creation. To this end, we propose an automatic annotating pipeline with filtering protocols to curate these humans from the web. Our pipeline surpasses state-of-the-art methods on the EMDB benchmark, and the filtering protocols boost verification metrics on web videos. We then curate WildAvatar, a web-scale in-the-wild human avatar creation dataset extracted from YouTube, with $10000+$ different human subjects and scenes. WildAvatar is at least $10\times$ richer than previous datasets for 3D human avatar creation and closer to the real world. To explore its potential, we demonstrate the quality and generalizability of avatar creation methods on WildAvatar. We will publicly release our code, data source links and annotations to push forward 3D human avatar creation and other related fields for real-world applications.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2406.13272.pdf' target='_blank'>https://arxiv.org/pdf/2406.13272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ken Chen, Sachith Seneviratne, Wei Wang, Dongting Hu, Sanjay Saha, Md. Tarek Hasan, Sanka Rasnayaka, Tamasha Malepathirana, Mingming Gong, Saman Halgamuge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13272">AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating stylized avatars with dynamic poses and expressions has attracted increasing attention for its broad range of applications. Previous research has made significant progress by training controllable generative models to synthesize animations based on reference characteristics, pose, and expression conditions. However, the mechanisms used in these methods to control pose and expression often inadvertently introduce unintended features from the target motion, while also causing a loss of expression-related details, particularly when applied to stylized animation. This paper proposes a new method based on Stable Diffusion, called AniFaceDiff, incorporating a new conditioning module for animating stylized avatars. First, we propose a refined spatial conditioning approach by Facial Alignment to prevent the inclusion of identity characteristics from the target motion. Then, we introduce an Expression Adapter that incorporates additional cross-attention layers to address the potential loss of expression-related information. Our approach effectively preserves pose and expression from the target video while maintaining input image consistency. Extensive experiments demonstrate that our method achieves state-of-the-art results, showcasing superior image quality, preservation of reference features, and expression accuracy, particularly for out-of-domain animation across diverse styles, highlighting its versatility and strong generalization capabilities. This work aims to enhance the quality of virtual stylized animation for positive applications. To promote responsible use in virtual environments, we contribute to the advancement of detection for generative content by evaluating state-of-the-art detectors, highlighting potential areas for improvement, and suggesting solutions.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2406.08475.pdf' target='_blank'>https://arxiv.org/pdf/2406.08475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08475">Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models will be released on https://yuxuan-xue.com/human-3diffusion.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2406.05209.pdf' target='_blank'>https://arxiv.org/pdf/2406.05209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o SimÃµes, Anderson Maciel, Catarina Moreira, Joaquim Jorge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05209">SPARC: Shared Perspective with Avatar Distortion for Remote Collaboration in VR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Telepresence VR systems allow for face-to-face communication, promoting the feeling of presence and understanding of nonverbal cues. However, when discussing virtual 3D objects, limitations to presence and communication cause deictic gestures to lose meaning due to disparities in orientation. Current approaches use shared perspective, and avatar overlap to restore these references, which cause occlusions and discomfort that worsen when multiple users participate. We introduce a new approach to shared perspective in multi-user collaboration where the avatars are not co-located. Each person sees the others' avatars at their positions around the workspace while having a first-person view of the workspace. Whenever a user manipulates an object, others will see his/her arms stretching to reach that object in their perspective. SPARC combines a shared orientation and supports nonverbal communication, minimizing occlusions. We conducted a user study (n=18) to understand how the novel approach impacts task performance and workspace awareness. We found evidence that SPARC is more efficient and less mentally demanding than life-like settings.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2405.14036.pdf' target='_blank'>https://arxiv.org/pdf/2405.14036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Su, Kunlin Cai, Reuben Beeler, Lukas Dresel, Allan Garcia, Ilya Grishchenko, Yuan Tian, Christopher Kruegel, Giovanni Vigna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14036">Remote Keylogging Attacks in Multi-user VR Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Virtual Reality (VR) applications grow in popularity, they have bridged distances and brought users closer together. However, with this growth, there have been increasing concerns about security and privacy, especially related to the motion data used to create immersive experiences. In this study, we highlight a significant security threat in multi-user VR applications, which are applications that allow multiple users to interact with each other in the same virtual space. Specifically, we propose a remote attack that utilizes the avatar rendering information collected from an adversary's game clients to extract user-typed secrets like credit card information, passwords, or private conversations. We do this by (1) extracting motion data from network packets, and (2) mapping motion data to keystroke entries. We conducted a user study to verify the attack's effectiveness, in which our attack successfully inferred 97.62% of the keystrokes. Besides, we performed an additional experiment to underline that our attack is practical, confirming its effectiveness even when (1) there are multiple users in a room, and (2) the attacker cannot see the victims. Moreover, we replicated our proposed attack on four applications to demonstrate the generalizability of the attack. Lastly, we proposed a defense against the attack, which has been implemented by major players in the VR industry. These results underscore the severity of the vulnerability and its potential impact on millions of VR social platform users.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2405.12069.pdf' target='_blank'>https://arxiv.org/pdf/2405.12069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Oztireli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12069">Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2404.10667.pdf' target='_blank'>https://arxiv.org/pdf/2404.10667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10667">VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only generating lip movements that are exquisitely synchronized with the audio, but also producing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2404.02686.pdf' target='_blank'>https://arxiv.org/pdf/2404.02686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiali Zheng, Rolandos Alexandros Potamias, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02686">Design2Cloth: 3D Cloth Generation from 2D Masks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a significant shift in the field of digital avatar research, towards modeling, animating and reconstructing clothed human representations, as a key step towards creating realistic avatars. However, current 3D cloth generation methods are garment specific or trained completely on synthetic data, hence lacking fine details and realism. In this work, we make a step towards automatic realistic garment design and propose Design2Cloth, a high fidelity 3D generative model trained on a real world dataset from more than 2000 subject scans. To provide vital contribution to the fashion industry, we developed a user-friendly adversarial model capable of generating diverse and detailed clothes simply by drawing a 2D cloth mask. Under a series of both qualitative and quantitative experiments, we showcase that Design2Cloth outperforms current state-of-the-art cloth generative models by a large margin. In addition to the generative properties of our network, we showcase that the proposed method can be used to achieve high quality reconstructions from single in-the-wild images and 3D scans. Dataset, code and pre-trained model will become publicly available.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2403.19773.pdf' target='_blank'>https://arxiv.org/pdf/2403.19773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rolandos Alexandros Potamias, Michail Tarasiou, Stylianos Ploumpis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19773">ShapeFusion: A 3D diffusion model for localized shape editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of 3D computer vision, parametric models have emerged as a ground-breaking methodology for the creation of realistic and expressive 3D avatars. Traditionally, they rely on Principal Component Analysis (PCA), given its ability to decompose data to an orthonormal space that maximally captures shape variations. However, due to the orthogonality constraints and the global nature of PCA's decomposition, these models struggle to perform localized and disentangled editing of 3D shapes, which severely affects their use in applications requiring fine control such as face sculpting. In this paper, we leverage diffusion models to enable diverse and fully localized edits on 3D meshes, while completely preserving the un-edited regions. We propose an effective diffusion masking training strategy that, by design, facilitates localized manipulation of any shape region, without being limited to predefined regions or to sparse sets of predefined control vertices. Following our framework, a user can explicitly set their manipulation region of choice and define an arbitrary set of vertices as handles to edit a 3D mesh. Compared to the current state-of-the-art our method leads to more interpretable shape manipulations than methods relying on latent code state, greater localization and generation diversity while offering faster inference than optimization based approaches. Project page: https://rolpotamias.github.io/Shapefusion/
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2403.17213.pdf' target='_blank'>https://arxiv.org/pdf/2403.17213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17213">AnimateMe: 4D Facial Expressions via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of photorealistic 3D avatar reconstruction and generation has garnered significant attention in recent years; however, animating such avatars remains challenging. Recent advances in diffusion models have notably enhanced the capabilities of generative models in 2D animation. In this work, we directly utilize these models within the 3D domain to achieve controllable and high-fidelity 4D facial animation. By integrating the strengths of diffusion processes and geometric deep learning, we employ Graph Neural Networks (GNNs) as denoising diffusion models in a novel approach, formulating the diffusion process directly on the mesh space and enabling the generation of 3D facial expressions. This facilitates the generation of facial deformations through a mesh-diffusion-based model. Additionally, to ensure temporal coherence in our animations, we propose a consistent noise sampling method. Under a series of both quantitative and qualitative experiments, we showcase that the proposed method outperforms prior work in 4D expression synthesis by generating high-fidelity extreme expressions. Furthermore, we applied our method to textured 4D facial expression generation, implementing a straightforward extension that involves training on a large-scale textured 4D facial expression database.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2402.05803.pdf' target='_blank'>https://arxiv.org/pdf/2402.05803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wamiq Reyaz Para, Abdelrahman Eldesokey, Zhenyu Li, Pradyumna Reddy, Jiankang Deng, Peter Wonka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05803">AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an approach for 3D head avatar generation and editing with multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and a Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatars given a single or no condition. However, it is challenging to generate samples that adhere to multiple conditions of different modalities. On the other hand, LDMs excel at learning complex conditional distributions. To this end, we propose to exploit the conditioning capabilities of LDMs to enable multi-modal control over the latent space of a pre-trained 3D GAN. Our method can generate and edit 3D head avatars given a mixture of control signals such as RGB input, segmentation masks, and global attributes. This provides better control over the generation and editing of synthetic avatars both globally and locally. Experiments show that our proposed approach outperforms a solely GAN-based approach both qualitatively and quantitatively on generation and editing tasks. To the best of our knowledge, our approach is the first to introduce multi-modal conditioning to 3D avatar generation and editing. \\href{avatarmmc-sig24.github.io}{Project Page}
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2401.03476.pdf' target='_blank'>https://arxiv.org/pdf/2401.03476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicheng Yang, Zunnan Xu, Haiwei Xue, Yongkang Cheng, Shaoli Huang, Mingming Gong, Zhiyong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03476">Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current talking avatars mostly generate co-speech gestures based on audio and text of the utterance, without considering the non-speaking motion of the speaker. Furthermore, previous works on co-speech gesture generation have designed network structures based on individual gesture datasets, which results in limited data volume, compromised generalizability, and restricted speaker movements. To tackle these issues, we introduce FreeTalker, which, to the best of our knowledge, is the first framework for the generation of both spontaneous (e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium) speaker motions. Specifically, we train a diffusion-based model for speaker motion generation that employs unified representations of both speech-driven gestures and text-driven motions, utilizing heterogeneous data sourced from various motion datasets. During inference, we utilize classifier-free guidance to highly control the style in the clips. Additionally, to create smooth transitions between clips, we utilize DoubleTake, a method that leverages a generative prior and ensures seamless motion blending. Extensive experiments show that our method generates natural and controllable speaker movements. Our code, model, and demo are are available at \url{https://youngseng.github.io/FreeTalker/}.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2312.09228.pdf' target='_blank'>https://arxiv.org/pdf/2312.09228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09228">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2312.03420.pdf' target='_blank'>https://arxiv.org/pdf/2312.03420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingyan Xu, Prashanth Chandran, Sebastian Weiss, Markus Gross, Gaspard Zoss, Derek Bradley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03420">Artist-Friendly Relightable and Animatable Neural Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An increasingly common approach for creating photo-realistic digital avatars is through the use of volumetric neural fields. The original neural radiance field (NeRF) allowed for impressive novel view synthesis of static heads when trained on a set of multi-view images, and follow up methods showed that these neural representations can be extended to dynamic avatars. Recently, new variants also surpassed the usual drawback of baked-in illumination in neural representations, showing that static neural avatars can be relit in any environment. In this work we simultaneously tackle both the motion and illumination problem, proposing a new method for relightable and animatable neural heads. Our method builds on a proven dynamic avatar approach based on a mixture of volumetric primitives, combined with a recently-proposed lightweight hardware setup for relightable neural fields, and includes a novel architecture that allows relighting dynamic neural avatars performing unseen expressions in any environment, even with nearfield illumination and viewpoints.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2312.02134.pdf' target='_blank'>https://arxiv.org/pdf/2312.02134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02134">GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2310.20436.pdf' target='_blank'>https://arxiv.org/pdf/2310.20436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengdi Yu, Shaoli Huang, Yongkang Cheng, Tolga Birdal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20436">SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SignAvatars, the first large-scale, multi-prompt 3D sign language (SL) motion dataset designed to bridge the communication gap for Deaf and hard-of-hearing individuals. While there has been an exponentially growing number of research regarding digital communication, the majority of existing communication technologies primarily cater to spoken or written languages, instead of SL, the essential communication method for Deaf and hard-of-hearing communities. Existing SL datasets, dictionaries, and sign language production (SLP) methods are typically limited to 2D as annotating 3D models and avatars for SL is usually an entirely manual and labor-intensive process conducted by SL experts, often resulting in unnatural avatars. In response to these challenges, we compile and curate the SignAvatars dataset, which comprises 70,000 videos from 153 signers, totaling 8.34 million frames, covering both isolated signs and continuous, co-articulated signs, with multiple prompts including HamNoSys, spoken language, and words. To yield 3D holistic annotations, including meshes and biomechanically-valid poses of body, hands, and face, as well as 2D and 3D keypoints, we introduce an automated annotation pipeline operating on our large corpus of SL videos. SignAvatars facilitates various tasks such as 3D sign language recognition (SLR) and the novel 3D SL production (SLP) from diverse inputs like text scripts, individual words, and HamNoSys notation. Hence, to evaluate the potential of SignAvatars, we further propose a unified benchmark of 3D SL holistic motion production. We believe that this work is a significant step forward towards bringing the digital world to the Deaf and hard-of-hearing communities as well as people interacting with them.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2310.03952.pdf' target='_blank'>https://arxiv.org/pdf/2310.03952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiali Zheng, Youngkyoon Jang, Athanasios Papaioannou, Christos Kampouris, Rolandos Alexandros Potamias, Foivos Paraperas Papantoniou, Efstathios Galanakis, Ales Leonardis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03952">ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the Imperial Light-Stage Head (ILSH) dataset, a novel light-stage-captured human head dataset designed to support view synthesis academic challenges for human heads. The ILSH dataset is intended to facilitate diverse approaches, such as scene-specific or generic neural rendering, multiple-view geometry, 3D vision, and computer graphics, to further advance the development of photo-realistic human avatars. This paper details the setup of a light-stage specifically designed to capture high-resolution (4K) human head images and describes the process of addressing challenges (preprocessing, ethical issues) in collecting high-quality data. In addition to the data collection, we address the split of the dataset into train, validation, and test sets. Our goal is to design and support a fair view synthesis challenge task for this novel dataset, such that a similar level of performance can be maintained and expected when using the test set, as when using the validation set. The ILSH dataset consists of 52 subjects captured using 24 cameras with all 82 lighting sources turned on, resulting in a total of 1,248 close-up head images, border masks, and camera pose pairs.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2309.02186.pdf' target='_blank'>https://arxiv.org/pdf/2309.02186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wu, Sicheng Xu, Jianfeng Xiang, Fangyun Wei, Qifeng Chen, Jiaolong Yang, Xin Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02186">AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous animatable 3D-aware GANs for human generation have primarily focused on either the human head or full body. However, head-only videos are relatively uncommon in real life, and full body generation typically does not deal with facial expression control and still has challenges in generating high-quality results. Towards applicable video avatars, we present an animatable 3D-aware GAN that generates portrait images with controllable facial expression, head pose, and shoulder movements. It is a generative model trained on unstructured 2D image collections without using 3D or video data. For the new task, we base our method on the generative radiance manifold representation and equip it with learnable facial and head-shoulder deformations. A dual-camera rendering and adversarial learning scheme is proposed to improve the quality of the generated faces, which is critical for portrait images. A pose deformation processing network is developed to generate plausible deformations for challenging regions such as long hair. Experiments show that our method, trained on unstructured 2D images, can generate diverse and high-quality 3D portraits with desired control over different properties.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2308.14847.pdf' target='_blank'>https://arxiv.org/pdf/2308.14847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xue, Bharat Lal Bhatnagar, Riccardo Marin, Nikolaos Sarafianos, Yuanlu Xu, Gerard Pons-Moll, Tony Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14847">NSF: Neural Surface Fields for Human Modeling from Monocular Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obtaining personalized 3D animatable avatars from a monocular camera has several real world applications in gaming, virtual try-on, animation, and VR/XR, etc. However, it is very challenging to model dynamic and fine-grained clothing deformations from such sparse data. Existing methods for modeling 3D humans from depth data have limitations in terms of computational efficiency, mesh coherency, and flexibility in resolution and topology. For instance, reconstructing shapes using implicit functions and extracting explicit meshes per frame is computationally expensive and cannot ensure coherent meshes across frames. Moreover, predicting per-vertex deformations on a pre-designed human template with a discrete surface lacks flexibility in resolution and topology. To overcome these limitations, we propose a novel method Neural Surface Fields for modeling 3D clothed humans from monocular depth. NSF defines a neural field solely on the base surface which models a continuous and flexible displacement field. NSF can be adapted to the base surface with different resolution and topology without retraining at inference time. Compared to existing approaches, our method eliminates the expensive per-frame surface extraction while maintaining mesh coherency, and is capable of reconstructing meshes with arbitrary resolution without retraining. To foster research in this direction, we release our code in project page at: https://yuxuan-xue.com/nsf.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2305.03713.pdf' target='_blank'>https://arxiv.org/pdf/2305.03713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekta Prashnani, Koki Nagano, Shalini De Mello, David Luebke, Orazio Gallo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03713">Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern avatar generators allow anyone to synthesize photorealistic real-time talking avatars, ushering in a new era of avatar-based human communication, such as with immersive AR/VR interactions or videoconferencing with limited bandwidths. Their safe adoption, however, requires a mechanism to verify if the rendered avatar is trustworthy: does it use the appearance of an individual without their consent? We term this task avatar fingerprinting. To tackle it, we first introduce a large-scale dataset of real and synthetic videos of people interacting on a video call, where the synthetic videos are generated using the facial appearance of one person and the expressions of another. We verify the identity driving the expressions in a synthetic video, by learning motion signatures that are independent of the facial appearance shown. Our solution, the first in this space, achieves an average AUC of 0.85. Critical to its practical use, it also generalizes to new generators never seen in training (average AUC of 0.83). The proposed dataset and other resources can be found at: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2301.09041.pdf' target='_blank'>https://arxiv.org/pdf/2301.09041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohd Sabra, Nisha Vinayaga Sureshkanth, Ari Sharma, Anindya Maiti, Murtuza Jadliwala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09041">Exploiting Out-of-band Motion Sensor Data to De-anonymize Virtual Reality Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) is an exciting new consumer technology which offers an immersive audio-visual experience to users through which they can navigate and interact with a digitally represented 3D space (i.e., a virtual world) using a headset device. By (visually) transporting users from the real or physical world to exciting and realistic virtual spaces, VR systems can enable true-to-life and more interactive versions of traditional applications such as gaming, remote conferencing, social networking and virtual tourism. However, as with any new consumer technology, VR applications also present significant user-privacy challenges. This paper studies a new type of privacy attack targeting VR users by connecting their activities visible in the virtual world (enabled by some VR application/service) to their physical state sensed in the real world. Specifically, this paper analyzes the feasibility of carrying out a de-anonymization or identification attack on VR users by correlating visually observed movements of users' avatars in the virtual world with some auxiliary data (e.g., motion sensor data from mobile/wearable devices held by users) representing their context/state in the physical world. To enable this attack, this paper proposes a novel framework which first employs a learning-based activity classification approach to translate the disparate visual movement data and motion sensor data into an activity-vector to ease comparison, followed by a filtering and identity ranking phase outputting an ordered list of potential identities corresponding to the target visual movement data. Extensive empirical evaluation of the proposed framework, under a comprehensive set of experimental settings, demonstrates the feasibility of such a de-anonymization attack.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2509.09595.pdf' target='_blank'>https://arxiv.org/pdf/2509.09595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikang Ding, Jiwen Liu, Wenyuan Zhang, Zekun Wang, Wentao Hu, Liyuan Cui, Mingming Lao, Yingchao Shao, Hui Liu, Xiaohan Li, Ming Chen, Xiaoqiang Liu, Yu-Shen Liu, Pengfei Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09595">Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2508.16401.pdf' target='_blank'>https://arxiv.org/pdf/2508.16401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NVIDIA, :, Chaeyeon Chung, Ilya Fedorov, Michael Huang, Aleksey Karmanov, Dmitry Korobchenko, Roger Ribera, Yeongho Seol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16401">Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven facial animation presents an effective solution for animating digital avatars. In this paper, we detail the technical aspects of NVIDIA Audio2Face-3D, including data acquisition, network architecture, retargeting methodology, evaluation metrics, and use cases. Audio2Face-3D system enables real-time interaction between human users and interactive avatars, facilitating facial animation authoring for game characters. To assist digital avatar creators and game developers in generating realistic facial animations, we have open-sourced Audio2Face-3D networks, SDK, training framework, and example dataset.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2508.09597.pdf' target='_blank'>https://arxiv.org/pdf/2508.09597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heyi Sun, Cong Wang, Tian-Xing Xu, Jingwei Huang, Di Kang, Chunchao Guo, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09597">SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity and editable head avatars is a pivotal challenge in computer vision and graphics, boosting many AR/VR applications. While recent advancements have achieved photorealistic renderings and plausible animation, head editing, especially real-time appearance editing, remains challenging due to the implicit representation and entangled modeling of the geometry and global appearance. To address this, we propose Surface-Volumetric Gaussian Head Avatar (SVG-Head), a novel hybrid representation that explicitly models the geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled texture images to capture the global appearance. Technically, it contains two types of Gaussians, in which surface Gaussians explicitly model the appearance of head avatars using learnable texture images, facilitating real-time texture editing, while volumetric Gaussians enhance the reconstruction quality of non-Lambertian regions (e.g., lips and hair). To model the correspondence between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping method, which leverages UV coordinates given by the FLAME mesh to obtain sharp texture images and real-time rendering speed. A hierarchical optimization strategy is further designed to pursue the optimal performance in both reconstruction quality and editing flexibility. Experiments on the NeRSemble dataset show that SVG-Head not only generates high-fidelity rendering results, but also is the first method to obtain explicit texture images for Gaussian head avatars and support real-time appearance editing.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2507.19481.pdf' target='_blank'>https://arxiv.org/pdf/2507.19481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byungjun Kim, Shunsuke Saito, Giljoo Nam, Tomas Simon, Jason Saragih, Hanbyul Joo, Junxuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19481">HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2507.15979.pdf' target='_blank'>https://arxiv.org/pdf/2507.15979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel C. BÃ¼hler, Ye Yuan, Xueting Li, Yangyi Huang, Koki Nagano, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15979">Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2507.05191.pdf' target='_blank'>https://arxiv.org/pdf/2507.05191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Doug Roble, Tuur Stuyck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05191">Neuralocks: Real-Time Dynamic Neural Hair Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time hair simulation is a vital component in creating believable virtual avatars, as it provides a sense of immersion and authenticity. The dynamic behavior of hair, such as bouncing or swaying in response to character movements like jumping or walking, plays a significant role in enhancing the overall realism and engagement of virtual experiences. Current methods for simulating hair have been constrained by two primary approaches: highly optimized physics-based systems and neural methods. However, state-of-the-art neural techniques have been limited to quasi-static solutions, failing to capture the dynamic behavior of hair. This paper introduces a novel neural method that breaks through these limitations, achieving efficient and stable dynamic hair simulation while outperforming existing approaches. We propose a fully self-supervised method which can be trained without any manual intervention or artist generated training data allowing the method to be integrated with hair reconstruction methods to enable automatic end-to-end methods for avatar reconstruction. Our approach harnesses the power of compact, memory-efficient neural networks to simulate hair at the strand level, allowing for the simulation of diverse hairstyles without excessive computational resources or memory requirements. We validate the effectiveness of our method through a variety of hairstyle examples, showcasing its potential for real-world applications.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2506.08805.pdf' target='_blank'>https://arxiv.org/pdf/2506.08805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stina Klein, Pooja Prajod, Katharina Weitz, Matteo Lavit Nicora, Dimitra Tsovaltzi, Elisabeth AndrÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08805">Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of collaborative robots (cobots) in industrial settings raises concerns about worker well-being, particularly due to reduced social interactions. Avatars - designed to facilitate worker interactions and engagement - are promising solutions to enhance the human-robot collaboration (HRC) experience. However, real-world perspectives on avatar-supported HRC remain unexplored. To address this gap, we conducted a focus group study with employees from a German manufacturing company that uses cobots. Before the discussion, participants engaged with a scripted, industry-like HRC demo in a lab setting. This qualitative approach provided valuable insights into the avatar's potential roles, improvements to its behavior, and practical considerations for deploying them in industrial workcells. Our findings also emphasize the importance of personalized communication and task assistance. Although our study's limitations restrict its generalizability, it serves as an initial step in recognizing the potential of adaptive, context-aware avatar interactions in real-world industrial environments.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2505.23397.pdf' target='_blank'>https://arxiv.org/pdf/2505.23397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Mohsin, Helge Janicke, Ahmed Ibrahim, Iqbal H. Sarker, Seyit Camtepe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23397">A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration. To address these limitations, we propose a novel autonomy tiered framework grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This enables adaptive and explainable AI integration across core SOC functions, including monitoring, protection, threat detection, alert triage, and incident response. The proposed framework differentiates itself from previous research by creating formal connections between autonomy, trust, and HITL across various SOC levels, which allows for adaptive task distribution according to operational complexity and associated risks. The framework is exemplified through a simulated cyber range that features the cybersecurity AI-Avatar, a fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing response coordination, and strategically calibrating trust. This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2504.13378.pdf' target='_blank'>https://arxiv.org/pdf/2504.13378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxiao Tu, Shuchang Ye, Hoijoon Jung, Jinman Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13378">SMPL-GPTexture: Dual-View 3D Human Texture Estimation using Text-to-Image Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-quality, photorealistic textures for 3D human avatars remains a fundamental yet challenging task in computer vision and multimedia field. However, real paired front and back images of human subjects are rarely available with privacy, ethical and cost of acquisition, which restricts scalability of the data. Additionally, learning priors from image inputs using deep generative models, such as GANs or diffusion models, to infer unseen regions such as the human back often leads to artifacts, structural inconsistencies, or loss of fine-grained detail. To address these issues, we present SMPL-GPTexture (skinned multi-person linear model - general purpose Texture), a novel pipeline that takes natural language prompts as input and leverages a state-of-the-art text-to-image generation model to produce paired high-resolution front and back images of a human subject as the starting point for texture estimation. Using the generated paired dual-view images, we first employ a human mesh recovery model to obtain a robust 2D-to-3D SMPL alignment between image pixels and the 3D model's UV coordinates for each views. Second, we use an inverted rasterization technique that explicitly projects the observed colour from the input images into the UV space, thereby producing accurate, complete texture maps. Finally, we apply a diffusion-based inpainting module to fill in the missing regions, and the fusion mechanism then combines these results into a unified full texture map. Extensive experiments shows that our SMPL-GPTexture can generate high resolution texture aligned with user's prompts.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2504.05265.pdf' target='_blank'>https://arxiv.org/pdf/2504.05265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>German Barquero, Nadine Bertsch, Manojkumar Marramreddy, Carlos ChacÃ³n, Filippo Arcadu, Ferran Rigual, Nicky Sijia He, Cristina Palmero, Sergio Escalera, Yuting Ye, Robin Kips
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05265">From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In extended reality (XR), generating full-body motion of the users is important to understand their actions, drive their virtual avatars for social interaction, and convey a realistic sense of presence. While prior works focused on spatially sparse and always-on input signals from motion controllers, many XR applications opt for vision-based hand tracking for reduced user friction and better immersion. Compared to controllers, hand tracking signals are less accurate and can even be missing for an extended period of time. To handle such unreliable inputs, we present Rolling Prediction Model (RPM), an online and real-time approach that generates smooth full-body motion from temporally and spatially sparse input signals. Our model generates 1) accurate motion that matches the inputs (i.e., tracking mode) and 2) plausible motion when inputs are missing (i.e., synthesis mode). More importantly, RPM generates seamless transitions from tracking to synthesis, and vice versa. To demonstrate the practical importance of handling noisy and missing inputs, we present GORP, the first dataset of realistic sparse inputs from a commercial virtual reality (VR) headset with paired high quality body motion ground truth. GORP provides >14 hours of VR gameplay data from 28 people using motion controllers (spatially sparse) and hand tracking (spatially and temporally sparse). We benchmark RPM against the state of the art on both synthetic data and GORP to highlight how we can bridge the gap for real-world applications with a realistic dataset and by handling unreliable input signals. Our code, pretrained models, and GORP dataset are available in the project webpage.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2503.00495.pdf' target='_blank'>https://arxiv.org/pdf/2503.00495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanchen Li, Jianyu Wang, Yuhao Cheng, Yikun Zeng, Xingyu Ren, Wenhan Zhu, Weiming Zhao, Yichao Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00495">Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant progress has been made for speech-driven 3D face animation, but most works focus on learning the motion of mesh/geometry, ignoring the impact of dynamic texture. In this work, we reveal that dynamic texture plays a key role in rendering high-fidelity talking avatars, and introduce a high-resolution 4D dataset \textbf{TexTalk4D}, consisting of 100 minutes of audio-synced scan-level meshes with detailed 8K dynamic textures from 100 subjects. Based on the dataset, we explore the inherent correlation between motion and texture, and propose a diffusion-based framework \textbf{TexTalker} to simultaneously generate facial motions and dynamic textures from speech. Furthermore, we propose a novel pivot-based style injection strategy to capture the complicity of different texture and motion styles, which allows disentangled control. TexTalker, as the first method to generate audio-synced facial motion with dynamic texture, not only outperforms the prior arts in synthesising facial motions, but also produces realistic textures that are consistent with the underlying facial movements. Project page: https://xuanchenli.github.io/TexTalk/.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2502.20323.pdf' target='_blank'>https://arxiv.org/pdf/2502.20323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuangeng Chu, Nabarun Goswami, Ziteng Cui, Hanqin Wang, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20323">ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D facial animation aims to generate realistic lip movements and facial expressions for 3D head models from arbitrary audio clips. Although existing diffusion-based methods are capable of producing natural motions, their slow generation speed limits their application potential. In this paper, we introduce a novel autoregressive model that achieves real-time generation of highly synchronized lip movements and realistic head poses and eye blinks by learning a mapping from speech to a multi-scale motion codebook. Furthermore, our model can adapt to unseen speaking styles, enabling the creation of 3D talking avatars with unique personal styles beyond the identities seen during training. Extensive evaluations and user studies demonstrate that our method outperforms existing approaches in lip synchronization accuracy and perceived quality.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2502.04976.pdf' target='_blank'>https://arxiv.org/pdf/2502.04976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Zhang, Zixiang Meng, Meng Luo, Hong Han, Lizi Liao, Erik Cambria, Hao Fei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04976">Towards Multimodal Empathetic Response Generation: A Rich Text-Speech-Vision Avatar-based Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empathetic Response Generation (ERG) is one of the key tasks of the affective computing area, which aims to produce emotionally nuanced and compassionate responses to user's queries. However, existing ERG research is predominantly confined to the singleton text modality, limiting its effectiveness since human emotions are inherently conveyed through multiple modalities. To combat this, we introduce an avatar-based Multimodal ERG (MERG) task, entailing rich text, speech, and facial vision information. We first present a large-scale high-quality benchmark dataset, \textbf{AvaMERG}, which extends traditional text ERG by incorporating authentic human speech audio and dynamic talking-face avatar videos, encompassing a diverse range of avatar profiles and broadly covering various topics of real-world scenarios. Further, we deliberately tailor a system, named \textbf{Empatheia}, for MERG. Built upon a Multimodal Large Language Model (MLLM) with multimodal encoder, speech and avatar generators, Empatheia performs end-to-end MERG, with Chain-of-Empathetic reasoning mechanism integrated for enhanced empathy understanding and reasoning. Finally, we devise a list of empathetic-enhanced tuning strategies, strengthening the capabilities of emotional accuracy and content, avatar-profile consistency across modalities. Experimental results on AvaMERG data demonstrate that Empatheia consistently shows superior performance than baseline methods on both textual ERG and MERG. Overall, this work is expected to pioneer the MERG research by introducing a novel benchmark and an end-to-end model, laying a solid foundation for future advancements in multimodal empathetic response generation.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2501.16557.pdf' target='_blank'>https://arxiv.org/pdf/2501.16557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Shi, Rahul Jain, Seungguen Chi, Hyungjun Doh, Hyunggun Chi, Alexander J. Quinn, Karthik Ramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16557">CARING-AI: Towards Authoring Context-aware Augmented Reality INstruction through Generative Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context-aware AR instruction enables adaptive and in-situ learning experiences. However, hardware limitations and expertise requirements constrain the creation of such instructions. With recent developments in Generative Artificial Intelligence (Gen-AI), current research tries to tackle these constraints by deploying AI-generated content (AIGC) in AR applications. However, our preliminary study with six AR practitioners revealed that the current AIGC lacks contextual information to adapt to varying application scenarios and is therefore limited in authoring. To utilize the strong generative power of GenAI to ease the authoring of AR instruction while capturing the context, we developed CARING-AI, an AR system to author context-aware humanoid-avatar-based instructions with GenAI. By navigating in the environment, users naturally provide contextual information to generate humanoid-avatar animation as AR instructions that blend in the context spatially and temporally. We showcased three application scenarios of CARING-AI: Asynchronous Instructions, Remote Instructions, and Ad Hoc Instructions based on a design space of AIGC in AR Instructions. With two user studies (N=12), we assessed the system usability of CARING-AI and demonstrated the easiness and effectiveness of authoring with Gen-AI.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2501.14726.pdf' target='_blank'>https://arxiv.org/pdf/2501.14726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Wang, Tomas Simon, Igor Santesteban, Timur Bagautdinov, Junxuan Li, Vasu Agrawal, Fabian Prada, Shoou-I Yu, Pace Nalbone, Matt Gramlich, Roman Lubachersky, Chenglei Wu, Javier Romero, Jason Saragih, Michael Zollhoefer, Andreas Geiger, Siyu Tang, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14726">Relightable Full-Body Gaussian Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for modeling relightable full-body avatars with fine-grained details including face and hands. The unique challenge for relighting full-body avatars lies in the large deformations caused by body articulation and the resulting impact on appearance caused by light transport. Changes in body pose can dramatically change the orientation of body surfaces with respect to lights, resulting in both local appearance changes due to changes in local light transport functions, as well as non-local changes due to occlusion between body parts. To address this, we decompose the light transport into local and non-local effects. Local appearance changes are modeled using learnable zonal harmonics for diffuse radiance transfer. Unlike spherical harmonics, zonal harmonics are highly efficient to rotate under articulation. This allows us to learn diffuse radiance transfer in a local coordinate frame, which disentangles the local radiance transfer from the articulation of the body. To account for non-local appearance changes, we introduce a shadow network that predicts shadows given precomputed incoming irradiance on a base mesh. This facilitates the learning of non-local shadowing between the body parts. Finally, we use a deferred shading approach to model specular radiance transfer and better capture reflections and highlights such as eye glints. We demonstrate that our approach successfully models both the local and non-local light transport required for relightable full-body avatars, with a superior generalization ability under novel illumination conditions and unseen poses.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2412.15171.pdf' target='_blank'>https://arxiv.org/pdf/2412.15171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Forrest Iandola, Stanislav Pidhorskyi, Igor Santesteban, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15171">SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian-based human avatars have achieved an unprecedented level of visual fidelity. However, existing approaches based on high-capacity neural networks typically require a desktop GPU to achieve real-time performance for a single avatar, and it remains non-trivial to animate and render such avatars on mobile devices including a standalone VR headset due to substantially limited memory and computational bandwidth. In this paper, we present SqueezeMe, a simple and highly effective framework to convert high-fidelity 3D Gaussian full-body avatars into a lightweight representation that supports both animation and rendering with mobile-grade compute. Our key observation is that the decoding of pose-dependent Gaussian attributes from a neural network creates non-negligible memory and computational overhead. Inspired by blendshapes and linear pose correctives widely used in Computer Graphics, we address this by distilling the pose correctives learned with neural networks into linear layers. Moreover, we further reduce the parameters by sharing the correctives among nearby Gaussians. Combining them with a custom splatting pipeline based on Vulkan, we achieve, for the first time, simultaneous animation and rendering of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset. Demo videos are available at https://forresti.github.io/squeezeme.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2412.13265.pdf' target='_blank'>https://arxiv.org/pdf/2412.13265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Ma, Ju Zhang, Lev Tankelevitch, Payod Panda, Torang Asadi, Charlie Hewitt, Lohit Petikam, James Clemoes, Marco Gillies, Xueni Pan, Sean Rintel, Marta Wilczkowiak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13265">Nods of Agreement: Webcam-Driven Avatars Improve Meeting Outcomes and Avatar Satisfaction Over Audio-Driven or Static Avatars in All-Avatar Work Videoconferencing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars are edging into mainstream videoconferencing, but evaluation of how avatar animation modalities contribute to work meeting outcomes has been limited. We report a within-group videoconferencing experiment in which 68 employees of a global technology company, in 16 groups, used the same stylized avatars in three modalities (static picture, audio-animation, and webcam-animation) to complete collaborative decision-making tasks. Quantitatively, for meeting outcomes, webcam-animated avatars improved meeting effectiveness over the picture modality and were also reported to be more comfortable and inclusive than both other modalities. In terms of avatar satisfaction, there was a similar preference for webcam animation as compared to both other modalities. Our qualitative analysis shows participants expressing a preference for the holistic motion of webcam animation, and that meaningful movement outweighs realism for meeting outcomes, as evidenced through a systematic overview of ten thematic factors. We discuss implications for research and commercial deployment and conclude that webcam-animated avatars are a plausible alternative to video in work meetings.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2412.10487.pdf' target='_blank'>https://arxiv.org/pdf/2412.10487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonello Ceravola, Frank Joublin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10487">HyperGraphOS: A Modern Meta-Operating System for the Scientific and Engineering Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents HyperGraphOS, a significant innovation in the domain of operating systems, specifically designed to address the needs of scientific and engineering domains. This platform aims to combine model-based engineering, graph modeling, data containers, and documents, along with tools for handling computational elements. HyperGraphOS functions as an Operating System offering to users an infinite workspace for creating and managing complex models represented as graphs with customizable semantics. By leveraging a web-based architecture, it requires only a modern web browser for access, allowing organization of knowledge, documents, and content into models represented in a network of workspaces. Elements of the workspace are defined in terms of domain-specific languages (DSLs). These DSLs are pivotal for navigating workspaces, generating code, triggering AI components, and organizing information and processes. The models' dual nature as both visual drawings and data structures allows dynamic modifications and inspections both interactively as well as programaticaly. We evaluated HyperGraphOS's efficiency and applicability across a large set of diverse domains, including the design and development of a virtual Avatar dialog system, a robotic task planner based on large language models (LLMs), a new meta-model for feature-based code development and many others. Our findings show that HyperGraphOS offers substantial benefits in the interaction with a computer as information system, as platoform for experiments and data analysis, as streamlined engineering processes, demonstrating enhanced flexibility in managing data, computation and documents, showing an innovative approaches to persistent desktop environments.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2412.10061.pdf' target='_blank'>https://arxiv.org/pdf/2412.10061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuur Stuyck, Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Aljaz Bozic, Nikolaos Sarafianos, Doug Roble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10061">Quaffure: Real-Time Quasi-Static Neural Hair Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic hair motion is crucial for high-quality avatars, but it is often limited by the computational resources available for real-time applications. To address this challenge, we propose a novel neural approach to predict physically plausible hair deformations that generalizes to various body poses, shapes, and hairstyles. Our model is trained using a self-supervised loss, eliminating the need for expensive data generation and storage. We demonstrate our method's effectiveness through numerous results across a wide range of pose and shape variations, showcasing its robust generalization capabilities and temporally smooth results. Our approach is highly suitable for real-time applications with an inference time of only a few milliseconds on consumer hardware and its ability to scale to predicting the drape of 1000 grooms in 0.3 seconds.
  Please see our project page here following https://tuurstuyck.github.io/quaffure/quaffure.html
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2412.04923.pdf' target='_blank'>https://arxiv.org/pdf/2412.04923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonello Ceravola, Frank Joublin, Ahmed R. Sadik, Bram Bolder, Juha-Pekka Tolvanen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04923">HyperGraphOS: A Meta Operating System for Science and Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents HyperGraphOS, an innovative Operating System designed for the scientific and engineering domains. It combines model based engineering, graph modeling, data containers, and computational tools, offering users a dynamic workspace for creating and managing complex models represented as customizable graphs. Using a web based architecture, HyperGraphOS requires only a modern browser to organize knowledge, documents, and content into interconnected models. Domain Specific Languages drive workspace navigation, code generation, AI integration, and process organization.The platform models function as both visual drawings and data structures, enabling dynamic modifications and inspection, both interactively and programmatically. HyperGraphOS was evaluated across various domains, including virtual avatars, robotic task planning using Large Language Models, and meta modeling for feature based code development. Results show significant improvements in flexibility, data management, computation, and document handling.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2410.24223.pdf' target='_blank'>https://arxiv.org/pdf/2410.24223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24223">URAvatar: Universal Relightable Gaussian Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new approach to creating photorealistic and relightable head avatars from a phone scan with unknown illumination. The reconstructed avatars can be animated and relit in real time with the global illumination of diverse environments. Unlike existing approaches that estimate parametric reflectance parameters via inverse rendering, our approach directly models learnable radiance transfer that incorporates global light transport in an efficient manner for real-time rendering. However, learning such a complex light transport that can generalize across identities is non-trivial. A phone scan in a single environment lacks sufficient information to infer how the head would appear in general environments. To address this, we build a universal relightable avatar model represented by 3D Gaussians. We train on hundreds of high-quality multi-view human scans with controllable point lights. High-resolution geometric guidance further enhances the reconstruction accuracy and generalization. Once trained, we finetune the pretrained model on a phone scan using inverse rendering to obtain a personalized relightable avatar. Our experiments establish the efficacy of our design, outperforming existing approaches while retaining real-time rendering capability.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2410.00441.pdf' target='_blank'>https://arxiv.org/pdf/2410.00441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyang Luo, Jenanan Vairavamurthy, Xiaoman Zhang, Abhinav Kumar, Ramon R. Ter-Oganesyan, Stuart T. Schroff, Dan Shilo, Rydhwana Hossain, Mike Moritz, Pranav Rajpurkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00441">ReXplain: Translating Radiology into Patient-Friendly Video Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiology reports, designed for efficient communication between medical experts, often remain incomprehensible to patients. This inaccessibility could potentially lead to anxiety, decreased engagement in treatment decisions, and poorer health outcomes, undermining patient-centered care. We present ReXplain (Radiology eXplanation), an innovative AI-driven system that translates radiology findings into patient-friendly video reports. ReXplain uniquely integrates a large language model for medical text simplification and text-anatomy association, an image segmentation model for anatomical region identification, and an avatar generation tool for engaging interface visualization. ReXplain enables producing comprehensive explanations with plain language, highlighted imagery, and 3D organ renderings in the form of video reports. To evaluate the utility of ReXplain-generated explanations, we conducted two rounds of user feedback collection from six board-certified radiologists. The results of this proof-of-concept study indicate that ReXplain could accurately deliver radiological information and effectively simulate one-on-one consultation, shedding light on enhancing patient-centered radiology with potential clinical usage. This work demonstrates a new paradigm in AI-assisted medical communication, potentially improving patient engagement and satisfaction in radiology care, and opens new avenues for research in multimodal medical communication.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2409.16990.pdf' target='_blank'>https://arxiv.org/pdf/2409.16990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqing Wang, Haosen Yang, Josef Kittler, Xiatian Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16990">Single Image, Any Face: Generalisable 3D Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of 3D human face avatars from a single unconstrained image is a fundamental task that underlies numerous real-world vision and graphics applications. Despite the significant progress made in generative models, existing methods are either less suited in design for human faces or fail to generalise from the restrictive training domain to unconstrained facial images. To address these limitations, we propose a novel model, Gen3D-Face, which generates 3D human faces with unconstrained single image input within a multi-view consistent diffusion framework. Given a specific input image, our model first produces multi-view images, followed by neural surface construction. To incorporate face geometry information in a generalisable manner, we utilise input-conditioned mesh estimation instead of ground-truth mesh along with synthetic multi-view training data. Importantly, we introduce a multi-view joint generation scheme to enhance appearance consistency among different views. To the best of our knowledge, this is the first attempt and benchmark for creating photorealistic 3D human face avatars from single images for generic human subject across domains. Extensive experiments demonstrate the superiority of our method over previous alternatives for out-of-domain singe image 3D face generation and top competition for in-domain setting.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2407.18380.pdf' target='_blank'>https://arxiv.org/pdf/2407.18380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark Roman Miller, Vivek Nair, Eugy Han, Cyan DeVeaux, Christian Rack, Rui Wang, Brandon Huang, Marc Erich Latoschik, James F. O'Brien, Jeremy N. Bailenson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18380">Effect of Duration and Delay on the Identifiability of VR Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social virtual reality is an emerging medium of communication. In this medium, a user's avatar (virtual representation) is controlled by the tracked motion of the user's headset and hand controllers. This tracked motion is a rich data stream that can leak characteristics of the user or can be effectively matched to previously-identified data to identify a user. To better understand the boundaries of motion data identifiability, we investigate how varying training data duration and train-test delay affects the accuracy at which a machine learning model can correctly classify user motion in a supervised learning task simulating re-identification. The dataset we use has a unique combination of a large number of participants, long duration per session, large number of sessions, and a long time span over which sessions were conducted. We find that training data duration and train-test delay affect identifiability; that minimal train-test delay leads to very high accuracy; and that train-test delay should be controlled in future experiments.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2406.15177.pdf' target='_blank'>https://arxiv.org/pdf/2406.15177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu, Erik Cambria
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15177">EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2406.06297.pdf' target='_blank'>https://arxiv.org/pdf/2406.06297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Grotta, Marco Coraggio, Antonio Spallone, Francesco De Lellis, Mario di Bernardo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06297">Learning-based cognitive architecture for enhancing coordination in human groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As interactions with autonomous agents-ranging from robots in physical settings to avatars in virtual and augmented realities-become more prevalent, developing advanced cognitive architectures is critical for enhancing the dynamics of human-avatar groups. This paper presents a reinforcement-learning-based cognitive architecture, trained via a sim-to-real approach, designed to improve synchronization in periodic motor tasks, crucial for applications in group rehabilitation and sports training. Extensive numerical validation consistently demonstrates improvements in synchronization. Theoretical derivations and numerical investigations are complemented by preliminary experiments with real participants, showing that our avatars can integrate seamlessly into human groups, often being indistinguishable from humans.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2405.19331.pdf' target='_blank'>https://arxiv.org/pdf/2405.19331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Giebenhain, Tobias Kirschstein, Martin RÃ¼nz, Lourdes Agapito, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19331">NPGA: Neural Parametric Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. For increased representational capacity of our avatars, we propose per-Gaussian latent features that condition each primitives dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2404.16053.pdf' target='_blank'>https://arxiv.org/pdf/2404.16053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Jacoby, Tianyi Zhang, Aanchan Mohan, Yvonne Coady
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16053">Human Latency Conversational Turns for Spoken Avatar Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A problem with many current Large Language Model (LLM) driven spoken dialogues is the response time. Some efforts such as Groq address this issue by lightning fast processing of the LLM, but we know from the cognitive psychology literature that in human-to-human dialogue often responses occur prior to the speaker completing their utterance. No amount of delay for LLM processing is acceptable if we wish to maintain human dialogue latencies. In this paper, we discuss methods for understanding an utterance in close to real time and generating a response so that the system can comply with human-level conversational turn delays. This means that the information content of the final part of the speaker's utterance is lost to the LLM. Using the Google NaturalQuestions (NQ) database, our results show GPT-4 can effectively fill in missing context from a dropped word at the end of a question over 60% of the time. We also provide some examples of utterances and the impacts of this information loss on the quality of LLM response in the context of an avatar that is currently under development. These results indicate that a simple classifier could be used to determine whether a question is semantically complete, or requires a filler phrase to allow a response to be generated within human dialogue time constraints.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2403.12477.pdf' target='_blank'>https://arxiv.org/pdf/2403.12477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Ishikawa, Kohei Konaka, Tomohiko Nakamura, Norihiro Takamune, Hiroshi Saruwatari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12477">Real-time Speech Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time speech extraction is an important challenge with various applications such as speech recognition in a human-like avatar/robot. In this paper, we propose the real-time extension of a speech extraction method based on independent low-rank matrix analysis (ILRMA) and rank-constrained spatial covariance matrix estimation (RCSCME). The RCSCME-based method is a multichannel blind speech extraction method that demonstrates superior speech extraction performance in diffuse noise environments. To improve the performance, we introduce spatial regularization into the ILRMA part of the RCSCME-based speech extraction and design two regularizers. Speech extraction experiments demonstrated that the proposed methods can function in real time and the designed regularizers improve the speech extraction performance.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2403.11453.pdf' target='_blank'>https://arxiv.org/pdf/2403.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongrui Cai, Yuting Xiao, Xuan Wang, Jiafei Li, Yudong Guo, Yanbo Fan, Shenghua Gao, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11453">Hybrid Explicit Representation for Ultra-Realistic Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel approach to creating ultra-realistic head avatars and rendering them in real-time (>30fps at $2048 \times 1334$ resolution). First, we propose a hybrid explicit representation that combines the advantages of two primitive-based efficient rendering techniques. UV-mapped 3D mesh is utilized to capture sharp and rich textures on smooth surfaces, while 3D Gaussian Splatting is employed to represent complex geometric structures. In the pipeline of modeling an avatar, after tracking parametric models based on captured multi-view RGB videos, our goal is to simultaneously optimize the texture and opacity map of mesh, as well as a set of 3D Gaussian splats localized and rigged onto the mesh facets. Specifically, we perform $Î±$-blending on the color and opacity values based on the merged and re-ordered z-buffer from the rasterization results of mesh and 3DGS. This process involves the mesh and 3DGS adaptively fitting the captured visual information to outline a high-fidelity digital avatar. To avoid artifacts caused by Gaussian splats crossing the mesh facets, we design a stable hybrid depth sorting strategy. Experiments illustrate that our modeled results exceed those of state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2403.06557.pdf' target='_blank'>https://arxiv.org/pdf/2403.06557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco De Lellis, Marco Coraggio, Nathan C. Foster, Riccardo Villa, Cristina Becchio, Mario di Bernardo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06557">Data-driven architecture to encode information in the kinematics of robots and artificial avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a data-driven control architecture for modifying the kinematics of robots and artificial avatars to encode specific information such as the presence or not of an emotion in the movements of an avatar or robot driven by a human operator. We validate our approach on an experimental dataset obtained during the reach-to-grasp phase of a pick-and-place task.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2401.15348.pdf' target='_blank'>https://arxiv.org/pdf/2401.15348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beijia Chen, Yuefan Shen, Qing Shuai, Xiaowei Zhou, Kun Zhou, Youyi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15348">AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent communities have seen significant progress in building photo-realistic animatable avatars from sparse multi-view videos. However, current workflows struggle to render realistic garment dynamics for loose-fitting characters as they predominantly rely on naked body models for human modeling while leaving the garment part un-modeled. This is mainly due to that the deformations yielded by loose garments are highly non-rigid, and capturing such deformations often requires dense views as supervision. In this paper, we introduce AniDress, a novel method for generating animatable human avatars in loose clothes using very sparse multi-view videos (4-8 in our setting). To allow the capturing and appearance learning of loose garments in such a situation, we employ a virtual bone-based garment rigging model obtained from physics-based simulation data. Such a model allows us to capture and render complex garment dynamics through a set of low-dimensional bone transformations. Technically, we develop a novel method for estimating temporal coherent garment dynamics from a sparse multi-view video. To build a realistic rendering for unseen garment status using coarse estimations, a pose-driven deformable neural radiance field conditioned on both body and garment motions is introduced, providing explicit control of both parts. At test time, the new garment poses can be captured from unseen situations, derived from a physics-based or neural network-based simulator to drive unseen garment dynamics. To evaluate our approach, we create a multi-view dataset that captures loose-dressed performers with diverse motions. Experiments show that our method is able to render natural garment dynamics that deviate highly from the body and generalize well to both unseen views and poses, surpassing the performance of existing methods. The code and data will be publicly available.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2401.05290.pdf' target='_blank'>https://arxiv.org/pdf/2401.05290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kris Hauser, Eleanor Watson, Joonbum Bae, Josh Bankston, Sven Behnke, Bill Borgia, Manuel G. Catalano, Stefano Dafarra, Jan B. F. van Erp, Thomas Ferris, Jeremy Fishel, Guy Hoffman, Serena Ivaldi, Fumio Kanehiro, Abderrahmane Kheddar, Gaelle Lannuzel, Jacqueline Ford Morie, Patrick Naughton, Steve NGuyen, Paul Oh, Taskin Padir, Jim Pippine, Jaeheung Park, Daniele Pucci, Jean Vaz, Peter Whitney, Peggy Wu, David Locke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05290">Analysis and Perspectives on the ANA Avatar XPRIZE Competition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ANA Avatar XPRIZE was a four-year competition to develop a robotic "avatar" system to allow a human operator to sense, communicate, and act in a remote environment as though physically present. The competition featured a unique requirement that judges would operate the avatars after less than one hour of training on the human-machine interfaces, and avatar systems were judged on both objective and subjective scoring metrics. This paper presents a unified summary and analysis of the competition from technical, judging, and organizational perspectives. We study the use of telerobotics technologies and innovations pursued by the competing teams in their avatar systems, and correlate the use of these technologies with judges' task performance and subjective survey ratings. It also summarizes perspectives from team leads, judges, and organizers about the competition's execution and impact to inform the future development of telerobotics and telepresence.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2312.09750.pdf' target='_blank'>https://arxiv.org/pdf/2312.09750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andre Rochow, Max Schwarz, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09750">Attention-Based VR Facial Animation with Visual Mouth Camera Guidance for Immersive Telepresence Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial animation in virtual reality environments is essential for applications that necessitate clear visibility of the user's face and the ability to convey emotional signals. In our scenario, we animate the face of an operator who controls a robotic Avatar system. The use of facial animation is particularly valuable when the perception of interacting with a specific individual, rather than just a robot, is intended. Purely keypoint-driven animation approaches struggle with the complexity of facial movements. We present a hybrid method that uses both keypoints and direct visual guidance from a mouth camera. Our method generalizes to unseen operators and requires only a quick enrolment step with capture of two short videos. Multiple source images are selected with the intention to cover different facial expressions. Given a mouth camera frame from the HMD, we dynamically construct the target keypoints and apply an attention mechanism to determine the importance of each source image. To resolve keypoint ambiguities and animate a broader range of mouth expressions, we propose to inject visual mouth camera information into the latent space. We enable training on large-scale speaking head datasets by simulating the mouth camera input with its perspective differences and facial deformations. Our method outperforms a baseline in quality, capability, and temporal consistency. In addition, we highlight how the facial animation contributed to our victory at the ANA Avatar XPRIZE Finals.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2312.03704.pdf' target='_blank'>https://arxiv.org/pdf/2312.03704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03704">Relightable Gaussian Codec Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2312.02069.pdf' target='_blank'>https://arxiv.org/pdf/2312.02069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02069">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2311.18635.pdf' target='_blank'>https://arxiv.org/pdf/2311.18635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Kirschstein, Simon Giebenhain, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18635">DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DiffusionAvatars synthesizes a high-fidelity 3D head avatar of a person, offering intuitive control over both pose and expression. We propose a diffusion-based neural renderer that leverages generic 2D priors to produce compelling images of faces. For coarse guidance of the expression and head pose, we render a neural parametric head model (NPHM) from the target viewpoint, which acts as a proxy geometry of the person. Additionally, to enhance the modeling of intricate facial expressions, we condition DiffusionAvatars directly on the expression codes obtained from NPHM via cross-attention. Finally, to synthesize consistent surface details across different viewpoints and expressions, we rig learnable spatial features to the head's surface via TriPlane lookup in NPHM's canonical space. We train DiffusionAvatars on RGB videos and corresponding fitted NPHM meshes of a person and test the obtained avatars in both self-reenactment and animation scenarios. Our experiments demonstrate that DiffusionAvatars generates temporally consistent and visually appealing videos for novel poses and expressions of a person, outperforming existing approaches.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2311.13655.pdf' target='_blank'>https://arxiv.org/pdf/2311.13655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Berna Kabadayi, Wojciech Zielonka, Bharat Lal Bhatnagar, Gerard Pons-Moll, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13655">GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital humans and, especially, 3D facial avatars have raised a lot of attention in the past years, as they are the backbone of several applications like immersive telepresence in AR or VR. Despite the progress, facial avatars reconstructed from commodity hardware are incomplete and miss out on parts of the side and back of the head, severely limiting the usability of the avatar. This limitation in prior work stems from their requirement of face tracking, which fails for profile and back views. To address this issue, we propose to learn person-specific animatable avatars from images without assuming to have access to precise facial expression tracking. At the core of our method, we leverage a 3D-aware generative model that is trained to reproduce the distribution of facial expressions from the training data. To train this appearance model, we only assume to have a collection of 2D images with the corresponding camera parameters. For controlling the model, we learn a mapping from 3DMM facial expression parameters to the latent space of the generative model. This mapping can be learned by sampling the latent space of the appearance model and reconstructing the facial parameters from a normalized frontal view, where facial expression estimation performs well. With this scheme, we decouple 3D appearance reconstruction and animation control to achieve high fidelity in image synthesis. In a series of experiments, we compare our proposed technique to state-of-the-art monocular methods and show superior quality while not requiring expression tracking of the training data.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2311.12194.pdf' target='_blank'>https://arxiv.org/pdf/2311.12194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Li, Hsiao-yu Chen, Egor Larionov, Nikolaos Sarafianos, Wojciech Matusik, Tuur Stuyck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12194">DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The realism of digital avatars is crucial in enabling telepresence applications with self-expression and customization. While physical simulations can produce realistic motions for clothed humans, they require high-quality garment assets with associated physical parameters for cloth simulations. However, manually creating these assets and calibrating their parameters is labor-intensive and requires specialized expertise. Current methods focus on reconstructing geometry, but don't generate complete assets for physics-based applications. To address this gap, we propose \papername,~a novel approach that performs body and garment co-optimization using differentiable simulation. By integrating physical simulation into the optimization loop and accounting for the complex nonlinear behavior of cloth and its intricate interaction with the body, our framework recovers body and garment geometry and extracts important material parameters in a physically plausible way. Our experiments demonstrate that our approach generates realistic clothing and body shape suitable for downstream applications. We provide additional insights and results on our webpage: https://people.csail.mit.edu/liyifei/publication/diffavatar/
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2309.01395.pdf' target='_blank'>https://arxiv.org/pdf/2309.01395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Cheng Wang, Tzu-Ting Yang, Hsin-Wei Wang, Bi-Cheng Yan, Berlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01395">AVATAR: Robust Voice Search Engine Leveraging Autoregressive Document Retrieval and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Voice, as input, has progressively become popular on mobiles and seems to transcend almost entirely text input. Through voice, the voice search (VS) system can provide a more natural way to meet user's information needs. However, errors from the automatic speech recognition (ASR) system can be catastrophic to the VS system. Building on the recent advanced lightweight autoregressive retrieval model, which has the potential to be deployed on mobiles, leading to a more secure and personal VS assistant. This paper presents a novel study of VS leveraging autoregressive retrieval and tackles the crucial problems facing VS, viz. the performance drop caused by ASR noise, via data augmentations and contrastive learning, showing how explicit and implicit modeling the noise patterns can alleviate the problems. A series of experiments conducted on the Open-Domain Question Answering (ODSQA) confirm our approach's effectiveness and robustness in relation to some strong baseline systems.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2304.12051.pdf' target='_blank'>https://arxiv.org/pdf/2304.12051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andre Rochow, Max Schwarz, Michael Schreiber, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12051">VR Facial Animation for Immersive Telepresence Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VR Facial Animation is necessary in applications requiring clear view of the face, even though a VR headset is worn. In our case, we aim to animate the face of an operator who is controlling our robotic avatar system. We propose a real-time capable pipeline with very fast adaptation for specific operators. In a quick enrollment step, we capture a sequence of source images from the operator without the VR headset which contain all the important operator-specific appearance information. During inference, we then use the operator keypoint information extracted from a mouth camera and two eye cameras to estimate the target expression and head pose, to which we map the appearance of a source still image. In order to enhance the mouth expression accuracy, we dynamically select an auxiliary expression frame from the captured sequence. This selection is done by learning to transform the current mouth keypoints into the source camera space, where the alignment can be determined accurately. We, furthermore, demonstrate an eye tracking pipeline that can be trained in less than a minute, a time efficient way to train the whole pipeline given a dataset that includes only complete faces, show exemplary results generated by our method, and discuss performance at the ANA Avatar XPRIZE semifinals.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2303.14471.pdf' target='_blank'>https://arxiv.org/pdf/2303.14471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Teotia, Mallikarjun B R, Xingang Pan, Hyeongwoo Kim, Pablo Garrido, Mohamed Elgharib, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14471">HQ3DAvatar: High Quality Controllable 3D Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view volumetric rendering techniques have recently shown great potential in modeling and synthesizing high-quality head avatars. A common approach to capture full head dynamic performances is to track the underlying geometry using a mesh-based template or 3D cube-based graphics primitives. While these model-based approaches achieve promising results, they often fail to learn complex geometric details such as the mouth interior, hair, and topological changes over time. This paper presents a novel approach to building highly photorealistic digital head avatars. Our method learns a canonical space via an implicit function parameterized by a neural network. It leverages multiresolution hash encoding in the learned feature space, allowing for high-quality, faster training and high-resolution rendering. At test time, our method is driven by a monocular RGB video. Here, an image encoder extracts face-specific features that also condition the learnable canonical space. This encourages deformation-dependent texture variations during training. We also propose a novel optical flow based loss that ensures correspondences in the learned canonical space, thus encouraging artifact-free and temporally consistent renderings. We show results on challenging facial expressions and show free-viewpoint renderings at interactive real-time rates for medium image resolutions. Our method outperforms all existing approaches, both visually and numerically. We will release our multiple-identity dataset to encourage further research. Our Project page is available at: https://vcai.mpi-inf.mpg.de/projects/HQ3DAvatar/
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2302.14717.pdf' target='_blank'>https://arxiv.org/pdf/2302.14717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoying Wei, Yizheng Gu, Emily Kuang, Xian Wang, Beiyan Cao, Xiaofu Jin, Mingming Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14717">Bridging the Generational Gap: Exploring How Virtual Reality Supports Remote Communication Between Grandparents and Grandchildren</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When living apart, grandparents and grandchildren often use audio-visual communication approaches to stay connected. However, these approaches seldom provide sufficient companionship and intimacy due to a lack of co-presence and spatial interaction, which can be fulfilled by immersive virtual reality (VR). To understand how grandparents and grandchildren might leverage VR to facilitate their remote communication and better inform future design, we conducted a user-centered participatory design study with twelve pairs of grandparents and grandchildren. Results show that VR affords casual and equal communication by reducing the generational gap, and promotes conversation by offering shared activities as bridges for connection. Participants preferred resemblant appearances on avatars for conveying well-being but created ideal selves for gaining playfulness. Based on the results, we contribute eight design implications that inform future VR-based grandparent-grandchild communications.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2302.04868.pdf' target='_blank'>https://arxiv.org/pdf/2302.04868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxuan Li, Shunsuke Saito, Tomas Simon, Stephen Lombardi, Hongdong Li, Jason Saragih
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04868">MEGANE: Morphable Eyeglass and Avatar Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eyeglasses play an important role in the perception of identity. Authentic virtual representations of faces can benefit greatly from their inclusion. However, modeling the geometric and appearance interactions of glasses and the face of virtual representations of humans is challenging. Glasses and faces affect each other's geometry at their contact points, and also induce appearance changes due to light transport. Most existing approaches do not capture these physical interactions since they model eyeglasses and faces independently. Others attempt to resolve interactions as a 2D image synthesis problem and suffer from view and temporal inconsistencies. In this work, we propose a 3D compositional morphable model of eyeglasses that accurately incorporates high-fidelity geometric and photometric interaction effects. To support the large variation in eyeglass topology efficiently, we employ a hybrid representation that combines surface geometry and a volumetric representation. Unlike volumetric approaches, our model naturally retains correspondences across glasses, and hence explicit modification of geometry, such as lens insertion and frame deformation, is greatly simplified. In addition, our model is relightable under point lights and natural illumination, supporting high-fidelity rendering of various frame materials, including translucent plastic and metal within a single morphable model. Importantly, our approach models global light transport effects, such as casting shadows between faces and glasses. Our morphable model for eyeglasses can also be fit to novel glasses via inverse rendering. We compare our approach to state-of-the-art methods and demonstrate significant quality improvements.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/1908.00379.pdf' target='_blank'>https://arxiv.org/pdf/1908.00379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Cmentowski, Andrey Krekhov, Jens KrÃ¼ger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1908.00379">Outstanding: A Multi-Perspective Travel Approach for Virtual Reality Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In virtual reality games, players dive into fictional environments and can experience a compelling and immersive world. State-of-the-art VR systems allow for natural and intuitive navigation through physical walking. However, the tracking space is still limited, and viable alternatives are required to reach further virtual destinations. Our work focuses on the exploration of vast open worlds - an area where existing local navigation approaches such as the arc-based teleport are not ideally suited and world-in-miniature techniques potentially reduce presence. We present a novel alternative for open environments: Our idea is to equip players with the ability to switch from first-person to a third-person bird's eye perspective on demand. From above, players can command their avatar and initiate travels over large distance. Our evaluation reveals a significant increase in spatial orientation while avoiding cybersickness and preserving presence, enjoyment, and competence. We summarize our findings in a set of comprehensive design guidelines to help developers integrate our technique.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2509.14739.pdf' target='_blank'>https://arxiv.org/pdf/2509.14739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlong Fan, Bingyu Hu, Xingguang Li, Yuxiang Yang, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14739">FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2509.12816.pdf' target='_blank'>https://arxiv.org/pdf/2509.12816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Axel Wiebe Werner, Jonas Beskow, Anna Deichler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12816">Gesture Evaluation in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gestures are central to human communication, enriching interactions through non-verbal expression. Virtual avatars increasingly use AI-generated gestures to enhance life-likeness, yet evaluations have largely been confined to 2D. Virtual Reality (VR) provides an immersive alternative that may affect how gestures are perceived. This paper presents a comparative evaluation of computer-generated gestures in VR and 2D, examining three models from the 2023 GENEA Challenge. Results show that gestures viewed in VR were rated slightly higher on average, with the strongest effect observed for motion-capture "true movement." While model rankings remained consistent across settings, VR influenced participants' overall perception and offered unique benefits over traditional 2D evaluation.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2508.19688.pdf' target='_blank'>https://arxiv.org/pdf/2508.19688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gangjian Zhang, Jian Shu, Nanjie Yao, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19688">SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular texture 3D human reconstruction aims to create a complete 3D digital avatar from just a single front-view human RGB image. However, the geometric ambiguity inherent in a single 2D image and the scarcity of 3D human training data are the main obstacles limiting progress in this field. To address these issues, current methods employ prior geometric estimation networks to derive various human geometric forms, such as the SMPL model and normal maps. However, they struggle to integrate these modalities effectively, leading to view inconsistencies, such as facial distortions. To this end, we propose a two-process 3D human reconstruction framework, SAT, which seamlessly learns various prior geometries in a unified manner and reconstructs high-quality textured 3D avatars as the final output. To further facilitate geometry learning, we introduce a Supervisor Feature Regularization module. By employing a multi-view network with the same structure to provide intermediate features as training supervision, these varied geometric priors can be better fused. To tackle data scarcity and further improve reconstruction quality, we also propose an Online Animation Augmentation module. By building a one-feed-forward animation network, we augment a massive number of samples from the original 3D human data online for model training. Extensive experiments on two benchmarks show the superiority of our approach compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2508.19320.pdf' target='_blank'>https://arxiv.org/pdf/2508.19320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, Xiaoqiang Liu, Pengfei Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19320">MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with heavy computational cost and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2506.12793.pdf' target='_blank'>https://arxiv.org/pdf/2506.12793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Shen, Gangjian Zhang, Jianfeng Zhang, Yu Feng, Nanjie Yao, Xuanmeng Zhang, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12793">SMPL Normal Map Is All You Need for Single-view Textured Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-view textured human reconstruction aims to reconstruct a clothed 3D digital human by inputting a monocular 2D image. Existing approaches include feed-forward methods, limited by scarce 3D human data, and diffusion-based methods, prone to erroneous 2D hallucinations. To address these issues, we propose a novel SMPL normal map Equipped 3D Human Reconstruction (SEHR) framework, integrating a pretrained large 3D reconstruction model with human geometry prior. SEHR performs single-view human reconstruction without using a preset diffusion model in one forward propagation. Concretely, SEHR consists of two key components: SMPL Normal Map Guidance (SNMG) and SMPL Normal Map Constraint (SNMC). SNMG incorporates SMPL normal maps into an auxiliary network to provide improved body shape guidance. SNMC enhances invisible body parts by constraining the model to predict an extra SMPL normal Gaussians. Extensive experiments on two benchmark datasets demonstrate that SEHR outperforms existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2505.22141.pdf' target='_blank'>https://arxiv.org/pdf/2505.22141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanwen Feng, Zhiyuan Ma, Yunan Li, Jiahao Yang, Junwei Jing, Qiguang Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22141">FaceEditTalker: Controllable Talking Head Generation with Facial Attribute Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is indispensable for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, flexible adjustment of visual attributes, such as hairstyle, accessories, and subtle facial features, is essential for aligning with user preferences, reflecting diverse brand identities and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method achieves comparable or superior performance to representative baseline methods in lip-sync accuracy, video quality, and attribute controllability. Project page: https://peterfanfan.github.io/FaceEditTalker/
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2505.18869.pdf' target='_blank'>https://arxiv.org/pdf/2505.18869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankan Dash, Jingyi Gu, Guiling Wang, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18869">Eye-See-You: Reverse Pass-Through VR and Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) headsets, while integral to the evolving digital ecosystem, present a critical challenge: the occlusion of users' eyes and portions of their faces, which hinders visual communication and may contribute to social isolation. To address this, we introduce RevAvatar, an innovative framework that leverages AI methodologies to enable reverse pass-through technology, fundamentally transforming VR headset design and interaction paradigms. RevAvatar integrates state-of-the-art generative models and multimodal AI techniques to reconstruct high-fidelity 2D facial images and generate accurate 3D head avatars from partially observed eye and lower-face regions. This framework represents a significant advancement in AI4Tech by enabling seamless interaction between virtual and physical environments, fostering immersive experiences such as VR meetings and social engagements. Additionally, we present VR-Face, a novel dataset comprising 200,000 samples designed to emulate diverse VR-specific conditions, including occlusions, lighting variations, and distortions. By addressing fundamental limitations in current VR systems, RevAvatar exemplifies the transformative synergy between AI and next-generation technologies, offering a robust platform for enhancing human connection and interaction in virtual environments.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2505.12009.pdf' target='_blank'>https://arxiv.org/pdf/2505.12009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Li, Guanggang Geng, Yeying Jin, Zhizhi Guo, Bruce Gu, Jidong Huo, Zhaoxin Fan, Wenjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12009">Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Expressive human pose and shape (EHPS) estimation is vital for digital human generation, particularly in live-streaming applications. However, most existing EHPS models focus primarily on minimizing estimation errors, with limited attention on potential security vulnerabilities. Current adversarial attacks on EHPS models often require white-box access (e.g., model details or gradients) or generate visually conspicuous perturbations, limiting their practicality and ability to expose real-world security threats. To address these limitations, we propose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA leverages the latent-space representations of natural images to generate an optimal adversarial noise pattern and iteratively refine its attack potency along an optimized direction in digital space. Crucially, this process relies solely on querying the model's output, requiring no internal knowledge of the EHPS architecture, while guiding the noise optimization toward greater stealth and effectiveness. Extensive experiments and visual analyses demonstrate the superiority of UBA. Notably, UBA increases the pose estimation errors of EHPS models by 17.27%-58.21% on average, revealing critical vulnerabilities. These findings underscore the urgent need to address and mitigate security risks associated with digital human generation systems.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2504.18215.pdf' target='_blank'>https://arxiv.org/pdf/2504.18215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nanjie Yao, Gangjian Zhang, Wenhao Shen, Jian Shu, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18215">Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : https://e2e3dgsrecon.github.io/e2e3dgsrecon/.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2504.17457.pdf' target='_blank'>https://arxiv.org/pdf/2504.17457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Li, Yeying Jin, Fan Shen, Zhi Liu, Weibin Chen, Pengju Zhang, Xiaomei Zhang, Boyu Chen, Michael Shen, Kejian Wu, Zhaoxin Fan, Jin Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17457">Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Expressive human pose and shape estimation (EHPS) is crucial for digital human generation, especially in applications like live streaming. While existing research primarily focuses on reducing estimation errors, it largely neglects robustness and security aspects, leaving these systems vulnerable to adversarial attacks. To address this significant challenge, we propose the \textbf{Tangible Attack (TBA)}, a novel framework designed to generate adversarial examples capable of effectively compromising any digital human generation model. Our approach introduces a \textbf{Dual Heterogeneous Noise Generator (DHNG)}, which leverages Variational Autoencoders (VAE) and ControlNet to produce diverse, targeted noise tailored to the original image features. Additionally, we design a custom \textbf{adversarial loss function} to optimize the noise, ensuring both high controllability and potent disruption. By iteratively refining the adversarial sample through multi-gradient signals from both the noise and the state-of-the-art EHPS model, TBA substantially improves the effectiveness of adversarial attacks. Extensive experiments demonstrate TBA's superiority, achieving a remarkable 41.0\% increase in estimation error, with an average improvement of approximately 17.0\%. These findings expose significant security vulnerabilities in current EHPS models and highlight the need for stronger defenses in digital human generation systems.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2504.15179.pdf' target='_blank'>https://arxiv.org/pdf/2504.15179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Yin, Mallikarjun B R, Chun-Han Yao, RafaÅ Mantiuk, Varun Jampani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15179">FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel framework for generating high-quality, animatable 4D avatar from a single image. While recent advances have shown promising results in 4D avatar creation, existing methods either require extensive multiview data or struggle with shape accuracy and identity consistency. To address these limitations, we propose a comprehensive system that leverages shape, image, and video priors to create full-view, animatable avatars. Our approach first obtains initial coarse shape through 3D-GAN inversion. Then, it enhances multiview textures using depth-guided warping signals for cross-view consistency with the help of the image diffusion model. To handle expression animation, we incorporate a video prior with synchronized driving signals across viewpoints. We further introduce a Consistent-Inconsistent training to effectively handle data inconsistencies during 4D reconstruction. Experimental results demonstrate that our method achieves superior quality compared to the prior art, while maintaining consistency across different viewpoints and expressions.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2503.02452.pdf' target='_blank'>https://arxiv.org/pdf/2503.02452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qipeng Yan, Mingyang Sun, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02452">2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem in computer vision and graphics. Over the past few years, the Neural Radiance Field (NeRF) has made significant progress in rendering quality but behaves poorly in run-time performance due to the low efficiency of volumetric rendering. Recently, methods based on 3D Gaussian Splatting (3DGS) have shown great potential in fast training and real-time rendering. However, they still suffer from artifacts caused by inaccurate geometry. To address these problems, we propose 2DGS-Avatar, a novel approach based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars with high-fidelity and fast training performance. Given monocular RGB videos as input, our method generates an avatar that can be driven by poses and rendered in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the advantages of fast training and rendering while also capturing detailed, dynamic, and photo-realistic appearances. We conduct abundant experiments on popular datasets such as AvatarRex and THuman4.0, demonstrating impressive performance in both qualitative and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2503.01610.pdf' target='_blank'>https://arxiv.org/pdf/2503.01610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Junxuan Li, Yash Kant, Yaser Sheikh, Shunsuke Saito, Chen Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01610">Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Vid2Avatar-Pro, a method to create photorealistic and animatable 3D human avatars from monocular in-the-wild videos. Building a high-quality avatar that supports animation with diverse poses from a monocular video is challenging because the observation of pose diversity and view points is inherently limited. The lack of pose variations typically leads to poor generalization to novel poses, and avatars can easily overfit to limited input view points, producing artifacts and distortions from other views. In this work, we address these limitations by leveraging a universal prior model (UPM) learned from a large corpus of multi-view clothed human performance capture data. We build our representation on top of expressive 3D Gaussians with canonical front and back maps shared across identities. Once the UPM is learned to accurately reproduce the large-scale multi-view human images, we fine-tune the model with an in-the-wild video via inverse rendering to obtain a personalized photorealistic human avatar that can be faithfully animated to novel human motions and rendered from novel views. The experiments show that our approach based on the learned universal prior sets a new state-of-the-art in monocular avatar reconstruction by substantially outperforming existing approaches relying only on heuristic regularization or a shape prior of minimally clothed bodies (e.g., SMPL) on publicly available datasets.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2502.20220.pdf' target='_blank'>https://arxiv.org/pdf/2502.20220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Kirschstein, Javier Romero, Artem Sevastopolsky, Matthias Nießner, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20220">Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditionally, creating photo-realistic 3D head avatars requires a studio-level multi-view capture setup and expensive optimization during test-time, limiting the use of digital human doubles to the VFX industry or offline renderings. To address this shortcoming, we present Avat3r, which regresses a high-quality and animatable 3D head avatar from just a few input images, vastly reducing compute requirements during inference. More specifically, we make Large Reconstruction Models animatable and learn a powerful prior over 3D human heads from a large multi-view video dataset. For better 3D head reconstructions, we employ position maps from DUSt3R and generalized feature maps from the human foundation model Sapiens. To animate the 3D head, our key discovery is that simple cross-attention to an expression code is already sufficient. Finally, we increase robustness by feeding input images with different expressions to our model during training, enabling the reconstruction of 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture with accidental movement, or frames from a monocular video. We compare Avat3r with current state-of-the-art methods for few-input and single-input scenarios, and find that our method has a competitive advantage in both tasks. Finally, we demonstrate the wide applicability of our proposed model, creating 3D head avatars from images of different sources, smartphone captures, single images, and even out-of-domain inputs like antique busts. Project website: https://tobias-kirschstein.github.io/avat3r/
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2502.18705.pdf' target='_blank'>https://arxiv.org/pdf/2502.18705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Fu, Samuel Schwamm, Amanda Baughan, Nicole M Powell, Zoe Kronberg, Alicia Owens, Emily Renee Izenman, Dania Alsabeh, Elizabeth Hunt, Michael Rich, David Bickham, Jenny Radesky, Alexis Hiniker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18705">Understanding Children's Avatar Making in Social Online Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social online games like Minecraft and Roblox have become increasingly integral to children's daily lives. Our study explores how children aged 8 to 13 create and customize avatars in these virtual environments. Through semi-structured interviews and gameplay observations with 48 participants, we investigate the motivations behind children's avatar-making. Our findings show that children's avatar creation is motivated by self-representation, experimenting with alter ego identities, fulfilling social needs, and improving in-game performance. In addition, designed monetization strategies play a role in shaping children's avatars. We identify the ''wardrobe effect,'' where children create multiple avatars but typically use only one favorite consistently. We discuss the impact of cultural consumerism and how social games can support children's identity exploration while balancing self-expression and social conformity. This work contributes to understanding how avatar shapes children's identity growth in social online games.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2502.06957.pdf' target='_blank'>https://arxiv.org/pdf/2502.06957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, Fernando De la Torre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06957">GAS: Generative Avatar Synthesis from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a unified and generalizable framework for synthesizing view-consistent and temporally coherent avatars from a single image, addressing the challenging task of single-image avatar generation. Existing diffusion-based methods often condition on sparse human templates (e.g., depth or normal maps), which leads to multi-view and temporal inconsistencies due to the mismatch between these signals and the true appearance of the subject. Our approach bridges this gap by combining the reconstruction power of regression-based 3D human reconstruction with the generative capabilities of a diffusion model. In a first step, an initial 3D reconstructed human through a generalized NeRF provides comprehensive conditioning, ensuring high-quality synthesis faithful to the reference appearance and structure. Subsequently, the derived geometry and appearance from the generalized NeRF serve as input to a video-based diffusion model. This strategic integration is pivotal for enforcing both multi-view and temporal consistency throughout the avatar's generation. Empirical results underscore the superior generalization ability of our proposed method, demonstrating its effectiveness across diverse in-domain and out-of-domain in-the-wild datasets.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2501.09978.pdf' target='_blank'>https://arxiv.org/pdf/2501.09978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyue Liu, Kunming Luo, Heng Li, Qi Zhang, Yuan Liu, Li Yi, Ping Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09978">GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GaussianAvatar-Editor, an innovative framework for text-driven editing of animatable Gaussian head avatars that can be fully controlled in expression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing animatable 4D Gaussian avatars presents challenges related to motion occlusion and spatial-temporal inconsistency. To address these issues, we propose the Weighted Alpha Blending Equation (WABE). This function enhances the blending weight of visible Gaussians while suppressing the influence on non-visible Gaussians, effectively handling motion occlusion during editing. Furthermore, to improve editing quality and ensure 4D consistency, we incorporate conditional adversarial learning into the editing process. This strategy helps to refine the edited results and maintain consistency throughout the animation. By integrating these methods, our GaussianAvatar-Editor achieves photorealistic and consistent results in animatable 4D Gaussian editing. We conduct comprehensive experiments across various subjects to validate the effectiveness of our proposed techniques, which demonstrates the superiority of our approach over existing methods. More results and code are available at: [Project Link](https://xiangyueliu.github.io/GaussianAvatar-Editor/).
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2501.01808.pdf' target='_blank'>https://arxiv.org/pdf/2501.01808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaize Liu, Wenzhang Sun, Donglin Di, Shibo Sun, Jiahui Yang, Changqing Zou, Hujun Bao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01808">MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of talking avatars has achieved significant advancements in precise audio synchronization. However, crafting lifelike talking head videos requires capturing a broad spectrum of emotions and subtle facial expressions. Current methods face fundamental challenges: a) the absence of frameworks for modeling single basic emotional expressions, which restricts the generation of complex emotions such as compound emotions; b) the lack of comprehensive datasets rich in human emotional expressions, which limits the potential of models. To address these challenges, we propose the following innovations: 1) the Mixture of Emotion Experts (MoEE) model, which decouples six fundamental emotions to enable the precise synthesis of both singular and compound emotional states; 2) the DH-FaceEmoVid-150 dataset, specifically curated to include six prevalent human emotional expressions as well as four types of compound emotions, thereby expanding the training potential of emotion-driven models. Furthermore, to enhance the flexibility of emotion control, we propose an emotion-to-latents module that leverages multimodal inputs, aligning diverse control signals-such as audio, text, and labels-to ensure more varied control inputs as well as the ability to control emotions using audio alone. Through extensive quantitative and qualitative evaluations, we demonstrate that the MoEE framework, in conjunction with the DH-FaceEmoVid-150 dataset, excels in generating complex emotional expressions and nuanced facial details, setting a new benchmark in the field. These datasets will be publicly released.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2410.03525.pdf' target='_blank'>https://arxiv.org/pdf/2410.03525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching Christie Pang, Yawei Zhao, Zhizhuo Yin, Jia Sun, Reza Hadi Mogavi, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03525">Artificial Human Lecturers: Initial Findings From Asia's First AI Lecturers in Class to Promote Innovation in Education</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, artificial intelligence (AI) has become increasingly integrated into education, reshaping traditional learning environments. Despite this, there has been limited investigation into fully operational artificial human lecturers. To the best of our knowledge, our paper presents the world's first study examining their deployment in a real-world educational setting. Specifically, we investigate the use of "digital teachers," AI-powered virtual lecturers, in a postgraduate course at the Hong Kong University of Science and Technology (HKUST). Our study explores how features such as appearance, non-verbal cues, voice, and verbal expression impact students' learning experiences. Findings suggest that students highly value naturalness, authenticity, and interactivity in digital teachers, highlighting areas for improvement, such as increased responsiveness, personalized avatars, and integration with larger learning platforms. We conclude that digital teachers have significant potential to enhance education by providing a more flexible, engaging, personalized, and accessible learning experience for students.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2408.03694.pdf' target='_blank'>https://arxiv.org/pdf/2408.03694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emna Baccour, Aiman Erbad, Amr Mohamed, Mounir Hamdi, Mohsen Guizani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03694">A Blockchain-based Reliable Federated Meta-learning for Metaverse: A Dual Game Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The metaverse, envisioned as the next digital frontier for avatar-based virtual interaction, involves high-performance models. In this dynamic environment, users' tasks frequently shift, requiring fast model personalization despite limited data. This evolution consumes extensive resources and requires vast data volumes. To address this, meta-learning emerges as an invaluable tool for metaverse users, with federated meta-learning (FML), offering even more tailored solutions owing to its adaptive capabilities. However, the metaverse is characterized by users heterogeneity with diverse data structures, varied tasks, and uneven sample sizes, potentially undermining global training outcomes due to statistical difference. Given this, an urgent need arises for smart coalition formation that accounts for these disparities. This paper introduces a dual game-theoretic framework for metaverse services involving meta-learners as workers to manage FML. A blockchain-based cooperative coalition formation game is crafted, grounded on a reputation metric, user similarity, and incentives. We also introduce a novel reputation system based on users' historical contributions and potential contributions to present tasks, leveraging correlations between past and new tasks. Finally, a Stackelberg game-based incentive mechanism is presented to attract reliable workers to participate in meta-learning, minimizing users' energy costs, increasing payoffs, boosting FML efficacy, and improving metaverse utility. Results show that our dual game framework outperforms best-effort, random, and non-uniform clustering schemes - improving training performance by up to 10%, cutting completion times by as much as 30%, enhancing metaverse utility by more than 25%, and offering up to 5% boost in training efficiency over non-blockchain systems, effectively countering misbehaving users.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2406.16737.pdf' target='_blank'>https://arxiv.org/pdf/2406.16737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shota Inoue, Hailong Liu, Takahiro Wada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16737">A Digital Human Model for Symptom Progression of Vestibular Motion Sickness based on Subjective Vertical Conflict Theory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital human models of motion sickness have been actively developed, among which models based on subjective vertical conflict (SVC) theory are the most actively studied. These models facilitate the prediction of motion sickness in various scenarios such as riding in a car. Most SVC theory models predict the motion sickness incidence (MSI), which is defined as the percentage of people who would vomit with the given specific motion stimulus. However, no model has been developed to describe milder forms of discomfort or specific symptoms of motion sickness, even though predicting milder symptoms is important for applications in automobiles and daily use vehicles. Therefore, the purpose of this study was to build a computational model of symptom progression of vestibular motion sickness based on SVC theory. We focused on a model of vestibular motion sickness with six degrees-of-freedom (6DoF) head motions. The model was developed by updating the output part of the state-of-the-art SVC model, termed the 6DoF-SVC (IN1) model, from MSI to the MIsery SCale (MISC), which is a subjective rating scale for symptom progression. We conducted an experiment to measure the progression of motion sickness during a straight fore-aft motion. It was demonstrated that our proposed method, with the parameters of the output parts optimized by the experimental results, fits well with the observed MISC.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2406.07516.pdf' target='_blank'>https://arxiv.org/pdf/2406.07516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Kolotouros, Thiemo Alldieck, Enric Corona, Eduard Gabriel Bazavan, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07516">Instant 3D Human Avatar Generation using Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape. The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network. We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs. We fine-tune latent diffusion networks with additional image conditioning for image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses. Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting. In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals. Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup wrt the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls. AvatarPopUp enables applications that require the controlled 3D generation of human avatars at scale. The project website can be found at https://www.nikoskolot.com/avatarpopup/.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2406.00637.pdf' target='_blank'>https://arxiv.org/pdf/2406.00637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00637">Representing Animatable Avatar via Factorized Neural Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results can be factorized into a pose-independent component and a corresponding pose-dependent equivalent to facilitate frame consistency. Pose adaptive textures can be further improved by restricting frequency bands of these two components. In detail, pose-independent outputs are expected to be low-frequency, while highfrequency information is linked to pose-dependent factors. We achieve a coherent preservation of both coarse body contours across the entire input video and finegrained texture features that are time variant with a dual-branch network with distinct frequency components. The first branch takes coordinates in canonical space as input, while the second branch additionally considers features outputted by the first branch and pose information of each frame. Our network integrates the information predicted by both branches and utilizes volume rendering to generate photo-realistic 3D human images. Through experiments, we demonstrate that our network surpasses the neural radiance fields (NeRF) based state-of-the-art methods in preserving high-frequency details and ensuring consistent body contours.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2405.13955.pdf' target='_blank'>https://arxiv.org/pdf/2405.13955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13955">Decoding Brain Dynamics in Motor Planning Based on EEG Microstates for Predicting Pedestrian Road-Crossing in Vehicle-to-Everything Architectures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pedestrians who cross roads, often emerge from occlusion or abruptly begin crossing from a standstill, frequently leading to unintended collisions with vehicular traffic that result in accidents and interruptions. Existing studies have predominantly relied on external network sensing and observational data to anticipate pedestrian motion. However, these methods are post hoc, reducing the vehicles' ability to respond in a timely manner. This study addresses these gaps by introducing a novel data stream and analytical framework derived from pedestrians' wearable electroencephalogram (EEG) signals to predict motor planning in road crossings. Experiments were conducted where participants were embodied in a visual avatar as pedestrians and interacted with varying traffic volumes, marked crosswalks, and traffic signals. To understand how human cognitive modules flexibly interplay with hemispheric asymmetries in functional specialization, we analyzed time-frequency representation and functional connectivity using collected EEG signals and constructed a Gaussian Hidden Markov Model to decompose EEG sequences into cognitive microstate transitions based on posterior probabilistic reasoning. Subsequently, datasets were constructed using a sliding window approach, and motor readiness was predicted using the K-nearest Neighbors algorithm combined with Dynamic Time Warping. Results showed that high-beta oscillations in the frontocentral cortex achieved an Area Under the Curve of 0.91 with approximately a 1-second anticipatory lead window before physical road crossing movement occurred. These preliminary results signify a transformative shift towards pedestrians proactively signaling their motor intentions to autonomous vehicles within intelligent V2X systems. The proposed framework is also adaptable to various human-robot interactions, enabling seamless collaboration in dynamic mobile environments.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2404.07403.pdf' target='_blank'>https://arxiv.org/pdf/2404.07403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark Armstrong, Chi-Lan Yang, Kinga Skiers, Mengzhen Lim, Tamil Selvan Gunasekaran, Ziyue Wang, Takuji Narumi, Kouta Minamizawa, Yun Suen Pai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07403">SealMates: Supporting Communication in Video Conferencing using a Collective Behavior-Driven Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The limited nonverbal cues and spatially distributed nature of remote communication make it challenging for unacquainted members to be expressive during social interactions over video conferencing. Though it enables seeing others' facial expressions, the visual feedback can instead lead to unexpected self-focus, resulting in users missing cues for others to engage in the conversation equally. To support expressive communication and equal participation among unacquainted counterparts, we propose SealMates, a behavior-driven avatar in which the avatar infers the engagement level of the group based on collective gaze and speech patterns and then moves across interlocutors' windows in the video conferencing. By conducting a controlled experiment with 15 groups of triads, we found the avatar's movement encouraged people to experience more self-disclosure and made them perceive everyone was equally engaged in the conversation than when there was no behavior-driven avatar. We discuss how a behavior-driven avatar influences distributed members' perceptions and the implications of avatar-mediated communication for future platforms.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2404.04037.pdf' target='_blank'>https://arxiv.org/pdf/2404.04037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayin Zhu, Linlin Yang, Angela Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04037">InstructHumans: Editing Animated 3D Human Textures with Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present InstructHumans, a novel framework for instruction-driven {animatable} 3D human texture editing. Existing text-based 3D editing methods often directly apply Score Distillation Sampling (SDS). SDS, designed for generation tasks, cannot account for the defining requirement of editing -- maintaining consistency with the source avatar. This work shows that naively using SDS harms editing, as it may destroy consistency. We propose a modified SDS for Editing (SDS-E) that selectively incorporates subterms of SDS across diffusion timesteps. We further enhance SDS-E with spatial smoothness regularization and gradient-based viewpoint sampling for edits with sharp and high-fidelity detailing. Incorporating SDS-E into a 3D human texture editing framework allows us to outperform existing 3D editing methods. Our avatars faithfully reflect the textual edits while remaining consistent with the original avatars. Project page: https://jyzhu.top/instruct-humans/.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2404.00485.pdf' target='_blank'>https://arxiv.org/pdf/2404.00485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akash Sengupta, Thiemo Alldieck, Nikos Kolotouros, Enric Corona, Andrei Zanfir, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00485">DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. Despite the ill-posed nature of this problem, most methods are deterministic and output a single solution, often resulting in a lack of geometric detail and blurriness in unseen or uncertain regions. In contrast, DiffHuman predicts a probability distribution over 3D reconstructions conditioned on an input 2D image, which allows us to sample multiple detailed 3D avatars that are consistent with the image. DiffHuman is implemented as a conditional diffusion model that denoises pixel-aligned 2D observations of an underlying 3D shape representation. During inference, we may sample 3D avatars by iteratively denoising 2D renders of the predicted 3D representation. Furthermore, we introduce a generator neural network that approximates rendering with considerably reduced runtime (55x speed up), resulting in a novel dual-branch diffusion framework. Our experiments show that DiffHuman can produce diverse and detailed reconstructions for the parts of the person that are unseen or uncertain in the input image, while remaining competitive with the state-of-the-art when reconstructing visible surfaces.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2403.08764.pdf' target='_blank'>https://arxiv.org/pdf/2403.08764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08764">VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2402.17292.pdf' target='_blank'>https://arxiv.org/pdf/2402.17292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17292">DivAvatar: Diverse 3D Avatar Generation with a Single Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Avatar generation has recently made significant strides due to advancements in diffusion models. However, most existing work remains constrained by limited diversity, producing avatars with subtle differences in appearance for a given text prompt. We design DivAvatar, a novel framework that generates diverse avatars, empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text prompt. Different from most existing work that exploits scene-specific 3D representations such as NeRF, DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss, the former producing appearances of high textual fidelity by separate fine-tuning of specific body parts and the latter improving geometry quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2401.01173.pdf' target='_blank'>https://arxiv.org/pdf/2401.01173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01173">En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present En3D, an enhanced generative scheme for sculpting high-quality 3D human avatars. Unlike previous works that rely on scarce 3D datasets or limited 2D collections with imbalanced viewing angles and imprecise pose priors, our approach aims to develop a zero-shot 3D generative scheme capable of producing visually realistic, geometrically accurate and content-wise diverse 3D humans without relying on pre-existing 3D or 2D assets. To address this challenge, we introduce a meticulously crafted workflow that implements accurate physical modeling to learn the enhanced 3D generative model from synthetic 2D data. During inference, we integrate optimization modules to bridge the gap between realistic appearances and coarse 3D shapes. Specifically, En3D comprises three modules: a 3D generator that accurately models generalizable 3D humans with realistic appearance from synthesized balanced, diverse, and structured human images; a geometry sculptor that enhances shape quality using multi-view normal constraints for intricate human anatomy; and a texturing module that disentangles explicit texture maps with fidelity and editability, leveraging semantical UV partitioning and a differentiable rasterizer. Experimental results show that our approach significantly outperforms prior works in terms of image quality, geometry accuracy and content diversity. We also showcase the applicability of our generated avatars for animation and editing, as well as the scalability of our approach for content-style free adaptation.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2312.16842.pdf' target='_blank'>https://arxiv.org/pdf/2312.16842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hansol Lee, Junuk Cha, Yunhoe Ku, Jae Shin Yoon, Seungryul Baek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16842">Dynamic Appearance Modeling of Clothed 3D Human Avatars using a Single Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The appearance of a human in clothing is driven not only by the pose but also by its temporal context, i.e., motion. However, such context has been largely neglected by existing monocular human modeling methods whose neural networks often struggle to learn a video of a person with large dynamics due to the motion ambiguity, i.e., there exist numerous geometric configurations of clothes that are dependent on the context of motion even for the same pose. In this paper, we introduce a method for high-quality modeling of clothed 3D human avatars using a video of a person with dynamic movements. The main challenge comes from the lack of 3D ground truth data of geometry and its temporal correspondences. We address this challenge by introducing a novel compositional human modeling framework that takes advantage of both explicit and implicit human modeling. For explicit modeling, a neural network learns to generate point-wise shape residuals and appearance features of a 3D body model by comparing its 2D rendering results and the original images. This explicit model allows for the reconstruction of discriminative 3D motion features from UV space by encoding their temporal correspondences. For implicit modeling, an implicit network combines the appearance and 3D motion features to decode high-fidelity clothed 3D human avatars with motion-dependent geometry and texture. The experiments show that our method can generate a large variation of secondary motion in a physically plausible way.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2312.16837.pdf' target='_blank'>https://arxiv.org/pdf/2312.16837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, Xuansong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16837">DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaptation by Combining 3D GANs and Diffusion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-guided domain adaptation and generation of 3D-aware portraits find many applications in various fields. However, due to the lack of training data and the challenges in handling the high variety of geometry and appearance, the existing methods for these tasks suffer from issues like inflexibility, instability, and low fidelity. In this paper, we propose a novel framework DiffusionGAN3D, which boosts text-guided 3D domain adaptation and generation by combining 3D GANs and diffusion priors. Specifically, we integrate the pre-trained 3D generative models (e.g., EG3D) and text-to-image diffusion models. The former provides a strong foundation for stable and high-quality avatar generation from text. And the diffusion models in turn offer powerful priors and guide the 3D generator finetuning with informative direction to achieve flexible and efficient text-guided domain adaptation. To enhance the diversity in domain adaptation and the generation capability in text-to-avatar, we introduce the relative distance loss and case-specific learnable triplane respectively. Besides, we design a progressive texture refinement module to improve the texture quality for both tasks above. Extensive experiments demonstrate that the proposed framework achieves excellent results in both domain adaptation and text-to-avatar tasks, outperforming existing methods in terms of generation quality and efficiency. The project homepage is at https://younglbw.github.io/DiffusionGAN3D-homepage/.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2312.04465.pdf' target='_blank'>https://arxiv.org/pdf/2312.04465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stathis Galanakis, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04465">FitDiff: Robust monocular 3D facial shape and reflectance estimation using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable progress in 3D face reconstruction has resulted in high-detail and photorealistic facial representations. Recently, Diffusion Models have revolutionized the capabilities of generative methods by surpassing the performance of GANs. In this work, we present FitDiff, a diffusion-based 3D facial avatar generative model. Leveraging diffusion principles, our model accurately generates relightable facial avatars, utilizing an identity embedding extracted from an "in-the-wild" 2D facial image. The introduced multi-modal diffusion model is the first to concurrently output facial reflectance maps (diffuse and specular albedo and normals) and shapes, showcasing great generalization capabilities. It is solely trained on an annotated subset of a public facial dataset, paired with 3D reconstructions. We revisit the typical 3D facial fitting approach by guiding a reverse diffusion process using perceptual and face recognition losses. Being the first 3D LDM conditioned on face recognition embeddings, FitDiff reconstructs relightable human avatars, that can be used as-is in common rendering engines, starting only from an unconstrained facial image, and achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2311.16610.pdf' target='_blank'>https://arxiv.org/pdf/2311.16610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Suen Pai, Mark Armstrong, Kinga Skiers, Anish Kundu, Danyang Peng, Yixin Wang, Tamil Selvan Gunasekaran, Chi-Lan Yang, Kouta Minamizawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16610">The Empathic Metaverse: An Assistive Bioresponsive Platform For Emotional Experience Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Metaverse is poised to be a future platform that redefines what it means to communicate, socialize, and interact with each other. Yet, it is important for us to consider avoiding the pitfalls of social media platforms we use today; cyberbullying, lack of transparency and an overall false mental model of society. In this paper, we propose the Empathic Metaverse, a virtual platform that prioritizes emotional sharing for assistance. It aims to cultivate prosocial behaviour, either egoistically or altruistically, so that our future society can better feel for each other and assist one another. To achieve this, we propose the platform to be bioresponsive; it reacts and adapts to an individual's physiological and cognitive state and reflects this via carefully designed avatars, environments, and interactions. We explore this concept in terms of three research directions: bioresponsive avatars, mediated communications and assistive tools.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2311.08581.pdf' target='_blank'>https://arxiv.org/pdf/2311.08581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael ZollhÃ¶fer, Justus Thies, Javier Romero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08581">Drivable 3D Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Drivable 3D Gaussian Avatars (D3GA), a multi-layered 3D controllable model for human bodies that utilizes 3D Gaussian primitives embedded into tetrahedral cages. The advantage of using cages compared to commonly employed linear blend skinning (LBS) is that primitives like 3D Gaussians are naturally re-oriented and their kernels are stretched via the deformation gradients of the encapsulating tetrahedron. Additional offsets are modeled for the tetrahedron vertices, effectively decoupling the low-dimensional driving poses from the extensive set of primitives to be rendered. This separation is achieved through the localized influence of each tetrahedron on 3D Gaussians, resulting in improved optimization. Using the cage-based deformation model, we introduce a compositional pipeline that decomposes an avatar into layers, such as garments, hands, or faces, improving the modeling of phenomena like garment sliding. These parts can be conditioned on different driving signals, such as keypoints for facial expressions or joint-angle vectors for garments and the body. Our experiments on two multi-view datasets with varied body shapes, clothes, and motions show higher-quality results. They surpass PSNR and SSIM metrics of other SOTA methods using the same data while offering greater flexibility and compactness.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2308.11951.pdf' target='_blank'>https://arxiv.org/pdf/2308.11951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunjin Song, Bastian Wandt, Helge Rhodin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11951">Pose Modulated Avatars from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2308.09716.pdf' target='_blank'>https://arxiv.org/pdf/2308.09716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumik Mukhopadhyay, Saksham Suri, Ravi Teja Gadde, Abhinav Shrivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09716">Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of lip synchronization (lip-sync) seeks to match the lips of human faces with different audio. It has various applications in the film industry as well as for creating virtual avatars and for video conferencing. This is a challenging problem as one needs to simultaneously introduce detailed, realistic lip movements while preserving the identity, pose, emotions, and image quality. Many of the previous methods trying to solve this problem suffer from image quality degradation due to a lack of complete contextual information. In this paper, we present Diff2Lip, an audio-conditioned diffusion-based model which is able to do lip synchronization in-the-wild while preserving these qualities. We train our model on Voxceleb2, a video dataset containing in-the-wild talking face videos. Extensive studies show that our method outperforms popular methods like Wav2Lip and PC-AVS in FrÃ©chet inception distance (FID) metric and Mean Opinion Scores (MOS) of the users. We show results on both reconstruction (same audio-video inputs) as well as cross (different audio-video inputs) settings on Voxceleb2 and LRW datasets. Video results and code can be accessed from our project page ( https://soumik-kanad.github.io/diff2lip ).
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2306.17278.pdf' target='_blank'>https://arxiv.org/pdf/2306.17278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vuthea Chheang, Shayla Sharmin, Rommy Marquez-Hernandez, Megha Patel, Danush Rajasekaran, Gavin Caulfield, Behdokht Kiafar, Jicheng Li, Pinar Kullu, Roghayeh Leila Barmaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17278">Towards Anatomy Education with Generative AI-based Virtual Assistants in Immersive Virtual Reality Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual reality (VR) and interactive 3D visualization systems have enhanced educational experiences and environments, particularly in complicated subjects such as anatomy education. VR-based systems surpass the potential limitations of traditional training approaches in facilitating interactive engagement among students. However, research on embodied virtual assistants that leverage generative artificial intelligence (AI) and verbal communication in the anatomy education context is underrepresented. In this work, we introduce a VR environment with a generative AI-embodied virtual assistant to support participants in responding to varying cognitive complexity anatomy questions and enable verbal communication. We assessed the technical efficacy and usability of the proposed environment in a pilot user study with 16 participants. We conducted a within-subject design for virtual assistant configuration (avatar- and screen-based), with two levels of cognitive complexity (knowledge- and analysis-based). The results reveal a significant difference in the scores obtained from knowledge- and analysis-based questions in relation to avatar configuration. Moreover, results provide insights into usability, cognitive task load, and the sense of presence in the proposed virtual assistant configurations. Our environment and results of the pilot study offer potential benefits and future research directions beyond medical education, using generative AI and embodied virtual agents as customized virtual conversational assistants.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2306.09329.pdf' target='_blank'>https://arxiv.org/pdf/2306.09329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09329">DreamHuman: Animatable 3D Avatars from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamHuman, a method to generate realistic animatable 3D human avatar models solely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than animated 3D human models, and anthropometric consistency for complex structures like people remains a challenge. DreamHuman connects large text-to-image synthesis models, neural radiance fields, and statistical human body models in a novel modeling and optimization framework. This makes it possible to generate dynamic 3D human avatars with high-quality textures and learned, instance-specific, surface deformations. We demonstrate that our method is capable to generate a wide variety of animatable, realistic 3D human models from text. Our 3D models have diverse appearance, clothing, skin tones and body shapes, and significantly outperform both generic text-to-3D approaches and previous text-based 3D avatar generators in visual fidelity. For more results and animations please check our website at https://dream-human.github.io.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2306.01096.pdf' target='_blank'>https://arxiv.org/pdf/2306.01096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Guan, Alexis Morris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01096">Extended-XRI Body Interfaces for Hyper-Connected Metaverse Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid mixed-reality (XR) internet-of-things (IoT) research, here called XRI, aims at a strong integration between physical and virtual objects, environments, and agents wherein IoT-enabled edge devices are deployed for sensing, context understanding, networked communication and control of device actuators. Likewise, as augmented reality systems provide an immersive overlay on the environments, and virtual reality provides fully immersive environments, the merger of these domains leads to immersive smart spaces that are hyper-connected, adaptive and dynamic components that anchor the metaverse to real-world constructs. Enabling the human-in-the-loop to remain engaged and connected across these virtual-physical hybrid environments requires advances in user interaction that are multi-dimensional. This work investigates the potential to transition the user interface to the human body as an extended-reality avatar with hybrid extended-body interfaces that can interact both with the physical and virtual sides of the metaverse. It contributes: i) an overview of metaverses, XRI, and avatarization concepts, ii) a taxonomy landscape for extended XRI body interfaces, iii) an architecture and potential interactions for XRI body designs, iv) a prototype XRI body implementation based on the architecture, v) a design-science evaluation, toward enabling future design research directions.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2305.11960.pdf' target='_blank'>https://arxiv.org/pdf/2305.11960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis Morris, Jie Guan, Nadine Lessio, Yiyi Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11960">Toward Mixed Reality Hybrid Objects with IoT Avatar Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The internet-of-things (IoT) refers to the growing field of interconnected pervasive computing devices and the networking that supports smart, embedded applications. The IoT has multiple human-computer interaction challenges due to its many formats and interlinked components, and central to these is the need to provide sensory information and situational context pertaining to users in a more human-friendly, easily understandable format. This work addresses this by applying mixed reality toward expressing the underlying behaviors and states internal to IoT devices and IoT-enabled objects. It extends the authors' previous research on IoT Avatars (mixed reality character representations of physical IoT devices), presenting a new head-mounted display framework and interconnection architecture. This contributes i) an exploration of mixed reality for smart spaces, ii) an approach toward expressive avatar behaviors using fuzzy inference, and iii) an early functional prototype of a hybrid physical and mixed reality IoT-enabled object. This approach is a step toward new information presentation, interaction, and engagement capabilities for smart devices and environments.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2304.09423.pdf' target='_blank'>https://arxiv.org/pdf/2304.09423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yang, Hong Shang, Tianyang Shi, Xinghan Chen, Jingkai Zhou, Zhongqian Sun, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09423">ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The research fields of parametric face model and 3D face reconstruction have been extensively studied. However, a critical question remains unanswered: how to tailor the face model for specific reconstruction settings. We argue that reconstruction with multi-view uncalibrated images demands a new model with stronger capacity. Our study shifts attention from data-dependent 3D Morphable Models (3DMM) to an understudied human-designed skinning model. We propose Adaptive Skinning Model (ASM), which redefines the skinning model with more compact and fully tunable parameters. With extensive experiments, we demonstrate that ASM achieves significantly improved capacity than 3DMM, with the additional advantage of model size and easy implementation for new topology. We achieve state-of-the-art performance with ASM for multi-view reconstruction on the Florence MICC Coop benchmark. Our quantitative analysis demonstrates the importance of a high-capacity model for fully exploiting abundant information from multi-view input in reconstruction. Furthermore, our model with physical-semantic parameters can be directly utilized for real-world applications, such as in-game avatar creation. As a result, our work opens up new research direction for parametric face model and facilitates future research on multi-view reconstruction.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2301.08153.pdf' target='_blank'>https://arxiv.org/pdf/2301.08153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shizun Wang, Weihong Zeng, Xu Wang, Hao Yang, Li Chen, Yi Yuan, Yunzhao Zeng, Min Zheng, Chuang Zhang, Ming Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08153">SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of a parameterized stylized character involves careful selection of numerous parameters, also known as the "avatar vectors" that can be interpreted by the avatar engine. Existing unsupervised avatar vector estimation methods that auto-create avatars for users, however, often fail to work because of the domain gap between realistic faces and stylized avatar images. To this end, we propose SwiftAvatar, a novel avatar auto-creation framework that is evidently superior to previous works. SwiftAvatar introduces dual-domain generators to create pairs of realistic faces and avatar images using shared latent codes. The latent codes can then be bridged with the avatar vectors as pairs, by performing GAN inversion on the avatar images rendered from the engine using avatar vectors. Through this way, we are able to synthesize paired data in high-quality as many as possible, consisting of avatar vectors and their corresponding realistic faces. We also propose semantic augmentation to improve the diversity of synthesis. Finally, a light-weight avatar vector estimator is trained on the synthetic pairs to implement efficient auto-creation. Our experiments demonstrate the effectiveness and efficiency of SwiftAvatar on two different avatar engines. The superiority and advantageous flexibility of SwiftAvatar are also verified in both subjective and objective evaluations.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2212.06820.pdf' target='_blank'>https://arxiv.org/pdf/2212.06820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.06820">Structured 3D Features for Reconstructing Controllable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Structured 3D Features, a model based on a novel implicit 3D representation that pools pixel-aligned image features onto dense 3D points sampled from a parametric, statistical human mesh surface. The 3D points have associated semantics and can move freely in 3D space. This allows for optimal coverage of the person of interest, beyond just the body shape, which in turn, additionally helps modeling accessories, hair, and loose clothing. Owing to this, we present a complete 3D transformer-based attention framework which, given a single image of a person in an unconstrained pose, generates an animatable 3D reconstruction with albedo and illumination decomposition, as a result of a single end-to-end model, trained semi-supervised, and with no additional postprocessing. We show that our S3F model surpasses the previous state-of-the-art on various tasks, including monocular 3D reconstruction, as well as albedo and shading estimation. Moreover, we show that the proposed methodology allows novel view synthesis, relighting, and re-posing the reconstruction, and can naturally be extended to handle multiple input images (e.g. different views of a person, or the same view, in different poses, in video). Finally, we demonstrate the editing capabilities of our model for 3D virtual try-on applications.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2509.01469.pdf' target='_blank'>https://arxiv.org/pdf/2509.01469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vanessa Sklyarova, Egor Zakharov, Malte Prinzler, Giorgio Becherini, Michael J. Black, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01469">Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach for 3D hair reconstruction from single photographs based on a global hair prior combined with local optimization. Capturing strand-based hair geometry from single photographs is challenging due to the variety and geometric complexity of hairstyles and the lack of ground truth training data. Classical reconstruction methods like multi-view stereo only reconstruct the visible hair strands, missing the inner structure of hairstyles and hampering realistic hair simulation. To address this, existing methods leverage hairstyle priors trained on synthetic data. Such data, however, is limited in both quantity and quality since it requires manual work from skilled artists to model the 3D hairstyles and create near-photorealistic renderings. To address this, we propose a novel approach that uses both, real and synthetic data to learn an effective hairstyle prior. Specifically, we train a transformer-based prior model on synthetic data to obtain knowledge of the internal hairstyle geometry and introduce real data in the learning process to model the outer structure. This training scheme is able to model the visible hair strands depicted in an input image, while preserving the general 3D structure of hairstyles. We exploit this prior to create a Gaussian-splatting-based reconstruction method that creates hairstyles from one or more images. Qualitative and quantitative comparisons with existing reconstruction pipelines demonstrate the effectiveness and superior performance of our method for capturing detailed hair orientation, overall silhouette, and backside consistency. For additional results and code, please refer to https://im2haircut.is.tue.mpg.de.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2508.17342.pdf' target='_blank'>https://arxiv.org/pdf/2508.17342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengyuan Zhang, Zhe Li, Xingqun Qi, Mengze Li, Muyi Sun, Man Zhang, Sirui Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17342">DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating coherent and diverse human dances from music signals has gained tremendous progress in animating virtual avatars. While existing methods support direct dance synthesis, they fail to recognize that enabling users to edit dance movements is far more practical in real-world choreography scenarios. Moreover, the lack of high-quality dance datasets incorporating iterative editing also limits addressing this challenge. To achieve this goal, we first construct DanceRemix, a large-scale multi-turn editable dance dataset comprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In addition, we propose a novel framework for iterative and editable dance generation coherently aligned with given music signals, namely DanceEditor. Considering the dance motion should be both musical rhythmic and enable iterative editing by user descriptions, our framework is built upon a prediction-then-editing paradigm unifying multi-modal conditions. At the initial prediction stage, our framework improves the authority of generated results by directly modeling dance movements from tailored, aligned music. Moreover, at the subsequent iterative editing stages, we incorporate text descriptions as conditioning information to draw the editable results through a specifically designed Cross-modality Editing Module (CEM). Specifically, CEM adaptively integrates the initial prediction with music and text prompts as temporal motion cues to guide the synthesized sequences. Thereby, the results display music harmonics while preserving fine-grained semantic alignment with text descriptions. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected DanceRemix dataset. Code is available at https://lzvsdy.github.io/DanceEditor/.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2508.09461.pdf' target='_blank'>https://arxiv.org/pdf/2508.09461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09461">Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different forms of customized 2D avatars are widely used in gaming applications, virtual communication, education, and content creation. However, existing approaches often fail to capture fine-grained facial expressions and struggle to preserve identity across different expressions. We propose GEN-AFFECT, a novel framework for personalized avatar generation that generates expressive and identity-consistent avatars with a diverse set of facial expressions. Our framework proposes conditioning a multimodal diffusion transformer on an extracted identity-expression representation. This enables identity preservation and representation of a wide range of facial expressions. GEN-AFFECT additionally employs consistent attention at inference for information sharing across the set of generated expressions, enabling the generation process to maintain identity consistency over the array of generated fine-grained expressions. GEN-AFFECT demonstrates superior performance compared to previous state-of-the-art methods on the basis of the accuracy of the generated expressions, the preservation of the identity and the consistency of the target identity across an array of fine-grained facial expressions.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2508.01218.pdf' target='_blank'>https://arxiv.org/pdf/2508.01218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujian Liu, Linlang Cao, Chuang Chen, Fanyu Geng, Dongxu Shen, Peng Cao, Shidang Xu, Xiaoli Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01218">MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing 3D head avatar reconstruction methods adopt a two-stage process, relying on tracked FLAME meshes derived from facial landmarks, followed by Gaussian-based rendering. However, misalignment between the estimated mesh and target images often leads to suboptimal rendering quality and loss of fine visual details. In this paper, we present MoGaFace, a novel 3D head avatar modeling framework that continuously refines facial geometry and texture attributes throughout the Gaussian rendering process. To address the misalignment between estimated FLAME meshes and target images, we introduce the Momentum-Guided Consistent Geometry module, which incorporates a momentum-updated expression bank and an expression-aware correction mechanism to ensure temporal and multi-view consistency. Additionally, we propose Latent Texture Attention, which encodes compact multi-view features into head-aware representations, enabling geometry-aware texture refinement via integration into Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity head avatar reconstruction and significantly improves novel-view synthesis quality, even under inaccurate mesh initialization and unconstrained real-world settings.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2507.15221.pdf' target='_blank'>https://arxiv.org/pdf/2507.15221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiying Xu, Haoze Liu, Mingshi Li, Siyu Cai, Guangxuan Zheng, Yuhuang Jia, Jinghua Zhao, Yong Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15221">EchoVoices: Preserving Generational Voices and Memories for Seniors and Children</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in intelligent speech and digital human technologies have primarily targeted mainstream adult users, often overlooking the distinct vocal patterns and interaction styles of seniors and children. These demographics possess distinct vocal characteristics, linguistic styles, and interaction patterns that challenge conventional ASR, TTS, and LLM systems. To address this, we introduce EchoVoices, an end-to-end digital human pipeline dedicated to creating persistent digital personas for seniors and children, ensuring their voices and memories are preserved for future generations. Our system integrates three core innovations: a k-NN-enhanced Whisper model for robust speech recognition of atypical speech; an age-adaptive VITS model for high-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that automatically generates persona cards and leverages a RAG-based memory system for conversational consistency. Our experiments, conducted on the SeniorTalk and ChildMandarin datasets, demonstrate significant improvements in recognition accuracy, synthesis quality, and speaker similarity. EchoVoices provides a comprehensive framework for preserving generational voices, offering a new means of intergenerational connection and the creation of lasting digital legacies.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2507.11138.pdf' target='_blank'>https://arxiv.org/pdf/2507.11138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adriano Castro, Simon Hanisch, Matin Fallahi, Thorsten Strufe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11138">FacialMotionID: Identifying Users of Mixed Reality Headsets using Abstract Facial Motion Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial motion capture in mixed reality headsets enables real-time avatar animation, allowing users to convey non-verbal cues during virtual interactions. However, as facial motion data constitutes a behavioral biometric, its use raises novel privacy concerns. With mixed reality systems becoming more immersive and widespread, understanding whether face motion data can lead to user identification or inference of sensitive attributes is increasingly important.
  To address this, we conducted a study with 116 participants using three types of headsets across three sessions, collecting facial, eye, and head motion data during verbal and non-verbal tasks. The data used is not raw video, but rather, abstract representations that are used to animate digital avatars. Our analysis shows that individuals can be re-identified from this data with up to 98% balanced accuracy, are even identifiable across device types, and that emotional states can be inferred with up to 86% accuracy. These results underscore the potential privacy risks inherent in face motion tracking in mixed reality environments.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2506.21865.pdf' target='_blank'>https://arxiv.org/pdf/2506.21865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haofeng Wang, Yilin Guo, Zehao Li, Tong Yue, Yizong Wang, Enci Zhang, Rongqun Lin, Feng Gao, Shiqi Wang, Siwei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21865">RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Yellow River is China's mother river and a cradle of human civilization. The ancient Yellow River culture is, moreover, an indispensable part of human art history. To conserve and inherit the ancient Yellow River culture, we designed RiverEcho, a real-time interactive system that responds to voice queries using a large language model and a cultural knowledge dataset, delivering explanations through a talking-head digital human. Specifically, we built a knowledge database focused on the ancient Yellow River culture, including the collection of historical texts and the processing pipeline. Experimental results demonstrate that leveraging Retrieval-Augmented Generation (RAG) on the proposed dataset enhances the response quality of the Large Language Model(LLM), enabling the system to generate more professional and informative responses. Our work not only diversifies the means of promoting Yellow River culture but also provides users with deeper cultural insights.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2506.01935.pdf' target='_blank'>https://arxiv.org/pdf/2506.01935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Tanmay Reddy Chakkera, Aggelina Chatziagapi, Md Moniruzzaman, Chen-Ping Yu, Yi-Hsuan Tsai, Dimitris Samaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01935">Low-Rank Head Avatar Personalization with Registers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel method for low-rank personalization of a generic model for head avatar generation. Prior work proposes generic models that achieve high-quality face animation by leveraging large-scale datasets of multiple identities. However, such generic models usually fail to synthesize unique identity-specific details, since they learn a general domain prior. To adapt to specific subjects, we find that it is still challenging to capture high-frequency facial details via popular solutions like low-rank adaptation (LoRA). This motivates us to propose a specific architecture, a Register Module, that enhances the performance of LoRA, while requiring only a small number of parameters to adapt to an unseen identity. Our module is applied to intermediate features of a pre-trained model, storing and re-purposing information in a learnable 3D feature space. To demonstrate the efficacy of our personalization method, we collect a dataset of talking videos of individuals with distinctive facial details, such as wrinkles and tattoos. Our approach faithfully captures unseen faces, outperforming existing methods quantitatively and qualitatively. We will release the code, models, and dataset to the public.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2505.21531.pdf' target='_blank'>https://arxiv.org/pdf/2505.21531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21531">How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the human motion knowledge of Large Language Models (LLMs) through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations. Using 20 representative motion instructions that cover fundamental movements and balance body part usage, we conduct comprehensive evaluations, including human and automatic scoring of both high-level movement plans and generated animations, as well as automatic comparison with oracle positions in low-level planning. Our findings show that LLMs are strong at interpreting high-level body movements but struggle with precise body part positioning. While decomposing motion queries into atomic components improves planning, LLMs face challenges in multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximations for general spatial descriptions, but fall short in handling precise spatial specifications. Notably, LLMs demonstrate promise in conceptualizing creative motions and distinguishing culturally specific motion patterns.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2504.21718.pdf' target='_blank'>https://arxiv.org/pdf/2504.21718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiying Li, Xingqun Qi, Bingkun Yang, Chen Weile, Zezhao Tian, Muyi Sun, Qifeng Liu, Man Zhang, Zhenan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21718">VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating responsive listener head dynamics with nuanced emotions and expressive reactions is crucial for practical dialogue modeling in various virtual avatar animations. Previous studies mainly focus on the direct short-term production of listener behavior. They overlook the fine-grained control over motion variations and emotional intensity, especially in long-sequence modeling. Moreover, the lack of long-term and large-scale paired speaker-listener corpora including head dynamics and fine-grained multi-modality annotations (e.g., text-based expression descriptions, emotional intensity) also limits the application of dialogue modeling.Therefore, we first newly collect a large-scale multi-turn dataset of 3D dyadic conversation containing more than 1.4M valid frames for multi-modal responsive interaction, dubbed ListenerX. Additionally, we propose VividListener, a novel framework enabling fine-grained, expressive and controllable listener dynamics modeling. This framework leverages multi-modal conditions as guiding principles for fostering coherent interactions between speakers and listeners.Specifically, we design the Responsive Interaction Module (RIM) to adaptively represent the multi-modal interactive embeddings. RIM ensures the listener dynamics achieve fine-grained semantic coordination with textual descriptions and adjustments, while preserving expressive reaction with speaker behavior. Meanwhile, we design the Emotional Intensity Tags (EIT) for emotion intensity editing with multi-modal information integration, applying to both text descriptions and listener motion amplitude.Extensive experiments conducted on our newly collected ListenerX dataset demonstrate that VividListener achieves state-of-the-art performance, realizing expressive and controllable listener dynamics.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2504.10486.pdf' target='_blank'>https://arxiv.org/pdf/2504.10486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeren Jiang, Shaofei Wang, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10486">DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable or even better relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2504.08353.pdf' target='_blank'>https://arxiv.org/pdf/2504.08353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08353">Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry -- especially for loose-fitting clothing -- remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2504.07945.pdf' target='_blank'>https://arxiv.org/pdf/2504.07945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07945">GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2502.13133.pdf' target='_blank'>https://arxiv.org/pdf/2502.13133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aggelina Chatziagapi, Louis-Philippe Morency, Hongyu Gong, Michael Zollhoefer, Dimitris Samaras, Alexander Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13133">AV-Flow: Transforming Text to Audio-Visual Human-like Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AV-Flow, an audio-visual generative model that animates photo-realistic 4D talking avatars given only text input. In contrast to prior work that assumes an existing speech signal, we synthesize speech and vision jointly. We demonstrate human-like speech synthesis, synchronized lip motion, lively facial expressions and head pose; all generated from just text characters. The core premise of our approach lies in the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized speech intonation and facial dynamics (e.g., eyebrow motion). Our model is trained with flow matching, leading to expressive results and fast inference. In case of dyadic conversations, AV-Flow produces an always-on avatar, that actively listens and reacts to the audio-visual input of a user. Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars. Project page: https://aggelinacha.github.io/AV-Flow/
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2412.01106.pdf' target='_blank'>https://arxiv.org/pdf/2412.01106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Xiang, Yudong Guo, Leipeng Hu, Boyang Guo, Yancheng Yuan, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01106">One Shot, One Talk: Whole-body Talking Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2410.01604.pdf' target='_blank'>https://arxiv.org/pdf/2410.01604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Si Chen, Haocong Cheng, Suzy Su, Stephanie Patterson, Raja Kushalnagar, Qi Wang, Yun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01604">Customizing Generated Signs and Voices of AI Avatars: Deaf-Centric Mixed-Reality Design for Deaf-Hearing Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates innovative interaction designs for communication and collaborative learning between learners of mixed hearing and signing abilities, leveraging advancements in mixed reality technologies like Apple Vision Pro and generative AI for animated avatars. Adopting a participatory design approach, we engaged 15 d/Deaf and hard of hearing (DHH) students to brainstorm ideas for an AI avatar with interpreting ability (sign language to English, voice to English) that would facilitate their face-to-face communication with hearing peers. Participants envisioned the AI avatars to address some issues with human interpreters, such as lack of availability, and provide affordable options to expensive personalized interpreting service. Our findings indicate a range of preferences for integrating the AI avatars with actual human figures of both DHH and hearing communication partners. The participants highlighted the importance of having control over customizing the AI avatar, such as AI-generated signs, voices, facial expressions, and their synchronization for enhanced emotional display in communication. Based on our findings, we propose a suite of design recommendations that balance respecting sign language norms with adherence to hearing social norms. Our study offers insights on improving the authenticity of generative AI in scenarios involving specific, and sometimes unfamiliar, social norms.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2409.08738.pdf' target='_blank'>https://arxiv.org/pdf/2409.08738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Gao, Haochun Huai, Sena Yildiz-Degirmenci, Maria Bannert, Enkelejda Kasneci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08738">DataliVR: Transformation of Data Literacy Education through Virtual Reality with ChatGPT-Powered Enhancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data literacy is essential in today's data-driven world, emphasizing individuals' abilities to effectively manage data and extract meaningful insights. However, traditional classroom-based educational approaches often struggle to fully address the multifaceted nature of data literacy. As education undergoes digital transformation, innovative technologies such as Virtual Reality (VR) offer promising avenues for immersive and engaging learning experiences. This paper introduces DataliVR, a pioneering VR application aimed at enhancing the data literacy skills of university students within a contextual and gamified virtual learning environment. By integrating Large Language Models (LLMs) like ChatGPT as a conversational artificial intelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides personalized learning assistance, enriching user learning experiences. Our study employed an experimental approach, with chatbot availability as the independent variable, analyzing learning experiences and outcomes as dependent variables with a sample of thirty participants. Our approach underscores the effectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering data literacy skills. Moreover, our study examines the impact of the ChatGPT-based AI chatbot on users' learning, revealing significant effects on both learning experiences and outcomes. Our study presents a robust tool for fostering data literacy skills, contributing significantly to the digital advancement of data literacy education through cutting-edge VR and AI technologies. Moreover, our research provides valuable insights and implications for future research endeavors aiming to integrate LLMs (e.g., ChatGPT) into educational VR platforms.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2407.13038.pdf' target='_blank'>https://arxiv.org/pdf/2407.13038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaojie Bai, Te-Li Wang, Chenghui Li, Akshay Venkatesh, Tomas Simon, Chen Cao, Gabriel Schwartz, Ryan Wrench, Jason Saragih, Yaser Sheikh, Shih-En Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13038">Universal Facial Encoding of Codec Avatars from VR Headsets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Faithful real-time facial animation is essential for avatar-mediated telepresence in Virtual Reality (VR). To emulate authentic communication, avatar animation needs to be efficient and accurate: able to capture both extreme and subtle expressions within a few milliseconds to sustain the rhythm of natural conversations. The oblique and incomplete views of the face, variability in the donning of headsets, and illumination variation due to the environment are some of the unique challenges in generalization to unseen faces. In this paper, we present a method that can animate a photorealistic avatar in realtime from head-mounted cameras (HMCs) on a consumer VR headset. We present a self-supervised learning approach, based on a cross-view reconstruction objective, that enables generalization to unseen users. We present a lightweight expression calibration mechanism that increases accuracy with minimal additional cost to run-time efficiency. We present an improved parameterization for precise ground-truth generation that provides robustness to environmental variation. The resulting system produces accurate facial animation for unseen users wearing VR headsets in realtime. We compare our approach to prior face-encoding methods demonstrating significant improvements in both quantitative metrics and qualitative results.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2407.07284.pdf' target='_blank'>https://arxiv.org/pdf/2407.07284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07284">MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that learns a single neural representation for multiple identities, using only monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human avatars require per-identity optimization. However, learning a multi-identity representation presents advantages in robustly animating humans under arbitrary poses. We propose to construct a high-order tensor that combines all the learnable 3DGS parameters for all the training identities. By assuming a low-rank structure and factorizing the tensor, we model the complex rigid and non-rigid deformations of multiple subjects in a unified network, significantly reducing the total number of parameters. Our proposed approach leverages information from all the training identities and enables robust animation under challenging unseen poses, outperforming existing approaches. It can also be extended to learn unseen identities.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2407.05324.pdf' target='_blank'>https://arxiv.org/pdf/2407.05324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Peng, Yunfan Tao, Haoyu Zhan, Yudong Guo, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05324">PICA: Physics-Integrated Clothed Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PICA, a novel representation for high-fidelity animatable clothed human avatars with physics-accurate dynamics, even for loose clothing. Previous neural rendering-based representations of animatable clothed humans typically employ a single model to represent both the clothing and the underlying body. While efficient, these approaches often fail to accurately represent complex garment dynamics, leading to incorrect deformations and noticeable rendering artifacts, especially for sliding or loose garments. Furthermore, previous works represent garment dynamics as pose-dependent deformations and facilitate novel pose animations in a data-driven manner. This often results in outcomes that do not faithfully represent the mechanics of motion and are prone to generating artifacts in out-of-distribution poses. To address these issues, we adopt two individual 3D Gaussian Splatting (3DGS) models with different deformation characteristics, modeling the human body and clothing separately. This distinction allows for better handling of their respective motion characteristics. With this representation, we integrate a graph neural network (GNN)-based clothed body physics simulation module to ensure an accurate representation of clothing dynamics. Our method, through its carefully designed features, achieves high-fidelity rendering of clothed human bodies in complex and novel driving poses, significantly outperforming previous methods under the same settings.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2406.04629.pdf' target='_blank'>https://arxiv.org/pdf/2406.04629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenghao Chai, Chen Tang, Yongkang Wong, Mohan Kankanhalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04629">STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network Motion Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The creation of 4D avatars (i.e., animated 3D avatars) from text description typically uses text-to-image (T2I) diffusion models to synthesize 3D avatars in the canonical space and subsequently applies animation with target motions. However, such an optimization-by-animation paradigm has several drawbacks. (1) For pose-agnostic optimization, the rendered images in canonical pose for naive Score Distillation Sampling (SDS) exhibit domain gap and cannot preserve view-consistency using only T2I priors, and (2) For post hoc animation, simply applying the source motions to target 3D avatars yields translation artifacts and misalignment. To address these issues, we propose Skeleton-aware Text-based 4D Avatar generation with in-network motion Retargeting (STAR). STAR considers the geometry and skeleton differences between the template mesh and target avatar, and corrects the mismatched source motion by resorting to the pretrained motion retargeting techniques. With the informatively retargeted and occlusion-aware skeleton, we embrace the skeleton-conditioned T2I and text-to-video (T2V) priors, and propose a hybrid SDS module to coherently provide multi-view and frame-consistent supervision signals. Hence, STAR can progressively optimize the geometry, texture, and motion in an end-to-end manner. The quantitative and qualitative experiments demonstrate our proposed STAR can synthesize high-quality 4D avatars with vivid animations that align well with the text description. Additional ablation studies shows the contributions of each component in STAR. The source code and demos are available at: \href{https://star-avatar.github.io}{https://star-avatar.github.io}.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2405.09551.pdf' target='_blank'>https://arxiv.org/pdf/2405.09551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Freire-ObregÃ³n, Daniel HernÃ¡ndez-Sosa, Oliverio J. Santana, Javier Lorenzo-Navarro, Modesto CastrillÃ³n-Santana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09551">Towards Bi-Hemispheric Emotion Mapping through EEG: A Dual-Stream Neural Network Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion classification through EEG signals plays a significant role in psychology, neuroscience, and human-computer interaction. This paper addresses the challenge of mapping human emotions using EEG data in the Mapping Human Emotions through EEG Signals FG24 competition. Subjects mimic the facial expressions of an avatar, displaying fear, joy, anger, sadness, disgust, and surprise in a VR setting. EEG data is captured using a multi-channel sensor system to discern brain activity patterns. We propose a novel two-stream neural network employing a Bi-Hemispheric approach for emotion inference, surpassing baseline methods and enhancing emotion recognition accuracy. Additionally, we conduct a temporal analysis revealing that specific signal intervals at the beginning and end of the emotion stimulus sequence contribute significantly to improve accuracy. Leveraging insights gained from this temporal analysis, our approach offers enhanced performance in capturing subtle variations in the states of emotions.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2403.15227.pdf' target='_blank'>https://arxiv.org/pdf/2403.15227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15227">LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D face stylization have made significant strides in few to zero-shot settings. However, the degree of stylization achieved by existing methods is often not sufficient for practical applications because they are mostly based on statistical 3D Morphable Models (3DMM) with limited variations. To this end, we propose a method that can produce a highly stylized 3D face model with desired topology. Our methods train a surface deformation network with 3DMM and translate its domain to the target style using a paired exemplar. The network achieves stylization of the 3D face mesh by mimicking the style of the target using a differentiable renderer and directional CLIP losses. Additionally, during the inference process, we utilize a Mesh Agnostic Encoder (MAGE) that takes deformation target, a mesh of diverse topologies as input to the stylization process and encodes its shape into our latent space. The resulting stylized face model can be animated by commonly used 3DMM blend shapes. A set of quantitative and qualitative evaluations demonstrate that our method can produce highly stylized face meshes according to a given style and output them in a desired topology. We also demonstrate example applications of our method including image-based stylized avatar generation, linear interpolation of geometric styles, and facial animation of stylized avatars.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2403.10805.pdf' target='_blank'>https://arxiv.org/pdf/2403.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, Shunman Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10805">Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture. The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy features, represented as a unified latent feature, are fed into the AdaLN transformer. The AdaLN transformer introduces a conditional mechanism that applies a uniform function across all tokens, thereby effectively modeling the correlation between the fuzzy features and the gesture sequence. This module ensures a high level of gesture-speech synchronization while preserving naturalness. Finally, we employ the diffusion model to train and infer various gestures. Extensive subjective and objective evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's superior performance to the current state-of-the-art approaches. Persona-Gestor improves the system's usability and generalization capabilities, setting a new benchmark in speech-driven gesture synthesis and broadening the horizon for virtual human technology. Supplementary videos and code can be accessed at https://zf223669.github.io/Diffmotion-v2-website/
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2401.12900.pdf' target='_blank'>https://arxiv.org/pdf/2401.12900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12900">PSAvatar: A Point-based Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress, achieving real-time high-fidelity head avatar animation is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency. Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a resolution of 512 $\times$ 512 ).
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2312.05210.pdf' target='_blank'>https://arxiv.org/pdf/2312.05210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Wang, BoÅ¾idar AntiÄ, Andreas Geiger, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05210">IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present IntrinsicAvatar, a novel approach to recovering the intrinsic properties of clothed human avatars including geometry, albedo, material, and environment lighting from only monocular videos. Recent advancements in human-based neural rendering have enabled high-quality geometry and appearance reconstruction of clothed humans from just monocular videos. However, these methods bake intrinsic properties such as albedo, material, and environment lighting into a single entangled neural representation. On the other hand, only a handful of works tackle the problem of estimating geometry and disentangled appearance properties of clothed humans from monocular videos. They usually achieve limited quality and disentanglement due to approximations of secondary shading effects via learned MLPs. In this work, we propose to model secondary shading effects explicitly via Monte-Carlo ray tracing. We model the rendering process of clothed humans as a volumetric scattering process, and combine ray tracing with body articulation. Our approach can recover high-quality geometry, albedo, material, and lighting properties of clothed humans from a single monocular video, without requiring supervised pre-training using ground truth materials. Furthermore, since we explicitly model the volumetric scattering process and ray tracing, our model naturally generalizes to novel poses, enabling animation of the reconstructed avatar in novel lighting conditions.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2312.02214.pdf' target='_blank'>https://arxiv.org/pdf/2312.02214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02214">FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D Gaussian field embedded in the surface of a parametric face model and learn extra spatial offset to model non-surface regions and subtle facial details. While full use of geometric priors can capture high-frequency facial details and preserve exaggerated expressions, proper initialization can help reduce the number of Gaussians, thus enabling super-fast rendering speed. Extensive experimental results demonstrate that FlashAvatar outperforms existing works regarding visual quality and personalized details and is almost an order of magnitude faster in rendering speed. Project page: https://ustc3dv.github.io/FlashAvatar/
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2308.12642.pdf' target='_blank'>https://arxiv.org/pdf/2308.12642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Ngo, Daniel Phelps, Derrick Lai, Thanyared Wong, Lucas Mathias, Anish Shivamurthy, Mustafa Ajmal, Minghao Liu, James Davis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12642">Tag-Based Annotation for Avatar Face Creation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, digital avatars can be created manually using human images as reference. Systems such as Bitmoji are excellent producers of detailed avatar designs, with hundreds of choices for customization. A supervised learning model could be trained to generate avatars automatically, but the hundreds of possible options create difficulty in securing non-noisy data to train a model. As a solution, we train a model to produce avatars from human images using tag-based annotations. This method provides better annotator agreement, leading to less noisy data and higher quality model predictions. Our contribution is an application of tag-based annotation to train a model for avatar face creation. We design tags for 3 different facial facial features offered by Bitmoji, and train a model using tag-based annotation to predict the nose.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2308.10899.pdf' target='_blank'>https://arxiv.org/pdf/2308.10899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10899">TADA! Text to Animatable Digital Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent alignment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2308.06076.pdf' target='_blank'>https://arxiv.org/pdf/2308.06076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Haozhe Wu, Junliang Xing, Jia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06076">Versatile Face Animator: Driving Arbitrary 3D Facial Avatar in RGBD Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic 3D facial animation is crucial for various applications in the movie production and gaming industry, especially with the burgeoning demand in the metaverse. However, prevalent methods such as blendshape-based approaches and facial rigging techniques are time-consuming, labor-intensive, and lack standardized configurations, making facial animation production challenging and costly. In this paper, we propose a novel self-supervised framework, Versatile Face Animator, which combines facial motion capture with motion retargeting in an end-to-end manner, eliminating the need for blendshapes or rigs. Our method has the following two main characteristics: 1) we propose an RGBD animation module to learn facial motion from raw RGBD videos by hierarchical motion dictionaries and animate RGBD images rendered from 3D facial mesh coarse-to-fine, enabling facial animation on arbitrary 3D characters regardless of their topology, textures, blendshapes, and rigs; and 2) we introduce a mesh retarget module to utilize RGBD animation to create 3D facial animation by manipulating facial mesh with controller transformations, which are estimated from dense optical flow fields and blended together with geodesic-distance-based weights. Comprehensive experiments demonstrate the effectiveness of our proposed framework in generating impressive 3D facial animation results, highlighting its potential as a promising solution for the cost-effective and efficient production of facial animation in the metaverse.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2308.05920.pdf' target='_blank'>https://arxiv.org/pdf/2308.05920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Ye, Jia Jia, Junliang Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05920">Semantics2Hands: Transferring Hand Motion Semantics between Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human hands, the primary means of non-verbal communication, convey intricate semantics in various scenarios. Due to the high sensitivity of individuals to hand motions, even minor errors in hand motions can significantly impact the user experience. Real applications often involve multiple avatars with varying hand shapes, highlighting the importance of maintaining the intricate semantics of hand motions across the avatars. Therefore, this paper aims to transfer the hand motion semantics between diverse avatars based on their respective hand models. To address this problem, we introduce a novel anatomy-based semantic matrix (ASM) that encodes the semantics of hand motions. The ASM quantifies the positions of the palm and other joints relative to the local frame of the corresponding joint, enabling precise retargeting of hand motions. Subsequently, we obtain a mapping function from the source ASM to the target hand joint rotations by employing an anatomy-based semantics reconstruction network (ASRN). We train the ASRN using a semi-supervised learning strategy on the Mixamo and InterHand2.6M datasets. We evaluate our method in intra-domain and cross-domain hand motion retargeting tasks. The qualitative and quantitative results demonstrate the significant superiority of our ASRN over the state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2307.02090.pdf' target='_blank'>https://arxiv.org/pdf/2307.02090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02090">Interactive Conversational Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new conversation head generation benchmark for synthesizing behaviors of a single interlocutor in a face-to-face conversation. The capability to automatically synthesize interlocutors which can participate in long and multi-turn conversations is vital and offer benefits for various applications, including digital humans, virtual agents, and social robots. While existing research primarily focuses on talking head generation (one-way interaction), hindering the ability to create a digital human for conversation (two-way) interaction due to the absence of listening and interaction parts. In this work, we construct two datasets to address this issue, ``ViCo'' for independent talking and listening head generation tasks at the sentence level, and ``ViCo-X'', for synthesizing interlocutors in multi-turn conversational scenarios. Based on ViCo and ViCo-X, we define three novel tasks targeting the interaction modeling during the face-to-face conversation: 1) responsive listening head generation making listeners respond actively to the speaker with non-verbal signals, 2) expressive talking head generation guiding speakers to be aware of listeners' behaviors, and 3) conversational head generation to integrate the talking/listening ability in one interlocutor. Along with the datasets, we also propose corresponding baseline solutions to the three aforementioned tasks. Experimental results show that our baseline method could generate responsive and vivid agents that can collaborate with real person to fulfil the whole conversation. Project page: https://vico.solutions/.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2305.19245.pdf' target='_blank'>https://arxiv.org/pdf/2305.19245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thu Nguyen-Phuoc, Gabriel Schwartz, Yuting Ye, Stephen Lombardi, Lei Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19245">AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method that can quickly adapt dynamic 3D avatars to arbitrary text descriptions of novel styles. Among existing approaches for avatar stylization, direct optimization methods can produce excellent results for arbitrary styles but they are unpleasantly slow. Furthermore, they require redoing the optimization process from scratch for every new input. Fast approximation methods using feed-forward networks trained on a large dataset of style images can generate results for new inputs quickly, but tend not to generalize well to novel styles and fall short in quality. We therefore investigate a new approach, AlteredAvatar, that combines those two approaches using the meta-learning framework. In the inner loop, the model learns to optimize to match a single target style well; while in the outer loop, the model learns to stylize efficiently across many styles. After training, AlteredAvatar learns an initialization that can quickly adapt within a small number of update steps to a novel style, which can be given using texts, a reference image, or a combination of both. We show that AlteredAvatar can achieve a good balance between speed, flexibility and quality, while maintaining consistency across a wide range of novel views and facial expressions.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2305.12649.pdf' target='_blank'>https://arxiv.org/pdf/2305.12649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbin Lin, Mingkui Tan, Yifan Zhang, Zhen Qiu, Shuaicheng Niu, Dong Liu, Qing Du, Yanxia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12649">Imbalance-Agnostic Source-Free Domain Adaptation via Avatar Prototype Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Source-free Unsupervised Domain Adaptation (SF-UDA) aims to adapt a well-trained source model to an unlabeled target domain without access to the source data. One key challenge is the lack of source data during domain adaptation. To handle this, we propose to mine the hidden knowledge of the source model and exploit it to generate source avatar prototypes. To this end, we propose a Contrastive Prototype Generation and Adaptation (CPGA) method. CPGA consists of two stages: Prototype generation and Prototype adaptation. Extensive experiments on three UDA benchmark datasets demonstrate the superiority of CPGA. However, existing SF.UDA studies implicitly assume balanced class distributions for both the source and target domains, which hinders their real applications. To address this issue, we study a more practical SF-UDA task, termed imbalance-agnostic SF-UDA, where the class distributions of both the unseen source domain and unlabeled target domain are unknown and could be arbitrarily skewed. This task is much more challenging than vanilla SF-UDA due to the co-occurrence of covariate shifts and unidentified class distribution shifts between the source and target domains. To address this task, we extend CPGA and propose a new Target-aware Contrastive Prototype Generation and Adaptation (T-CPGA) method. Specifically, for better prototype adaptation in the imbalance-agnostic scenario, T-CPGA applies a new pseudo label generation strategy to identify unknown target class distribution and generate accurate pseudo labels, by utilizing the collective intelligence of the source model and an additional contrastive language-image pre-trained model. Meanwhile, we further devise a target label-distribution-aware classifier to adapt the model to the unknown target class distribution. We empirically show that T-CPGA significantly outperforms CPGA and other SF-UDA methods in imbalance-agnostic SF-UDA.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2305.00936.pdf' target='_blank'>https://arxiv.org/pdf/2305.00936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihun Cha, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00936">Generating Texture for 3D Human Avatar from a Single Image using Sampling and Refinement Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been significant progress in generating an animatable 3D human avatar from a single image. However, recovering texture for the 3D human avatar from a single image has been relatively less addressed. Because the generated 3D human avatar reveals the occluded texture of the given image as it moves, it is critical to synthesize the occluded texture pattern that is unseen from the source image. To generate a plausible texture map for 3D human avatars, the occluded texture pattern needs to be synthesized with respect to the visible texture from the given image. Moreover, the generated texture should align with the surface of the target 3D mesh. In this paper, we propose a texture synthesis method for a 3D human avatar that incorporates geometry information. The proposed method consists of two convolutional networks for the sampling and refining process. The sampler network fills in the occluded regions of the source image and aligns the texture with the surface of the target 3D mesh using the geometry information. The sampled texture is further refined and adjusted by the refiner network. To maintain the clear details in the given image, both sampled and refined texture is blended to produce the final texture map. To effectively guide the sampler network to achieve its goal, we designed a curriculum learning scheme that starts from a simple sampling task and gradually progresses to the task where the alignment needs to be considered. We conducted experiments to show that our method outperforms previous methods qualitatively and quantitatively.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2303.06370.pdf' target='_blank'>https://arxiv.org/pdf/2303.06370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stevo RackoviÄ, ClÃ¡udia Soares, DuÅ¡an JakovetiÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06370">Distributed Solution of the Inverse Rig Problem in Blendshape Facial Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of rig inversion is central in facial animation as it allows for a realistic and appealing performance of avatars. With the increasing complexity of modern blendshape models, execution times increase beyond practically feasible solutions. A possible approach towards a faster solution is clustering, which exploits the spacial nature of the face, leading to a distributed method. In this paper, we go a step further, involving cluster coupling to get more confident estimates of the overlapping components. Our algorithm applies the Alternating Direction Method of Multipliers, sharing the overlapping weights between the subproblems. The results obtained with this technique show a clear advantage over the naive clustered approach, as measured in different metrics of success and visual inspection. The method applies to an arbitrary clustering of the face. We also introduce a novel method for choosing the number of clusters in a data-free manner. The method tends to find a clustering such that the resulting clustering graph is sparse but without losing essential information. Finally, we give a new variant of a data-free clustering algorithm that produces good scores with respect to the mentioned strategy for choosing the optimal clustering.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2302.07354.pdf' target='_blank'>https://arxiv.org/pdf/2302.07354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Liu, Zeyu Cheng, Shen Sang, Jing Liu, James Davis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07354">Tag-based annotation creates better avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatar creation from human images allows users to customize their digital figures in different styles. Existing rendering systems like Bitmoji, MetaHuman, and Google Cartoonset provide expressive rendering systems that serve as excellent design tools for users. However, twenty-plus parameters, some including hundreds of options, must be tuned to achieve ideal results. Thus it is challenging for users to create the perfect avatar. A machine learning model could be trained to predict avatars from images, however the annotators who label pairwise training data have the same difficulty as users, causing high label noise. In addition, each new rendering system or version update requires thousands of new training pairs. In this paper, we propose a Tag-based annotation method for avatar creation. Compared to direct annotation of labels, the proposed method: produces higher annotator agreements, causes machine learning to generates more consistent predictions, and only requires a marginal cost to add new rendering systems.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2301.06471.pdf' target='_blank'>https://arxiv.org/pdf/2301.06471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Lavit Nicora, Sebastian Beyrodt, Dimitra Tsovaltzi, Fabrizio Nunnari, Patrick Gebhard, Matteo Malosio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06471">Towards social embodied cobots: The integration of an industrial cobot with a social virtual agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of the physical capabilities of an industrial collaborative robot with a social virtual character may represent a viable solution to enhance the workers' perception of the system as an embodied social entity and increase social engagement and well-being at the workplace. An online study was setup using prerecorded video interactions in order to pilot potential advantages of different embodied configurations of the cobot-avatar system in terms of perceptions of Social Presence, cobot-avatar Unity and Social Role of the system, and explore the relation of these. In particular, two different configurations were explored and compared: the virtual character was displayed either on a tablet strapped onto the base of the cobot or on a large TV screen positioned at the back of the workcell. The results imply that participants showed no clear preference based on the constructs, and both configurations fulfill these basic criteria. In terms of the relations between the constructs, there were strong correlations between perception of Social Presence, Unity and Social Role (Collegiality). This gives a valuable insight into the role of these constructs in the perception of cobots as embodied social entities, and towards building cobots that support well-being at the workplace.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2203.04179.pdf' target='_blank'>https://arxiv.org/pdf/2203.04179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Hanisch, Evelyn Muschter, Admantini Hatzipanayioti, Shu-Chen Li, Thorsten Strufe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.04179">Understanding Person Identification through Gait</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait recognition is the process of identifying humans from their bipedal locomotion such as walking or running. As such, gait data is privacy sensitive information and should be anonymized where possible. With the rise of higher quality gait recording techniques, such as depth cameras or motion capture suits, an increasing amount of detailed gait data is captured and processed. The introduction and rise of the Metaverse is an example of a potentially popular application scenario in which the gait of users is transferred onto digital avatars. As a first step towards developing effective anonymization techniques for high-quality gait data, we study different aspects of movement data to quantify their contribution to gait recognition. We first extract categories of features from the literature on human gait perception and then design experiments for each category to assess how much the information they contain contributes to recognition success. We evaluated the utility of gait perturbation by means of naturalness ratings in a user study. Our results show that gait anonymization will be challenging, as the data is highly redundant and inter-dependent.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2108.11590.pdf' target='_blank'>https://arxiv.org/pdf/2108.11590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, Kai-Wei Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.11590">AVATAR: A Parallel Corpus for Java-Python Program Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples. Therefore, we present AVATAR, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python. AVATAR is collected from competitive programming sites, online platforms, and open-source repositories. Furthermore, AVATAR includes unit tests for 250 examples to facilitate functional correctness evaluation. We benchmark several pre-trained language models fine-tuned on AVATAR. Experiment results show that the models lack in generating functionally accurate code.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2509.20817.pdf' target='_blank'>https://arxiv.org/pdf/2509.20817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiluo Wei, Yupeng He, Gareth Tyson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20817">Even More Kawaii than Real-Person-Driven VTubers? Understanding How Viewers Perceive AI-Driven VTubers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VTubers, digital personas represented by animated avatars, have gained massive popularity. Traditionally, VTubers are operated and voiced by human controllers known as Nakanohito. The reliance on Nakanohito, however, poses risks due to potential personal controversies and operational disruptions. The emergence of AI-driven VTubers offers a new model free from these human constraints. While AI-driven VTubers present benefits such as continuous operation and reduced scandal risk, they also raise questions about authenticity and audience engagement. Therefore, to gain deeper insights, we conduct a case study, investigating viewer perceptions of Neuro-sama, the most popular AI-driven VTuber with 845k followers on Twitch and 753k followers on YouTube. We analyze 108k Reddit posts and 136k YouTube comments, aiming to better understand viewer motivations, how AI constructs the virtual persona, and perceptions of the AI as Nakanohito. Our findings enhance the understanding of AI-driven VTubers and their impact on digital streaming culture.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2509.01420.pdf' target='_blank'>https://arxiv.org/pdf/2509.01420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Evan G. Center, Matti Pouke, Alessandro Nardi, Lukas Gehrke, Klaus Gramann, Timo Ojala, Steven M. LaValle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01420">Body Ownership Affects the Processing of Sensorimotor Contingencies in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Presence in virtual reality (VR), the subjective sense of "being there" in a virtual environment, is notoriously difficult to measure. Electroencephalography (EEG) may offer a promising, unobtrusive means of assessing a user's momentary state of presence. Unlike traditional questionnaires, EEG does not interrupt the experience or rely on users' retrospective self-reports, thereby avoiding interference with the very state it aims to capture. Previous research has attempted to quantify presence in virtual environments using event-related potentials (ERPs). We contend, however, that previous efforts have fallen short of fully realizing this goal, failing to either A) independently manipulate presence, B) validate their measure of presence against traditional techniques, C) adequately separate the constructs of presence and attention, and/or D) implement a realistic and immersive environment and task. We address these shortcomings in a preregistered ERP experiment in which participants play an engaging target shooting game in VR. ERPs are time-locked to the release of a ball from a sling. We induce breaks in presence (BIPs) by freezing the ball's release on a minority of trials. Embodiment is manipulated by allowing manual manipulation of the sling with a realistic avatar in one condition (embodied condition) and passive manipulation with only controllers in another (non-embodied condition). We support our predictions that the N2, the P3b, and the N400, are selectively sensitive towards specific components of these manipulations. The pattern of findings carries significant implications for theories of presence, which have been seldom addressed in previous ERP investigations on this topic.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2507.19359.pdf' target='_blank'>https://arxiv.org/pdf/2507.19359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanmiao Liu, Esam Ghaleb, AslÄ± ÃzyÃ¼rek, Zerrin Yumak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19359">SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at https://semgesture.github.io/.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2507.18758.pdf' target='_blank'>https://arxiv.org/pdf/2507.18758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Liu, Shengjun Zhang, Chensheng Dai, Yang Chen, Hao Liu, Chen Li, Yueqi Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18758">Learning Efficient and Generalizable Human Representation with Human Gaussian Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling animatable human avatars from videos is a long-standing and challenging problem. While conventional methods require per-instance optimization, recent feed-forward methods have been proposed to generate 3D Gaussians with a learnable network. However, these methods predict Gaussians for each frame independently, without fully capturing the relations of Gaussians from different timestamps. To address this, we propose Human Gaussian Graph to model the connection between predicted Gaussians and human SMPL mesh, so that we can leverage information from all frames to recover an animatable human representation. Specifically, the Human Gaussian Graph contains dual layers where Gaussians are the first layer nodes and mesh vertices serve as the second layer nodes. Based on this structure, we further propose the intra-node operation to aggregate various Gaussians connected to one mesh vertex, and inter-node operation to support message passing among mesh node neighbors. Experimental results on novel view synthesis and novel pose animation demonstrate the efficiency and generalization of our method.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2507.04667.pdf' target='_blank'>https://arxiv.org/pdf/2507.04667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hahyeon Choi, Junhoo Lee, Nojun Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04667">What's Making That Sound Right Now? Video-centric Audio-Visual Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2506.18866.pdf' target='_blank'>https://arxiv.org/pdf/2506.18866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, Steven Hoi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18866">OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2505.20582.pdf' target='_blank'>https://arxiv.org/pdf/2505.20582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhao, Chunjiang Liu, Haoyu Chen, Bhiksha Raj, Min Xu, Tadas Baltrusaitis, Mitch Rundle, HsiangTao Wu, Kamran Ghasedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20582">Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face reenactment and portrait relighting are essential tasks in portrait editing, yet they are typically addressed independently, without much synergy. Most face reenactment methods prioritize motion control and multiview consistency, while portrait relighting focuses on adjusting shading effects. To take advantage of both geometric consistency and illumination awareness, we introduce Total-Editing, a unified portrait editing framework that enables precise control over appearance, motion, and lighting. Specifically, we design a neural radiance field decoder with intrinsic decomposition capabilities. This allows seamless integration of lighting information from portrait images or HDR environment maps into synthesized portraits. We also incorporate a moving least squares based deformation field to enhance the spatiotemporal coherence of avatar motion and shading effects. With these innovations, our unified framework significantly improves the quality and realism of portrait editing results. Further, the multi-source nature of Total-Editing supports more flexible applications, such as illumination transfer from one portrait to another, or portrait animation with customized backgrounds.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2505.16512.pdf' target='_blank'>https://arxiv.org/pdf/2505.16512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Long Ma, Renwang Pei, Zhaofeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16512">Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the explosive advancement of deepfake technology has posed a critical and escalating threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency via multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Leveraging five of the latest digital human generation methods and a voice cloning method, we systematically construct a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that the misrecognition rate by participants for DigiFakeAV reaches as high as 68%. Moreover, the substantial performance degradation of existing detection models on our dataset further highlights its challenges. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2505.05672.pdf' target='_blank'>https://arxiv.org/pdf/2505.05672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05672">TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2505.05376.pdf' target='_blank'>https://arxiv.org/pdf/2505.05376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rachmadio Noval Lazuardi, Artem Sevastopolsky, Egor Zakharov, Matthias Niessner, Vanessa Sklyarova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05376">GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel method that reconstructs hair strands directly from colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair strand reconstruction is a fundamental problem in computer vision and graphics that can be used for high-fidelity digital avatar synthesis, animation, and AR/VR applications. However, accurately recovering hair strands from raw scan data remains challenging due to human hair's complex and fine-grained structure. Existing methods typically rely on RGB captures, which can be sensitive to the environment and can be a challenging domain for extracting the orientation of guiding strands, especially in the case of challenging hairstyles. To reconstruct the hair purely from the observed geometry, our method finds sharp surface features directly on the scan and estimates strand orientation through a neural 2D line detector applied to the renderings of scan shading. Additionally, we incorporate a diffusion prior trained on a diverse set of synthetic hair scans, refined with an improved noise schedule, and adapted to the reconstructed contents via a scan-specific text prompt. We demonstrate that this combination of supervision signals enables accurate reconstruction of both simple and intricate hairstyles without relying on color information. To facilitate further research, we introduce Strands400, the largest publicly available dataset of hair strands with detailed surface geometry extracted from real-world data, which contains reconstructed hair strands from the scans of 400 subjects.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2504.15835.pdf' target='_blank'>https://arxiv.org/pdf/2504.15835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Wu, Malte Prinzler, Xiaogang Jin, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15835">Text-based Animatable 3D Avatars with Morphable Model Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of high-quality, animatable 3D head avatars from text has enormous potential in content creation applications such as games, movies, and embodied virtual assistants. Current text-to-3D generation methods typically combine parametric head models with 2D diffusion models using score distillation sampling to produce 3D-consistent results. However, they struggle to synthesize realistic details and suffer from misalignments between the appearance and the driving parametric model, resulting in unnatural animation results. We discovered that these limitations stem from ambiguities in the 2D diffusion predictions during 3D avatar distillation, specifically: i) the avatar's appearance and geometry is underconstrained by the text input, and ii) the semantic alignment between the predictions and the parametric head model is insufficient because the diffusion model alone cannot incorporate information from the parametric model. In this work, we propose a novel framework, AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with morphable model alignment, and introduce two key strategies to address these challenges. First, we tackle appearance and geometry ambiguities by utilizing prior information from a pretrained text-to-3D model to initialize a 3D avatar with robust appearance, geometry, and rigging relationships to the morphable model. Second, we refine the initial 3D avatar for dynamic expressions using a ControlNet that is conditioned on semantic and normal maps of the morphable model to ensure accurate alignment. As a result, our method outperforms existing approaches in terms of synthesis quality, alignment, and animation fidelity. Our experiments show that the proposed method advances the state of the art in text-based, animatable 3D head avatar generation.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2504.04842.pdf' target='_blank'>https://arxiv.org/pdf/2504.04842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, Mu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04842">FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2502.19441.pdf' target='_blank'>https://arxiv.org/pdf/2502.19441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengtian Li, Shengxiang Yao, Chen Kai, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19441">AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Gaussian-based human body reconstruction have achieved notable success in creating animatable avatars. However, there are ongoing challenges to fully exploit the SMPL model's prior knowledge and enhance the visual fidelity of these models to achieve more refined avatar reconstructions. In this paper, we introduce AniGaussian which addresses the above issues with two insights. First, we propose an innovative pose guided deformation strategy that effectively constrains the dynamic Gaussian avatar with SMPL pose guidance, ensuring that the reconstructed model not only captures the detailed surface nuances but also maintains anatomical correctness across a wide range of motions. Second, we tackle the expressiveness limitations of Gaussian models in representing dynamic human bodies. We incorporate rigid-based priors from previous works to enhance the dynamic transform capabilities of the Gaussian model. Furthermore, we introduce a split-with-scale strategy that significantly improves geometry quality. The ablative study experiment demonstrates the effectiveness of our innovative model design. Through extensive comparisons with existing methods, AniGaussian demonstrates superior performance in both qualitative result and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2502.01553.pdf' target='_blank'>https://arxiv.org/pdf/2502.01553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiluo Wei, Gareth Tyson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01553">Virtual Stars, Real Fans: Understanding the VTuber Ecosystem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Livestreaming by VTubers -- animated 2D/3D avatars controlled by real individuals -- have recently garnered substantial global followings and achieved significant monetary success. Despite prior research highlighting the importance of realism in audience engagement, VTubers deliberately conceal their identities, cultivating dedicated fan communities through virtual personas. While previous studies underscore that building a core fan community is essential to a streamer's success, we lack an understanding of the characteristics of viewers of this new type of streamer. Gaining a deeper insight into these viewers is critical for VTubers to enhance audience engagement, foster a more robust fan base, and attract a larger viewership. To address this gap, we conduct a comprehensive analysis of VTuber viewers on Bilibili, a leading livestreaming platform where nearly all VTubers in China stream. By compiling a first-of-its-kind dataset covering 2.7M livestreaming sessions, we investigate the characteristics, engagement patterns, and influence of VTuber viewers. Our research yields several valuable insights, which we then leverage to develop a tool to "recommend" future subscribers to VTubers. By reversing the typical approach of recommending streams to viewers, this tool assists VTubers in pinpointing potential future fans to pay more attention to, and thereby effectively growing their fan community.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2411.16729.pdf' target='_blank'>https://arxiv.org/pdf/2411.16729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Siyuan Zhao, Naye Ji, Zhaohan Wang, Jingmei Wu, Fuxing Gao, Zhenqing Ye, Leyao Yan, Lanxin Dai, Weidong Geng, Xin Lyu, Bozuo Zhao, Dingguo Yu, Hui Du, Bin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16729">DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation using transformer-based generative models represents a rapidly advancing area within virtual human creation. However, existing models face significant challenges due to their quadratic time and space complexities, limiting scalability and efficiency. To address these limitations, we introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamba-2 architecture. DiM-Gestor features a dual-component framework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping module, both built on the Mamba-2. The fuzzy feature extractor, integrated with a Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit, continuous speech features. These features are synthesized into a unified latent representation and then processed by the speech-to-gesture mapping module. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced Mamba-2 mechanism to uniformly apply transformations across all sequence tokens. This enables precise modeling of the nuanced interplay between speech features and gesture dynamics. We utilize a diffusion model to train and infer diverse gesture outputs. Extensive subjective and objective evaluations conducted on the newly released Chinese Co-Speech Gestures dataset corroborate the efficacy of our proposed model. Compared with Transformer-based architecture, the assessments reveal that our approach delivers competitive results and significantly reduces memory usage, approximately 2.4 times, and enhances inference speeds by 2 to 4 times. Additionally, we released the CCG dataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six styles across five scenarios) of 3D full-body skeleton gesture motion performed by professional Chinese TV broadcasters.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2409.14778.pdf' target='_blank'>https://arxiv.org/pdf/2409.14778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Egor Zakharov, Vanessa Sklyarova, Michael Black, Giljoo Nam, Justus Thies, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14778">Human Hair Reconstruction with Strand-Aligned 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2409.13268.pdf' target='_blank'>https://arxiv.org/pdf/2409.13268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Shi, Xuyang Cao, Jun Zhao, Guoxin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13268">JoyHallo: Digital human model for Mandarin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In audio-driven video generation, creating Mandarin videos presents significant challenges. Collecting comprehensive Mandarin datasets is difficult, and the complex lip movements in Mandarin further complicate model training compared to English. In this study, we collected 29 hours of Mandarin speech video from JD Health International Inc. employees, resulting in the jdh-Hallo dataset. This dataset includes a diverse range of ages and speaking styles, encompassing both conversational and specialized medical topics. To adapt the JoyHallo model for Mandarin, we employed the Chinese wav2vec2 model for audio feature embedding. A semi-decoupled structure is proposed to capture inter-feature relationships among lip, expression, and pose features. This integration not only improves information utilization efficiency but also accelerates inference speed by 14.3%. Notably, JoyHallo maintains its strong ability to generate English videos, demonstrating excellent cross-language generation capabilities. The code and models are available at https://jdh-algo.github.io/JoyHallo.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2409.07649.pdf' target='_blank'>https://arxiv.org/pdf/2409.07649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian, Xiaohu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07649">DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking video generation has advanced significantly, but existing methods often depend on video-to-video translation techniques and traditional generative networks like GANs and they typically generate taking heads and co-speech gestures separately, leading to less coherent outputs. Furthermore, the gestures produced by these methods often appear overly smooth or subdued, lacking in diversity, and many gesture-centric approaches do not integrate talking head generation. To address these limitations, we introduce DiffTED, a new approach for one-shot audio-driven TED-style talking video generation from a single image. Specifically, we leverage a diffusion model to generate sequences of keypoints for a Thin-Plate Spline motion model, precisely controlling the avatar's animation while ensuring temporally coherent and diverse gestures. This innovative approach utilizes classifier-free guidance, empowering the gestures to flow naturally with the audio input without relying on pre-trained classifiers. Experiments demonstrate that DiffTED generates temporally coherent talking videos with diverse co-speech gestures.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2409.00383.pdf' target='_blank'>https://arxiv.org/pdf/2409.00383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anish Kundu, Giulia Barbareschi, Midori Kawaguchi, Yuichiro Yano, Mizuki Ohashi, Kaori Kitaoka, Aya Seike, Kouta Minamizawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00383">"I Wanted to Create my Ideal Self": Exploring Avatar Perception of LGBTQ+ Users for Therapy in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we explore the potential of utilizing Virtual Reality (VR) as a therapeutic tool for supporting individuals in the LGBTQ+ community, who often face elevated risks of mental health issues. Specifically, we investigated the effectiveness of using pre-existing avatars compared to allowing individuals to create their own avatars through a website, and their experience in a VR space when using these avatars. We conducted a user study (n=10) measuring heart rate variability (HRV) and gathering subjective feedback through semi-structured interviews conducted in VR. Avatar creation was facilitated using an online platform, and conversations took place within a two-user VR space developed in a commercially available VR application. Our findings suggest that users significantly prefer creating their own avatars in the context of therapy sessions, and while there was no statistically significant difference, there was a consistent trend of enhanced physiological response when using self-made avatars in VR. This study provides initial empirical support for the importance of custom avatar creation in utilizing VR for therapy within the LGBTQ+ community.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2408.10588.pdf' target='_blank'>https://arxiv.org/pdf/2408.10588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijing Shao, Duotun Wang, Qing-Yao Tian, Yao-Dong Yang, Hengyu Meng, Zeyu Cai, Bo Dong, Yu Zhang, Kang Zhang, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10588">DEGAS: Detailed Expressions on Full-Body Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although neural rendering has made significant advances in creating lifelike, animatable full-body and head avatars, incorporating detailed expressions into full-body avatars remains largely unexplored. We present DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for full-body avatars with rich facial expressions. Trained on multiview videos of a given subject, our method learns a conditional variational autoencoder that takes both the body motion and facial expression as driving signals to generate Gaussian maps in the UV layout. To drive the facial expressions, instead of the commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to adopt the expression latent space trained solely on 2D portrait images, bridging the gap between 2D talking faces and 3D avatars. Leveraging the rendering capability of 3DGS and the rich expressiveness of the expression latent space, the learned avatars can be reenacted to reproduce photorealistic rendering images with subtle and accurate facial expressions. Experiments on an existing dataset and our newly proposed dataset of full-body talking avatars demonstrate the efficacy of our method. We also propose an audio-driven extension of our method with the help of 2D talking faces, opening new possibilities for interactive AI agents.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2408.06019.pdf' target='_blank'>https://arxiv.org/pdf/2408.06019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06019">HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2408.00370.pdf' target='_blank'>https://arxiv.org/pdf/2408.00370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Bozuo Zhao, Jingmei Wu, Yanbing Jiang, Hui Du, Zhenqing Ye, Jiayang Zhu, WeiFan Zhong, Leyao Yan, Xiaomeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00370">DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2 framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation is an emerging domain within virtual human creation, where current methods predominantly utilize Transformer-based architectures that necessitate extensive memory and are characterized by slow inference speeds. In response to these limitations, we propose \textit{DiM-Gestures}, a novel end-to-end generative model crafted to create highly personalized 3D full-body gestures solely from raw speech audio, employing Mamba-based architectures. This model integrates a Mamba-based fuzzy feature extractor with a non-autoregressive Adaptive Layer Normalization (AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba framework and a WavLM pre-trained model, autonomously derives implicit, continuous fuzzy features, which are then unified into a singular latent feature. This feature is processed by the AdaLN Mamba-2, which implements a uniform conditional mechanism across all tokens to robustly model the interplay between the fuzzy features and the resultant gesture sequence. This innovative approach guarantees high fidelity in gesture-speech synchronization while maintaining the naturalness of the gestures. Employing a diffusion model for training and inference, our framework has undergone extensive subjective and objective evaluations on the ZEGGS and BEAT datasets. These assessments substantiate our model's enhanced performance relative to contemporary state-of-the-art methods, demonstrating competitive outcomes with the DiTs architecture (Persona-Gestors) while optimizing memory usage and accelerating inference speed.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2407.08011.pdf' target='_blank'>https://arxiv.org/pdf/2407.08011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton, Reza Keshavarz, Alejandro Beacco, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08011">Stretch your reach: Studying Self-Avatar and Controller Misalignment in Virtual Reality Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive Virtual Reality typically requires a head-mounted display (HMD) to visualize the environment and hand-held controllers to interact with the virtual objects. Recently, many applications display full-body avatars to represent the user and animate the arms to follow the controllers. Embodiment is higher when the self-avatar movements align correctly with the user. However, having a full-body self-avatar following the user's movements can be challenging due to the disparities between the virtual body and the user's body. This can lead to misalignments in the hand position that can be noticeable when interacting with virtual objects. In this work, we propose five different interaction modes to allow the user to interact with virtual objects despite the self-avatar and controller misalignment and study their influence on embodiment, proprioception, preference, and task performance. We modify aspects such as whether the virtual controllers are rendered, whether controllers are rendered in their real physical location or attached to the user's hand, and whether stretching the avatar arms to always reach the real controllers. We evaluate the interaction modes both quantitatively (performance metrics) and qualitatively (embodiment, proprioception, and user preference questionnaires). Our results show that the stretching arms solution, which provides body continuity and guarantees that the virtual hands or controllers are in the correct location, offers the best results in embodiment, user preference, proprioception, and performance. Also, rendering the controller does not have an effect on either embodiment or user preference.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2406.19070.pdf' target='_blank'>https://arxiv.org/pdf/2406.19070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Xuan, Xinyang Li, Gongxin Yao, Shiwei Zhou, Donghui Sun, Xiaoxin Chen, Yu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19070">FAGhead: Fully Animate Gaussian Head from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity reconstruction of 3D human avatars has a wild application in visual reality. In this paper, we introduce FAGhead, a method that enables fully controllable human portraits from monocular videos. We explicit the traditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to reconstruct with complex expressions. Furthermore, we employ a novel Point-based Learnable Representation Field (PLRF) with learnable Gaussian point positions to enhance reconstruction performance. Meanwhile, to effectively manage the edges of avatars, we introduced the alpha rendering to supervise the alpha value of each pixel. Extensive experimental results on the open-source datasets and our capturing datasets demonstrate that our approach is able to generate high-fidelity 3D head avatars and fully control the expression and pose of the virtual avatars, which is outperforming than existing works.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2406.04254.pdf' target='_blank'>https://arxiv.org/pdf/2406.04254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salvatore Esposito, Qingshan Xu, Kacper Kania, Charlie Hewitt, Octave Mariotti, Lohit Petikam, Julien Valentin, Arno Onken, Oisin Mac Aodha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04254">GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections. Most existing approaches predict volumetric density to render multi-view consistent images. By employing volumetric rendering using neural radiance fields, they inherit a key limitation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes. To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner. Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF). This allows us to introduce useful priors to generate valid meshes. However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios. To alleviate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF. Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes. For evaluation, we introduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges presented by real-world datasets, which often lack 3D consistency and do not cover all camera angles. Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2405.11993.pdf' target='_blank'>https://arxiv.org/pdf/2405.11993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11993">GGAvatar: Geometric Adjustment of Gaussian Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2405.05928.pdf' target='_blank'>https://arxiv.org/pdf/2405.05928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyan Guo, Freeman Guo, Hongxin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05928">Moderating Embodied Cyber Threats Using Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement in computing and hardware, like spatial computing and VR headsets (e.g., Apple's Vision Pro) [1], has boosted the popularity of social VR platforms (VRChat, Rec Room, Meta HorizonWorlds) [2, 3, 4]. Unlike traditional digital interactions, social VR allows for more immersive experiences, with avatars that mimic users' real-time movements and enable physical-like interactions. However, the immersive nature of social VR may introduce intensified and more physicalized cyber threats-we define as "embodied cyber threats", including trash-talking, virtual "groping", and such virtual harassment and assault. These new cyber threats are more realistic and invasive due to direct, virtual interactions, underscoring the urgent need for comprehensive understanding and practical strategies to enhance safety and security in virtual environments.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2404.00524.pdf' target='_blank'>https://arxiv.org/pdf/2404.00524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00524">TexVocab: Texture Vocabulary-conditioned Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at https://texvocab.github.io/.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2403.09326.pdf' target='_blank'>https://arxiv.org/pdf/2403.09326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Xiaohang Zhan, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09326">HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current text-to-avatar methods often rely on implicit representations (e.g., NeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit and animate in graphics software. This paper introduces a novel framework for generating stylized head avatars from text guidance, which leverages locally learnable mesh deformation and 2D diffusion priors to achieve high-quality digital assets for attribute-preserving manipulation. Given a template mesh, our method represents mesh deformation with per-face Jacobians and adaptively modulates local deformation using a learnable vector field. This vector field enables anisotropic scaling while preserving the rotation of vertices, which can better express identity and geometric details. We employ landmark- and contour-based regularization terms to balance the expressiveness and plausibility of generated avatars from multiple views without relying on any specific shape prior. Our framework can generate realistic shapes and textures that can be further edited via text, while supporting seamless editing using the preserved attributes from the template mesh, such as 3DMM parameters, blendshapes, and UV coordinates. Extensive experiments demonstrate that our framework can generate diverse and expressive head avatars with high-quality meshes that artists can easily manipulate in graphics software, facilitating downstream applications such as efficient asset creation and animation with preserved attributes.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2403.06375.pdf' target='_blank'>https://arxiv.org/pdf/2403.06375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Tan, Bin Ji, Ye Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06375">FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating emotional talking faces is a practical yet challenging endeavor. To create a lifelike avatar, we draw upon two critical insights from a human perspective: 1) The connection between audio and the non-deterministic facial dynamics, encompassing expressions, blinks, poses, should exhibit synchronous and one-to-many mapping. 2) Vibrant expressions are often accompanied by emotion-aware high-definition (HD) textures and finely detailed teeth. However, both aspects are frequently overlooked by existing methods. To this end, this paper proposes using normalizing Flow and Vector-Quantization modeling to produce emotional talking faces that satisfy both insights concurrently (FlowVQTalker). Specifically, we develop a flow-based coefficient generator that encodes the dynamics of facial emotion into a multi-emotion-class latent space represented as a mixture distribution. The generation process commences with random sampling from the modeled distribution, guided by the accompanying audio, enabling both lip-synchronization and the uncertain nonverbal facial cues generation. Furthermore, our designed vector-quantization image generator treats the creation of expressive facial images as a code query task, utilizing a learned codebook to provide rich, high-quality textures that enhance the emotional perception of the results. Extensive experiments are conducted to showcase the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2402.10636.pdf' target='_blank'>https://arxiv.org/pdf/2402.10636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunsoo Cha, Byungjun Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10636">PEGASUS: Personalized Generative 3D Avatars with Composable Attributes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PEGASUS, a method for constructing a personalized generative 3D face avatar from monocular video sources. Our generative 3D avatar enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) while preserving the identity. Our approach consists of two stages: synthetic database generation and constructing a personalized generative avatar. We generate a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing the attributes from monocular videos of diverse identities. Then, we build a person-specific generative 3D avatar that can modify its attributes continuously while preserving its identity. Through extensive experiments, we demonstrate that our method of generating a synthetic database and creating a 3D generative avatar is the most effective in preserving identity while achieving high realism. Subsequently, we introduce a zero-shot approach to achieve the same goal of generative modeling more efficiently by leveraging a previously constructed personalized generative model.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2402.05592.pdf' target='_blank'>https://arxiv.org/pdf/2402.05592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anisha Ghosh, Aditya Mitra, Anik Saha, Sibi Chakkaravarthy Sethuraman, Anitha Subramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05592">MERP: Metaverse Extended Realtiy Portal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A standardized control system called Metaverse Extended Reality Portal (MERP) is presented as a solution to the issues with conventional VR eyewear. The MERP system improves user awareness of the physical world while offering an immersive 3D view of the metaverse by using a shouldermounted projector to display a Heads-Up Display (HUD) in a designated Metaverse Experience Room. To provide natural and secure interaction inside the metaverse, a compass module and gyroscope integration enable accurate mapping of real-world motions to avatar actions. Through user tests and research, the MERP system shows that it may reduce mishaps brought on by poor spatial awareness, offering an improved metaverse experience and laying the groundwork for future developments in virtual reality technology. MERP, which is compared with existing Virtual Reality (VR) glasses used to traverse the metaverse, is projected to become a seamless, novel and better alternative. Existing VR headsets and AR glasses have well-known drawbacks that making them ineffective for prolonged usage as it causes harm to the eyes.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2401.12979.pdf' target='_blank'>https://arxiv.org/pdf/2401.12979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeksoo Kim, Byungjun Kim, Shunsuke Saito, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12979">GALA: Generating Animatable Layered Assets from a Single Scan</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2401.08739.pdf' target='_blank'>https://arxiv.org/pdf/2401.08739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08739">EgoGen: An Egocentric Synthetic Data Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2312.04784.pdf' target='_blank'>https://arxiv.org/pdf/2312.04784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Rao, Eduardo Perez Pellitero, Benjamin Busam, Yiren Zhou, Jifei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04784">Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D avatar generation excel with multi-view supervision for photorealistic models. However, monocular counterparts lag in quality despite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB is a fully-differentiable pipeline that learns high-fidelity 3D human avatars from just a single RGB video. A pose-conditioned deformable NeRF is optimized to volumetrically represent a human subject in canonical T-pose. The canonical representation is then leveraged to efficiently associate neural textures using 2D-3D correspondences. This enables the separation of diffused color generation and lighting correction branches that jointly compose an RGB prediction. The design allows to control intermediate results for human pose, body shape, texture, and lighting with text prompts. An image-conditioned diffusion model thereby helps to animate appearance and pose of the 3D avatar to create video sequences with previously unseen human motion. Extensive experiments show that ReCaLaB outperforms previous monocular approaches in terms of image quality for image synthesis tasks. Moreover, natural language offers an intuitive user interface for creative manipulation of 3D human avatars.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2312.02902.pdf' target='_blank'>https://arxiv.org/pdf/2312.02902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo PÃ©rez-Pellitero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02902">HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, a model that uses 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit 3DGS representation with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, surpassing baselines by up to 2dB, while accelerating rendering speed by over x10.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2312.02521.pdf' target='_blank'>https://arxiv.org/pdf/2312.02521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Tang, Jieren Deng, Zhihong Pan, Hao Tian, Pratik Chaudhari, Xin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02521">RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images for Subject-Driven Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based methods have demonstrated remarkable capabilities in generating a diverse array of high-quality images, sparking interests for styled avatars, virtual try-on, and more. Previous methods use the same reference image as the target. An overlooked aspect is the leakage of the target's spatial information, style, etc. from the reference, harming the generated diversity and causing shortcuts. However, this approach continues as widely available datasets usually consist of single images not grouped by identities, and it is expensive to recollect large-scale same-identity data. Moreover, existing metrics adopt decoupled evaluation on text alignment and identity preservation, which fail at distinguishing between balanced outputs and those that over-fit to one aspect. In this paper, we propose a multi-level, same-identity dataset RetriBooru, which groups anime characters by both face and cloth identities. RetriBooru enables adopting reference images of the same character and outfits as the target, while keeping flexible gestures and actions. We benchmark previous methods on our dataset, and demonstrate the effectiveness of training with a reference image different from target (but same identity). We introduce a new concept composition task, where the conditioning encoder learns to retrieve different concepts from several reference images, and modify a baseline network RetriNet for the new task. Finally, we introduce a novel class of metrics named Similarity Weighted Diversity (SWD), to measure the overlooked diversity and better evaluate the alignment between similarity and diversity.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2311.17113.pdf' target='_blank'>https://arxiv.org/pdf/2311.17113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo PÃ©rez-Pellitero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17113">Human Gaussian Splatting: Real-time Rendering of Animatable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the problem of real-time rendering of photorealistic human body avatars learned from multi-view videos. While the classical approaches to model and render virtual humans generally use a textured mesh, recent research has developed neural body representations that achieve impressive visual quality. However, these models are difficult to render in real-time and their quality degrades when the character is animated with body poses different than the training observations. We propose an animatable human model based on 3D Gaussian Splatting, that has recently emerged as a very efficient alternative to neural radiance fields. The body is represented by a set of gaussian primitives in a canonical space which is deformed with a coarse to fine approach that combines forward skinning and local non-rigid refinement. We describe how to learn our Human Gaussian Splatting (HuGS) model in an end-to-end fashion from multi-view observations, and evaluate it against the state-of-the-art approaches for novel pose synthesis of clothed body. Our method achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4 dataset while being able to render in real-time (80 fps for 512x512 resolution).
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2311.02191.pdf' target='_blank'>https://arxiv.org/pdf/2311.02191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton, Haoran Yun, Andreas Aristidou, Carlos Andujar, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02191">SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable human motion reconstruction is crucial for creating natural interactions of full-body avatars in Virtual Reality (VR) and entertainment applications. As the Metaverse and social applications gain popularity, users are seeking cost-effective solutions to create full-body animations that are comparable in quality to those produced by commercial motion capture systems. In order to provide affordable solutions, though, it is important to minimize the number of sensors attached to the subject's body. Unfortunately, reconstructing the full-body pose from sparse data is a heavily under-determined problem. Some studies that use IMU sensors face challenges in reconstructing the pose due to positional drift and ambiguity of the poses. In recent years, some mainstream VR systems have released 6-degree-of-freedom (6-DoF) tracking devices providing positional and rotational information. Nevertheless, most solutions for reconstructing full-body poses rely on traditional inverse kinematics (IK) solutions, which often produce non-continuous and unnatural poses. In this article, we introduce SparsePoser, a novel deep learning-based solution for reconstructing a full-body pose from a reduced set of six tracking devices. Our system incorporates a convolutional-based autoencoder that synthesizes high-quality continuous human poses by learning the human motion manifold from motion capture data. Then, we employ a learned IK component, made of multiple lightweight feed-forward neural networks, to adjust the hands and feet toward the corresponding trackers. We extensively evaluate our method on publicly available motion capture datasets and with real-time live demos. We show that our method outperforms state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and can be used for users with different body dimensions and proportions.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2310.16287.pdf' target='_blank'>https://arxiv.org/pdf/2310.16287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejas S. Prabhune, Peter Wu, Bohan Yu, Gopala K. Anumanchipalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16287">Towards Streaming Speech-to-Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Streaming speech-to-avatar synthesis creates real-time animations for a virtual character from audio data. Accurate avatar representations of speech are important for the visualization of sound in linguistics, phonetics, and phonology, visual feedback to assist second language acquisition, and virtual embodiment for paralyzed patients. Previous works have highlighted the capability of deep articulatory inversion to perform high-quality avatar animation using electromagnetic articulography (EMA) features. However, these models focus on offline avatar synthesis with recordings rather than real-time audio, which is necessary for live avatar visualization or embodiment. To address this issue, we propose a method using articulatory inversion for streaming high quality facial and inner-mouth avatar animation from real-time audio. Our approach achieves 130ms average streaming latency for every 0.1 seconds of audio with a 0.792 correlation with ground truth articulations. Finally, we show generated mouth and tongue animations to demonstrate the efficacy of our methodology.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2310.04194.pdf' target='_blank'>https://arxiv.org/pdf/2310.04194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyuan Wang, Yiqian Wu, Yongliang Yang, Chen Liu, Xiaogang Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04194">Enhancing the Authenticity of Rendered Portraits with Identity-Consistent Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite rapid advances in computer graphics, creating high-quality photo-realistic virtual portraits is prohibitively expensive. Furthermore, the well-know ''uncanny valley'' effect in rendered portraits has a significant impact on the user experience, especially when the depiction closely resembles a human likeness, where any minor artifacts can evoke feelings of eeriness and repulsiveness. In this paper, we present a novel photo-realistic portrait generation framework that can effectively mitigate the ''uncanny valley'' effect and improve the overall authenticity of rendered portraits. Our key idea is to employ transfer learning to learn an identity-consistent mapping from the latent space of rendered portraits to that of real portraits. During the inference stage, the input portrait of an avatar can be directly transferred to a realistic portrait by changing its appearance style while maintaining the facial identity. To this end, we collect a new dataset, Daz-Rendered-Faces-HQ (DRFHQ), that is specifically designed for rendering-style portraits. We leverage this dataset to fine-tune the StyleGAN2 generator, using our carefully crafted framework, which helps to preserve the geometric and color features relevant to facial identity. We evaluate our framework using portraits with diverse gender, age, and race variations. Qualitative and quantitative evaluations and ablation studies show the advantages of our method compared to state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2309.05782.pdf' target='_blank'>https://arxiv.org/pdf/2309.05782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Grishchenko, Geng Yan, Eduard Gabriel Bazavan, Andrei Zanfir, Nikolai Chinaev, Karthik Raveendran, Matthias Grundmann, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05782">Blendshapes GHUM: Real-time Monocular Facial Blendshape Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Blendshapes GHUM, an on-device ML pipeline that predicts 52 facial blendshape coefficients at 30+ FPS on modern mobile phones, from a single monocular RGB image and enables facial motion capture applications like virtual avatars. Our main contributions are: i) an annotation-free offline method for obtaining blendshape coefficients from real-world human scans, ii) a lightweight real-time model that predicts blendshape coefficients based on facial landmarks.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2308.05995.pdf' target='_blank'>https://arxiv.org/pdf/2308.05995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05995">Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2307.14770.pdf' target='_blank'>https://arxiv.org/pdf/2307.14770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Wu, Hao Xu, Xiangjun Tang, Yue Shangguan, Hongbo Fu, Xiaogang Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14770">3DPortraitGAN: Learning One-Quarter Headshot 3D GANs from a Single-View Portrait Dataset with Diverse Body Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D-aware face generators are typically trained on 2D real-life face image datasets that primarily consist of near-frontal face data, and as such, they are unable to construct one-quarter headshot 3D portraits with complete head, neck, and shoulder geometry. Two reasons account for this issue: First, existing facial recognition methods struggle with extracting facial data captured from large camera angles or back views. Second, it is challenging to learn a distribution of 3D portraits covering the one-quarter headshot region from single-view data due to significant geometric deformation caused by diverse body poses. To this end, we first create the dataset 360Â°-Portrait-HQ (360Â°PHQ for short) which consists of high-quality single-view real portraits annotated with a variety of camera parameters (the yaw angles span the entire 360Â° range) and body poses. We then propose 3DPortraitGAN, the first 3D-aware one-quarter headshot portrait generator that learns a canonical 3D avatar distribution from the 360Â°PHQ dataset with body pose self-learning. Our model can generate view-consistent portrait images from all camera angles with a canonical one-quarter headshot 3D representation. Our experiments show that the proposed framework can accurately predict portrait body poses and generate view-consistent, realistic portrait images with complete geometry from all camera angles.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2307.09558.pdf' target='_blank'>https://arxiv.org/pdf/2307.09558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Luis Ponton, VÃ­ctor Ceballos, Lesly Acosta, Alejandro RÃ­os, Eva MonclÃºs, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09558">Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of the metaverse, self-avatars are gaining popularity, as they can enhance presence and provide embodiment when a user is immersed in Virtual Reality. They are also very important in collaborative Virtual Reality to improve communication through gestures. Whether we are using a complex motion capture solution or a few trackers with inverse kinematics (IK), it is essential to have a good match in size between the avatar and the user, as otherwise mismatches in self-avatar posture could be noticeable for the user. To achieve such a correct match in dimensions, a manual process is often required, with the need for a second person to take measurements of body limbs and introduce them into the system. This process can be time-consuming, and prone to errors. In this paper, we propose an automatic measuring method that simply requires the user to do a small set of exercises while wearing a Head-Mounted Display (HMD), two hand controllers, and three trackers. Our work provides an affordable and quick method to automatically extract user measurements and adjust the virtual humanoid skeleton to the exact dimensions. Our results show that our method can reduce the misalignment produced by the IK system when compared to other solutions that simply apply a uniform scaling to an avatar based on the height of the HMD, and make assumptions about the locations of joints with respect to the trackers.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2307.00804.pdf' target='_blank'>https://arxiv.org/pdf/2307.00804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongjin Luo, Dong Du, Heming Zhu, Yizhou Yu, Hongbo Fu, Xiaoguang Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00804">SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling 3D avatars benefits various application scenarios such as AR/VR, gaming, and filming. Character faces contribute significant diversity and vividity as a vital component of avatars. However, building 3D character face models usually requires a heavy workload with commercial tools, even for experienced artists. Various existing sketch-based tools fail to support amateurs in modeling diverse facial shapes and rich geometric details. In this paper, we present SketchMetaFace - a sketching system targeting amateur users to model high-fidelity 3D faces in minutes. We carefully design both the user interface and the underlying algorithm. First, curvature-aware strokes are adopted to better support the controllability of carving facial details. Second, considering the key problem of mapping a 2D sketch map to a 3D model, we develop a novel learning-based method termed "Implicit and Depth Guided Mesh Modeling" (IDGMM). It fuses the advantages of mesh, implicit, and depth representations to achieve high-quality results with high efficiency. In addition, to further support usability, we present a coarse-to-fine 2D sketching interface design and a data-driven stroke suggestion tool. User studies demonstrate the superiority of our system over existing modeling tools in terms of the ease to use and visual quality of results. Experimental analyses also show that IDGMM reaches a better trade-off between accuracy and efficiency. SketchMetaFace is available at https://zhongjinluo.github.io/SketchMetaFace/.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2306.17123.pdf' target='_blank'>https://arxiv.org/pdf/2306.17123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai-En Lin, Alex Trevithick, Keli Cheng, Michel Sarkis, Mohsen Ghafoorian, Ning Bi, Gerhard Reitmayr, Ravi Ramamoorthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17123">PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Portrait synthesis creates realistic digital avatars which enable users to interact with others in a compelling way. Recent advances in StyleGAN and its extensions have shown promising results in synthesizing photorealistic and accurate reconstruction of human faces. However, previous methods often focus on frontal face synthesis and most methods are not able to handle large head rotations due to the training data distribution of StyleGAN. In this work, our goal is to take as input a monocular video of a face, and create an editable dynamic portrait able to handle extreme head poses. The user can create novel viewpoints, edit the appearance, and animate the face. Our method utilizes pivotal tuning inversion (PTI) to learn a personalized video prior from a monocular video sequence. Then we can input pose and expression coefficients to MLPs and manipulate the latent vectors to synthesize different viewpoints and expressions of the subject. We also propose novel loss functions to further disentangle pose and expression in the latent space. Our algorithm shows much better performance over previous approaches on monocular video datasets, and it is also capable of running in real-time at 54 FPS on an RTX 3080.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2306.08990.pdf' target='_blank'>https://arxiv.org/pdf/2306.08990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Radek DanÄÄek, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael J. Black, Timo Bolkart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08990">Emotional Speech-Driven Animation with Content-Emotion Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2306.07579.pdf' target='_blank'>https://arxiv.org/pdf/2306.07579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricong Huang, Peiwen Lai, Yipeng Qin, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07579">Parametric Implicit Face Representation for Audio-Driven Facial Reenactment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven facial reenactment is a crucial technique that has a range of applications in film-making, virtual avatars and video conferences. Existing works either employ explicit intermediate face representations (e.g., 2D facial landmarks or 3D face models) or implicit ones (e.g., Neural Radiance Fields), thus suffering from the trade-offs between interpretability and expressive power, hence between controllability and quality of the results. In this work, we break these trade-offs with our novel parametric implicit face representation and propose a novel audio-driven facial reenactment framework that is both controllable and can generate high-quality talking heads. Specifically, our parametric implicit representation parameterizes the implicit representation with interpretable parameters of 3D face models, thereby taking the best of both explicit and implicit methods. In addition, we propose several new techniques to improve the three components of our framework, including i) incorporating contextual information into the audio-to-expression parameters encoding; ii) using conditional image synthesis to parameterize the implicit representation and implementing it with an innovative tri-plane structure for efficient learning; iii) formulating facial reenactment as a conditional image inpainting problem and proposing a novel data augmentation technique to improve model generalizability. Extensive experiments demonstrate that our method can generate more realistic results than previous methods with greater fidelity to the identities and talking styles of speakers.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2306.07399.pdf' target='_blank'>https://arxiv.org/pdf/2306.07399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthieu Armando, Laurence Boissieux, Edmond Boyer, Jean-Sebastien Franco, Martin Humenberger, Christophe Legras, Vincent Leroy, Mathieu Marsot, Julien Pansiot, Sergi Pujades, Rim Rekik, Gregory Rogez, Anilkumar Swamy, Stefanie Wuhrer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07399">4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in varying outfits exhibiting large displacements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents 4DHumanOutfit, a new dataset of densely sampled spatio-temporal 4D human motion data of different actors, outfits and motions. The dataset is designed to contain different actors wearing different outfits while performing different motions in each outfit. In this way, the dataset can be seen as a cube of data containing 4D motion sequences along 3 axes with identity, outfit and motion. This rich dataset has numerous potential applications for the processing and creation of digital humans, e.g. augmented reality, avatar creation and virtual try on. 4DHumanOutfit is released for research purposes at https://kinovis.inria.fr/4dhumanoutfit/. In addition to image data and 4D reconstructions, the dataset includes reference solutions for each axis. We present independent baselines along each axis that demonstrate the value of these reference solutions for evaluation tasks.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2305.11870.pdf' target='_blank'>https://arxiv.org/pdf/2305.11870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11870">Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a 3D generation pipeline that uses diffusion models to generate realistic human digital avatars. Due to the wide variety of human identities, poses, and stochastic details, the generation of 3D human meshes has been a challenging problem. To address this, we decompose the problem into 2D normal map generation and normal map-based 3D reconstruction. Specifically, we first simultaneously generate realistic normal maps for the front and backside of a clothed human, dubbed dual normal maps, using a pose-conditional diffusion model. For 3D reconstruction, we "carve" the prior SMPL-X mesh to a detailed 3D mesh according to the normal maps through mesh optimization. To further enhance the high-frequency details, we present a diffusion resampling scheme on both body and facial regions, thus encouraging the generation of realistic digital avatars. We also seamlessly incorporate a recent text-to-image diffusion model to support text-based human identity control. Our method, namely, Chupa, is capable of generating realistic 3D clothed humans with better perceptual quality and identity variety.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2304.05334.pdf' target='_blank'>https://arxiv.org/pdf/2304.05334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Yun, Jose Luis Ponton, Carlos Andujar, Nuria Pelechano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05334">Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of self-avatars is gaining popularity thanks to affordable VR headsets. Unfortunately, mainstream VR devices often use a small number of trackers and provide low-accuracy animations. Previous studies have shown that the Sense of Embodiment, and in particular the Sense of Agency, depends on the extent to which the avatar's movements mimic the user's movements. However, few works study such effect for tasks requiring a precise interaction with the environment, i.e., tasks that require accurate manipulation, precise foot stepping, or correct body poses. In these cases, users are likely to notice inconsistencies between their self-avatars and their actual pose. In this paper, we study the impact of the animation fidelity of the user avatar on a variety of tasks that focus on arm movement, leg movement and body posture. We compare three different animation techniques: two of them using Inverse Kinematics to reconstruct the pose from sparse input (6 trackers), and a third one using a professional motion capture system with 17 inertial sensors. We evaluate these animation techniques both quantitatively (completion time, unintentional collisions, pose accuracy) and qualitatively (Sense of Embodiment). Our results show that the animation quality affects the Sense of Embodiment. Inertial-based MoCap performs significantly better in mimicking body poses. Surprisingly, IK-based solutions using fewer sensors outperformed MoCap in tasks requiring accurate positioning, which we attribute to the higher latency and the positional drift that causes errors at the end-effectors, which are more noticeable in contact areas such as the feet.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2304.02001.pdf' target='_blank'>https://arxiv.org/pdf/2304.02001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, Kwan-Yee Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02001">MonoHuman: Animatable Human Neural Field from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual avatars with free-view control is crucial for various applications like virtual reality and digital entertainment. Previous studies have attempted to utilize the representation power of the neural radiance field (NeRF) to reconstruct the human body from monocular videos. Recent works propose to graft a deformation network into the NeRF to further model the dynamics of the human neural field for animating vivid human motions. However, such pipelines either rely on pose-dependent representations or fall short of motion coherency due to frame-independent optimization, making it difficult to generalize to unseen pose sequences realistically. In this paper, we propose a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under arbitrary novel poses. Our key insight is to model the deformation field with bi-directional constraints and explicitly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Specifically, we first propose a Shared Bidirectional Deformation module, which creates a pose-independent generalizable deformation field by disentangling backward and forward deformation correspondences into shared skeletal motion weight and separate non-rigid motions. Then, we devise a Forward Correspondence Search module, which queries the correspondence feature of keyframes to guide the rendering network. The rendered results are thus multi-view consistent with high fidelity, even under challenging novel pose settings. Extensive experiments demonstrate the superiority of our proposed MonoHuman over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2303.17606.pdf' target='_blank'>https://arxiv.org/pdf/2303.17606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17606">AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural implicit fields are powerful for representing 3D scenes and generating high-quality novel views, but it remains challenging to use such implicit representations for creating a 3D human avatar with a specific identity and artistic style that can be easily animated. Our proposed method, AvatarCraft, addresses this challenge by using diffusion models to guide the learning of geometry and texture for a neural avatar based on a single text prompt. We carefully design the optimization framework of neural implicit fields, including a coarse-to-fine multi-bounding box training strategy, shape regularization, and diffusion-based constraints, to produce high-quality geometry and texture. Additionally, we make the human avatar animatable by deforming the neural implicit field with an explicit warping field that maps the target human mesh to a template human mesh, both represented using parametric human models. This simplifies animation and reshaping of the generated avatar by controlling pose and shape parameters. Extensive experiments on various text descriptions show that AvatarCraft is effective and robust in creating human avatars and rendering novel views, poses, and shapes. Our project page is: https://avatar-craft.github.io/.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2303.13831.pdf' target='_blank'>https://arxiv.org/pdf/2303.13831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulia Barbareschi, Midori Kawaguchi, Hiroki Kato, Masato Nagahiro, Kazuaki Takehuchi, Yoshifumi Shiiba, Shunichi Kasahara, Kai Kunze, Kouta Minamizawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13831">'I am both here and there' Parallel Control of Multiple Robotic Avatars by Disabled Workers in a CafÃ©</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic avatars can help disabled people extend their reach in interacting with the world. Technological advances make it possible for individuals to embody multiple avatars simultaneously. However, existing studies have been limited to laboratory conditions and did not involve disabled participants. In this paper, we present a real-world implementation of a parallel control system allowing disabled workers in a cafÃ© to embody multiple robotic avatars at the same time to carry out different tasks. Our data corpus comprises semi-structured interviews with workers, customer surveys, and videos of cafÃ© operations. Results indicate that the system increases workers' agency, enabling them to better manage customer journeys. Parallel embodiment and transitions between avatars create multiple interaction loops where the links between disabled workers and customers remain consistent, but the intermediary avatar changes. Based on our observations, we theorize that disabled individuals possess specific competencies that increase their ability to manage multiple avatar bodies.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2303.06537.pdf' target='_blank'>https://arxiv.org/pdf/2303.06537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungbok Shin, Sanghyun Hong, Niklas Elmqvist
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06537">Perceptual Pat: A Virtual Human System for Iterative Visualization Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing a visualization is often a process of iterative refinement where the designer improves a chart over time by adding features, improving encodings, and fixing mistakes. However, effective design requires external critique and evaluation. Unfortunately, such critique is not always available on short notice and evaluation can be costly. To address this need, we present Perceptual Pat, an extensible suite of AI and computer vision techniques that forms a virtual human visual system for supporting iterative visualization design. The system analyzes snapshots of a visualization using an extensible set of filters - including gaze maps, text recognition, color analysis, etc - and generates a report summarizing the findings. The web-based Pat Design Lab provides a version tracking system that enables the designer to track improvements over time. We validate Perceptual Pat using a longitudinal qualitative study involving 4 professional visualization designers that used the tool over a few days to design a new visualization.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2303.00377.pdf' target='_blank'>https://arxiv.org/pdf/2303.00377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runchuan Zhu, Naye Ji, Youbing Zhao, Fan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00377">Few-shots Portrait Generation with Style Enhancement and Identity Preservation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, the wide application of virtual digital human promotes the comprehensive prosperity and development of digital culture supported by digital economy. The personalized portrait automatically generated by AI technology needs both the natural artistic style and human sentiment. In this paper, we propose a novel StyleIdentityGAN model, which can ensure the identity and artistry of the generated portrait at the same time. Specifically, the style-enhanced module focuses on artistic style features decoupling and transferring to improve the artistry of generated virtual face images. Meanwhile, the identity-enhanced module preserves the significant features extracted from the input photo. Furthermore, the proposed method requires a small number of reference style data. Experiments demonstrate the superiority of StyleIdentityGAN over state-of-art methods in artistry and identity effects, with comparisons done qualitatively, quantitatively and through a perceptual user study. Code has been released on Github3.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2301.10047.pdf' target='_blank'>https://arxiv.org/pdf/2301.10047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10047">DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture synthesis is a field of growing interest in virtual human creation. However, a critical challenge is the inherent intricate one-to-many mapping between speech and gestures. Previous studies have explored and achieved significant progress with generative models. Notwithstanding, most synthetic gestures are still vastly less natural. This paper presents DiffMotion, a novel speech-driven gesture synthesis architecture based on diffusion models. The model comprises an autoregressive temporal encoder and a denoising diffusion probability Module. The encoder extracts the temporal context of the speech input and historical gestures. The diffusion module learns a parameterized Markov chain to gradually convert a simple distribution into a complex distribution and generates the gestures according to the accompanied speech. Compared with baselines, objective and subjective evaluations confirm that our approach can produce natural and diverse gesticulation and demonstrate the benefits of diffusion-based models on speech-driven gesture synthesis.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2301.01770.pdf' target='_blank'>https://arxiv.org/pdf/2301.01770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibi Chakkaravarthy Sethuraman, Aditya Mitra, Anisha Ghosh, Gautam Galada, Anitha Subramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01770">MetaSecure: A Passwordless Authentication for the Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse in general holds a potential future for cyberspace. At the beginning of Web 2.0, it was witnessed that people were signing in with various pseudonyms or 'nyms', risking their online identities by increasing presence of fake accounts leading to difficulty in unique identification for different roles. However, in Web 3.0, the metaverse, a user's identity is tied to their original identity, where risking one poses a significant risk to the other. Therefore, this paper proposes a novel authentication system for securing digital assets, online identity, avatars, and accounts called Metasecure where a unique id for every entity or user to develop a human establishment is essential on a digital platform. The proposed passwordless system provides three layers of security using device attestation, facial recognition and use of physical security keys, security keys, or smartcards in accordance to Fast IDentity Online (FIDO2) specifications. It provides SDKs for authentication on any system including VR/XR glasses, thus ensuring seamlessness in accessing services in the Metaverse.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2301.00764.pdf' target='_blank'>https://arxiv.org/pdf/2301.00764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Lenz, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00764">Bimanual Telemanipulation with Force and Haptic Feedback through an Anthropomorphic Avatar System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic teleoperation is a key technology for a wide variety of applications. It allows sending robots instead of humans in remote, possibly dangerous locations while still using the human brain with its enormous knowledge and creativity, especially for solving unexpected problems. A main challenge in teleoperation consists of providing enough feedback to the human operator for situation awareness and thus create full immersion, as well as offering the operator suitable control interfaces to achieve efficient and robust task fulfillment. We present a bimanual telemanipulation system consisting of an anthropomorphic avatar robot and an operator station providing force and haptic feedback to the human operator. The avatar arms are controlled in Cartesian space with a direct mapping of the operator movements. The measured forces and torques on the avatar side are haptically displayed to the operator. We developed a predictive avatar model for limit avoidance which runs on the operator side, ensuring low latency. The system was successfully evaluated during the ANA Avatar XPRIZE competition semifinals. In addition, we performed in lab experiments and carried out a small user study with mostly untrained operators.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2509.17803.pdf' target='_blank'>https://arxiv.org/pdf/2509.17803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nabila Amadou, Kazi Injamamul Haque, Zerrin Yumak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17803">Effect of Appearance and Animation Realism on the Perception of Emotionally Expressive Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Virtual Human technology is growing with several potential applications in health, education, business and telecommunications. Investigating the perception of these virtual humans can help guide to develop better and more effective applications. Recent developments show that the appearance of the virtual humans reached to a very realistic level. However, there is not yet adequate analysis on the perception of appearance and animation realism for emotionally expressive virtual humans. In this paper, we designed a user experiment and analyzed the effect of a realistic virtual human's appearance realism and animation realism in varying emotion conditions. We found that higher appearance realism and higher animation realism leads to higher social presence and higher attractiveness ratings. We also found significant effects of animation realism on perceived realism and emotion intensity levels. Our study sheds light into how appearance and animation realism effects the perception of highly realistic virtual humans in emotionally expressive scenarios and points out to future directions.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2509.17748.pdf' target='_blank'>https://arxiv.org/pdf/2509.17748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Liu, Kazi Injamamul Haque, Zerrin Yumak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17748">"I don't like my avatar": Investigating Human Digital Doubles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating human digital doubles is becoming easier and much more accessible to everyone using consumer grade devices. In this work, we investigate how avatar style (realistic vs cartoon) and avatar familiarity (self, acquaintance, unknown person) affect self/other-identification, perceived realism, affinity and social presence with a controlled offline experiment. We created two styles of avatars (realistic-looking MetaHumans and cartoon-looking ReadyPlayerMe avatars) and facial animations stimuli for them using performance capture. Questionnaire responses demonstrate that higher appearance realism leads to a higher level of identification, perceived realism and social presence. However, avatars with familiar faces, especially those with high appearance realism, lead to a lower level of identification, perceived realism, and affinity. Although participants identified their digital doubles as their own, they consistently did not like their avatars, especially of realistic appearance. But they were less critical and more forgiving about their acquaintance's or an unknown person's digital double.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2509.09889.pdf' target='_blank'>https://arxiv.org/pdf/2509.09889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulia Botta, Marco Botta, Cristina Gena, Alessandro Mazzei, Massimo Donini, Alberto Lillo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09889">Using the Pepper Robot to Support Sign Language Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social robots are increasingly experimented in public and assistive settings, but their accessibility for Deaf users remains quite underexplored. Italian Sign Language (LIS) is a fully-fledged natural language that relies on complex manual and non-manual components. Enabling robots to communicate using LIS could foster more inclusive human robot interaction, especially in social environments such as hospitals, airports, or educational settings. This study investigates whether a commercial social robot, Pepper, can produce intelligible LIS signs and short signed LIS sentences. With the help of a Deaf student and his interpreter, an expert in LIS, we co-designed and implemented 52 LIS signs on Pepper using either manual animation techniques or a MATLAB based inverse kinematics solver. We conducted a exploratory user study involving 12 participants proficient in LIS, both Deaf and hearing. Participants completed a questionnaire featuring 15 single-choice video-based sign recognition tasks and 2 open-ended questions on short signed sentences. Results shows that the majority of isolated signs were recognized correctly, although full sentence recognition was significantly lower due to Pepper's limited articulation and temporal constraints. Our findings demonstrate that even commercially available social robots like Pepper can perform a subset of LIS signs intelligibly, offering some opportunities for a more inclusive interaction design. Future developments should address multi-modal enhancements (e.g., screen-based support or expressive avatars) and involve Deaf users in participatory design to refine robot expressivity and usability.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2509.01681.pdf' target='_blank'>https://arxiv.org/pdf/2509.01681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Ilyes Lakhal, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01681">GaussianGAN: Real-Time Photorealistic controllable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2508.15988.pdf' target='_blank'>https://arxiv.org/pdf/2508.15988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Ilyes Lakhal, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15988">Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The diversity of sign representation is essential for Sign Language Production (SLP) as it captures variations in appearance, facial expressions, and hand movements. However, existing SLP models are often unable to capture diversity while preserving visual quality and modelling non-manual attributes such as emotions. To address this problem, we propose a novel approach that leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital avatars from a generated reference image. We propose a novel sign feature aggregation module that explicitly models the non-manual features (\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands). We show that our proposed module ensures the preservation of linguistic content while seamlessly using reference images with different ethnic backgrounds to ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2508.12438.pdf' target='_blank'>https://arxiv.org/pdf/2508.12438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaron Aloni, Rotem Shalev-Arkushin, Yonatan Shafir, Guy Tevet, Ohad Fried, Amit Haim Bermano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12438">Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic facial expression generation from natural language is a crucial task in Computer Graphics, with applications in Animation, Virtual Avatars, and Human-Computer Interaction. However, current generative models suffer from datasets that are either speech-driven or limited to coarse emotion labels, lacking the nuanced, expressive descriptions needed for fine-grained control, and were captured using elaborate and expensive equipment. We hence present a new dataset of facial motion sequences featuring nuanced performances and semantic annotation. The data is easily collected using commodity equipment and LLM-generated natural language instructions, in the popular ARKit blendshape format. This provides riggable motion, rich with expressive performances and labels. We accordingly train two baseline models, and evaluate their performance for future benchmarking. Using our Express4D dataset, the trained models can learn meaningful text-to-expression motion generation and capture the many-to-many mapping of the two modalities. The dataset, code, and video examples are available on our webpage: https://jaron1990.github.io/Express4D/
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2507.22153.pdf' target='_blank'>https://arxiv.org/pdf/2507.22153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Wilson, Vincent Bindschaedler, Sophie JÃ¶rg, Sean Sheikholeslam, Kevin Butler, Eakta Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22153">Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic 3D avatar generation has rapidly improved in recent years, and realistic avatars that match a user's true appearance are more feasible in Mixed Reality (MR) than ever before. Yet, there are known risks to sharing one's likeness online, and photorealistic MR avatars could exacerbate these risks. If user likenesses were to be shared broadly, there are risks for cyber abuse or targeted fraud based on user appearances. We propose an alternate avatar rendering scheme for broader social MR -- synthesizing realistic avatars that preserve a user's demographic identity while being distinct enough from the individual user to protect facial biometric information. We introduce a methodology for privatizing appearance by isolating identity within the feature space of identity-encoding generative models. We develop two algorithms that then obfuscate identity: \epsmethod{} provides differential privacy guarantees and \thetamethod{} provides fine-grained control for the level of identity offset. These methods are shown to successfully generate de-identified virtual avatars across multiple generative architectures in 2D and 3D. With these techniques, it is possible to protect user privacy while largely preserving attributes related to sense of self. Employing these techniques in public settings could enable the use of photorealistic avatars broadly in MR, maintaining high realism and immersion without privacy risk.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2507.18649.pdf' target='_blank'>https://arxiv.org/pdf/2507.18649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Liu, Xiaolin Hong, Xuancheng Yang, Yudi Ruan, Xiang Lian, Michael Lingelbach, Hongwei Yi, Wei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18649">Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Livatar, a real-time audio-driven talking heads videos generation framework. Existing baselines suffer from limited lip-sync accuracy and long-term pose drift. We address these limitations with a flow matching based framework. Coupled with system optimizations, Livatar achieves competitive lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single A10 GPU. This makes high-fidelity avatars accessible to broader applications. Our project is available at https://www.hedra.com/ with with examples at https://h-liu1997.github.io/Livatar-1/
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2507.09862.pdf' target='_blank'>https://arxiv.org/pdf/2507.09862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09862">SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2506.21632.pdf' target='_blank'>https://arxiv.org/pdf/2506.21632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Li, Donggang Jia, Markus Hadwiger, Ivan Viola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21632">SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an interactive human avatar and the background from a monocular video of a dynamic human scene is highly challenging. In this work we adopt a strategy of point cloud decoupling and joint optimization to achieve the decoupled reconstruction of backgrounds and human bodies while preserving the interactivity of human motion. We introduce a position texture to subdivide the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human point cloud. To capture fine details of human dynamics and deformations, we incorporate a convolutional neural network structure to predict human body point cloud features based on texture. This strategy makes our approach free of hyperparameter tuning for densification and efficiently represents human points with half the point cloud of HUGS. This approach ensures high-quality human reconstruction and reduces GPU resource consumption during training. As a result, our method surpasses the previous state-of-the-art HUGS in reconstruction metrics while maintaining the ability to generalize to novel poses and views. Furthermore, our technique achieves real-time rendering at over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning (LBS) weights for human transformation. Additionally, this work demonstrates that this framework can be extended to animal scene reconstruction when an accurately-posed model of an animal is available.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2505.06411.pdf' target='_blank'>https://arxiv.org/pdf/2505.06411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangyu Du, Yang Yang, Xuehao Gao, Hongye Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06411">MAGE:A Multi-stage Avatar Generator with Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inferring full-body poses from Head Mounted Devices, which capture only 3-joint observations from the head and wrists, is a challenging task with wide AR/VR applications. Previous attempts focus on learning one-stage motion mapping and thus suffer from an over-large inference space for unobserved body joint motions. This often leads to unsatisfactory lower-body predictions and poor temporal consistency, resulting in unrealistic or incoherent motion sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator named MAGE that factorizes this one-stage direct motion mapping learning with a progressive prediction strategy. Specifically, given initial 3-joint motions, MAGE gradually inferring multi-scale body part poses at different abstract granularity levels, starting from a 6-part body representation and gradually refining to 22 joints. With decreasing abstract levels step by step, MAGE introduces more motion context priors from former prediction stages and thus improves realistic motion completion with richer constraint conditions and less ambiguity. Extensive experiments on large-scale datasets verify that MAGE significantly outperforms state-of-the-art methods with better accuracy and continuity.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2504.18805.pdf' target='_blank'>https://arxiv.org/pdf/2504.18805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jong Inn Park, Maanas Taneja, Qianwen Wang, Dongyeop Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18805">Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2504.12292.pdf' target='_blank'>https://arxiv.org/pdf/2504.12292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Schoneveld, Zhe Chen, Davide Davoli, Jiapeng Tang, Saimon Terazawa, Ko Nishino, Matthias NieÃner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12292">SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach. Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2504.07949.pdf' target='_blank'>https://arxiv.org/pdf/2504.07949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kefan Chen, Sergiu Oprea, Justin Theiss, Sreyas Mohan, Srinath Sridhar, Aayush Prakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07949">InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rising interest from the community in digital avatars coupled with the importance of expressions and gestures in communication, modeling natural avatar behavior remains an important challenge across many industries such as teleconferencing, gaming, and AR/VR. Human hands are the primary tool for interacting with the environment and essential for realistic human behavior modeling, yet existing 3D hand and head avatar models often overlook the crucial aspect of hand-body interactions, such as between hand and face. We present InteracttAvatar, the first model to faithfully capture the photorealistic appearance of dynamic hand and non-rigid hand-face interactions. Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian Splatting as well as a dynamic refinement module, captures pose-dependent change, e.g. the fine wrinkles and complex shadows that occur during articulation. Importantly, our hand-face interaction module models the subtle geometry and appearance dynamics that underlie common gestures. Through experiments of novel view synthesis, self reenactment and cross-identity reenactment, we demonstrate that InteracttAvatar can reconstruct hand and hand-face interactions from monocular or multiview videos with high-fidelity details and be animated with novel poses.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2504.01559.pdf' target='_blank'>https://arxiv.org/pdf/2504.01559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yahui Li, Zhi Zeng, Liming Pang, Guixuan Zhang, Shuwu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01559">RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling animatable human avatars from monocular or multi-view videos has been widely studied, with recent approaches leveraging neural radiance fields (NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in novel-view and novel-pose synthesis. However, existing methods often struggle to accurately capture the dynamics of loose clothing, as they primarily rely on global pose conditioning or static per-frame representations, leading to oversmoothing and temporal inconsistencies in non-rigid regions. To address this, We propose RealityAvatar, an efficient framework for high-fidelity digital human modeling, specifically targeting loosely dressed avatars. Our method leverages 3D Gaussian Splatting to capture complex clothing deformations and motion dynamics while ensuring geometric consistency. By incorporating a motion trend module and a latentbone encoder, we explicitly model pose-dependent deformations and temporal variations in clothing behavior. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach in capturing fine-grained clothing deformations and motion-driven shape variations. Our method significantly enhances structural fidelity and perceptual quality in dynamic human reconstruction, particularly in non-rigid regions, while achieving better consistency across temporal frames.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2503.15809.pdf' target='_blank'>https://arxiv.org/pdf/2503.15809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Gao, Jingtao Zhou, Dongyu Liu, Yuqi Zhou, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15809">Controlling Avatar Diffusion with Learnable Gaussian Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have made significant progress in digital human generation. However, most existing models still struggle to maintain 3D consistency, temporal coherence, and motion accuracy. A key reason for these shortcomings is the limited representation ability of commonly used control signals(e.g., landmarks, depth maps, etc.). In addition, the lack of diversity in identity and pose variations in public datasets further hinders progress in this area. In this paper, we analyze the shortcomings of current control signals and introduce a novel control signal representation that is optimizable, dense, expressive, and 3D consistent. Our method embeds a learnable neural Gaussian onto a parametric head surface, which greatly enhances the consistency and expressiveness of diffusion-based head models. Regarding the dataset, we synthesize a large-scale dataset with multiple poses and identities. In addition, we use real/synthetic labels to effectively distinguish real and synthetic data, minimizing the impact of imperfections in synthetic data on the generated head images. Extensive experiments show that our model outperforms existing methods in terms of realism, expressiveness, and 3D consistency. Our code, synthetic datasets, and pre-trained models will be released in our project page: https://ustc3dv.github.io/Learn2Control/
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2503.09293.pdf' target='_blank'>https://arxiv.org/pdf/2503.09293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Moreau, Mohammed Brahimi, Richard Shaw, Athanasios Papaioannou, Thomas Tanay, Zhensong Zhang, Eduardo PÃ©rez-Pellitero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09293">Better Together: Unified Motion Capture and 3D Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Better Together, a method that simultaneously solves the human pose estimation problem while reconstructing a photorealistic 3D human avatar from multi-view videos. While prior art usually solves these problems separately, we argue that joint optimization of skeletal motion with a 3D renderable body model brings synergistic effects, i.e. yields more precise motion capture and improved visual quality of real-time rendering of avatars. To achieve this, we introduce a novel animatable avatar with 3D Gaussians rigged on a personalized mesh and propose to optimize the motion sequence with time-dependent MLPs that provide accurate and temporally consistent pose estimates. We first evaluate our method on highly challenging yoga poses and demonstrate state-of-the-art accuracy on multi-view human pose estimation, reducing error by 35% on body joints and 45% on hand joints compared to keypoint-based methods. At the same time, our method significantly boosts the visual quality of animatable avatars (+2dB PSNR on novel view synthesis) on diverse challenging subjects.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2502.07239.pdf' target='_blank'>https://arxiv.org/pdf/2502.07239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinxin Liu, Pengfei Zhang, Hyeongwoo Kim, Pablo Garrido, Ari Shapiro, Kyle Olszewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07239">Contextual Gesture: Co-Speech Gesture Video Generation through Context-aware Gesture Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gesture generation is crucial for creating lifelike avatars and enhancing human-computer interactions by synchronizing gestures with speech. Despite recent advancements, existing methods struggle with accurately identifying the rhythmic or semantic triggers from audio for generating contextualized gesture patterns and achieving pixel-level realism. To address these challenges, we introduce Contextual Gesture, a framework that improves co-speech gesture video generation through three innovative components: (1) a chronological speech-gesture alignment that temporally connects two modalities, (2) a contextualized gesture tokenization that incorporate speech context into motion pattern representation through distillation, and (3) a structure-aware refinement module that employs edge connection to link gesture keypoints to improve video generation. Our extensive experiments demonstrate that Contextual Gesture not only produces realistic and speech-aligned gesture videos but also supports long-sequence generation and video gesture editing applications, shown in Fig.1.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2502.03069.pdf' target='_blank'>https://arxiv.org/pdf/2502.03069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Rasch, Julia TÃ¶ws, Teresa Hirzle, Florian MÃ¼ller, Martin Schmitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03069">CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI's contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2501.07870.pdf' target='_blank'>https://arxiv.org/pdf/2501.07870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Liu, Yutong Wang, Jiahao Chen, Jianfang Li, Tangli Xue, Longlong Li, Jianqiang Ren, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07870">Make-A-Character 2: Animatable 3D Character Generation From a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report introduces Make-A-Character 2, an advanced system for generating high-quality 3D characters from single portrait photographs, ideal for game development and digital human applications. Make-A-Character 2 builds upon its predecessor by incorporating several significant improvements for image-based head generation. We utilize the IC-Light method to correct non-ideal illumination in input photos and apply neural network-based color correction to harmonize skin tones between the photos and game engine renders. We also employ the Hierarchical Representation Network to capture high-frequency facial structures and conduct adaptive skeleton calibration for accurate and expressive facial animations. The entire image-to-3D-character generation process takes less than 2 minutes. Furthermore, we leverage transformer architecture to generate co-speech facial and gesture actions, enabling real-time conversation with the generated character. These technologies have been integrated into our conversational AI avatar products.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2501.06903.pdf' target='_blank'>https://arxiv.org/pdf/2501.06903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06903">Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participant's consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2501.01174.pdf' target='_blank'>https://arxiv.org/pdf/2501.01174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumyaratna Debnath, Harish Katti, Shashikant Verma, Shanmuganathan Raman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01174">L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While 2D pose estimation has advanced our ability to interpret body movements in animals and primates, it is limited by the lack of depth information, constraining its application range. 3D pose estimation provides a more comprehensive solution by incorporating spatial depth, yet creating extensive 3D pose datasets for animals is challenging due to their dynamic and unpredictable behaviours in natural settings. To address this, we propose a hybrid approach that utilizes rigged avatars and the pipeline to generate synthetic datasets to acquire the necessary 3D annotations for training. Our method introduces a simple attention-based MLP network for converting 2D poses to 3D, designed to be independent of the input image to ensure scalability for poses in natural environments. Additionally, we identify that existing anatomical keypoint detectors are insufficient for accurate pose retargeting onto arbitrary avatars. To overcome this, we present a lookup table based on a deep pose estimation method using a synthetic collection of diverse actions rigged avatars perform. Our experiments demonstrate the effectiveness and efficiency of this lookup table-based retargeting approach. Overall, we propose a comprehensive framework with systematically synthesized datasets for lifting poses from 2D to 3D and then utilize this to re-target motion from wild settings onto arbitrary avatars.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2412.15166.pdf' target='_blank'>https://arxiv.org/pdf/2412.15166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15166">Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2412.13479.pdf' target='_blank'>https://arxiv.org/pdf/2412.13479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzhong Guo, Hongwei Yi, Daquan Zhou, Alexander William Bergman, Michael Lingelbach, Yizhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13479">Real-time One-Step Diffusion-based Expressive Portrait Videos Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent diffusion models have made great strides in generating expressive portrait videos with accurate lip-sync and natural motion from a single reference image and audio input. However, these models are far from real-time, often requiring many sampling steps that take minutes to generate even one second of video-significantly limiting practical use. We introduce OSA-LCM (One-Step Avatar Latent Consistency Model), paving the way for real-time diffusion-based avatars. Our method achieves comparable video quality to existing methods but requires only one sampling step, making it more than 10x faster. To accomplish this, we propose a novel avatar discriminator design that guides lip-audio consistency and motion expressiveness to enhance video quality in limited sampling steps. Additionally, we employ a second-stage training architecture using an editing fine-tuned method (EFT), transforming video generation into an editing task during training to effectively address the temporal gap challenge in single-step generation. Experiments demonstrate that OSA-LCM outperforms existing open-source portrait video generation models while operating more efficiently with a single sampling step.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2408.13084.pdf' target='_blank'>https://arxiv.org/pdf/2408.13084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernhard Hilpert, Claudio Alves da Silva, Leon Christidis, Chirag Bhuvaneshwara, Patrick Gebhard, Fabrizio Nunnari, Dimitra Tsovaltzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13084">Avatar Visual Similarity for Social HCI: Increasing Self-Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-awareness is a critical factor in social human-human interaction and, hence, in social HCI interaction. Increasing self-awareness through mirrors or video recordings is common in face-to-face trainings, since it influences antecedents of self-awareness like explicit identification and implicit affective identification (affinity). However, increasing self-awareness has been scarcely examined in virtual trainings with virtual avatars, which allow for adjusting the similarity, e.g. to avoid negative effects of self-consciousness. Automatic visual similarity in avatars is an open issue related to high costs. It is important to understand which features need to be manipulated and which degree of similarity is necessary for self-awareness to leverage the added value of using avatars for self-awareness. This article examines the relationship between avatar visual similarity and increasing self-awareness in virtual training environments. We define visual similarity based on perceptually important facial features for human-human identification and develop a theory-based methodology to systematically manipulate visual similarity of virtual avatars and support self-awareness. Three personalized versions of virtual avatars with varying degrees of visual similarity to participants were created (weak, medium and strong facial features manipulation). In a within-subject study (N=33), we tested effects of degree of similarity on perceived similarity, explicit identification and implicit affective identification (affinity). Results show significant differences between the weak similarity manipulation, and both the strong manipulation and the random avatar for all three antecedents of self-awareness. An increasing degree of avatar visual similarity influences antecedents of self-awareness in virtual environments.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2408.01826.pdf' target='_blank'>https://arxiv.org/pdf/2408.01826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Lin, Zhaoxin Fan, Xianjia Wu, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01826">GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven talking head generation is a critical yet challenging task with applications in augmented reality and virtual human modeling. While recent approaches using autoregressive and diffusion-based models have achieved notable progress, they often suffer from modality inconsistencies, particularly misalignment between audio and mesh, leading to reduced motion diversity and lip-sync accuracy. To address this, we propose GLDiTalker, a novel speech-driven 3D facial animation model based on a Graph Latent Diffusion Transformer. GLDiTalker resolves modality misalignment by diffusing signals within a quantized spatiotemporal latent space. It employs a two-stage training pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion diversity. Together, these stages enable GLDiTalker to generate realistic, temporally stable 3D facial animations. Extensive evaluations on standard benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving superior results in both lip-sync accuracy and motion diversity.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2408.01732.pdf' target='_blank'>https://arxiv.org/pdf/2408.01732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Tan, Xize Cheng, Lingyu Xiong, Lei Zhu, Xiandong Li, Xianjia Wu, Kai Gong, Minglei Li, Yi Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01732">Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given speech. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2407.21686.pdf' target='_blank'>https://arxiv.org/pdf/2407.21686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyeongsik Moon, Takaaki Shiratori, Shunsuke Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21686">Expressive Whole-Body 3D Gaussian Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expression and hand motions are necessary to express our emotions and interact with the world. Nevertheless, most of the 3D human avatars modeled from a casually captured video only support body motions without facial expressions and hand motions.In this work, we present ExAvatar, an expressive whole-body 3D human avatar learned from a short monocular video. We design ExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and 3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of facial expressions and poses in the video and 2) the absence of 3D observations, such as 3D scans and RGBD images. The limited diversity in the video makes animations with novel facial expressions and poses non-trivial. In addition, the absence of 3D observations could cause significant ambiguity in human parts that are not observed in the video, which can result in noticeable artifacts under novel motions. To address them, we introduce our hybrid representation of the mesh and 3D Gaussians. Our hybrid representation treats each 3D Gaussian as a vertex on the surface with pre-defined connectivity information (i.e., triangle faces) between them following the mesh topology of SMPL-X. It makes our ExAvatar animatable with novel facial expressions by driven by the facial expression space of SMPL-X. In addition, by using connectivity-based regularizers, we significantly reduce artifacts in novel facial expressions and poses.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2407.19593.pdf' target='_blank'>https://arxiv.org/pdf/2407.19593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ShahRukh Athar, Shunsuke Saito, Zhengyu Yang, Stanislav Pidhorsky, Chen Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19593">Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic avatars for individuals traditionally involves extensive capture sessions with complex and expensive studio devices like the LightStage system. While recent strides in neural representations have enabled the generation of photorealistic and animatable 3D avatars from quick phone scans, they have the capture-time lighting baked-in, lack facial details and have missing regions in areas such as the back of the ears. Thus, they lag in quality compared to studio-captured avatars. In this paper, we propose a method that bridges this gap by generating studio-like illuminated texture maps from short, monocular phone captures. We do this by parameterizing the phone texture maps using the $W^+$ space of a StyleGAN2, enabling near-perfect reconstruction. Then, we finetune a StyleGAN2 by sampling in the $W^+$ parameterized space using a very small set of studio-captured textures as an adversarial training signal. To further enhance the realism and accuracy of facial details, we super-resolve the output of the StyleGAN2 using carefully designed diffusion model that is guided by image gradients of the phone-captured texture map. Once trained, our method excels at producing studio-like facial texture maps from casual monocular smartphone videos. Demonstrating its capabilities, we showcase the generation of photorealistic, uniformly lit, complete avatars from monocular phone captures. The project page can be found at http://shahrukhathar.github.io/2024/07/22/Bridging.html
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2407.15070.pdf' target='_blank'>https://arxiv.org/pdf/2407.15070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelang Xu, Zhaoqi Su, Qingyao Wu, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15070">GPHM: Gaussian Parametric Head Model for Monocular Head Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-fidelity 3D human head avatars is crucial for applications in VR/AR, digital human, and film production. Recent advances have leveraged morphable face models to generate animated head avatars from easily accessible data, representing varying identities and expressions within a low-dimensional parametric space. However, existing methods often struggle with modeling complex appearance details, e.g., hairstyles, and suffer from low rendering quality and efficiency. In this paper we introduce a novel approach, 3D Gaussian Parametric Head Model, which employs 3D Gaussians to accurately represent the complexities of the human head, allowing precise control over both identity and expression. The Gaussian model can handle intricate details, enabling realistic representations of varying appearances and complex expressions. Furthermore, we presents a well-designed training framework to ensure smooth convergence, providing a robust guarantee for learning the rich content. Our method achieves high-quality, photo-realistic rendering with real-time efficiency, making it a valuable contribution to the field of parametric head models. Finally, we apply the 3D Gaussian Parametric Head Model to monocular video or few-shot head avatar reconstruction tasks, which enables instant reconstruction of high-quality 3D head avatars even when input data is extremely limited, surpassing previous methods in terms of reconstruction quality and training speed.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2407.04545.pdf' target='_blank'>https://arxiv.org/pdf/2407.04545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04545">Gaussian Eigen Models for Human Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current personalized neural head avatars face a trade-off: lightweight models lack detail and realism, while high-quality, animatable avatars require significant computational resources, making them unsuitable for commodity devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars. GEM utilizes 3D Gaussian primitives for representing the appearance combined with Gaussian splatting for rendering. Building on the success of mesh-based 3D morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases for representing the head appearance of a specific subject. In particular, we construct linear bases to represent the position, scale, rotation, and opacity of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives of a specific head shape by a linear combination of the basis vectors, only requiring a low-dimensional parameter vector that contains the respective coefficients. We propose to construct these linear bases (GEM) by distilling high-quality compute-intense CNN-based Gaussian avatar models that can generate expression-dependent appearance changes like wrinkles. These high-quality models are trained on multi-view videos of a subject and are distilled using a series of principal component analyses. Once we have obtained the bases that represent the animatable appearance space of a specific human, we learn a regressor that takes a single RGB image as input and predicts the low-dimensional parameter vector that corresponds to the shown facial expression. In a series of experiments, we compare GEM's self-reenactment and cross-person reenactment results to state-of-the-art 3D avatar methods, demonstrating GEM's higher visual quality and better generalization to new expressions.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2406.15754.pdf' target='_blank'>https://arxiv.org/pdf/2406.15754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishi Jain, Bohan Yu, Peter Wu, Tejas Prabhune, Gopala Anumanchipalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15754">Multimodal Segmentation for Vocal Tract Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable speech processing and linguistics. However, vocal tract modeling is challenging because many internal articulators are occluded from external motion capture technologies. Real-time magnetic resonance imaging (RT-MRI) allows measuring precise movements of internal articulators during speech, but annotated datasets of MRI are limited in size due to time-consuming and computationally expensive labeling methods. We first present a deep labeling strategy for the RT-MRI video using a vision-only segmentation approach. We then introduce a multimodal algorithm using audio to improve segmentation of vocal articulators. Together, we set a new benchmark for vocal tract modeling in MRI video segmentation and use this to release labels for a 75-speaker RT-MRI dataset, increasing the amount of labeled public RT-MRI data of the vocal tract by over a factor of 9. The code and dataset labels can be found at \url{rishiraij.github.io/multimodal-mri-avatar/}.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2406.11570.pdf' target='_blank'>https://arxiv.org/pdf/2406.11570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Xuan Wei Lim, Lynnette Hui Xian Ng, Nicholas Kyger, Tomo Michigami, Faraz Baghernezhad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11570">Projecting Radiance Fields to Mesh Surfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiance fields produce high fidelity images with high rendering speed, but are difficult to manipulate. We effectively perform avatar texture transfer across different appearances by combining benefits from radiance fields and mesh surfaces. We represent the source as a radiance field using 3D Gaussian Splatter, then project the Gaussians on the target mesh. Our pipeline consists of Source Preconditioning, Target Vectorization and Texture Projection. The projection completes in 1.12s in a pure CPU compute, compared to baselines techniques of Per Face Texture Projection and Ray Casting (31s, 4.1min). This method lowers the computational requirements, which makes it applicable to a broader range of devices from low-end mobiles to high end computers.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2405.19609.pdf' target='_blank'>https://arxiv.org/pdf/2405.19609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujiao Jiang, Qingmin Liao, Zhaolong Wang, Xiangru Lin, Zongqing Lu, Yuxi Zhao, Hanqing Wei, Jingrui Ye, Yu Zhang, Zhijing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19609">SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering photorealistic and drivable full-body avatars is crucial for numerous applications, including virtual reality, 3D games, and tele-presence. Most methods, whether reconstruction or generation, require large numbers of human motion sequences and corresponding textured meshes. To easily learn a drivable avatar, a reasonable parametric body model with unified topology is paramount. However, existing human body datasets either have images or textured models and lack parametric models which fit clothes well. We propose a new parametric model SMPLX-Lite-D, which can fit detailed geometry of the scanned mesh while maintaining stable geometry in the face, hand and foot regions. We present SMPLX-Lite dataset, the most comprehensive clothing avatar dataset with multi-view RGB sequences, keypoints annotations, textured scanned meshes, and textured SMPLX-Lite-D models. With the SMPLX-Lite dataset, we train a conditional variational autoencoder model that takes human pose and facial keypoints as input, and generates a photorealistic drivable human avatar.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2405.11270.pdf' target='_blank'>https://arxiv.org/pdf/2405.11270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11270">HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2404.19038.pdf' target='_blank'>https://arxiv.org/pdf/2404.19038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyong Wang, Xiangyu Liang, Wangguandong Zheng, Dan Niu, Haifeng Xia, Siyu Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19038">Embedded Representation Learning Network for Animating Styled Video Portrait</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The talking head generation recently attracted considerable attention due to its widespread application prospects, especially for digital avatars and 3D animation design. Inspired by this practical demand, several works explored Neural Radiance Fields (NeRF) to synthesize the talking heads. However, these methods based on NeRF face two challenges: (1) Difficulty in generating style-controllable talking heads. (2) Displacement artifacts around the neck in rendered images. To overcome these two challenges, we propose a novel generative paradigm \textit{Embedded Representation Learning Network} (ERLNet) with two learning stages. First, the \textit{ audio-driven FLAME} (ADF) module is constructed to produce facial expression and head pose sequences synchronized with content audio and style video. Second, given the sequence deduced by the ADF, one novel \textit{dual-branch fusion NeRF} (DBF-NeRF) explores these contents to render the final images. Extensive empirical studies demonstrate that the collaboration of these two stages effectively facilitates our method to render a more realistic talking head than the existing algorithms.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2404.14745.pdf' target='_blank'>https://arxiv.org/pdf/2404.14745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Wang, Caoyuan Ma, Guopeng Li, Hanrui Xu, Yuke Li, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14745">You Think, You ACT: The New Task of Arbitrary Text to Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text to Motion aims to generate human motions from texts. Existing settings rely on limited Action Texts that include action labels, which limits flexibility and practicability in scenarios difficult to describe directly. This paper extends limited Action Texts to arbitrary ones. Scene texts without explicit action labels can enhance the practicality of models in complex and diverse industries such as virtual human interaction, robot behavior generation, and film production, while also supporting the exploration of potential implicit behavior patterns. However, newly introduced Scene Texts may yield multiple reasonable output results, causing significant challenges in existing data, framework, and evaluation. To address this practical issue, we first create a new dataset HUMANML3D++ by extending texts of the largest existing dataset HUMANML3D. Secondly, we propose a simple yet effective framework that extracts action instructions from arbitrary texts and subsequently generates motions. Furthermore, we also benchmark this new setting with multi-solution metrics to address the inadequacies of existing single-solution metrics. Extensive experiments indicate that Text to Motion in this realistic setting is challenging, fostering new research in this practical direction.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2404.06152.pdf' target='_blank'>https://arxiv.org/pdf/2404.06152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06152">HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2404.02411.pdf' target='_blank'>https://arxiv.org/pdf/2404.02411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhao, Nan Gao, Zhi Zeng, Guixuan Zhang, Jie Liu, Shuwu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02411">A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have shown great success in generating high-quality co-speech gestures for interactive humanoid robots or digital avatars from noisy input with the speech audio or text as conditions. However, they rarely focus on providing rich editing capabilities for content creators other than high-level specialized measures like style conditioning. To resolve this, we propose a unified framework utilizing diffusion inversion that enables multi-level editing capabilities for co-speech gesture generation without re-training. The method takes advantage of two key capabilities of invertible diffusion models. The first is that through inversion, we can reconstruct the intermediate noise from gestures and regenerate new gestures from the noise. This can be used to obtain gestures with high-level similarities to the original gestures for different speech conditions. The second is that this reconstruction reduces activation caching requirements during gradient calculation, making the direct optimization on input noises possible on current hardware with limited memory. With different loss functions designed for, e.g., joint rotation or velocity, we can control various low-level details by automatically tweaking the input noises through optimization. Extensive experiments on multiple use cases show that this framework succeeds in unifying high-level and low-level co-speech gesture editing.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2403.13570.pdf' target='_blank'>https://arxiv.org/pdf/2403.13570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Deng, Duomin Wang, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13570">Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel learning approach for feed-forward one-shot 4D head avatar synthesis. Different from existing methods that often learn from reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view videos to learn a 4D head synthesizer in a data-driven manner, avoiding reliance on inaccurate 3DMM reconstruction that could be detrimental to the synthesis performance. The key idea is to first learn a 3D head synthesizer using synthetic multi-view images to convert monocular real videos into multi-view ones, and then utilize the pseudo multi-view videos to learn a 4D head synthesizer via cross-view self-reenactment. By leveraging a simple vision transformer backbone with motion-aware cross-attentions, our method exhibits superior performance compared to previous methods in terms of reconstruction fidelity, geometry consistency, and motion control accuracy. We hope our method offers novel insights into integrating 3D priors with 2D supervisions for improved 4D head avatar creation.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2403.10737.pdf' target='_blank'>https://arxiv.org/pdf/2403.10737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liupei Lu, Yufeng Yin, Yuming Gu, Yizhen Wu, Pratusha Prasad, Yajie Zhao, Mohammad Soleymani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10737">Leveraging Synthetic Data for Generalizable and Fair Facial Action Unit Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial action unit (AU) detection is a fundamental block for objective facial expression analysis. Supervised learning approaches require a large amount of manual labeling which is costly. The limited labeled data are also not diverse in terms of gender which can affect model fairness. In this paper, we propose to use synthetically generated data and multi-source domain adaptation (MSDA) to address the problems of the scarcity of labeled data and the diversity of subjects. Specifically, we propose to generate a diverse dataset through synthetic facial expression re-targeting by transferring the expressions from real faces to synthetic avatars. Then, we use MSDA to transfer the AU detection knowledge from a real dataset and the synthetic dataset to a target dataset. Instead of aligning the overall distributions of different domains, we propose Paired Moment Matching (PM2) to align the features of the paired real and synthetic data with the same facial expression. To further improve gender fairness, PM2 matches the features of the real data with a female and a male synthetic image. Our results indicate that synthetic data and the proposed model improve both AU detection performance and fairness across genders, demonstrating its potential to solve AU detection in-the-wild.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2403.03561.pdf' target='_blank'>https://arxiv.org/pdf/2403.03561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Dai, Yang Zhang, Tao Liu, Zhen Fan, Tianyuan Du, Zhuo Su, Xiaozheng Zheng, Zeming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03561">HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users' choices for both high tracking accuracy and easy-to-wear. A lightweight temporal-spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the-art results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser's on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are available https://pico-ai-team.github.io/hmd-poser
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2402.18969.pdf' target='_blank'>https://arxiv.org/pdf/2402.18969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozheng Zheng, Chao Wen, Zhuo Su, Zeran Xu, Zhaohu Li, Yang Zhao, Zhou Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18969">OHTA: One-shot Hand Avatar via Data-driven Implicit Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image. With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical. Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios. To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image. OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors. Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge. OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image. Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2402.09459.pdf' target='_blank'>https://arxiv.org/pdf/2402.09459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier GonzÃ¡lez-Alonso, David Oviedo-Pastor, HÃ©ctor J. Aguado, Francisco J. DÃ­az-Pernas, David GonzÃ¡lez-Ortega, Mario MartÃ­nez-Zarzuela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09459">Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body Parts Orientation Tracking and 3D Movement Visualization on an Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies confirm the applicability of Inertial Measurement Unit (IMU)-based systems for human motion analysis. Notwithstanding, high-end IMU-based commercial solutions are yet too expensive and complex to democratize their use among a wide range of potential users. Less featured entry-level commercial solutions are being introduced in the market, trying to fill this gap, but still present some limitations that need to be overcome. At the same time, there is a growing number of scientific papers using not commercial, but custom do-it-yourself IMU-based systems in medical and sports applications. Even though these solutions can help to popularize the use of this technology, they have more limited features and the description on how to design and build them from scratch is yet too scarce in the literature. The aim of this work is two-fold: (1) Proving the feasibility of building an affordable custom solution aimed at simultaneous multiple body parts orientation tracking; while providing a detailed bottom-up description of the required hardware, tools, and mathematical operations to estimate and represent 3D movement in real-time. (2) Showing how the introduction of a custom 2.4 GHz communication protocol including a channel hopping strategy can address some of the current communication limitations of entry-level commercial solutions. The proposed system can be used for wireless real-time human body parts orientation tracking with up to 10 custom sensors, at least at 50 Hz. In addition, it provides a more reliable motion data acquisition in Bluetooth and Wi-Fi crowded environments, where the use of entry-level commercial solutions might be unfeasible. This system can be used as a groundwork for developing affordable human motion analysis solutions that do not require an accurate kinematic analysis.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2402.06385.pdf' target='_blank'>https://arxiv.org/pdf/2402.06385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dragos Costea, Alina Marcu, Cristina Lazar, Marius Leordeanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06385">Maia: A Real-time Non-Verbal Chat for Human-AI Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling face-to-face communication in computer vision, which focuses on recognizing and analyzing nonverbal cues and behaviors during interactions, serves as the foundation for our proposed alternative to text-based Human-AI interaction. By leveraging nonverbal visual communication, through facial expressions, head and body movements, we aim to enhance engagement and capture the user's attention through a novel improvisational element, that goes beyond mirroring gestures. Our goal is to track and analyze facial expressions, and other nonverbal cues in real-time, and use this information to build models that can predict and understand human behavior. Operating in real-time and requiring minimal computational resources, our approach signifies a major leap forward in making AI interactions more natural and accessible. We offer three different complementary approaches, based on retrieval, statistical, and deep learning techniques. A key novelty of our work is the integration of an artistic component atop an efficient human-computer interaction system, using art as a medium to transmit emotions. Our approach is not art-specific and can be adapted to various paintings, animations, and avatars. In our experiments, we compare state-of-the-art diffusion models as mediums for emotion translation in 2D, and our 3D avatar, Maia, that we introduce in this work, with not just facial movements but also body motions for a more natural and engaging experience. We demonstrate the effectiveness of our approach in translating AI-generated emotions into human-relatable expressions, through both human and automatic evaluation procedures, highlighting its potential to significantly enhance the naturalness and engagement of Human-AI interactions across various applications.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2402.03188.pdf' target='_blank'>https://arxiv.org/pdf/2402.03188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Wilson, Frederick Shic, Sophie JÃ¶rg, Eakta Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03188">Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss terms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in face swapping have enabled the automatic generation of highly realistic faces. Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes. Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process. We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect. We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes. We confirm that viewed face swaps do elicit uncanny responses from viewers. Our proposed improvements significant reduce viewing angle errors between face swaps and their source material. Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks. Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications. Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2402.02904.pdf' target='_blank'>https://arxiv.org/pdf/2402.02904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yu, Zebin Huang, Qingbo Liu, Ignacio Carlucho, Mustafa Suphi Erden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02904">Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-controlled digital twin with complete musculoskeletal models of the human body is expected to be useful in designing experiments and validating rehabilitation theory before experiments on real human subjects.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2401.01885.pdf' target='_blank'>https://arxiv.org/pdf/2401.01885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, Alexander Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01885">From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction. Given speech audio, we output multiple possibilities of gestural motion for an individual, including face, body, and hands. The key behind our method is in combining the benefits of sample diversity from vector quantization with the high-frequency details obtained through diffusion to generate more dynamic, expressive motion. We visualize the generated motion using highly photorealistic avatars that can express crucial nuances in gestures (e.g. sneers and smirks). To facilitate this line of research, we introduce a first-of-its-kind multi-view conversational dataset that allows for photorealistic reconstruction. Experiments show our model generates appropriate and diverse gestures, outperforming both diffusion- and VQ-only methods. Furthermore, our perceptual evaluation highlights the importance of photorealism (vs. meshes) in accurately assessing subtle motion details in conversational gestures. Code and dataset available online.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2312.15430.pdf' target='_blank'>https://arxiv.org/pdf/2312.15430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianqiang Ren, Chao He, Lin Liu, Jiahao Chen, Yutong Wang, Yafei Song, Jianfang Li, Tangli Xue, Siqi Hu, Tao Chen, Kunkun Zheng, Jianjing Xiang, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15430">Make-A-Character: High Quality Text-to-3D Character Generation within Minutes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing demand for customized and expressive 3D characters with the emergence of AI agents and Metaverse, but creating 3D characters using traditional computer graphics tools is a complex and time-consuming task. To address these challenges, we propose a user-friendly framework named Make-A-Character (Mach) to create lifelike 3D avatars from text descriptions. The framework leverages the power of large language and vision models for textual intention understanding and intermediate image generation, followed by a series of human-oriented visual perception and 3D generation modules. Our system offers an intuitive approach for users to craft controllable, realistic, fully-realized 3D characters that meet their expectations within 2 minutes, while also enabling easy integration with existing CG pipeline for dynamic expressiveness. For more information, please visit the project page at https://human3daigc.github.io/MACH/.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2311.18729.pdf' target='_blank'>https://arxiv.org/pdf/2311.18729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18729">Portrait4D: Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing one-shot 4D head synthesis methods usually learn from monocular videos with the aid of 3DMM reconstruction, yet the latter is evenly challenging which restricts them from reasonable 4D head synthesis. We present a method to learn one-shot 4D head synthesis via large-scale synthetic data. The key is to first learn a part-wise 4D generative model from monocular images via adversarial learning, to synthesize multi-view images of diverse identities and full motions as training data; then leverage a transformer-based animatable triplane reconstructor to learn 4D head reconstruction using the synthetic data. A novel learning strategy is enforced to enhance the generalizability to real images by disentangling the learning process of 3D reconstruction and reenactment. Experiments demonstrate our superiority over the prior art.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2311.17465.pdf' target='_blank'>https://arxiv.org/pdf/2311.17465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duomin Wang, Bin Dai, Yu Deng, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17465">AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, our goal is to create interactive avatar agents that can autonomously plan and animate nuanced facial movements realistically, from both visual and behavioral perspectives. Given high-level inputs about the environment and agent profile, our framework harnesses LLMs to produce a series of detailed text descriptions of the avatar agents' facial motions. These descriptions are then processed by our task-agnostic driving engine into motion token sequences, which are subsequently converted into continuous motion embeddings that are further consumed by our standalone neural-based renderer to generate the final photorealistic avatar animations. These streamlined processes allow our framework to adapt to a variety of non-verbal avatar interactions, both monadic and dyadic. Our extensive study, which includes experiments on both newly compiled and existing datasets featuring two types of agents -- one capable of monadic interaction with the environment, and the other designed for dyadic conversation -- validates the effectiveness and versatility of our approach. To our knowledge, we advanced a leap step by combining LLMs and neural rendering for generalized non-verbal prediction and photo-realistic rendering of avatar agents.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2311.15672.pdf' target='_blank'>https://arxiv.org/pdf/2311.15672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xihe Yang, Xingyu Chen, Daiheng Gao, Shaohui Wang, Xiaoguang Han, Baoyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15672">HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As for human avatar reconstruction, contemporary techniques commonly necessitate the acquisition of costly data and struggle to achieve satisfactory results from a small number of casual images. In this paper, we investigate this task from a few-shot unconstrained photo album. The reconstruction of human avatars from such data sources is challenging because of limited data amount and dynamic articulated poses. For handling dynamic data, we integrate a skinning mechanism with deep marching tetrahedra (DMTet) to form a drivable tetrahedral representation, which drives arbitrary mesh topologies generated by the DMTet for the adaptation of unconstrained images. To effectively mine instructive information from few-shot data, we devise a two-phase optimization method with few-shot reference and few-shot guidance. The former focuses on aligning avatar identity with reference images, while the latter aims to generate plausible appearances for unseen regions. Overall, our framework, called HaveFun, can undertake avatar reconstruction, rendering, and animation. Extensive experiments on our developed benchmarks demonstrate that HaveFun exhibits substantially superior performance in reconstructing the human body and hand. Project website: https://seanchenxy.github.io/HaveFunWeb/.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2308.11261.pdf' target='_blank'>https://arxiv.org/pdf/2308.11261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadegh Aliakbarian, Fatemeh Saleh, David Collier, Pashmina Cameron, Darren Cosker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11261">HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating both plausible and accurate full body avatar motion is the key to the quality of immersive experiences in mixed reality scenarios. Head-Mounted Devices (HMDs) typically only provide a few input signals, such as head and hands 6-DoF. Recently, different approaches achieved impressive performance in generating full body motion given only head and hands signal. However, to the best of our knowledge, all existing approaches rely on full hand visibility. While this is the case when, e.g., using motion controllers, a considerable proportion of mixed reality experiences do not involve motion controllers and instead rely on egocentric hand tracking. This introduces the challenge of partial hand visibility owing to the restricted field of view of the HMD. In this paper, we propose the first unified approach, HMD-NeMo, that addresses plausible and accurate full body motion generation even when the hands may be only partially visible. HMD-NeMo is a lightweight neural network that predicts the full body motion in an online and real-time fashion. At the heart of HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask tokens that encourage plausible motion in the absence of hand observations. We perform extensive analysis of the impact of different components in HMD-NeMo and introduce a new state-of-the-art on AMASS dataset through our evaluation.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2308.08855.pdf' target='_blank'>https://arxiv.org/pdf/2308.08855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozheng Zheng, Zhuo Su, Chao Wen, Zhou Xue, Xiaojie Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08855">Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To bridge the physical and virtual worlds for rapidly developed VR/AR applications, the ability to realistically drive 3D full-body avatars is of great significance. Although real-time body tracking with only the head-mounted displays (HMDs) and hand controllers is heavily under-constrained, a carefully designed end-to-end neural network is of great potential to solve the problem by learning from large-scale motion data. To this end, we propose a two-stage framework that can obtain accurate and smooth full-body motions with the three tracking signals of head and hands only. Our framework explicitly models the joint-level features in the first stage and utilizes them as spatiotemporal tokens for alternating spatial and temporal transformer blocks to capture joint-level correlations in the second stage. Furthermore, we design a set of loss terms to constrain the task of a high degree of freedom, such that we can exploit the potential of our joint-level modeling. With extensive experiments on the AMASS motion dataset and real-captured data, we validate the effectiveness of our designs and show our proposed method can achieve more accurate and smooth motion compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2307.11025.pdf' target='_blank'>https://arxiv.org/pdf/2307.11025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Wan, Zhicong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11025">Investigating VTubing as a Reconstruction of Streamer Self-Presentation: Identity, Performance, and Gender</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VTubers, or Virtual YouTubers, are live streamers who create streaming content using animated 2D or 3D virtual avatars. In recent years, there has been a significant increase in the number of VTuber creators and viewers across the globe. This practise has drawn research attention into topics such as viewers' engagement behaviors and perceptions, however, as animated avatars offer more identity and performance flexibility than traditional live streaming where one uses their own body, little research has focused on how this flexibility influences how creators present themselves. This research thus seeks to fill this gap by presenting results from a qualitative study of 16 Chinese-speaking VTubers' streaming practices. The data revealed that the virtual avatars that were used while live streaming afforded creators opportunities to present themselves using inflated presentations and resulted in inclusive interactions with viewers. The results also unveiled the inflated, and often sexualized, gender expressions of VTubers while they were situated in misogynistic environments. The socio-technical facets of VTubing were found to potentially reduce sexual harassment and sexism, whilst also raising self-objectification concerns.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2307.10533.pdf' target='_blank'>https://arxiv.org/pdf/2307.10533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillermo Colin, Joseph Byrnes, Youngwoo Sim, Patrick Wensing, Joao Ramos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10533">Whole-Body Dynamic Telelocomotion: A Step-to-Step Dynamics Approach to Human Walking Reference Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperated humanoid robots hold significant potential as physical avatars for humans in hazardous and inaccessible environments, with the goal of channeling human intelligence and sensorimotor skills through these robotic counterparts. Precise coordination between humans and robots is crucial for accomplishing whole-body behaviors involving locomotion and manipulation. To progress successfully, dynamic synchronization between humans and humanoid robots must be achieved. This work enhances advancements in whole-body dynamic telelocomotion, addressing challenges in robustness. By embedding the hybrid and underactuated nature of bipedal walking into a virtual human walking interface, we achieve dynamically consistent walking gait generation. Additionally, we integrate a reactive robot controller into a whole-body dynamic telelocomotion framework. Thus, allowing the realization of telelocomotion behaviors on the full-body dynamics of a bipedal robot. Real-time telelocomotion simulation experiments validate the effectiveness of our methods, demonstrating that a trained human pilot can dynamically synchronize with a simulated bipedal robot, achieving sustained locomotion, controlling walking speeds within the range of 0.0 m/s to 0.3 m/s, and enabling backward walking for distances of up to 2.0 m. This research contributes to advancing teleoperated humanoid robots and paves the way for future developments in synchronized locomotion between humans and bipedal robots.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2307.01938.pdf' target='_blank'>https://arxiv.org/pdf/2307.01938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniele Reda, Jungdam Won, Yuting Ye, Michiel van de Panne, Alexander Winkler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01938">Physics-based Motion Retargeting from Sparse Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user's motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user's pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human. We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2306.14484.pdf' target='_blank'>https://arxiv.org/pdf/2306.14484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jann Philipp Freiwald, Susanne Schmidt, Bernhard E. Riecke, Frank Steinicke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14484">Moving Avatars and Agents in Social Extended Reality Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural interaction between multiple users within a shared virtual environment (VE) relies on each other's awareness of the current position of the interaction partners. This, however, cannot be warranted when users employ noncontinuous locomotion techniques, such as teleportation, which may cause confusion among bystanders. In this paper, we pursue two approaches to create a pleasant experience for both the moving user and the bystanders observing that movement. First, we will introduce a Smart Avatar system that delivers continuous full-body human representations for noncontinuous locomotion in shared virtual reality (VR) spaces. Smart Avatars imitate their assigned user's real-world movements when close-by and autonomously navigate to their user when the distance between them exceeds a certain threshold, i.e., after the user teleports. As part of the Smart Avatar system, we implemented four avatar transition techniques and compared them to conventional avatar locomotion in a user study, revealing significant positive effects on the observer's spatial awareness, as well as pragmatic and hedonic quality scores. Second, we introduce the concept of Stuttered Locomotion, which can be applied to any continuous locomotion method. By converting a continuous movement into short-interval teleport steps, we provide the merits of non-continuous locomotion for the moving user while observers can easily keep track of their path. Thus, while the experience for observers is similarly positive as with continuous motion, a user study confirmed that Stuttered Locomotion can significantly reduce the occurrence of cybersickness symptoms for the moving user, making it an attractive choice for shared VEs. We will discuss the potential of Smart Avatars and Stuttered Locomotion for shared VR experiences, both when applied individually and in combination.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2306.02850.pdf' target='_blank'>https://arxiv.org/pdf/2306.02850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Sun, Qian Bao, Wu Liu, Tao Mei, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02850">TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new "maps" to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2305.04995.pdf' target='_blank'>https://arxiv.org/pdf/2305.04995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunghwan Lee, Yifeng Jiang, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04995">Anatomically Detailed Simulation of Human Torso</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing digital human models approximate the human skeletal system using rigid bodies connected by rotational joints. While the simplification is considered acceptable for legs and arms, it significantly lacks fidelity to model rich torso movements in common activities such as dancing, Yoga, and various sports. Research from biomechanics provides more detailed modeling for parts of the torso, but their models often operate in isolation and are not fast and robust enough to support computationally heavy applications and large-scale data generation for full-body digital humans. This paper proposes a new torso model that aims to achieve high fidelity both in perception and in functionality, while being computationally feasible for simulation and optimal control tasks. We build a detailed human torso model consisting of various anatomical components, including facets, ligaments, and intervertebral discs, by coupling efficient finite-element and rigid-body simulations. Given an existing motion capture sequence without dense markers placed on the torso, our new model is able to recover the underlying torso bone movements. Our method is remarkably robust that it can be used to automatically "retrofit" the entire Mixamo motion database of highly diverse human motions without user intervention. We also show that our model is computationally efficient for solving trajectory optimization of highly dynamic full-body movements, without relying on any reference motion. Physiological validity of the model is validated against established literature.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2211.12782.pdf' target='_blank'>https://arxiv.org/pdf/2211.12782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Chen, Baoyuan Wang, Heung-Yeung Shum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12782">Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present HandAvatar, a novel representation for hand animation and rendering, which can generate smoothly compositional geometry and self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as a high-resolution mesh topology to fit personalized hand shapes. Sequentially, we decompose hand geometry into per-bone rigid parts, and then re-compose paired geometry encodings to derive an across-part consistent occupancy field. As for texture modeling, we propose a self-occlusion-aware shading field (SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record albedo information under a wide variety of hand poses. Moreover, directed soft occupancy is designed to describe the ray-to-surface relation, which is leveraged to generate an illumination field for the disentanglement of pose-independent albedo and pose-dependent illumination. Trained from monocular video data, our HandAvatar can perform free-pose hand animation and rendering while at the same time achieving superior appearance fidelity. We also demonstrate that HandAvatar provides a route for hand appearance editing. Project website: https://seanchenxy.github.io/HandAvatarWeb.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2211.12499.pdf' target='_blank'>https://arxiv.org/pdf/2211.12499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timo Bolkart, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12499">Instant Volumetric Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Instant Volumetric Head Avatars (INSTA), a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2211.01426.pdf' target='_blank'>https://arxiv.org/pdf/2211.01426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julie Jiang, Ron Dotsch, Mireia Triguero Roura, Yozen Liu, VÃ­tor Silva, Maarten W. Bos, Francesco Barbieri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.01426">Reciprocity, Homophily, and Social Network Effects in Pictorial Communication: A Case Study of Bitmoji Stickers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pictorial emojis and stickers are commonly used in online social networking to facilitate and aid communications. We delve into the use of Bitmoji stickers, a highly expressive form of pictorial communication using avatars resembling actual users. We collect a large-scale dataset of the metadata of 3 billion Bitmoji stickers shared among 300 million Snapchat users. We find that individual Bitmoji sticker usage patterns can be characterized jointly on dimensions of reciprocity and selectivity: Users are either both reciprocal and selective about whom they use Bitmoji stickers with or neither reciprocal nor selective. We additionally provide evidence of network homophily in that friends use Bitmoji stickers at similar rates. Finally, using a quasi-experimental approach, we show that receiving Bitmoji stickers from a friend encourages future Bitmoji sticker usage and overall Snapchat engagement. We discuss broader implications of our work towards a better understanding of pictorial communication behaviors in social networks.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2209.00776.pdf' target='_blank'>https://arxiv.org/pdf/2209.00776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanhang Yan, Yu Sun, Qian Bao, Jinhui Pang, Wu Liu, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00776">WOC: A Handy Webcam-based 3D Online Chatroom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop WOC, a webcam-based 3D virtual online chatroom for multi-person interaction, which captures the 3D motion of users and drives their individual 3D virtual avatars in real-time. Compared to the existing wearable equipment-based solution, WOC offers convenient and low-cost 3D motion capture with a single camera. To promote the immersive chat experience, WOC provides high-fidelity virtual avatar manipulation, which also supports the user-defined characters. With the distributed data flow service, the system delivers highly synchronized motion and voice for all users. Deployed on the website and no installation required, users can freely experience the virtual online chat at https://yanch.cloud.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2509.19965.pdf' target='_blank'>https://arxiv.org/pdf/2509.19965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19965">SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <https://novicemm.github.io/synchrorama>.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2508.20623.pdf' target='_blank'>https://arxiv.org/pdf/2508.20623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Xin, Xiaolin Zhang, Yanbin Liu, Peng Zhang, Caifeng Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20623">AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2508.13537.pdf' target='_blank'>https://arxiv.org/pdf/2508.13537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shikun Zhang, Cunjian Chen, Yiqun Wang, Qiuhong Ke, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13537">EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2508.03100.pdf' target='_blank'>https://arxiv.org/pdf/2508.03100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yogesh Kulkarni, Pooyan Fazli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03100">AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal reasoning over long-horizon video is challenging due to the need for precise spatiotemporal fusion and alignment across modalities. While recent methods such as Group Relative Policy Optimization (GRPO) have shown promise in this domain, they suffer from three key limitations: (1) data inefficiency from their on-policy design, (2) a vanishing advantage problem, where identical or near-identical rewards within a group eliminate the learning signal by producing zero-valued advantages, and (3) uniform credit assignment that fails to emphasize critical reasoning steps. We introduce AVATAR (Audio-Video Agent for Alignment and Reasoning), a framework that addresses these limitations through two core components: (1) an off-policy training architecture that improves sample efficiency and resolves vanishing advantages by reusing past experiences with greater reward diversity, and (2) Temporal Advantage Shaping (TAS), a novel credit assignment strategy that upweights key reasoning phases during learning. AVATAR achieves strong performance across various benchmarks, outperforming the Qwen2.5-Omni baseline by +5.4on MMVU, +4.9 on OmniBench, and +4.5 on Video-Holmes, while demonstrating over 35% higher sample efficiency.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2508.02958.pdf' target='_blank'>https://arxiv.org/pdf/2508.02958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Killough, Justin Feng, Zheng Xue "ZX" Ching, Daniel Wang, Rithvik Dyava, Yapeng Tian, Yuhang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02958">VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) is inaccessible to blind people. While research has investigated many techniques to enhance VR accessibility, they require additional developer effort to integrate. As such, most mainstream VR apps remain inaccessible as the industry de-prioritizes accessibility. We present VRSight, an end-to-end system that recognizes VR scenes post hoc through a set of AI models (e.g., object detection, depth estimation, LLM-based atmosphere interpretation) and generates tone-based, spatial audio feedback, empowering blind users to interact in VR without developer intervention. To enable virtual element detection, we further contribute DISCOVR, a VR dataset consisting of 30 virtual object classes from 17 social VR apps, substituting real-world datasets that remain not applicable to VR contexts. Nine participants used VRSight to explore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in facilitating social tasks like avatar awareness and available seat identification.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2508.02376.pdf' target='_blank'>https://arxiv.org/pdf/2508.02376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matus Krajcovic, Peter Demcak, Eduard Kuric
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02376">Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied conversational agents (ECAs) are increasingly more realistic and capable of dynamic conversations. In online surveys, anthropomorphic agents could help address issues like careless responding and satisficing, which originate from the lack of personal engagement and perceived accountability. However, there is a lack of understanding of how ECAs in user experience research may affect participant engagement, satisfaction, and the quality of responses. As a proof of concept, we propose an instrument that enables the incorporation of conversations with a virtual avatar into surveys, using on AI-driven video generation, speech recognition, and Large Language Models. In our between-subjects study, 80 participants (UK, stratified random sample of general population) either talked to a voice-based agent with an animated video avatar, or interacted with a chatbot. Across surveys based on two self-reported psychometric tests, 2,265 conversation responses were obtained. Statistical comparison of results indicates that embodied agents can contribute significantly to more informative, detailed responses, as well as higher yet more time-efficient engagement. Furthermore, qualitative analysis provides valuable insights for causes of no significant change to satisfaction, linked to personal preferences, turn-taking delays and Uncanny Valley reactions. These findings support the pursuit and development of new methods toward human-like agents for the transformation of online surveys into more natural interactions resembling in-person interviews.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2507.22326.pdf' target='_blank'>https://arxiv.org/pdf/2507.22326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qun Ma, Xiao Xue, Ming Zhang, Yifan Shen, Zihan Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22326">An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse service is a product of the convergence between Metaverse and service systems, designed to address service-related challenges concerning digital avatars, digital twins, and digital natives within Metaverse. With the rise of large language models (LLMs), agents now play a pivotal role in Metaverse service ecosystem, serving dual functions: as digital avatars representing users in the virtual realm and as service assistants (or NPCs) providing personalized support. However, during the modeling of Metaverse service ecosystems, existing LLM-based agents face significant challenges in bridging virtual-world services with real-world services, particularly regarding issues such as character data fusion, character knowledge association, and ethical safety concerns. This paper proposes an explainable emotion alignment framework for LLM-based agents in Metaverse Service Ecosystem. It aims to integrate factual factors into the decision-making loop of LLM-based agents, systematically demonstrating how to achieve more relational fact alignment for these agents. Finally, a simulation experiment in the Offline-to-Offline food delivery scenario is conducted to evaluate the effectiveness of this framework, obtaining more realistic social emergence.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2505.08293.pdf' target='_blank'>https://arxiv.org/pdf/2505.08293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhuo Yin, Yuk Hang Tsui, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08293">M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2505.01932.pdf' target='_blank'>https://arxiv.org/pdf/2505.01932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinmu Wang, Xiang Gao, Xiyun Song, Heather Yu, Zongfang Lin, Liang Peng, Xianfeng Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01932">OT-Talk: Animating 3D Talking Head with Optimal Transportation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating 3D head meshes using audio inputs has significant applications in AR/VR, gaming, and entertainment through 3D avatars. However, bridging the modality gap between speech signals and facial dynamics remains a challenge, often resulting in incorrect lip syncing and unnatural facial movements. To address this, we propose OT-Talk, the first approach to leverage optimal transportation to optimize the learning model in talking head animation. Building on existing learning frameworks, we utilize a pre-trained Hubert model to extract audio features and a transformer model to process temporal sequences. Unlike previous methods that focus solely on vertex coordinates or displacements, we introduce Chebyshev Graph Convolution to extract geometric features from triangulated meshes. To measure mesh dissimilarities, we go beyond traditional mesh reconstruction errors and velocity differences between adjacent frames. Instead, we represent meshes as probability measures and approximate their surfaces. This allows us to leverage the sliced Wasserstein distance for modeling mesh variations. This approach facilitates the learning of smooth and accurate facial motions, resulting in coherent and natural facial animations. Our experiments on two public audio-mesh datasets demonstrate that our method outperforms state-of-the-art techniques both quantitatively and qualitatively in terms of mesh reconstruction accuracy and temporal alignment. In addition, we conducted a user perception study with 20 volunteers to further assess the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2504.20844.pdf' target='_blank'>https://arxiv.org/pdf/2504.20844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelika Kothe, Volker Hohmann, Giso Grimm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20844">Effect of Avatar Head Movement on Communication Behaviour, Experience of Presence and Conversation Success in Triadic Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive communication in virtual reality can be used in experimental paradigms to increase the ecological validity of hearing device evaluations. This requires the virtual environment to elicit natural communication behaviour in listeners. This study evaluates the effect of virtual animated characters' head movements on participants' communication behaviour and experience.
  Triadic conversations were conducted between a test participant and two confederates. To facilitate the manipulation of head movements, the conversation was conducted in telepresence using a system that transmitted audio, head movement data and video with low delay. The confederates were represented by virtual animated characters (avatars) with different levels of animation: Static heads, automated head movement animations based on speech level onsets, and animated head movements based on the transmitted head movements of the interlocutors. A condition was also included in which the videos of the interlocutors' heads were embedded in the visual scene.
  The results show significant effects of animation level on the participants' speech and head movement behaviour as recorded by physical sensors, as well as on the subjective sense of presence and the success of the conversation. The largest effects were found for the range of head orientation during speech and the perceived realism of avatars. Participants reported that they were spoken to in a more helpful way when the avatars showed head movements transmitted from the interlocutors than when the avatars' heads were static.
  We therefore conclude that the representation of interlocutors must include sufficiently realistic head movements in order to elicit natural communication behaviour.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2504.18332.pdf' target='_blank'>https://arxiv.org/pdf/2504.18332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuting Zhao, Linxin Bai, Liangjing Shao, Ye Zhang, Xinrong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18332">SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing applications of AR/VR increase the demand for real-time full-body pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint signals from the head and hands, reconstructing a full-body pose remains challenging due to the unconstrained lower body. Recent advancements often rely on conventional neural networks and generative models to improve performance in this task, such as Transformers and diffusion models. However, these approaches struggle to strike a balance between achieving precise pose reconstruction and maintaining fast inference speed. To overcome these challenges, a lightweight and efficient model, SSD-Poser, is designed for robust full-body motion estimation from sparse observations. SSD-Poser incorporates a well-designed hybrid encoder, State Space Attention Encoders, to adapt the state space duality to complex motion poses and enable real-time realistic pose reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate jitter caused by variable-frequency motion signals, remarkably enhancing the motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate that SSD-Poser achieves exceptional accuracy and computational efficiency, showing outstanding inference efficiency compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2504.14373.pdf' target='_blank'>https://arxiv.org/pdf/2504.14373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Zhuo Su, Jian Wang, Shuang Li, Xu Chang, Zhaohu Li, Yang Zhao, Guidong Wang, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14373">SEGA: Drivable 3D Gaussian Head Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2504.06751.pdf' target='_blank'>https://arxiv.org/pdf/2504.06751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leszek Luchowski, Dariusz Pojda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06751">Visualization of a multidimensional point cloud as a 3D swarm of avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an innovative technique for representing multidimensional datasets using icons inspired by Chernoff faces. Our approach combines classical projection techniques with the explicit assignment of selected data dimensions to avatar (facial) features, leveraging the innate human ability to interpret facial traits. We introduce a semantic division of data dimensions into intuitive and technical categories, assigning the former to avatar features and projecting the latter into a four-dimensional (or higher) spatial embedding. The technique is implemented as a plugin for the open-source dpVision visualization platform, enabling users to interactively explore data in the form of a swarm of avatars whose spatial positions and visual features jointly encode various aspects of the dataset. Experimental results with synthetic test data and a 12-dimensional dataset of Portuguese Vinho Verde wines demonstrate that the proposed method enhances interpretability and facilitates the analysis of complex data structures.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2503.21886.pdf' target='_blank'>https://arxiv.org/pdf/2503.21886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pilseo Park, Ze Zhang, Michel Sarkis, Ning Bi, Xiaoming Liu, Yiying Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21886">Refined Geometry-guided Head Avatar Reconstruction from Monocular RGB Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity reconstruction of head avatars from monocular videos is highly desirable for virtual human applications, but it remains a challenge in the fields of computer graphics and computer vision. In this paper, we propose a two-phase head avatar reconstruction network that incorporates a refined 3D mesh representation. Our approach, in contrast to existing methods that rely on coarse template-based 3D representations derived from 3DMM, aims to learn a refined mesh representation suitable for a NeRF that captures complex facial nuances. In the first phase, we train 3DMM-stored NeRF with an initial mesh to utilize geometric priors and integrate observations across frames using a consistent set of latent codes. In the second phase, we leverage a novel mesh refinement procedure based on an SDF constructed from the density field of the initial NeRF. To mitigate the typical noise in the NeRF density field without compromising the features of the 3DMM, we employ Laplace smoothing on the displacement field. Subsequently, we apply a second-phase training with these refined meshes, directing the learning process of the network towards capturing intricate facial details. Our experiments demonstrate that our method further enhances the NeRF rendering based on the initial mesh and achieves performance superior to state-of-the-art methods in reconstructing high-fidelity head avatars with such input.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2503.16739.pdf' target='_blank'>https://arxiv.org/pdf/2503.16739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geonsun Lee, Yue Yang, Jennifer Healey, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16739">Since U Been Gone: Augmenting Context-Aware Transcriptions for Re-engaging in Immersive VR Meetings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining engagement in immersive meetings is challenging, particularly when users must catch up on missed content after disruptions. While transcription interfaces can help, table-fixed panels have the potential to distract users from the group, diminishing social presence, while avatar-fixed captions fail to provide past context. We present EngageSync, a context-aware avatar-fixed transcription interface that adapts based on user engagement, offering live transcriptions and LLM-generated summaries to enhance catching up while preserving social presence. We implemented a live VR meeting setup for a 12-participant formative study and elicited design considerations. In two user studies with small (3 avatars) and mid-sized (7 avatars) groups, EngageSync significantly improved social presence (p < .05) and time spent gazing at others in the group instead of the interface over table-fixed panels. Also, it reduced re-engagement time and increased information recall (p < .05) over avatar-fixed interfaces, with stronger effects in mid-sized groups (p < .01).
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2502.09811.pdf' target='_blank'>https://arxiv.org/pdf/2502.09811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexin Zhang, Edward Glenn Scott Spencer, Abijith Manikandan, Andric Li, Ang Li, Yaxing Yao, Yuhang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09811">Inclusive Avatar Guidelines for People with Disabilities: Supporting Disability Representation in Social Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatar is a critical medium for identity representation in social virtual reality (VR). However, options for disability expression are highly limited on current avatar interfaces. Improperly designed disability features may even perpetuate misconceptions about people with disabilities (PWD). As more PWD use social VR, there is an emerging need for comprehensive design standards that guide developers and designers to create inclusive avatars. Our work aim to advance the avatar design practices by delivering a set of centralized, comprehensive, and validated design guidelines that are easy to adopt, disseminate, and update. Through a systematic literature review and interview with 60 participants with various disabilities, we derived 20 initial design guidelines that cover diverse disability expression methods through five aspects, including avatar appearance, body dynamics, assistive technology design, peripherals around avatars, and customization control. We further evaluated the guidelines via a heuristic evaluation study with 10 VR practitioners, validating the guideline coverage, applicability, and actionability. Our evaluation resulted in a final set of 17 design guidelines with recommendation levels.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2502.08085.pdf' target='_blank'>https://arxiv.org/pdf/2502.08085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tri Tung Nguyen Nguyen, Fujii Yasuyuki, Dinh Tuan Tran, Joo-Ho Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08085">Interactive Holographic Visualization for 3D Facial Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional methods for visualizing dynamic human expressions, particularly in medical training, often rely on flat-screen displays or static mannequins, which have proven inefficient for realistic simulation. In response, we propose a platform that leverages a 3D interactive facial avatar capable of displaying non-verbal feedback, including pain signals. This avatar is projected onto a stereoscopic, view-dependent 3D display, offering a more immersive and realistic simulated patient experience for pain assessment practice. However, there is no existing solution that dynamically predicts and projects interactive 3D facial avatars in real-time. To overcome this, we emphasize the need for a 3D display projection system that can project the facial avatar holographically, allowing users to interact with the avatar from any viewpoint. By incorporating 3D Gaussian Splatting (3DGS) and real-time view-dependent calibration, we significantly improve the training environment for accurate pain recognition and assessment.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2501.14646.pdf' target='_blank'>https://arxiv.org/pdf/2501.14646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujian Liu, Shidang Xu, Jing Guo, Dingbin Wang, Zairan Wang, Xianfeng Tan, Xiaoli Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14646">SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating talking avatar driven by audio remains a significant challenge. Existing methods typically require high computational costs and often lack sufficient facial detail and realism, making them unsuitable for applications that demand high real-time performance and visual quality. Additionally, while some methods can synchronize lip movement, they still face issues with consistency between facial expressions and upper body movement, particularly during silent periods. In this paper, we introduce SyncAnimation, the first NeRF-based method that achieves audio-driven, stable, and real-time generation of speaking avatar by combining generalized audio-to-pose matching and audio-to-expression synchronization. By integrating AudioPose Syncer and AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression generation, progressively producing audio-synchronized upper body, head, and lip shapes. Furthermore, the High-Synchronization Human Renderer ensures seamless integration of the head and upper body, and achieves audio-sync lip. The project page can be found at https://syncanimation.github.io
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2501.07104.pdf' target='_blank'>https://arxiv.org/pdf/2501.07104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07104">RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures low-frequency motion and surface deformation of the avatar. Due to the limitations of LBS formula, the human skeleton is hard to control complex non-rigid transformations. We then design a pose-related Gaussian rectification module to learn fine-detailed non-rigid deformations, further improving the realism and expressiveness of the avatar. We conduct extensive experiments on public datasets, RMAvatar shows state-of-the-art performance on both rendering quality and quantitative evaluations. Please see our project page at https://rm-avatar.github.io.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2501.00168.pdf' target='_blank'>https://arxiv.org/pdf/2501.00168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mykola Maslych, Christian Pumarada, Amirpouya Ghasemaghaei, Joseph J. LaViola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00168">Takeaways from Applying LLM Capabilities to Multiple Conversational Avatars in a VR Pilot Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a virtual reality (VR) environment featuring conversational avatars powered by a locally-deployed LLM, integrated with automatic speech recognition (ASR), text-to-speech (TTS), and lip-syncing. Through a pilot study, we explored the effects of three types of avatar status indicators during response generation. Our findings reveal design considerations for improving responsiveness and realism in LLM-driven conversational systems. We also detail two system architectures: one using an LLM-based state machine to control avatar behavior and another integrating retrieval-augmented generation (RAG) for context-grounded responses. Together, these contributions offer practical insights to guide future work in developing task-oriented conversational AI in VR environments.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2412.00845.pdf' target='_blank'>https://arxiv.org/pdf/2412.00845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghan Chen, Yang Cong, Jiayue Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00845">SAGA: Surface-Aligned Gaussian Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a Surface-Aligned Gaussian representation for creating animatable human avatars from monocular videos,aiming at improving the novel view and pose synthesis performance while ensuring fast training and real-time rendering. Recently,3DGS has emerged as a more efficient and expressive alternative to NeRF, and has been used for creating dynamic human avatars. However,when applied to the severely ill-posed task of monocular dynamic reconstruction, the Gaussians tend to overfit the constantly changing regions such as clothes wrinkles or shadows since these regions cannot provide consistent supervision, resulting in noisy geometry and abrupt deformation that typically fail to generalize under novel views and poses.To address these limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns the Gaussians with a mesh to enforce well-defined geometry and consistent deformation, thereby improving generalization under novel views and poses. Unlike existing strict alignment methods that suffer from limited expressive power and low realism,SAGA employs a two-stage alignment strategy where the Gaussians are first adhered on while then detached from the mesh, thus facilitating both good geometry and high expressivity. In the Adhered Stage, we improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow on the mesh, in contrast to existing methods that rigidly bind Gaussians to fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh Alignment regularization, which allows us to unleash the expressivity by detaching the Gaussians but maintain the geometric alignment by minimizing their location and orientation offsets from the bound triangles. Finally, since the Gaussians may drift outside the bound triangles during optimization, an efficient Walking-on-Mesh strategy is proposed to dynamically update the bound triangles.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2411.04490.pdf' target='_blank'>https://arxiv.org/pdf/2411.04490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joni-Roy Piispanen, Tinja Myllyviita, Ville Vakkuri, Rebekah Rousi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04490">Smoke Screens and Scapegoats: The Reality of General Data Protection Regulation Compliance -- Privacy and Ethics in the Case of Replika AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently artificial intelligence (AI)-enabled chatbots are capturing the hearts and imaginations of the public at large. Chatbots that users can build and personalize, as well as pre-designed avatars ready for users' selection, all of these are on offer in applications to provide social companionship, friends and even love. These systems, however, have demonstrated challenges on the privacy and ethics front. This paper takes a critical approach towards examining the intricacies of these issues within AI companion services. We chose Replika as a case and employed close reading to examine the service's privacy policy. We additionally analyze articles from public media about the company and its practices to gain insight into the trustworthiness and integrity of the information provided in the policy. The aim is to ascertain whether seeming General Data Protection Regulation (GDPR) compliance equals reliability of required information, or whether the area of GDPR compliance in itself is one riddled with ethical challenges. The paper contributes to a growing body of scholarship on ethics and privacy related matters in the sphere of social chatbots. The results reveal that despite privacy notices, data collection practices might harvest personal data without users' full awareness. Cross-textual comparison reveals that privacy notice information does not fully correspond with other information sources.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2410.23800.pdf' target='_blank'>https://arxiv.org/pdf/2410.23800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoyang Pan, Angjoo Kanazawa, Hang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23800">SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-occlusion is common when capturing people in the wild, where the performer do not follow predefined motion scripts. This challenges existing monocular human reconstruction systems that assume full body visibility. We introduce Self-Occluded Avatar Recovery (SOAR), a method for complete human reconstruction from partial observations where parts of the body are entirely unobserved. SOAR leverages structural normal prior and generative diffusion prior to address such an ill-posed reconstruction problem. For structural normal prior, we model human with an reposable surfel model with well-defined and easily readable shapes. For generative diffusion prior, we perform an initial reconstruction and refine it using score distillation. On various benchmarks, we show that SOAR performs favorably than state-of-the-art reconstruction and generation methods, and on-par comparing to concurrent works. Additional video results and code are available at https://soar-avatar.github.io/.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2409.12968.pdf' target='_blank'>https://arxiv.org/pdf/2409.12968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lara Chehayeb, Chirag Bhuvaneshwara, Manuel Anglet, Bernhard Hilpert, Ann-Kristin Meyer, Dimitra Tsovaltzi, Patrick Gebhard, Antje Biermann, Sinah Auchtor, Nils Lauinger, Julia Knopf, Andreas Kaiser, Fabian Kersting, Gregor Mehlmann, Florian Lingenfelser, Elisabeth AndrÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12968">MITHOS: Interactive Mixed Reality Training to Support Professional Socio-Emotional Interactions at Schools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teachers in challenging conflict situations often experience shame and self-blame, which relate to the feeling of incompetence but may externalise as anger. Sensing mixed signals fails the contingency rule for developing affect regulation and may result in confusion for students about their own emotions and hinder their emotion regulation. Therefore, being able to constructively regulate emotions not only benefits individual experience of emotions but also fosters effective interpersonal emotion regulation and influences how a situation is managed. MITHOS is a system aimed at training teachers' conflict resolution skills through realistic situative learning opportunities during classroom conflicts. In four stages, MITHOS supports teachers' socio-emotional self-awareness, perspective-taking and positive regard. It provides: a) a safe virtual environment to train free social interaction and receive natural social feedback from reciprocal student-agent reactions, b) spatial situational perspective taking through an avatar, c) individual virtual reflection guidance on emotional experiences through co-regulation processes, and d) expert feedback on professional behavioural strategies. This chapter presents the four stages and their implementation in a semi-automatic Wizard-of-Oz (WoZ) System. The WoZ system affords collecting data that are used for developing the fully automated hybrid (machine learning and model-based) system, and to validate the underlying psychological and conflict resolution models. We present results validating the approach in terms of scenario realism, as well as a systematic testing of the effects of external avatar similarity on antecedents of self-awareness with behavior similarity. The chapter contributes to a common methodology of conducting interdisciplinary research for human-centered and generalisable XR and presents a system designed to support it.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2409.08122.pdf' target='_blank'>https://arxiv.org/pdf/2409.08122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqiu Wang, Zihao Zhan, Haoqi Shan, Siqi Dai, Max Panoff, Shuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08122">GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from Avatar Views in VR/MR Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent and growing popularity of Virtual Reality (VR) and Mixed Reality (MR) solutions have revolutionized the way we interact with digital platforms. The cutting-edge gaze-controlled typing methods, now prevalent in high-end models of these devices, e.g., Apple Vision Pro, have not only improved user experience but also mitigated traditional keystroke inference attacks that relied on hand gestures, head movements and acoustic side-channels. However, this advancement has paradoxically given birth to a new, potentially more insidious cyber threat, GAZEploit.
  In this paper, we unveil GAZEploit, a novel eye-tracking based attack specifically designed to exploit these eye-tracking information by leveraging the common use of virtual appearances in VR applications. This widespread usage significantly enhances the practicality and feasibility of our attack compared to existing methods. GAZEploit takes advantage of this vulnerability to remotely extract gaze estimations and steal sensitive keystroke information across various typing scenarios-including messages, passwords, URLs, emails, and passcodes. Our research, involving 30 participants, achieved over 80% accuracy in keystroke inference. Alarmingly, our study also identified over 15 top-rated apps in the Apple Store as vulnerable to the GAZEploit attack, emphasizing the urgent need for bolstered security measures for this state-of-the-art VR/MR text entry method.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2408.09320.pdf' target='_blank'>https://arxiv.org/pdf/2408.09320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunsung Cho, Alexander Wang, Divya Kartik, Emily Liying Xie, Yukang Yan, David Lindlbauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09320">Auptimize: Optimal Placement of Spatial Audio Cues for Extended Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial audio in Extended Reality (XR) provides users with better awareness of where virtual elements are placed, and efficiently guides them to events such as notifications, system alerts from different windows, or approaching avatars. Humans, however, are inaccurate in localizing sound cues, especially with multiple sources due to limitations in human auditory perception such as angular discrimination error and front-back confusion. This decreases the efficiency of XR interfaces because users misidentify from which XR element a sound is coming. To address this, we propose Auptimize, a novel computational approach for placing XR sound sources, which mitigates such localization errors by utilizing the ventriloquist effect. Auptimize disentangles the sound source locations from the visual elements and relocates the sound sources to optimal positions for unambiguous identification of sound cues, avoiding errors due to inter-source proximity and front-back confusion. Our evaluation shows that Auptimize decreases spatial audio-based source identification errors compared to playing sound cues at the paired visual-sound locations. We demonstrate the applicability of Auptimize for diverse spatial audio-based interactive XR scenarios.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2408.08193.pdf' target='_blank'>https://arxiv.org/pdf/2408.08193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ria J. Gualano, Lucy Jiang, Kexin Zhang, Tanisha Shende, Andrea Stevenson Won, Shiri Azenkot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08193">"I Try to Represent Myself as I Am": Self-Presentation Preferences of People with Invisible Disabilities through Embodied Social VR Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing adoption of social virtual reality (VR), it is critical to design inclusive avatars. While researchers have investigated how and why blind and d/Deaf people wish to disclose their disabilities in VR, little is known about the preferences of many others with invisible disabilities (e.g., ADHD, dyslexia, chronic conditions). We filled this gap by interviewing 15 participants, each with one to three invisible disabilities, who represented 22 different invisible disabilities in total. We found that invisibly disabled people approached avatar-based disclosure through contextualized considerations informed by their prior experiences. For example, some wished to use VR's embodied affordances, such as facial expressions and body language, to dynamically represent their energy level or willingness to engage with others, while others preferred not to disclose their disability identity in any context. We define a binary framework for embodied invisible disability expression (public and private) and discuss three disclosure patterns (Activists, Non-Disclosers, and Situational Disclosers) to inform the design of future inclusive VR experiences.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2408.03646.pdf' target='_blank'>https://arxiv.org/pdf/2408.03646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Wang, Nan Li, Yansha Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03646">Goal-oriented Semantic Communication for the Metaverse Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the emergence of the metaverse and its role in enabling real-time simulation and analysis of real-world counterparts, an increasing number of personalized metaverse scenarios are being created to influence entertainment experiences and social behaviors. However, compared to traditional image and video entertainment applications, the exact transmission of the vast amount of metaverse-associated information significantly challenges the capacity of existing bit-oriented communication networks. Moreover, the current metaverse also witnesses a growing goal shift for transmitting the meaning behind custom-designed content, such as user-designed buildings and avatars, rather than exact copies of physical objects. To meet this growing goal shift and bandwidth challenge, this paper proposes a goal-oriented semantic communication framework for metaverse application (GSCM) to explore and define semantic information through the goal levels. Specifically, we first analyze the traditional image communication framework in metaverse construction and then detail our proposed semantic information along with the end-to-end wireless communication. We then describe the designed modules of the GSCM framework, including goal-oriented semantic information extraction, base knowledge definition, and neural radiance field (NeRF) based metaverse construction. Finally, numerous experiments have been conducted to demonstrate that, compared to image communication, our proposed GSCM framework decreases transmission latency by up to 92.6% and enhances the virtual object operation accuracy and metaverse construction clearance by up to 45.6% and 44.7%, respectively.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2408.00407.pdf' target='_blank'>https://arxiv.org/pdf/2408.00407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Wang, Yansha Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00407">Task-oriented and Semantics-aware Communications for Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Upon the advent of the emerging metaverse and its related applications in Augmented Reality (AR), the current bit-oriented network struggles to support real-time changes for the vast amount of associated information, creating a significant bottleneck in its development. To address the above problem, we present a novel task-oriented and semantics-aware communication framework for augmented reality (TSAR) to enhance communication efficiency and effectiveness significantly. We first present an analysis of the traditional wireless AR point cloud communication framework, followed by a detailed summary of our proposed semantic information extraction within the end-to-end communication. Then, we detail the components of the TSAR framework, incorporating semantics extraction with deep learning, task-oriented base knowledge selection, and avatar pose recovery. Through rigorous experimentation, we demonstrate that our proposed TSAR framework considerably outperforms traditional point cloud communication framework, reducing wireless AR application transmission latency by 95.6% and improving communication effectiveness in geometry and color aspects by up to 82.4% and 20.4%, respectively.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2407.11989.pdf' target='_blank'>https://arxiv.org/pdf/2407.11989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, CÃ©dric Plessiet, RÃ©my Sohier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11989">Interconnected virtual space and Theater. Practice as research on theater stage in the era of the network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since 2014, we have been conducting experiments based on a multidisciplinary collaboration between specialists in theatrical staging and researchers in virtual reality, digital art, and video games. This team focused its work on the similarities and differencesthat exist between real physical actors (actor-performers) and virtual digital actors (avatars). From this multidisciplinary approach, experimental research-creation projects have emerged and rely on a physical actor playing with the image of an avatar, controlled by another physical actor via the intermediary of a low-cost motion-capture system. In the first part of the paper, we will introduce the scenographic design on which our presentation is based, and the modifications we have made in relation to our previous work. Next, in the second section, we will discuss in detail the impact of augmenting the player's game using an avatar, compared to the scenic limitations of the theatrical stage. In part three of the paper, we will discuss the software-related aspects of the project, focusing on exchanges between the different components of our design and describing the algorithms enabling us to utilize the real-time movement of a player via various capture devices. To conclude, we will examine in detail how our experimental system linking physical actors and avatars profoundly alters the nature of collaboration between directors, actors, and digital artists in terms of actor/avatar direction.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2407.10412.pdf' target='_blank'>https://arxiv.org/pdf/2407.10412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiffany D. Do, Juanita Benjamin, Camille Isabella Protko, Ryan P. McMahan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10412">Cultural Reflections in Virtual Reality: The Effects of User Ethnicity in Avatar Matching Experiences on Sense of Embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Matching avatar characteristics to a user can impact sense of embodiment (SoE) in VR. However, few studies have examined how participant demographics may interact with these matching effects. We recruited a diverse and racially balanced sample of 78 participants to investigate the differences among participant groups when embodying both demographically matched and unmatched avatars. We found that participant ethnicity emerged as a significant factor, with Asian and Black participants reporting lower total SoE compared to Hispanic participants. Furthermore, we found that user ethnicity significantly influences ownership (a subscale of SoE), with Asian and Black participants exhibiting stronger effects of matched avatar ethnicity compared to White participants. Additionally, Hispanic participants showed no significant differences, suggesting complex dynamics in ethnic-racial identity. Our results also reveal significant main effects of matched avatar ethnicity and gender on SoE, indicating the importance of considering these factors in VR experiences. These findings contribute valuable insights into understanding the complex dynamics shaping VR experiences across different demographic groups.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2407.00229.pdf' target='_blank'>https://arxiv.org/pdf/2407.00229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirban Mukherjee, Venkat Suprabath Bitra, Vignesh Bondugula, Tarun Reddy Tallapureddy, Dinesh Babu Jayagopi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00229">SemUV: Deep Learning based semantic manipulation over UV texture map of virtual human heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing and manipulating virtual human heads is essential across various applications, including AR, VR, gaming, human-computer interaction and VFX. Traditional graphic-based approaches require manual effort and resources to achieve accurate representation of human heads. While modern deep learning techniques can generate and edit highly photorealistic images of faces, their focus remains predominantly on 2D facial images. This limitation makes them less suitable for 3D applications. Recognizing the vital role of editing within the UV texture space as a key component in the 3D graphics pipeline, our work focuses on this aspect to benefit graphic designers by providing enhanced control and precision in appearance manipulation. Research on existing methods within the UV texture space is limited, complex, and poses challenges. In this paper, we introduce SemUV: a simple and effective approach using the FFHQ-UV dataset for semantic manipulation directly within the UV texture space. We train a StyleGAN model on the publicly available FFHQ-UV dataset, and subsequently train a boundary for interpolation and semantic feature manipulation. Through experiments comparing our method with 2D manipulation technique, we demonstrate its superior ability to preserve identity while effectively modifying semantic features such as age, gender, and facial hair. Our approach is simple, agnostic to other 3D components such as structure, lighting, and rendering, and also enables seamless integration into standard 3D graphics pipelines without demanding extensive domain expertise, time, or resources.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2405.18565.pdf' target='_blank'>https://arxiv.org/pdf/2405.18565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keiichi Ihara, Kyzyl Monteiro, Mehrad Faridan, Rubaiat Habib Kazi, Ryo Suzuki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18565">Video2MR: Automatically Generating Mixed Reality 3D Instructions by Augmenting Extracted Motion from 2D Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Video2MR, a mixed reality system that automatically generates 3D sports and exercise instructions from 2D videos. Mixed reality instructions have great potential for physical training, but existing works require substantial time and cost to create these 3D experiences. Video2MR overcomes this limitation by transforming arbitrary instructional videos available online into MR 3D avatars with AI-enabled motion capture (DeepMotion). Then, it automatically enhances the avatar motion through the following augmentation techniques: 1) contrasting and highlighting differences between the user and avatar postures, 2) visualizing key trajectories and movements of specific body parts, 3) manipulation of time and speed using body motion, and 4) spatially repositioning avatars for different perspectives. Developed on Hololens 2 and Azure Kinect, we showcase various use cases, including yoga, dancing, soccer, tennis, and other physical exercises. The study results confirm that Video2MR provides more engaging and playful learning experiences, compared to existing 2D video instructions.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2404.18628.pdf' target='_blank'>https://arxiv.org/pdf/2404.18628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Maiorca, Seyed Abolfazl Ghasemzadeh, Thierry Ravet, FranÃ§ois Cresson, Thierry Dutoit, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18628">Self-Avatar Animation in Virtual Reality: Impact of Motion Signals Artifacts on the Full-Body Pose Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) applications have revolutionized user experiences by immersing individuals in interactive 3D environments. These environments find applications in numerous fields, including healthcare, education, or architecture. A significant aspect of VR is the inclusion of self-avatars, representing users within the virtual world, which enhances interaction and embodiment. However, generating lifelike full-body self-avatar animations remains challenging, particularly in consumer-grade VR systems, where lower-body tracking is often absent. One method to tackle this problem is by providing an external source of motion information that includes lower body information such as full Cartesian positions estimated from RGB(D) cameras. Nevertheless, the limitations of these systems are multiples: the desynchronization between the two motion sources and occlusions are examples of significant issues that hinder the implementations of such systems. In this paper, we aim to measure the impact on the reconstruction of the articulated self-avatar's full-body pose of (1) the latency between the VR motion features and estimated positions, (2) the data acquisition rate, (3) occlusions, and (4) the inaccuracy of the position estimation algorithm. In addition, we analyze the motion reconstruction errors using ground truth and 3D Cartesian coordinates estimated from \textit{YOLOv8} pose estimation. These analyzes show that the studied methods are significantly sensitive to any degradation tested, especially regarding the velocity reconstruction error.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2404.16040.pdf' target='_blank'>https://arxiv.org/pdf/2404.16040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megan A. Witherow, Norou Diawara, Janice Keener, John W. Harrington, Khan M. Iftekharuddin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16040">Pilot Study to Discover Candidate Biomarkers for Autism based on Perception and Production of Facial Expressions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: Facial expression production and perception in autism spectrum disorder (ASD) suggest potential presence of behavioral biomarkers that may stratify individuals on the spectrum into prognostic or treatment subgroups. Construct validity and group discriminability have been recommended as criteria for identification of candidate stratification biomarkers.
  Methods: In an online pilot study of 11 children and young adults diagnosed with ASD and 11 age- and gender-matched neurotypical (NT) individuals, participants recognize and mimic static and dynamic facial expressions of 3D avatars. Webcam-based eye-tracking (ET) and facial video tracking (VT), including activation and asymmetry of action units (AUs) from the Facial Action Coding System (FACS) are collected. We assess validity of constructs for each dependent variable (DV) based on the expected response in the NT group. Then, the Boruta statistical method identifies DVs that are significant to group discriminability (ASD or NT).
  Results: We identify one candidate ET biomarker (percentage gaze duration to the face while mimicking static 'disgust' expression) and 14 additional DVs of interest for future study, including 4 ET DVs, 5 DVs related to VT AU activation, and 4 DVs related to AU asymmetry in VT. Based on a power analysis, we provide sample size recommendations for future studies.
  Conclusion: This pilot study provides a framework for ASD stratification biomarker discovery based on perception and production of facial expressions.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2404.12322.pdf' target='_blank'>https://arxiv.org/pdf/2404.12322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Liang, Haotian Liu, Hongteng Xu, Dixin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12322">Generalizable Face Landmarking Guided by Conditional Face Warping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a significant step for human face modeling, editing, and generation, face landmarking aims at extracting facial keypoints from images. A generalizable face landmarker is required in practice because real-world facial images, e.g., the avatars in animations and games, are often stylized in various ways. However, achieving generalizable face landmarking is challenging due to the diversity of facial styles and the scarcity of labeled stylized faces. In this study, we propose a simple but effective paradigm to learn a generalizable face landmarker based on labeled real human faces and unlabeled stylized faces. Our method learns the face landmarker as the key module of a conditional face warper. Given a pair of real and stylized facial images, the conditional face warper predicts a warping field from the real face to the stylized one, in which the face landmarker predicts the ending points of the warping field and provides us with high-quality pseudo landmarks for the corresponding stylized facial images. Applying an alternating optimization strategy, we learn the face landmarker to minimize $i)$ the discrepancy between the stylized faces and the warped real ones and $ii)$ the prediction errors of both real and pseudo landmarks. Experiments on various datasets show that our method outperforms existing state-of-the-art domain adaptation methods in face landmarking tasks, leading to a face landmarker with better generalizability. Code is available at https://plustwo0.github.io/project-face-landmarker.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2404.01296.pdf' target='_blank'>https://arxiv.org/pdf/2404.01296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Armand Comas-MassaguÃ©, Di Qiu, Menglei Chai, Marcel BÃ¼hler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01296">MagicMirror: Fast and High-Quality Avatar Generation with a Constrained Search Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: https://syntec-research.github.io/MagicMirror
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2404.00368.pdf' target='_blank'>https://arxiv.org/pdf/2404.00368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Liu, Qiong Cao, Yandong Wen, Huaiguang Jiang, Changxing Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00368">Towards Variable and Coordinated Holistic Co-Speech Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of generating lifelike holistic co-speech motions for 3D avatars, focusing on two key aspects: variability and coordination. Variability allows the avatar to exhibit a wide range of motions even with similar speech content, while coordination ensures a harmonious alignment among facial expressions, hand gestures, and body poses. We aim to achieve both with ProbTalk, a unified probabilistic framework designed to jointly model facial, hand, and body movements in speech. ProbTalk builds on the variational autoencoder (VAE) architecture and incorporates three core designs. First, we introduce product quantization (PQ) to the VAE, which enriches the representation of complex holistic motion. Second, we devise a novel non-autoregressive model that embeds 2D positional encoding into the product-quantized representation, thereby preserving essential structure information of the PQ codes. Last, we employ a secondary stage to refine the preliminary prediction, further sharpening the high-frequency details. Coupling these three designs enables ProbTalk to generate natural and diverse holistic co-speech motions, outperforming several state-of-the-art methods in qualitative and quantitative evaluations, particularly in terms of realism. Our code and model will be released for research purposes at https://feifeifeiliu.github.io/probtalk/.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2403.17103.pdf' target='_blank'>https://arxiv.org/pdf/2403.17103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Remy Sabathier, Niloy J. Mitra, David Novotny
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17103">Animal Avatars: Reconstructing Animatable 3D Animals from Casual Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method to build animatable dog avatars from monocular videos. This is challenging as animals display a range of (unpredictable) non-rigid movements and have a variety of appearance details (e.g., fur, spots, tails). We develop an approach that links the video frames via a 4D solution that jointly solves for animal's pose variation, and its appearance (in a canonical pose). To this end, we significantly improve the quality of template-based shape fitting by endowing the SMAL parametric model with Continuous Surface Embeddings, which brings image-to-mesh reprojection constaints that are denser, and thus stronger, than the previously used sparse semantic keypoint correspondences. To model appearance, we propose an implicit duplex-mesh texture that is defined in the canonical pose, but can be deformed using SMAL pose coefficients and later rendered to enforce a photometric compatibility with the input video frames. On the challenging CoP3D and APTv2 datasets, we demonstrate superior results (both in terms of pose estimates and predicted appearance) to existing template-free (RAC) and template-based approaches (BARC, BITE).
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2403.13553.pdf' target='_blank'>https://arxiv.org/pdf/2403.13553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>H. Zhang, Z. Qiao, H. Wang, B. Duan, J. Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13553">VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational artificial intelligence can already independently engage in brief conversations with clients with psychological problems and provide evidence-based psychological interventions. The main objective of this study is to improve the effectiveness and credibility of the large language model in psychological intervention by creating a specialized agent, the VCounselor, to address the limitations observed in popular large language models such as ChatGPT in domain applications. We achieved this goal by proposing a new affective interaction structure and knowledge-enhancement structure. In order to evaluate VCounselor, this study compared the general large language model, the fine-tuned large language model, and VCounselor's knowledge-enhanced large language model. At the same time, the general large language model and the fine-tuned large language model will also be provided with an avatar to compare them as an agent with VCounselor. The comparison results indicated that the affective interaction structure and knowledge-enhancement structure of VCounselor significantly improved the effectiveness and credibility of the psychological intervention, and VCounselor significantly provided positive tendencies for clients' emotions. The conclusion of this study strongly supports that VConselor has a significant advantage in providing psychological support to clients by being able to analyze the patient's problems with relative accuracy and provide professional-level advice that enhances support for clients.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2403.07314.pdf' target='_blank'>https://arxiv.org/pdf/2403.07314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megan A. Witherow, Crystal Butler, Winston J. Shields, Furkan Ilgin, Norou Diawara, Janice Keener, John W. Harrington, Khan M. Iftekharuddin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07314">Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more. However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert. To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss encourages feature correlation with AUs while discouraging correlation with subject identities for improved generalization. We train BeCoME-Net for unilateral and bilateral AU detection and compare with state-of-the-art approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty healthy adult volunteers complete expression recognition and mimicry tasks in an online feasibility study while webcam-based eye-tracking and video are collected. We test validity of multiple constructs, including face preference during recognition and AUs during mimicry.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2403.05218.pdf' target='_blank'>https://arxiv.org/pdf/2403.05218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05218">3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech. Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning. However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes. To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process. Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based Graph Convolution Encoder applied to the facial mesh. This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW benchmark.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2402.09211.pdf' target='_blank'>https://arxiv.org/pdf/2402.09211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongseok Yang, Jiho Kang, Lingni Ma, Joseph Greer, Yuting Ye, Sung-Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09211">DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced Three-Point Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Full-body avatar presence is crucial for immersive social and environmental interactions in digital reality. However, current devices only provide three six degrees of freedom (DOF) poses from the headset and two controllers (i.e. three-point trackers). Because it is a highly under-constrained problem, inferring full-body pose from these inputs is challenging, especially when supporting the full range of body proportions and use cases represented by the general population. In this paper, we propose a deep learning framework, DivaTrack, which outperforms existing methods when applied to diverse body sizes and activities. We augment the sparse three-point inputs with linear accelerations from Inertial Measurement Units (IMU) to improve foot contact prediction. We then condition the otherwise ambiguous lower-body pose with the predictions of foot contact and upper-body pose in a two-stage model. We further stabilize the inferred full-body pose in a wide range of configurations by learning to blend predictions that are computed in two reference frames, each of which is designed for different types of motions. We demonstrate the effectiveness of our design on a large dataset that captures 22 subjects performing challenging locomotion for three-point tracking, including lunges, hula-hooping, and sitting. As shown in a live demo using the Meta VR headset and Xsens IMUs, our method runs in real-time while accurately tracking a user's motion when they perform a diverse set of movements.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2402.03279.pdf' target='_blank'>https://arxiv.org/pdf/2402.03279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiffany D. Do, Camille Isabella Protko, Ryan P. McMahan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03279">Stepping into the Right Shoes: The Effects of User-Matched Avatar Ethnicity and Gender on Sense of Embodiment in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many consumer virtual reality (VR) applications, users embody predefined characters that offer minimal customization options, frequently emphasizing storytelling over user choice. We explore whether matching a user's physical characteristics, specifically ethnicity and gender, with their virtual self-avatar affects their sense of embodiment in VR. We conducted a 2 x 2 within-subjects experiment (n=32) with a diverse user population to explore the impact of matching or not matching a user's self-avatar to their ethnicity and gender on their sense of embodiment. Our results indicate that matching the ethnicity of the user and their self-avatar significantly enhances sense of embodiment regardless of gender, extending across various aspects, including appearance, response, and ownership. We also found that matching gender significantly enhanced ownership, suggesting that this aspect is influenced by matching both ethnicity and gender. Interestingly, we found that matching ethnicity specifically affects self-location while matching gender specifically affects one's body ownership.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2401.11002.pdf' target='_blank'>https://arxiv.org/pdf/2401.11002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaitanya Patel, Shaojie Bai, Te-Li Wang, Jason Saragih, Shih-En Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11002">Fast Registration of Photorealistic Avatars for VR Facial Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) bares promise of social interactions that can feel more immersive than other media. Key to this is the ability to accurately animate a personalized photorealistic avatar, and hence the acquisition of the labels for headset-mounted camera (HMC) images need to be efficient and accurate, while wearing a VR headset. This is challenging due to oblique camera views and differences in image modality. In this work, we first show that the domain gap between the avatar and HMC images is one of the primary sources of difficulty, where a transformer-based architecture achieves high accuracy on domain-consistent data, but degrades when the domain-gap is re-introduced. Building on this finding, we propose a system split into two parts: an iterative refinement module that takes in-domain inputs, and a generic avatar-guided image-to-image domain transfer module conditioned on current estimates. These two modules reinforce each other: domain transfer becomes easier when close-to-groundtruth examples are shown, and better domain-gap removal in turn improves the registration. Our system obviates the need for costly offline optimization, and produces online registration of higher quality than direct regression method. We validate the accuracy and efficiency of our approach through extensive experiments on a commodity headset, demonstrating significant improvements over these baselines. To stimulate further research in this direction, we make our large-scale dataset and code publicly available.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2312.12096.pdf' target='_blank'>https://arxiv.org/pdf/2312.12096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunjie Luo, Fei Luo, Yusen Wang, Enxu Zhao, Chunxia Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12096">DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing a dynamic human with loose clothing is an important but difficult task. To address this challenge, we propose a method named DLCA-Recon to create human avatars from monocular videos. The distance from loose clothing to the underlying body rapidly changes in every frame when the human freely moves and acts. Previous methods lack effective geometric initialization and constraints for guiding the optimization of deformation to explain this dramatic change, resulting in the discontinuous and incomplete reconstruction surface. To model the deformation more accurately, we propose to initialize an estimated 3D clothed human in the canonical space, as it is easier for deformation fields to learn from the clothed human than from SMPL. With both representations of explicit mesh and implicit SDF, we utilize the physical connection information between consecutive frames and propose a dynamic deformation field (DDF) to optimize deformation fields. DDF accounts for contributive forces on loose clothing to enhance the interpretability of deformations and effectively capture the free movement of loose clothing. Moreover, we propagate SMPL skinning weights to each individual and refine pose and skinning weights during the optimization to improve skinning transformation. Based on more reasonable initialization and DDF, we can simulate real-world physics more accurately. Extensive experiments on public and our own datasets validate that our method can produce superior results for humans with loose clothing compared to the SOTA methods.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2312.08679.pdf' target='_blank'>https://arxiv.org/pdf/2312.08679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Wang, Giljoo Nam, Aljaz Bozic, Chen Cao, Jason Saragih, Michael Zollhoefer, Jessica Hodgins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08679">A Local Appearance Model for Volumetric Capture of Diverse Hairstyle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hair plays a significant role in personal identity and appearance, making it an essential component of high-quality, photorealistic avatars. Existing approaches either focus on modeling the facial region only or rely on personalized models, limiting their generalizability and scalability. In this paper, we present a novel method for creating high-fidelity avatars with diverse hairstyles. Our method leverages the local similarity across different hairstyles and learns a universal hair appearance prior from multi-view captures of hundreds of people. This prior model takes 3D-aligned features as input and generates dense radiance fields conditioned on a sparse point cloud with color. As our model splits different hairstyles into local primitives and builds prior at that level, it is capable of handling various hair topologies. Through experiments, we demonstrate that our model captures a diverse range of hairstyles and generalizes well to challenging new hairstyles. Empirical results show that our method improves the state-of-the-art approaches in capturing and generating photorealistic, personalized avatars with complete hair.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2311.11614.pdf' target='_blank'>https://arxiv.org/pdf/2311.11614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixiang Lin, Jianke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11614">Semantic-Preserved Point-based Human Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable realistic experience in AR/VR and digital entertainment, we present the first point-based human avatar model that embodies the entirety expressive range of digital humans. We employ two MLPs to model pose-dependent deformation and linear skinning (LBS) weights. The representation of appearance relies on a decoder and the features that attached to each point. In contrast to alternative implicit approaches, the oriented points representation not only provides a more intuitive way to model human avatar animation but also significantly reduces both training and inference time. Moreover, we propose a novel method to transfer semantic information from the SMPL-X model to the points, which enables to better understand human body movements. By leveraging the semantic information of points, we can facilitate virtual try-on and human avatar composition through exchanging the points of same category across different subjects. Experimental results demonstrate the efficacy of our presented method.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2311.05521.pdf' target='_blank'>https://arxiv.org/pdf/2311.05521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, Yan-Pei Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05521">BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing photorealistic 4D human head avatars from videos is essential for VR/AR, telepresence, and video game applications. Although existing Neural Radiance Fields (NeRF)-based methods achieve high-fidelity results, the computational expense limits their use in real-time applications. To overcome this limitation, we introduce BakedAvatar, a novel representation for real-time neural head avatar synthesis, deployable in a standard polygon rasterization pipeline. Our approach extracts deformable multi-layer meshes from learned isosurfaces of the head and computes expression-, pose-, and view-dependent appearances that can be baked into static textures for efficient rasterization. We thus propose a three-stage pipeline for neural head avatar synthesis, which includes learning continuous deformation, manifold, and radiance fields, extracting layered meshes and textures, and fine-tuning texture details with differential rasterization. Experimental results demonstrate that our representation generates synthesis results of comparable quality to other state-of-the-art methods while significantly reducing the inference time required. We further showcase various head avatar synthesis results from monocular videos, including view synthesis, face reenactment, expression editing, and pose editing, all at interactive frame rates.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2311.04218.pdf' target='_blank'>https://arxiv.org/pdf/2311.04218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijuan Liu, Xiangyu Xu, Zhijie Lin, Jiabin Liang, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04218">Towards Garment Sewing Pattern Reconstruction from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Garment sewing pattern represents the intrinsic rest shape of a garment, and is the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. Code, dataset, and pre-trained models are available at: https://sewformer.github.io.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2310.13717.pdf' target='_blank'>https://arxiv.org/pdf/2310.13717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, CÃ©dric Plessiet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13717">Quand le jeu vidÃ©o est le catalyseur d'expÃ©rimentations thÃ©Ã¢trales (2014-2019)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The meeting between the director and researcher Georges Gagner{Ã©} and the digital artist and researcher CÃ©dric Plessiet at Paris 8 University through experiments crossing live performance and video game led to the development of tools from game techniques video and dedicated to theatrical experimentation on a mixed stage involving physical actors and avatars. The genesis of AvatarStaging setup and AKN_Regie tool is detailed through the various research-creations on which they were formalized and used. AKN_Regie is a plugin of  the videogame engine Unreal Engine, programmed in blueprint, and dedicated to real time 3D previsualization and avatar direction.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2309.12841.pdf' target='_blank'>https://arxiv.org/pdf/2309.12841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ariel Kwiatkowski, Vicky Kalogeiton, Julien PettrÃ©, Marie-Paule Cani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12841">Reward Function Design for Crowd Simulation via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd simulation is important for video-games design, since it enables to populate virtual worlds with autonomous avatars that navigate in a human-like manner. Reinforcement learning has shown great potential in simulating virtual crowds, but the design of the reward function is critical to achieving effective and efficient results. In this work, we explore the design of reward functions for reinforcement learning-based crowd simulation. We provide theoretical insights on the validity of certain reward functions according to their analytical properties, and evaluate them empirically using a range of scenarios, using the energy efficiency as the metric. Our experiments show that directly minimizing the energy usage is a viable strategy as long as it is paired with an appropriately scaled guiding potential, and enable us to study the impact of the different reward components on the behavior of the simulated crowd. Our findings can inform the development of new crowd simulation techniques, and contribute to the wider study of human-like navigation.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2309.10902.pdf' target='_blank'>https://arxiv.org/pdf/2309.10902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiffany D. Do, Steve Zelenty, Mar Gonzalez-Franco, Ryan P. McMahan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10902">VALID: A perceptually validated Virtual Avatar Library for Inclusion and Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As consumer adoption of immersive technologies grows, virtual avatars will play a prominent role in the future of social computing. However, as people begin to interact more frequently through virtual avatars, it is important to ensure that the research community has validated tools to evaluate the effects and consequences of such technologies. We present the first iteration of a new, freely available 3D avatar library called the Virtual Avatar Library for Inclusion and Diversity (VALID), which includes 210 fully rigged avatars with a focus on advancing racial diversity and inclusion. We present a detailed process for creating, iterating, and validating avatars of diversity. Through a large online study (n=132) with participants from 33 countries, we provide statistically validated labels for each avatar's perceived race and gender. Through our validation study, we also advance knowledge pertaining to the perception of an avatar's race. In particular, we found that avatars of some races were more accurately identified by participants of the same race.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2309.09132.pdf' target='_blank'>https://arxiv.org/pdf/2309.09132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anas El Fathi, Mohammadreza Ganji, Dimitri Boiroux, Henrik Bengtsson, Marc D. Breton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09132">Intermittent Control for Safe Long-Acting Insulin Intensification for Type 2 Diabetes: In-Silico Experiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Around a third of type 2 diabetes patients (T2D) are escalated to basal insulin injections. Basal insulin dose is titrated to achieve a tight glycemic target without undue hypoglycemic risk. In the standard of care (SoC), titration is based on intermittent fasting blood glucose (FBG) measurements. Lack of adherence and the day-to-day variabilities in FBG measurements are limiting factors to the existing insulin titration procedure. We propose an adaptive receding horizon control strategy where a glucose-insulin fasting model is identified and used to predict the optimal basal insulin dose. This algorithm is evaluated in \textit{in-silico} experiments using the new UVA virtual lab (UVlab) and a set of T2D avatars matched to clinical data (NCT01336023). Compared to SoC, we show that this control strategy can achieve the same glucose targets faster (as soon as week 8) and safer (increased hypoglycemia protection and robustness to missing FBG measurements). Specifically, when insulin is titrated daily, a time-in-range (TIR, 70--180 mg/dL) of 71.4$\pm$20.0\% can be achieved at week eight and maintained at week 52 (72.6$\pm$19.6%) without an increased hypoglycemia risk as measured by time under 70 mg/dL (TBR, week 8: 1.3$\pm$1.9% and week 52: 1.2$\pm$1.9%), when compared to the SoC (TIR at week 8: 59.3$\pm$28.0% and week:52 72.1$\pm$22.3%, TBR at week 8: 0.5$\pm$1.3% and week 52: 2.8$\pm$3.4%). Such an approach can potentially reduce treatment inertia and prescription complexity, resulting in improved glycemic outcomes for T2D using basal insulin injections.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2309.07289.pdf' target='_blank'>https://arxiv.org/pdf/2309.07289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunus Bicer, Niklas Smedemark-Margulies, Basak Celik, Elifnur Sunger, Ryan Orendorff, Stephanie Naufel, Tales Imbiriba, Deniz ErdoÄmuÅ, Eugene Tunik, Mathew Yarossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07289">User Training with Error Augmentation for Electromyogram-based Gesture Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We designed and tested a system for real-time control of a user interface by extracting surface electromyographic (sEMG) activity from eight electrodes in a wrist-band configuration. sEMG data were streamed into a machine-learning algorithm that classified hand gestures in real-time. After an initial model calibration, participants were presented with one of three types of feedback during a human-learning stage: veridical feedback, in which predicted probabilities from the gesture classification algorithm were displayed without alteration, modified feedback, in which we applied a hidden augmentation of error to these probabilities, and no feedback. User performance was then evaluated in a series of minigames, in which subjects were required to use eight gestures to manipulate their game avatar to complete a task. Experimental results indicated that, relative to baseline, the modified feedback condition led to significantly improved accuracy and improved gesture class separation. These findings suggest that real-time feedback in a gamified user interface with manipulation of feedback may enable intuitive, rapid, and accurate task acquisition for sEMG-based gesture recognition applications.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2308.15839.pdf' target='_blank'>https://arxiv.org/pdf/2308.15839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Myungjin Shin, Dohae Lee, In-Kwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15839">Utilizing Task-Generic Motion Prior to Recover Full-Body Motion from Very Sparse Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The most popular type of devices used to track a user's posture in a virtual reality experience consists of a head-mounted display and two controllers held in both hands. However, due to the limited number of tracking sensors (three in total), faithfully recovering the user in full-body is challenging, limiting the potential for interactions among simulated user avatars within the virtual world. Therefore, recent studies have attempted to reconstruct full-body poses using neural networks that utilize previously learned human poses or accept a series of past poses over a short period. In this paper, we propose a method that utilizes information from a neural motion prior to improve the accuracy of reconstructed user's motions. Our approach aims to reconstruct user's full-body poses by predicting the latent representation of the user's overall motion from limited input signals and integrating this information with tracking sensor inputs. This is based on the premise that the ultimate goal of pose reconstruction is to reconstruct the motion, which is a series of poses. Our results show that this integration enables more accurate reconstruction of the user's full-body motion, particularly enhancing the robustness of lower body motion reconstruction from impoverished signals. Web: https://https://mjsh34.github.io/mp-sspe/
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2308.13551.pdf' target='_blank'>https://arxiv.org/pdf/2308.13551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyue Yao, Mingjie Sun, Bingliang Li, Fengyu Yang, Junle Wang, Ruimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13551">Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as references for motion generation. Then, we introduce a hyper-parameter that coordinates the similarity between dancers by masking poses to prevent the generation of sequences that are over-diverse or consistent. To avoid the rigidity of movements, we design a Dance Pre-generated stage to pre-generate these masked poses instead of filling them with zeros. After that, a Dance Motion Transfer stage is adopted with leader sequences and music, in which a multi-conditional sampling formula is rewritten to transfer the pre-generated poses into a sequence with a partner style. In practice, to address the lack of multi-person datasets, we introduce AIST-M, a new dataset for partner dancer generation, which is publicly availiable. Comprehensive evaluations on our AIST-M dataset demonstrate that the proposed DanY can synthesize satisfactory partner dancer results with controllable diversity.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2308.10559.pdf' target='_blank'>https://arxiv.org/pdf/2308.10559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leila Ismail, Rajkumar Buyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10559">Metaverse: A Vision, Architectural Elements, and Future Directions for Scalable and Realtime Virtual Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the emergence of Cloud computing, Internet of Things-enabled Human-Computer Interfaces, Generative Artificial Intelligence, and high-accurate Machine and Deep-learning recognition and predictive models, along with the Post Covid-19 proliferation of social networking, and remote communications, the Metaverse gained a lot of popularity. Metaverse has the prospective to extend the physical world using virtual and augmented reality so the users can interact seamlessly with the real and virtual worlds using avatars and holograms. It has the potential to impact people in the way they interact on social media, collaborate in their work, perform marketing and business, teach, learn, and even access personalized healthcare. Several works in the literature examine Metaverse in terms of hardware wearable devices, and virtual reality gaming applications. However, the requirements of realizing the Metaverse in realtime and at a large-scale need yet to be examined for the technology to be usable. To address this limitation, this paper presents the temporal evolution of Metaverse definitions and captures its evolving requirements. Consequently, we provide insights into Metaverse requirements. In addition to enabling technologies, we lay out architectural elements for scalable, reliable, and efficient Metaverse systems, and a classification of existing Metaverse applications along with proposing required future research directions.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2308.10115.pdf' target='_blank'>https://arxiv.org/pdf/2308.10115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Trung Phan, Hamed Alimohammadzadeh, Heather Culbertson, Shahram Ghandeharizadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10115">An Evaluation of Three Distance Measurement Technologies for Flying Light Specks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study evaluates the accuracy of three different types of time-of-flight sensors to measure distance. We envision the possible use of these sensors to localize swarms of flying light specks (FLSs) to illuminate objects and avatars of a metaverse. An FLS is a miniature-sized drone configured with RGB light sources. It is unable to illuminate a point cloud by itself. However, the inter-FLS relationship effect of an organizational framework will compensate for the simplicity of each individual FLS, enabling a swarm of cooperating FLSs to illuminate complex shapes and render haptic interactions. Distance between FLSs is an important criterion of the inter-FLS relationship. We consider sensors that use radio frequency (UWB), infrared light (IR), and sound (ultrasonic) to quantify this metric. Obtained results show only one sensor is able to measure distances as small as 1 cm with a high accuracy. A sensor may require a calibration process that impacts its accuracy in measuring distance.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2308.05112.pdf' target='_blank'>https://arxiv.org/pdf/2308.05112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Zhang, Jie Chen, Qiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05112">Explicifying Neural Implicit Fields for Efficient Dynamic Human Avatar Modeling via a Neural Explicit Surface</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a technique for efficiently modeling dynamic humans by explicifying the implicit neural fields via a Neural Explicit Surface (NES). Implicit neural fields have advantages over traditional explicit representations in modeling dynamic 3D content from sparse observations and effectively representing complex geometries and appearances. Implicit neural fields defined in 3D space, however, are expensive to render due to the need for dense sampling during volumetric rendering. Moreover, their memory efficiency can be further optimized when modeling sparse 3D space. To overcome these issues, the paper proposes utilizing Neural Explicit Surface (NES) to explicitly represent implicit neural fields, facilitating memory and computational efficiency. To achieve this, the paper creates a fully differentiable conversion between the implicit neural fields and the explicit rendering interface of NES, leveraging the strengths of both implicit and explicit approaches. This conversion enables effective training of the hybrid representation using implicit methods and efficient rendering by integrating the explicit rendering interface with a newly proposed rasterization-based neural renderer that only incurs a texture color query once for the initial ray interaction with the explicit surface, resulting in improved inference efficiency. NES describes dynamic human geometries with pose-dependent neural implicit surface deformation fields and their dynamic neural textures both in 2D space, which is a more memory-efficient alternative to traditional 3D methods, reducing redundancy and computational load. The comprehensive experiments show that NES performs similarly to previous 3D approaches, with greatly improved rendering speed and reduced memory cost.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2308.03610.pdf' target='_blank'>https://arxiv.org/pdf/2308.03610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, Min Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03610">AvatarVerse: High-quality & Stable 3D Avatar Creation from Text and Pose</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating expressive, diverse and high-quality 3D avatars from highly customized text descriptions and pose guidance is a challenging task, due to the intricacy of modeling and texturing in 3D that ensure details and various styles (realistic, fictional, etc). We present AvatarVerse, a stable pipeline for generating expressive high-quality 3D avatars from nothing but text descriptions and pose guidance. In specific, we introduce a 2D diffusion model conditioned on DensePose signal to establish 3D pose control of avatars through 2D images, which enhances view consistency from partially observed scenarios. It addresses the infamous Janus Problem and significantly stablizes the generation process. Moreover, we propose a progressive high-resolution 3D synthesis strategy, which obtains substantial improvement over the quality of the created 3D avatars. To this end, the proposed AvatarVerse pipeline achieves zero-shot 3D modeling of 3D avatars that are not only more expressive, but also in higher quality and fidelity than previous works. Rigorous qualitative evaluations and user studies showcase AvatarVerse's superiority in synthesizing high-fidelity 3D avatars, leading to a new standard in high-quality and stable 3D avatar creation. Our project page is: https://avatarverse3d.github.io
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2307.05501.pdf' target='_blank'>https://arxiv.org/pdf/2307.05501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruslan Isaev, Radmir Gumerov, Gulzada Esenalieva, Remudin Reshid Mekuria, Ermek Doszhanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05501">HIVA: Holographic Intellectual Voice Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Holographic Intellectual Voice Assistant (HIVA) aims to facilitate human computer interaction using audiovisual effects and 3D avatar. HIVA provides complete information about the university, including requests of various nature: admission, study issues, fees, departments, university structure and history, canteen, human resources, library, student life and events, information about the country and the city, etc. There are other ways for receiving the data listed above: the university's official website and other supporting apps, HEI (Higher Education Institution) official social media, directly asking the HEI staff, and other channels. However, HIVA provides the unique experience of "face-to-face" interaction with an animated 3D mascot, helping to get a sense of 'real-life' communication. The system includes many sub-modules and connects a family of applications such as mobile applications, Telegram chatbot, suggestion categorization, and entertainment services. The Voice assistant uses Russian language NLP models and tools, which are pipelined for the best user experience.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2306.16541.pdf' target='_blank'>https://arxiv.org/pdf/2306.16541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanyue Shen, Letian Zhang, Zhangsihao Yang, Masood Mortazavi, Xiyun Song, Liang Peng, Heather Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16541">Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meeting online is becoming the new normal. Creating an immersive experience for online meetings is a necessity towards more diverse and seamless environments. Efficient photorealistic rendering of human 3D dynamics is the core of immersive meetings. Current popular applications achieve real-time conferencing but fall short in delivering photorealistic human dynamics, either due to limited 2D space or the use of avatars that lack realistic interactions between participants. Recent advances in neural rendering, such as the Neural Radiance Field (NeRF), offer the potential for greater realism in metaverse meetings. However, the slow rendering speed of NeRF poses challenges for real-time conferencing. We envision a pipeline for a future extended reality metaverse conferencing system that leverages monocular video acquisition and free-viewpoint synthesis to enhance data and hardware efficiency. Towards an immersive conferencing experience, we explore an accelerated NeRF-based free-viewpoint synthesis algorithm for rendering photorealistic human dynamics more efficiently. We show that our algorithm achieves comparable rendering quality while performing training and inference 44.5% and 213% faster than state-of-the-art methods, respectively. Our exploration provides a design basis for constructing metaverse conferencing systems that can handle complex application scenarios, including dynamic scene relighting with customized themes and multi-user conferencing that harmonizes real-world people into an extended world.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2306.15470.pdf' target='_blank'>https://arxiv.org/pdf/2306.15470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Wang, Yansha Deng, A. Hamid Aghvami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15470">Goal-oriented Semantic Communications for Avatar-centric Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Upon the advent of the emerging metaverse and its related applications in Augmented Reality (AR), the current bit-oriented network struggles to support real-time changes for the vast amount of associated information, hindering its development. Thus, a critical revolution in the Sixth Generation (6G) networks is envisioned through the joint exploitation of information context and its importance to the task, leading to a communication paradigm shift towards semantic and effectiveness levels. However, current research has not yet proposed any explicit and systematic communication framework for AR applications that incorporate these two levels. To fill this research gap, this paper presents a task-oriented and semantics-aware communication framework for augmented reality (TSAR) to enhance communication efficiency and effectiveness in 6G. Specifically, we first analyse the traditional wireless AR point cloud communication framework and then summarize our proposed semantic information along with the end-to-end wireless communication. We then detail the design blocks of the TSAR framework, covering both semantic and effectiveness levels. Finally, numerous experiments have been conducted to demonstrate that, compared to the traditional point cloud communication framework, our proposed TSAR significantly reduces wireless AR application transmission latency by 95.6%, while improving communication effectiveness in geometry and color aspects by up to 82.4% and 20.4%, respectively.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2303.11463.pdf' target='_blank'>https://arxiv.org/pdf/2303.11463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio MartÃ­n Serrano, RubÃ©n Izquierdo, IvÃ¡n GarcÃ­a Daza, Miguel Ãngel Sotelo, David FernÃ¡ndez Llorca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11463">Digital twin in virtual reality for human-vehicle interactions in the context of autonomous driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the results of tests of interactions between real humans and simulated vehicles in a virtual scenario. Human activity is inserted into the virtual world via a virtual reality interface for pedestrians. The autonomous vehicle is equipped with a virtual Human-Machine interface (HMI) and drives through the digital twin of a real crosswalk. The HMI was combined with gentle and aggressive braking maneuvers when the pedestrian intended to cross. The results of the interactions were obtained through questionnaires and measurable variables such as the distance to the vehicle when the pedestrian initiated the crossing action. The questionnaires show that pedestrians feel safer whenever HMI is activated and that varying the braking maneuver does not influence their perception of danger as much, while the measurable variables show that both HMI activation and the gentle braking maneuver cause the pedestrian to cross earlier.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2303.06987.pdf' target='_blank'>https://arxiv.org/pdf/2303.06987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, Andy Lavender, CÃ©dric Plessiet, Tim White
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06987">Challenges of movement quality using motion capture in theatre</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe1 two case studies of AvatarStaging theatrical mixed reality framework combining avatars and performers acting in an artistic context. We outline a qualitative approach toward the condition for stage presence for the avatars. We describe the motion control solutions we experimented with from the perspective of building a protocol of avatar direction in a mixed reality appropriate to live performance.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2303.06984.pdf' target='_blank'>https://arxiv.org/pdf/2303.06984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, CÃ©dric Plessiet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06984">Experiencing avatar direction in low cost theatrical mixed reality setup</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the setup and programming framework of AvatarStaging theatrical mixed reality experiment. We focus on a configuration addressing movement issues between physical and 3D digital spaces from performers and directors' points of view. We propose 3 practical exercises.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2303.06795.pdf' target='_blank'>https://arxiv.org/pdf/2303.06795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Huang, Stan Z. Li, Xin Yuan, Shenghui Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06795">Roadmap towards Meta-being</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse is a perpetual and persistent multi-user environment that merges physical reality with digital virtuality. It is widely considered to be the next revolution of the Internet. Digital humans are a critical part of Metaverse. They are driven by artificial intelligence (AI) and deployed in many applications. However, it is a complex process to construct digital humans which can be well combined with the Metaverse elements, such as immersion creation, connection construction, and economic operation. In this paper, we present the roadmap of Meta-being to construct the digital human in Metaverse. In this roadmap, we first need to model and render a digital human model for immersive display in Metaverse. Then we add a dialogue system with audio, facial expressions, and movements for this digital human. Finally, we can apply our digital human in the fields of the economy in Metaverse with the consideration of security. We also construct a digital human in Metaverse to implement our roadmap. Numerous advanced technologies and devices, such as AI, Natural Language Processing (NLP), and motion capture, are used in our implementation. This digital human can be applied to many applications, such as education and exhibition.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2303.00744.pdf' target='_blank'>https://arxiv.org/pdf/2303.00744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Saunders, Vinay Namboodiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00744">READ Avatars: Realistic Emotion-controllable Audio Driven Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present READ Avatars, a 3D-based approach for generating 2D avatars that are driven by audio input with direct and granular control over the emotion. Previous methods are unable to achieve realistic animation due to the many-to-many nature of audio to expression mappings. We alleviate this issue by introducing an adversarial loss in the audio-to-expression generation process. This removes the smoothing effect of regression-based models and helps to improve the realism and expressiveness of the generated avatars. We note furthermore, that audio should be directly utilized when generating mouth interiors and that other 3D-based methods do not attempt this. We address this with audio-conditioned neural textures, which are resolution-independent. To evaluate the performance of our method, we perform quantitative and qualitative experiments, including a user study. We also propose a new metric for comparing how well an actor's emotion is reconstructed in the generated avatar. Our results show that our approach outperforms state of the art audio-driven avatar generation methods across several metrics. A demo video can be found at \url{https://youtu.be/QSyMl3vV0pA}
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2302.01880.pdf' target='_blank'>https://arxiv.org/pdf/2302.01880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kelly Mack, Rai Ching Ling Hsu, AndrÃ©s Monroy-HernÃ¡ndez, Brian A. Smith, Fannie Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01880">Towards Inclusive Avatars: Disability Representation in Avatar Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital avatars are an important part of identity representation, but there is little work on understanding how to represent disability. We interviewed 18 people with disabilities and related identities about their experiences and preferences in representing their identities with avatars. Participants generally preferred to represent their disability identity if the context felt safe and platforms supported their expression, as it was important for feeling authentically represented. They also utilized avatars in strategic ways: as a means to signal and disclose current abilities, access needs, and to raise awareness. Some participants even found avatars to be a more accessible way to communicate than alternatives. We discuss how avatars can support disability identity representation because of their easily customizable format that is not strictly tied to reality. We conclude with design recommendations for creating platforms that better support people in representing their disability and other minoritized identities.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2212.00613.pdf' target='_blank'>https://arxiv.org/pdf/2212.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Wang, Giljoo Nam, Tuur Stuyck, Stephen Lombardi, Chen Cao, Jason Saragih, Michael Zollhoefer, Jessica Hodgins, Christoph Lassner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.00613">NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capture and animation of human hair are two of the major challenges in the creation of realistic avatars for the virtual reality. Both problems are highly challenging, because hair has complex geometry and appearance, as well as exhibits challenging motion. In this paper, we present a two-stage approach that models hair independently from the head to address these challenges in a data-driven manner. The first stage, state compression, learns a low-dimensional latent space of 3D hair states containing motion and appearance, via a novel autoencoder-as-a-tracker strategy. To better disentangle the hair and head in appearance learning, we employ multi-view hair segmentation masks in combination with a differentiable volumetric renderer. The second stage learns a novel hair dynamics model that performs temporal hair transfer based on the discovered latent codes. To enforce higher stability while driving our dynamics model, we employ the 3D point-cloud autoencoder from the compression stage for de-noising of the hair state. Our model outperforms the state of the art in novel view synthesis and is capable of creating novel hair animations without having to rely on hair observations as a driving signal. Project page is here https://ziyanw1.github.io/neuwigs/.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2211.11903.pdf' target='_blank'>https://arxiv.org/pdf/2211.11903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Purva Tendulkar, DÃ­dac SurÃ­s, Carl Vondrick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11903">FLEX: Full-Body Grasping Without Full-Body Grasps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in AR/VR, video games and robotics. Towards this goal, we address the task of generating a virtual human -- hands and full body -- grasping everyday objects. Existing methods approach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object positions and orientations, or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasping data. Our key insight is to leverage the existence of both full-body pose and hand grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are superior to baselines both quantitatively and qualitatively. See our webpage for more details: https://flex.cs.columbia.edu/.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2201.12894.pdf' target='_blank'>https://arxiv.org/pdf/2201.12894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhi Cheng, Nan Wu, Songqing Chen, Bo Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.12894">Will Metaverse be NextG Internet? Vision, Hype, and Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse, with the combination of the prefix "meta" (meaning transcending) and the word "universe", has been deemed as the next-generation (NextG) Internet. It aims to create a shared virtual space that connects all virtual worlds via the Internet, where users, represented as digital avatars, can communicate and collaborate as if they are in the physical world. Nevertheless, there is still no unified definition of the Metaverse. This article first presents our vision of what the key requirements of Metaverse should be and reviews what has been heavily advocated by the industry and the positions of various high-tech companies. It then briefly introduces existing social virtual reality (VR) platforms that can be viewed as early prototypes of Metaverse and conducts a reality check by diving into the network operation and performance of two representative platforms, Workrooms from Meta and AltspaceVR from Microsoft. Finally, it concludes by discussing several opportunities and future directions for further innovation.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2509.11411.pdf' target='_blank'>https://arxiv.org/pdf/2509.11411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Zioulis, Nikolaos Kotarelas, Georgios Albanis, Spyridon Thermos, Anargyros Chatzitofis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11411">On the Skinning of Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2509.04356.pdf' target='_blank'>https://arxiv.org/pdf/2509.04356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04356">SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SRWToolkit, an open-source Wizard of Oz toolkit designed to facilitate the rapid prototyping of social robotic avatars powered by local large language models (LLMs). Our web-based toolkit enables multimodal interaction through text input, button-activated speech, and wake-word command. The toolkit offers real-time configuration of avatar appearance, behavior, language, and voice via an intuitive control panel. In contrast to prior works that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and ensures on-device functionality through local LLM inference. In our small-scale user study ($n=11$), participants created and interacted with diverse robotic roles (hospital receptionist, mathematics teacher, and driving assistant), which demonstrated positive outcomes in the toolkit's usability, trust, and user experience. The toolkit enables rapid and efficient development of robot characters customized to researchers' needs, supporting scalable research in human-robot interaction.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2509.04174.pdf' target='_blank'>https://arxiv.org/pdf/2509.04174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Merz, Lukas Schach, Marie Luisa Fiedler, Jean-Luc Lugrin, Carolin Wienrich, Marc Erich Latoschik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04174">Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric Similarity Learning of Motion Patterns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an unobtrusive in-situ measurement method to detect user behavior changes during arbitrary exposures in XR systems. Here, such behavior changes are typically associated with the Proteus effect or bodily affordances elicited by different avatars that the users embody in XR. We present a biometric user model based on deep metric similarity learning, which uses high-dimensional embeddings as reference vectors to identify behavior changes of individual users. We evaluate our model against two alternative approaches: a (non-learned) motion analysis based on central tendencies of movement patterns and subjective post-exposure embodiment questionnaires frequently used in various XR exposures. In a within-subject study, participants performed a fruit collection task while embodying avatars of different body heights (short, actual-height, and tall). Subjective assessments confirmed the effective manipulation of perceived body schema, while the (non-learned) objective analyses of head and hand movements revealed significant differences across conditions. Our similarity learning model trained on the motion data successfully identified the elicited behavior change for various query and reference data pairings of the avatar conditions. The approach has several advantages in comparison to existing methods: 1) In-situ measurement without additional user input, 2) generalizable and scalable motion analysis for various use cases, 3) user-specific analysis on the individual level, and 4) with a trained model, users can be added and evaluated in real time to study how avatar changes affect behavior.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2508.19754.pdf' target='_blank'>https://arxiv.org/pdf/2508.19754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wu, Yufan Wu, Wen Li, Yuxi Lu, Kairui Feng, Xuanhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19754">FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2508.04505.pdf' target='_blank'>https://arxiv.org/pdf/2508.04505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daisheng Jin, Ying He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04505">MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing realistic 3D human avatars from monocular videos is a challenging task due to the limited geometric information and complex non-rigid motion involved. We present MonoCloth, a new method for reconstructing and animating clothed human avatars from monocular videos. To overcome the limitations of monocular input, we introduce a part-based decomposition strategy that separates the avatar into body, face, hands, and clothing. This design reflects the varying levels of reconstruction difficulty and deformation complexity across these components. Specifically, we focus on detailed geometry recovery for the face and hands. For clothing, we propose a dedicated cloth simulation module that captures garment deformation using temporal motion cues and geometric constraints. Experimental results demonstrate that MonoCloth improves both visual reconstruction quality and animation realism compared to existing methods. Furthermore, thanks to its part-based design, MonoCloth also supports additional tasks such as clothing transfer, underscoring its versatility and practical utility.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2507.17327.pdf' target='_blank'>https://arxiv.org/pdf/2507.17327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao He, Jianqiang Ren, Jianjing Xiang, Xiejie Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17327">CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of large foundation models, AIGC, cloud rendering, and real-time motion capture technologies, digital humans are now capable of achieving synchronized facial expressions and body movements, engaging in intelligent dialogues driven by natural language, and enabling the fast creation of personalized avatars. While current mainstream approaches to digital humans primarily focus on 3D models and 2D video-based representations, interactive 2D cartoon-style digital humans have received relatively less attention. Compared to 3D digital humans that require complex modeling and high rendering costs, and 2D video-based solutions that lack flexibility and real-time interactivity, 2D cartoon-style Live2D models offer a more efficient and expressive alternative. By simulating 3D-like motion through layered segmentation without the need for traditional 3D modeling, Live2D enables dynamic and real-time manipulation. In this technical report, we present CartoonAlive, an innovative method for generating high-quality Live2D digital humans from a single input portrait image. CartoonAlive leverages the shape basis concept commonly used in 3D face modeling to construct facial blendshapes suitable for Live2D. It then infers the corresponding blendshape weights based on facial keypoints detected from the input image. This approach allows for the rapid generation of a highly expressive and visually accurate Live2D model that closely resembles the input portrait, within less than half a minute. Our work provides a practical and scalable solution for creating interactive 2D cartoon characters, opening new possibilities in digital content creation and virtual character animation. The project homepage is https://human3daigc.github.io/CartoonAlive_webpage/.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2507.16542.pdf' target='_blank'>https://arxiv.org/pdf/2507.16542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiong Wu, Yan Dong, Zipeng Zhang, Ruochen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16542">The Effect of Scale Consistency between Real and Virtual Spaces on Immersion in Exhibition Hybrid Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In exhibition hybrid spaces, scale consistency between real and virtual spaces is crucial for user immersion. However, there is currently a lack of systematic research to determine appropriate virtual-to-real mapping ratios. This study developed an immersive interaction system based on Intel 3D Athlete Tracking body mapping technology. Two experiments investigated the impact of virtual space and virtual avatar scale on immersion. Experiment 1 investigated 30 participants' preferences for virtual space scale, while Experiment 2 tested the effect of 6 different virtual avatar sizes (25%-150%) on immersion. A 5-point Likert scale was used to assess immersion, followed by analysis of variance and Tukey HSD post-hoc tests. Experiment 1 showed that participants preferred a virtual space ratio of 130% (mean 127.29%, SD 8.55%). Experiment 2 found that virtual avatar sizes within the 75%-100% range produced optimal immersion (p < 0.05). Immersion decreased significantly when virtual avatar sizes deviated from users' actual height (below 50% or above 125%). Participants were more sensitive to size changes in the 25%-75% range, while perception was weaker for changes in the 75%-100% range. Virtual environments slightly larger than real space (130%) and virtual avatars slightly smaller than users (75%-100%) optimize user immersion. These findings have been applied in the Intel Global Trade Center exhibition hall, demonstrating actionable insights for designing hybrid spaces that enhance immersion and coherence.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2506.14268.pdf' target='_blank'>https://arxiv.org/pdf/2506.14268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Aymerich-Franch, Tarek Taha, Takahiro Miyashita, Hiroko Kamide, Hiroshi Ishiguro, Paolo Dario
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14268">Public Acceptance of Cybernetic Avatars in the service sector: Evidence from a Large-Scale Survey in Dubai</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybernetic avatars are hybrid interaction robots or digital representations that combine autonomous capabilities with teleoperated control. This study investigates the acceptance of cybernetic avatars in the highly multicultural society of Dubai, with particular emphasis on robotic avatars for customer service. Specifically, we explore how acceptance varies as a function of robot appearance (e.g., android, robotic-looking, cartoonish), deployment settings (e.g., shopping malls, hotels, hospitals), and functional tasks (e.g., providing information, patrolling). To this end, we conducted a large-scale survey with over 1,000 participants. Overall, cybernetic avatars received a high level of acceptance, with physical robot avatars receiving higher acceptance than digital avatars. In terms of appearance, robot avatars with a highly anthropomorphic robotic appearance were the most accepted, followed by cartoonish designs and androids. Animal-like appearances received the lowest level of acceptance. Among the tasks, providing information and guidance was rated as the most valued. Shopping malls, airports, public transport stations, and museums were the settings with the highest acceptance, whereas healthcare-related spaces received lower levels of support. An analysis by community cluster revealed among others that Emirati respondents showed significantly greater acceptance of android appearances compared to the overall sample, while participants from the 'Other Asia' cluster were significantly more accepting of cartoonish appearances. Our study underscores the importance of incorporating citizen feedback into the design and deployment of cybernetic avatars from the early stages to enhance acceptance of this technology in society.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2506.05806.pdf' target='_blank'>https://arxiv.org/pdf/2506.05806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Yu, Zhaonian Wang, Yihan Pan, Meng Cheng, Hao Yang, Chao Wang, Tao Xie, Xiaoming Xu, Xiaoming Wei, Xunliang Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05806">LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based models have gained wide adoption in the virtual human generation due to their outstanding expressiveness. However, their substantial computational requirements have constrained their deployment in real-time interactive avatar applications, where stringent speed, latency, and duration requirements are paramount. We present a novel audio-driven portrait video generation framework based on the diffusion model to address these challenges. Firstly, we propose robust variable-length video generation to reduce the minimum time required to generate the initial video clip or state transitions, which significantly enhances the user experience. Secondly, we propose a consistency model training strategy for Audio-Image-to-Video to ensure real-time performance, enabling a fast few-step generation. Model quantization and pipeline parallelism are further employed to accelerate the inference speed. To mitigate the stability loss incurred by the diffusion process and model quantization, we introduce a new inference strategy tailored for long-duration video generation. These methods ensure real-time performance and low latency while maintaining high-fidelity output. Thirdly, we incorporate class labels as a conditional input to seamlessly switch between speaking, listening, and idle states. Lastly, we design a novel mechanism for fine-grained facial expression control to exploit our model's inherent capacity. Extensive experiments demonstrate that our approach achieves low-latency, fluid, and authentic two-way communication. On an NVIDIA RTX 4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45 FPS at a resolution of 512x512, with an initial video generation latency of 140 ms and 215 ms, respectively.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2505.23301.pdf' target='_blank'>https://arxiv.org/pdf/2505.23301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rim Rekik, Stefanie Wuhrer, Ludovic Hoyet, Katja Zibrek, Anne-HÃ©lÃ¨ne Olivier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23301">Quality assessment of 3D human animation: Subjective and objective evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual human animations have a wide range of applications in virtual and augmented reality. While automatic generation methods of animated virtual humans have been developed, assessing their quality remains challenging. Recently, approaches introducing task-oriented evaluation metrics have been proposed, leveraging neural network training. However, quality assessment measures for animated virtual humans that are not generated with parametric body models have yet to be developed. In this context, we introduce a first such quality assessment measure leveraging a novel data-driven framework. First, we generate a dataset of virtual human animations together with their corresponding subjective realism evaluation scores collected with a user study. Second, we use the resulting dataset to learn predicting perceptual evaluation scores. Results indicate that training a linear regressor on our dataset results in a correlation of 90%, which outperforms a state of the art deep learning baseline.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2505.20082.pdf' target='_blank'>https://arxiv.org/pdf/2505.20082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryo Ohara, Chi-Lan Yang, Takuji Narumi, Hideaki Kuzuoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20082">Understanding and Supporting Co-viewing Comedy in VR with Embodied Expressive Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-viewing videos with family and friends remotely has become prevalent with the support of communication channels such as text messaging or real-time voice chat. However, current co-viewing platforms often lack visible embodied cues, such as body movements and facial expressions. This absence can reduce emotional engagement and the sense of co-presence when people are watching together remotely. Although virtual reality (VR) is an emerging technology that allows individuals to participate in various social activities while embodied as avatars, we still do not fully understand how this embodiment in VR affects co-viewing experiences, particularly in terms of engagement, emotional contagion, and expressive norms. In a controlled experiment involving eight triads of three participants each (N=24), we compared the participants' perceptions and reactions while watching comedy in VR using embodied expressive avatars that displayed visible laughter cues. This was contrasted with a control condition where no such embodied expressions were presented. With a mixed-method analysis, we found that embodied laughter cues shifted participants' engagement from individual immersion to socially coordinated participation. Participants reported heightened self-awareness of emotional expression, greater emotional contagion, and the development of expressive norms surrounding co-viewers' laughter. The result highlighted the tension between individual engagement and interpersonal emotional accommodation when co-viewing with embodied expressive avatars.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2505.10584.pdf' target='_blank'>https://arxiv.org/pdf/2505.10584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huafeng Shi, Jianzhong Liang, Rongchang Xie, Xian Wu, Cheng Chen, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10584">Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report introduces Aquarius, a family of industry-level video generation models for marketing scenarios designed for thousands-xPU clusters and models with hundreds of billions of parameters. Leveraging efficient engineering architecture and algorithmic innovation, Aquarius demonstrates exceptional performance in high-fidelity, multi-aspect-ratio, and long-duration video synthesis. By disclosing the framework's design details, we aim to demystify industrial-scale video generation systems and catalyze advancements in the generative video community. The Aquarius framework consists of five components: Distributed Graph and Video Data Processing Pipeline: Manages tens of thousands of CPUs and thousands of xPUs via automated task distribution, enabling efficient video data processing. Additionally, we are about to open-source the entire data processing framework named "Aquarius-Datapipe". Model Architectures for Different Scales: Include a Single-DiT architecture for 2B models and a Multimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios, multi-resolution, and multi-duration video generation. High-Performance infrastructure designed for video generation model training: Incorporating hybrid parallelism and fine-grained memory optimization strategies, this infrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference Acceleration: Utilizes diffusion cache and attention optimization to achieve a 2.35x inference speedup. Multiple marketing-scenarios applications: Including image-to-video, text-to-video (avatar), video inpainting and video personalization, among others. More downstream applications and multi-dimensional evaluation metrics will be added in the upcoming version updates.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2505.02694.pdf' target='_blank'>https://arxiv.org/pdf/2505.02694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurtis Haut, Masum Hasan, Thomas Carroll, Ronald Epstein, Taylan Sen, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02694">AI Standardized Patient Improves Human Conversations in Advanced Cancer Care</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Serious illness communication (SIC) in end-of-life care faces challenges such as emotional stress, cultural barriers, and balancing hope with honesty. Despite its importance, one of the few available ways for clinicians to practice SIC is with standardized patients, which is expensive, time-consuming, and inflexible. In this paper, we present SOPHIE, an AI-powered standardized patient simulation and automated feedback system. SOPHIE combines large language models (LLMs), a lifelike virtual avatar, and automated, personalized feedback based on clinical literature to provide remote, on-demand SIC training. In a randomized control study with healthcare students and professionals, SOPHIE users demonstrated significant improvement across three critical SIC domains: Empathize, Be Explicit, and Empower. These results suggest that AI-driven tools can enhance complex interpersonal communication skills, offering scalable, accessible solutions to address a critical gap in clinician education.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2505.00421.pdf' target='_blank'>https://arxiv.org/pdf/2505.00421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00421">Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2504.13854.pdf' target='_blank'>https://arxiv.org/pdf/2504.13854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Aymerich-Franch, Tarek Taha, Hiroshi Ishiguro, Takahiro Miyashita, Paolo Dario
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13854">Stakeholder perspectives on designing socially acceptable social robots and robot avatars for Dubai and multicultural societies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot avatars for customer service are gaining traction in Japan. However, their acceptance in other societal contexts remains underexplored, complicating efforts to design robot avatars suitable for diverse cultural environments. To address this, we interviewed key stakeholders in Dubai's service sector to gain insights into their experiences deploying social robots for customer service, as well as their opinions on the most useful tasks and design features that could maximize customer acceptance of robot avatars in Dubai. Providing information and guiding individuals to specific locations were identified as the most valued functions. Regarding appearance, robotic-looking, highly anthropomorphic designs were the most preferred. Ultra-realistic androids and cartoonish-looking robots elicited mixed reactions, while hybrid androids, low-anthropomorphic robotic designs, and animal-looking robots were considered less suitable or discouraged. Additionally, a psycho-sociological analysis revealed that interactions with robot avatars are influenced by their symbolic meaning, context, and affordances. These findings offer pioneering insights into culturally adaptive robot avatar design, addressing a significant research gap and providing actionable guidelines for deploying socially acceptable robots and avatars in multicultural contexts worldwide.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2504.13386.pdf' target='_blank'>https://arxiv.org/pdf/2504.13386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Radek DanÄÄek, Carolin Schmitt, Senya Polikovsky, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13386">Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without rich expressions, whereas stochastic models generate diverse expressions but with lower lip-sync quality. To get the best of both, we seek a stochastic model with accurate lip-sync. To that end, we develop a new approach based on the following observation: if a method generates realistic 3D lip motions, it should be possible to infer the spoken audio from the lip motion. The inferred speech should match the original input audio, and erroneous predictions create a novel supervision signal for training 3D talking head avatars with accurate lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under Neural Differentiable Elocution Reconstruction), a 3D talking head avatar framework that introduces a novel supervision mechanism via differentiable sound production. First, we train a novel mesh-to-speech model that regresses audio from facial animation. Then, we incorporate this model into a diffusion-based talking avatar framework. During training, the mesh-to-speech model takes the generated animation and produces a sound that is compared to the input speech, creating a differentiable analysis-by-audio-synthesis supervision loop. Our extensive qualitative and quantitative experiments demonstrate that THUNDER significantly improves the quality of the lip-sync of talking head avatars while still allowing for generation of diverse, high-quality, expressive facial animations. The code and models will be available at https://thunder.is.tue.mpg.de/
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2504.09018.pdf' target='_blank'>https://arxiv.org/pdf/2504.09018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Yin, Chenxinran Shen, Robert Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09018">Entertainers Between Real and Virtual -- Investigating Viewer Interaction, Engagement, and Relationships with Avatarized Virtual Livestreamers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual YouTubers (VTubers) are avatar-based livestreamers that are voiced and played by human actors. VTubers have been popular in East Asia for years and have more recently seen widespread international growth. Despite their emergent popularity, research has been scarce into the interactions and relationships that exist between avatarized VTubers and their viewers, particularly in contrast to non-avatarized streamers. To address this gap, we performed in-depth interviews with self-reported VTuber viewers (n=21). Our findings first reveal that the avatarized nature of VTubers fosters new forms of theatrical engagement, as factors of the virtual blend with the real to create a mixture of fantasy and realism in possible livestream interactions. Avatarization furthermore results in a unique audience perception regarding the identity of VTubers - an identity which comprises a dynamic, distinct mix of the real human (the voice actor/actress) and the virtual character. Our findings suggest that each of these dual identities both individually and symbiotically affect viewer interactions and relationships with VTubers. Whereas the performer's identity mediates social factors such as intimacy, relatability, and authenticity, the virtual character's identity offers feelings of escapism, novelty in interactions, and a sense of continuity beyond the livestream. We situate our findings within existing livestreaming literature to highlight how avatarization drives unique, character-based interactions as well as reshapes the motivations and relationships that viewers form with livestreamers. Finally, we provide suggestions and recommendations for areas of future exploration to address the challenges involved in present livestreamed avatarized entertainment.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2504.03141.pdf' target='_blank'>https://arxiv.org/pdf/2504.03141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuya Izumi, Akihisa Shitara, Yoichi Ochiai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03141">See-Through Face Display for DHH People: Enhancing Gaze Awareness in Remote Sign Language Conversations with Camera-Behind Displays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a sign language conversation system based on the See-Through Face Display to address the challenge of maintaining eye contact in remote sign language interactions. A camera positioned behind a transparent display allows users to look at the face of their conversation partner while appearing to maintain direct eye contact. Unlike conventional methods that rely on software-based gaze correction or large-scale half-mirror setups, this design reduces visual distortions and simplifies installation. We implemented and evaluated a videoconferencing system that integrates See-Through Face Display, comparing it to traditional videoconferencing methods. We explore its potential applications for Deaf and Hard of Hearing (DHH), including multi-party sign language conversations, corpus collection, remote interpretation, and AI-driven sign language avatars. Collaboration with DHH communities will be key to refining the system for real-world use and ensuring its practical deployment.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2503.22728.pdf' target='_blank'>https://arxiv.org/pdf/2503.22728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Fu, Ziqi Ni, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22728">Dual Audio-Centric Modality Coupling for Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2503.17032.pdf' target='_blank'>https://arxiv.org/pdf/2503.17032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17032">TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we "bake" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2503.06324.pdf' target='_blank'>https://arxiv.org/pdf/2503.06324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuya Izumi, Shuhey Koyama, Yoichi Ochiai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06324">AnimeGaze: Real-Time Mutual Gaze Synthesis for Anime-Style Avatars in Physical Environments via Behind-Display Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatars on displays lack the ability to engage with the physical environment through gaze. To address this limitation, we propose a gaze synthesis method that enables animated avatars to establish gaze communication with the physical environment using a camera-behind-the-display system. The system uses a display that rapidly alternates between visible and transparent states. During the transparent state, a camera positioned behind the display captures the physical environment. This configuration physically aligns the position of the avatar's eyes with the camera, enabling two-way gaze communication with people and objects in the physical environment. Building on this system, we developed a framework for mutual gaze communication between avatars and people. The framework detects the user's gaze and dynamically synthesizes the avatar's gaze towards people or objects in the environment. This capability was integrated into an AI agent system to generate real-time, context-aware gaze behaviors during conversations, enabling more seamless and natural interactions. To evaluate the system, we conducted a user study to assess its effectiveness in supporting physical gaze awareness and generating human-like gaze behaviors. The results show that the behind-display approach significantly enhances the user's perception of being observed and attended to by the avatar. By bridging the gap between virtual avatars and the physical environment through enhanced gaze interactions, our system offers a promising avenue for more immersive and human-like AI-mediated communication in everyday environments.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2503.05196.pdf' target='_blank'>https://arxiv.org/pdf/2503.05196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzhi Guo, Yixiao Chen, Dongye Xiaonuo, Zeyu Tian, Dongdong Weng, Le Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05196">STGA: Selective-Training Gaussian Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose selective-training Gaussian head avatars (STGA) to enhance the details of dynamic head Gaussian. The dynamic head Gaussian model is trained based on the FLAME parameterized model. Each Gaussian splat is embedded within the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before training, our selection strategy calculates the 3D Gaussian splat to be optimized in each frame. The parameters of these 3D Gaussian splats are optimized in the training of each frame, while those of the other splats are frozen. This means that the splats participating in the optimization process differ in each frame, to improve the realism of fine details. Compared with network-based methods, our method achieves better results with shorter training time. Compared with mesh-based methods, our method produces more realistic details within the same training time. Additionally, the ablation experiment confirms that our method effectively enhances the quality of details.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2502.19455.pdf' target='_blank'>https://arxiv.org/pdf/2502.19455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingzhou Mu, Baiji Liu, Ruonan Zhang, Guiming Mo, Jiawei Jin, Kai Zhang, Haozhi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19455">FLAP: Fully-controllable Audio-driven Portrait Video Generation through 3D head conditioned diffusion model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based video generation techniques have significantly improved zero-shot talking-head avatar generation, enhancing the naturalness of both head motion and facial expressions. However, existing methods suffer from poor controllability, making them less applicable to real-world scenarios such as filmmaking and live streaming for e-commerce. To address this limitation, we propose FLAP, a novel approach that integrates explicit 3D intermediate parameters (head poses and facial expressions) into the diffusion model for end-to-end generation of realistic portrait videos. The proposed architecture allows the model to generate vivid portrait videos from audio while simultaneously incorporating additional control signals, such as head rotation angles and eye-blinking frequency. Furthermore, the decoupling of head pose and facial expression allows for independent control of each, offering precise manipulation of both the avatar's pose and facial expressions. We also demonstrate its flexibility in integrating with existing 3D head generation methods, bridging the gap between 3D model-based approaches and end-to-end diffusion techniques. Extensive experiments show that our method outperforms recent audio-driven portrait video models in both naturalness and controllability.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2502.17856.pdf' target='_blank'>https://arxiv.org/pdf/2502.17856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dakyeom Ahn, Seora Park, Seolhee Lee, Jieun Cho, Hajin Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17856">I Stan Alien Idols and Also the People Behind Them: Understanding How Seams Between Virtual and Real Identities Engage VTuber Fans -- A Case Study of PLAVE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual YouTubers (VTubers) have recently gained popularity as streamers using computer-generated avatars and real-time motion capture to create distinct virtual identities. While prior research has explored how VTubers construct virtual personas and engage audiences, little attention has been given to viewers' reactions when virtual and real identities blur-what we refer to as "seams." To address this gap, we conducted a case study on PLAVE, a popular Korean VTuber Kpop idol group, interviewing 24 of their fans. Our findings identified two main sources of seams: technical glitches and identity collapses, where VTubers act inconsistently with their virtual personas, revealing aspects of their real selves. These seams played a pivotal role in shaping diverse fan engagements, with some valuing authenticity linked to real identities, while others prioritized the coherence of virtual personas. Overall, our findings underscore the importance of seams in shaping viewer experiences.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2502.07030.pdf' target='_blank'>https://arxiv.org/pdf/2502.07030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07030">PrismAvatar: Real-time animated 3D neural head avatars on edge devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PrismAvatar: a 3D head avatar model which is designed specifically to enable real-time animation and rendering on resource-constrained edge devices, while still enjoying the benefits of neural volumetric rendering at training time. By integrating a rigged prism lattice with a 3D morphable head model, we use a hybrid rendering model to simultaneously reconstruct a mesh-based head and a deformable NeRF model for regions not represented by the 3DMM. We then distill the deformable NeRF into a rigged mesh and neural textures, which can be animated and rendered efficiently within the constraints of the traditional triangle rendering pipeline. In addition to running at 60 fps with low memory usage on mobile devices, we find that our trained models have comparable quality to state-of-the-art 3D avatar models on desktop devices.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2501.16870.pdf' target='_blank'>https://arxiv.org/pdf/2501.16870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Josep Lopez Camunas, Cristina Bustos, Yanjun Zhu, Raquel Ros, Agata Lapedriza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16870">Experimenting with Affective Computing Models in Video Interviews with Spanish-speaking Older Adults</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding emotional signals in older adults is crucial for designing virtual assistants that support their well-being. However, existing affective computing models often face significant limitations: (1) limited availability of datasets representing older adults, especially in non-English-speaking populations, and (2) poor generalization of models trained on younger or homogeneous demographics. To address these gaps, this study evaluates state-of-the-art affective computing models -- including facial expression recognition, text sentiment analysis, and smile detection -- using videos of older adults interacting with either a person or a virtual avatar. As part of this effort, we introduce a novel dataset featuring Spanish-speaking older adults engaged in human-to-human video interviews. Through three comprehensive analyses, we investigate (1) the alignment between human-annotated labels and automatic model outputs, (2) the relationships between model outputs across different modalities, and (3) individual variations in emotional signals. Using both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover limited agreement between human annotations and model predictions, weak consistency across modalities, and significant variability among individuals. These findings highlight the shortcomings of generalized emotion perception models and emphasize the need of incorporating personal variability and cultural nuances into future systems.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2501.09442.pdf' target='_blank'>https://arxiv.org/pdf/2501.09442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takato Mizuho, Takuji Narumi, Hideaki Kuzuoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09442">Effects of Social Contextual Variation Using Partner Avatars on Memory Acquisition and Retention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates how partner avatar design affects learning and memory when an avatar serves as a lecturer. Based on earlier research on the environmental context dependency of memory, we hypothesize that the use of diverse partner avatars results in a slower learning rate but better memory retention than that of a constant partner avatar. Accordingly, participants were tasked with memorizing Tagalog--Japanese word pairs. On the first day of the experiment, they repeatedly learned the pairs over six sessions from a partner avatar in an immersive virtual environment. One week later, on the second day of the experiment, they underwent a recall test in a real environment. We employed a between-participants design to compare the following conditions: the varied avatar condition, in which each repetition used a different avatar, and the constant avatar condition, in which the same avatar was used throughout the experiment. Results showed that, compared to the constant avatar condition, the varied avatar condition resulted in significantly lower recall performance in the repeated learning trials conducted on the first day. However, the avatar conditions showed no significant differences in the final recall test on the second day. We discuss these effects in relation to the social presence of the partner avatar. This study opens up a novel approach to optimizing the effectiveness of instructor avatars in immersive virtual environments.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2501.01589.pdf' target='_blank'>https://arxiv.org/pdf/2501.01589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghu Chen, Bo Peng, Yunfan Tao, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01589">D$^3$-Human: Dynamic Disentangled Digital Human from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce D$^3$-Human, a method for reconstructing Dynamic Disentangled Digital Human geometry from monocular videos. Past monocular video human reconstruction primarily focuses on reconstructing undecoupled clothed human bodies or only reconstructing clothing, making it difficult to apply directly in applications such as animation production. The challenge in reconstructing decoupled clothing and body lies in the occlusion caused by clothing over the body. To this end, the details of the visible area and the plausibility of the invisible area must be ensured during the reconstruction process. Our proposed method combines explicit and implicit representations to model the decoupled clothed human body, leveraging the robustness of explicit representations and the flexibility of implicit representations. Specifically, we reconstruct the visible region as SDF and propose a novel human manifold signed distance field (hmSDF) to segment the visible clothing and visible body, and then merge the visible and invisible body. Extensive experimental results demonstrate that, compared with existing reconstruction schemes, D$^3$-Human can achieve high-quality decoupled reconstruction of the human body wearing different clothing, and can be directly applied to clothing transfer and animation.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2412.21206.pdf' target='_blank'>https://arxiv.org/pdf/2412.21206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunsoo Cha, Inhee Lee, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.21206">PERSE: Personalized 3D Generative Avatars from A Single Portrait</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with a variation in a specific facial attribute from the original input. We propose a novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present a personalized avatar creation method based on the 3D Gaussian Splatting, learning a continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce a latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2412.12093.pdf' target='_blank'>https://arxiv.org/pdf/2412.12093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12093">CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints $-$ for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2412.12034.pdf' target='_blank'>https://arxiv.org/pdf/2412.12034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas TesÃ¡n, David GonzÃ¡lez, Pedro Martins, ElÃ­as Cueto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12034">Thermodynamics-informed graph neural networks for real-time simulation of digital human twins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing importance of real-time simulation in the medical field has exposed the limitations and bottlenecks inherent in the digital representation of complex biological systems. This paper presents a novel methodology aimed at advancing current lines of research in soft tissue simulation. The proposed approach introduces a hybrid model that integrates the geometric bias of graph neural networks with the physical bias derived from the imposition of a metriplectic structure as soft and hard constrains in the architecture, being able to simulate hepatic tissue with dissipative properties. This approach provides an efficient solution capable of generating predictions at high feedback rate while maintaining a remarkable generalization ability for previously unseen anatomies. This makes these features particularly relevant in the context of precision medicine and haptic rendering.
  Based on the adopted methodologies, we propose a model that predicts human liver responses to traction and compression loads in as little as 7.3 milliseconds for optimized configurations and as fast as 1.65 milliseconds in the most efficient cases, all in the forward pass. The model achieves relative position errors below 0.15\%, with stress tensor and velocity estimations maintaining relative errors under 7\%. This demonstrates the robustness of the approach developed, which is capable of handling diverse load states and anatomies effectively. This work highlights the feasibility of integrating real-time simulation with patient-specific geometries through deep learning, paving the way for more robust digital human twins in medical applications.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2412.10209.pdf' target='_blank'>https://arxiv.org/pdf/2412.10209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10209">GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2412.07739.pdf' target='_blank'>https://arxiv.org/pdf/2412.07739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay P. Namboodiri, Benjamin E Lundell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07739">GASP: Gaussian Avatars with Synthetic Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360$^\circ$ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page (https://microsoft.github.io/GASP/) for results.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2412.02421.pdf' target='_blank'>https://arxiv.org/pdf/2412.02421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwei Pan, Yang Li, Hongsheng Li, Kwan-Yee Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02421">TimeWalker: Personalized Neural Space for Lifelong Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized 'time traveling' in a breeze.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2411.06719.pdf' target='_blank'>https://arxiv.org/pdf/2411.06719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Osman Akar, Yushan Han, Yizhou Chen, Weixian Lan, Benn Gallagher, Ronald Fedkiw, Joseph Teran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06719">Shallow Signed Distance Functions for Kinematic Collision Bodies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present learning-based implicit shape representations designed for real-time avatar collision queries arising in the simulation of clothing. Signed distance functions (SDFs) have been used for such queries for many years due to their computational efficiency. Recently deep neural networks have been used for implicit shape representations (DeepSDFs) due to their ability to represent multiple shapes with modest memory requirements compared to traditional representations over dense grids. However, the computational expense of DeepSDFs prevents their use in real-time clothing simulation applications. We design a learning-based representation of SDFs for human avatars whoes bodies change shape kinematically due to joint-based skinning. Rather than using a single DeepSDF for the entire avatar, we use a collection of extremely computationally efficient (shallow) neural networks that represent localized deformations arising from changes in body shape induced by the variation of a single joint. This requires a stitching process to combine each shallow SDF in the collection together into one SDF representing the signed closest distance to the boundary of the entire body. To achieve this we augment each shallow SDF with an additional output that resolves whether or not the individual shallow SDF value is referring to a closest point on the boundary of the body, or to a point on the interior of the body (but on the boundary of the individual shallow SDF). Our model is extremely fast and accurate and we demonstrate its applicability with real-time simulation of garments driven by animated characters.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2410.20789.pdf' target='_blank'>https://arxiv.org/pdf/2410.20789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaonuo Dongye, Hanzhi Guo, Le Luo, Haiyan Jiang, Yihua Bao, Zeyu Tian, Dongdong Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20789">LoDAvatar: Hierarchical Embedding and Adaptive Levels of Detail with Gaussian Splatting for Enhanced Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of virtual reality, the demand for 3D human avatars is increasing. The emergence of Gaussian Splatting technology has enabled the rendering of Gaussian avatars with superior visual quality and reduced computational costs. Despite numerous methods researchers propose for implementing drivable Gaussian avatars, limited attention has been given to balancing visual quality and computational costs. In this paper, we introduce LoDAvatar, a method that introduces levels of detail into Gaussian avatars through hierarchical embedding and selective detail enhancement methods. The key steps of LoDAvatar encompass data preparation, Gaussian embedding, Gaussian optimization, and selective detail enhancement. We conducted experiments involving Gaussian avatars at various levels of detail, employing both objective assessments and subjective evaluations. The outcomes indicate that incorporating levels of detail into Gaussian avatars can decrease computational costs during rendering while upholding commendable visual quality, thereby enhancing runtime frame rates. We advocate adopting LoDAvatar to render multiple dynamic Gaussian avatars or extensive Gaussian scenes to balance visual quality and computational costs.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2410.17262.pdf' target='_blank'>https://arxiv.org/pdf/2410.17262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqing Wang, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17262">EmoGene: Audio-Driven Emotional 3D Talking-Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking-head generation is a crucial and useful technology for virtual human interaction and film-making. While recent advances have focused on improving image fidelity and lip synchronization, generating accurate emotional expressions remains underexplored. In this paper, we introduce EmoGene, a novel framework for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Our approach employs a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks, which are concatenated with emotional embedding in a motion-to-emotion module to produce emotional landmarks. These landmarks drive a Neural Radiance Fields (NeRF)-based emotion-to-video module to render realistic emotional talking-head videos. Additionally, we propose a pose sampling method to generate natural idle-state (non-speaking) videos for silent audio inputs. Extensive experiments demonstrate that EmoGene outperforms previous methods in generating high-fidelity emotional talking-head videos.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2410.16503.pdf' target='_blank'>https://arxiv.org/pdf/2410.16503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saif Punjwani, Larry Heck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16503">Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for Allocentric Avatar Gesture Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of high-quality, multimodal training data severely hinders the creation of lifelike avatar animations for conversational AI in virtual environments. Existing datasets often lack the intricate synchronization between speech, facial expressions, and body movements that characterize natural human communication. To address this critical gap, we introduce Allo-AVA, a large-scale dataset specifically designed for text and audio-driven avatar gesture animation in an allocentric (third person point-of-view) context. Allo-AVA consists of $\sim$1,250 hours of diverse video content, complete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely maps these keypoints to precise timestamps, enabling accurate replication of human movements (body and facial gestures) in synchronization with speech. This comprehensive resource enables the development and evaluation of more natural, context-aware avatar animation models, potentially transforming applications ranging from virtual reality to digital assistants.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2410.13503.pdf' target='_blank'>https://arxiv.org/pdf/2410.13503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Wagner, Mario Botsch, Ulrich Schwanecke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13503">NePHIM: A Neural Physics-Based Head-Hand Interaction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the increasing use of virtual avatars, the animation of head-hand interactions has recently gained attention. To this end, we present a novel volumetric and physics-based interaction simulation. In contrast to previous work, our simulation incorporates temporal effects such as collision paths, respects anatomical constraints, and can detect and simulate skin pulling. As a result, we can achieve more natural-looking interaction animations and take a step towards greater realism. However, like most complex and computationally expensive simulations, ours is not real-time capable even on high-end machines. Therefore, we train small and efficient neural networks as accurate approximations that achieve about 200 FPS on consumer GPUs, about 50 FPS on CPUs, and are learned in less than four hours for one person. In general, our focus is not to generalize the approximation networks to low-resolution head models but to adapt them to more detailed personalized avatars. Nevertheless, we show that these networks can learn to approximate our head-hand interaction model for multiple identities while maintaining computational efficiency.
  Since the quality of the simulations can only be judged subjectively, we conducted a comprehensive user study which confirms the improved realism of our approach. In addition, we provide extensive visual results and inspect the neural approximations quantitatively. All data used in this work has been recorded with a multi--view camera rig and will be made available upon publication. We will also publish relevant implementations.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2410.03181.pdf' target='_blank'>https://arxiv.org/pdf/2410.03181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjong Sun, Eungu Lee, Seo Yeon Baek, Seunghyun Hwang, Wonbyung Lee, Dongyan Nan, Bernard J. Jansen, Jang Hyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03181">Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study is the first to explore whether multi-modal large language models (LLMs) can align their behaviors with visual personas, addressing a significant gap in the literature that predominantly focuses on text-based personas. We developed a novel dataset of 5K fictional avatar images for assignment as visual personas to LLMs, and analyzed their negotiation behaviors based on the visual traits depicted in these images, with a particular focus on aggressiveness. The results indicate that LLMs assess the aggressiveness of images in a manner similar to humans and output more aggressive negotiation behaviors when prompted with an aggressive visual persona. Interestingly, the LLM exhibited more aggressive negotiation behaviors when the opponent's image appeared less aggressive than their own, and less aggressive behaviors when the opponents image appeared more aggressive.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2410.01585.pdf' target='_blank'>https://arxiv.org/pdf/2410.01585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuetong Wang, Ziyan Wang, Mingmin Zhang, Kangyou Yu, Pan Hui, Mingming Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01585">Avatar Appearance and Behavior of Potential Harassers Affect Users' Perceptions and Response Strategies in Social Virtual Reality (VR): A Mixed-Methods Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sexual harassment has been recognized as a significant social issue. In recent years, the emergence of harassment in social virtual reality (VR) has become an important and urgent research topic. We employed a mixed-methods approach by conducting online surveys with VR users (N = 166) and semi-structured interviews with social VR users (N = 18) to investigate how users perceive sexual harassment in social VR, focusing on the influence of avatar appearance. Moreover, we derived users' response strategies to sexual harassment and gained insights on platform regulation. This study contributes to the research on sexual harassment in social VR by examining the moderating effect of avatar appearance on user perception of sexual harassment and uncovering the underlying reasons behind response strategies. Moreover, it presents novel prospects and challenges in platform design and regulation domains.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2409.10239.pdf' target='_blank'>https://arxiv.org/pdf/2409.10239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mine Dastan, Michele Fiorentino, Antonio E. Uva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10239">Precise Tool to Target Positioning Widgets (TOTTA) in Spatial Environments: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>TOTTA outlines the spatial position and rotation guidance of a real/virtual tool (TO) towards a real/virtual target (TA), which is a key task in Mixed Reality applications. The task error can have critical consequences regarding safety, performance, and quality, such as in surgical implantology or industrial maintenance scenarios. The TOTTA problem lacks a dedicated study and is scattered across different domains with isolated designs. This work contributes to a systematic review of the TOTTA visual widgets, studying 70 unique designs from 24 papers. TOTTA is commonly guided by visual overlap an intuitive, pre-attentive 'collimation' feedback of simple-shaped widgets: Box, 3D Axes, 3D Model, 2D Crosshair, Globe, Tetrahedron, Line, and Plane. Our research discovers that TO and TA are often represented with the same shape. They are distinguished by topological elements (e.g., edges, vertices, faces), colors, transparency levels, and added shapes, widget quantity, and size. Meanwhile, some designs provide continuous 'during manipulation feedback' relative to the distance between TO and TA by text, dynamic color, sonification, and amplified graphical visualization. Some approaches trigger discrete 'TA reached feedback,' such as color alteration, added sound, TA shape change, and added text. We found a lack of golden standards, including in testing procedures, as current ones are limited to partial sets with different and incomparable setups (different target configurations, avatar, background, etc.). We also found a bias in participants: right-handed, young male, non-color impaired.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2407.11174.pdf' target='_blank'>https://arxiv.org/pdf/2407.11174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pramish Paudel, Anubhav Khanal, Ajad Chhatkuli, Danda Pani Paudel, Jyoti Tandukar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11174">iHuman: Instant Animatable Digital Humans From Monocular Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized 3D avatars require an animatable representation of digital humans. Doing so instantly from monocular videos offers scalability to broad class of users and wide-scale applications. In this paper, we present a fast, simple, yet effective method for creating animatable 3D digital humans from monocular videos. Our method utilizes the efficiency of Gaussian splatting to model both 3D geometry and appearance. However, we observed that naively optimizing Gaussian splats results in inaccurate geometry, thereby leading to poor animations. This work achieves and illustrates the need of accurate 3D mesh-type modelling of the human body for animatable digitization through Gaussian splats. This is achieved by developing a novel pipeline that benefits from three key aspects: (a) implicit modelling of surface's displacements and the color's spherical harmonics; (b) binding of 3D Gaussians to the respective triangular faces of the body template; (c) a novel technique to render normals followed by their auxiliary supervision. Our exhaustive experiments on three different benchmark datasets demonstrates the state-of-the-art results of our method, in limited time settings. In fact, our method is faster by an order of magnitude (in terms of training time) than its closest competitor. At the same time, we achieve superior rendering and 3D reconstruction performance under the change of poses.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2407.05833.pdf' target='_blank'>https://arxiv.org/pdf/2407.05833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuya Izumi, Ryosuke Hyakuta, Ippei Suzuki, Yoichi Ochiai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05833">See-Through Face Display: Enabling Gaze Communication for Any Face$\unicode{x2013}$Human or AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present See-Through Face Display, an eye-contact display system designed to enhance gaze awareness in both human-to-human and human-to-avatar communication. The system addresses the limitations of existing gaze correction methods by combining a transparent display with a strategically positioned camera. The display alternates rapidly between a visible and transparent state, thereby enabling the camera to capture clear images of the user's face from behind the display. This configuration allows for mutual gaze awareness among remote participants without the necessity of a large form factor or computationally resource-intensive image processing. In comparison to conventional methodologies, See-Through Face Display offers a number of practical advantages. The system requires minimal physical space, operates with low computational overhead, and avoids the visual artifacts typically associated with software-based gaze redirection. These features render the system suitable for a variety of applications, including multi-party teleconferencing and remote customer service. Furthermore, the alignment of the camera's field of view with the displayed face position facilitates more natural gaze-based interactions with AI avatars. This paper presents the implementation of See-Through Face Display and examines its potential applications, demonstrating how this compact eye-contact system can enhance gaze communication in both human-to-human and human-AI interactions.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2407.03486.pdf' target='_blank'>https://arxiv.org/pdf/2407.03486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pronay Debnath, Usafa Akther Rifa, Busra Kamal Rafa, Ali Haider Talukder Akib, Md. Aminur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03486">Celeb-FBI: A Benchmark Dataset on Human Full Body Images and Age, Gender, Height and Weight Estimation using Deep Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of comprehensive datasets in surveillance, identification, image retrieval systems, and healthcare poses a significant challenge for researchers in exploring new methodologies and advancing knowledge in these respective fields. Furthermore, the need for full-body image datasets with detailed attributes like height, weight, age, and gender is particularly significant in areas such as fashion industry analytics, ergonomic design assessment, virtual reality avatar creation, and sports performance analysis. To address this gap, we have created the 'Celeb-FBI' dataset which contains 7,211 full-body images of individuals accompanied by detailed information on their height, age, weight, and gender. Following the dataset creation, we proceed with the preprocessing stages, including image cleaning, scaling, and the application of Synthetic Minority Oversampling Technique (SMOTE). Subsequently, utilizing this prepared dataset, we employed three deep learning approaches: Convolutional Neural Network (CNN), 50-layer ResNet, and 16-layer VGG, which are used for estimating height, weight, age, and gender from human full-body images. From the results obtained, ResNet-50 performed best for the system with an accuracy rate of 79.18% for age, 95.43% for gender, 85.60% for height and 81.91% for weight.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2406.16815.pdf' target='_blank'>https://arxiv.org/pdf/2406.16815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Liu, Junshu Tang, Chu Zheng, Shijie Zhang, Jinkun Hao, Junwei Zhu, Dongjin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16815">ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity 3D garment synthesis from text is desirable yet challenging for digital avatar creation. Recent diffusion-based approaches via Score Distillation Sampling (SDS) have enabled new possibilities but either intricately couple with human body or struggle to reuse. We introduce ClotheDreamer, a 3D Gaussian-based method for generating wearable, production-ready 3D garment assets from text prompts. We propose a novel representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate optimization. DCGS represents clothed avatar as one Gaussian model but freezes body Gaussian splats. To enhance quality and completeness, we incorporate bidirectional SDS to supervise clothed avatar and garment RGBD renderings respectively with pose conditions and propose a new pruning strategy for loose clothing. Our approach can also support custom clothing templates as input. Benefiting from our design, the synthetic 3D garment can be easily applied to virtual try-on and support physically accurate animation. Extensive experiments showcase our method's superior and competitive performance. Our project page is at https://ggxxii.github.io/clothedreamer.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2406.00370.pdf' target='_blank'>https://arxiv.org/pdf/2406.00370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MaurÃ­cio Sousa, Daniel Mendes, Alfredo Ferreira, JoÃ£o Madeiras Pereira, Joaquim Jorge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00370">Eery Space: Facilitating Virtual Meetings Through Remote Proxemics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual meetings have become increasingly common with modern video-conference and collaborative software. While they allow obvious savings in time and resources, current technologies add unproductive layers of protocol to the flow of communication between participants, rendering the interactions far from seamless. In this work we introduce Remote Proxemics, an extension of proxemics aimed at bringing the syntax of co-located proximal interactions to virtual meetings. We propose Eery Space, a shared virtual locus that results from merging multiple remote areas, where meeting participants' are located side-by-side as if they shared the same physical location. Eery Space promotes collaborative content creation and seamless mediation of communication channels based on virtual proximity. Results from user evaluation suggest that our approach is effective at enhancing mutual awareness between participants and sufficient to initiate proximal exchanges regardless of their geolocation, while promoting smooth interactions between local and remote people alike. These results happen even in the absence of visual avatars and other social devices such as eye contact, which are largely the focus of previous approaches.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2405.20786.pdf' target='_blank'>https://arxiv.org/pdf/2405.20786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Feng, Wenchao Ma, Quankai Gao, Xianwei Zheng, Nan Xue, Huijuan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20786">Stratified Avatar Generation from Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating 3D full-body avatars from AR/VR devices is essential for creating immersive experiences in AR/VR applications. This task is challenging due to the limited input from Head Mounted Devices, which capture only sparse observations from the head and hands. Predicting the full-body avatars, particularly the lower body, from these sparse observations presents significant difficulties. In this paper, we are inspired by the inherent property of the kinematic tree defined in the Skinned Multi-Person Linear (SMPL) model, where the upper body and lower body share only one common ancestor node, bringing the potential of decoupled reconstruction. We propose a stratified approach to decouple the conventional full-body avatar reconstruction pipeline into two stages, with the reconstruction of the upper body first and a subsequent reconstruction of the lower body conditioned on the previous stage. To implement this straightforward idea, we leverage the latent diffusion model as a powerful probabilistic generator, and train it to follow the latent distribution of decoupled motions explored by a VQ-VAE encoder-decoder model. Extensive experiments on AMASS mocap dataset demonstrate our state-of-the-art performance in the reconstruction of full-body motions.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2405.16204.pdf' target='_blank'>https://arxiv.org/pdf/2405.16204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phong Tran, Egor Zakharov, Long-Nhat Ho, Liwen Hu, Adilbek Karmanov, Aviral Agarwal, McLean Goldwhite, Ariana Bermudez Venegas, Anh Tuan Tran, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16204">VOODOO XP: Expressive One-Shot Head Reenactment for VR Telepresence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce VOODOO XP: a 3D-aware one-shot head reenactment method that can generate highly expressive facial expressions from any input driver video and a single 2D portrait. Our solution is real-time, view-consistent, and can be instantly used without calibration or fine-tuning. We demonstrate our solution on a monocular video setting and an end-to-end VR telepresence system for two-way communication. Compared to 2D head reenactment methods, 3D-aware approaches aim to preserve the identity of the subject and ensure view-consistent facial geometry for novel camera poses, which makes them suitable for immersive applications. While various facial disentanglement techniques have been introduced, cutting-edge 3D-aware neural reenactment techniques still lack expressiveness and fail to reproduce complex and fine-scale facial expressions. We present a novel cross-reenactment architecture that directly transfers the driver's facial expressions to transformer blocks of the input source's 3D lifting module. We show that highly effective disentanglement is possible using an innovative multi-stage self-supervision approach, which is based on a coarse-to-fine strategy, combined with an explicit face neutralization and 3D lifted frontalization during its initial training stage. We further integrate our novel head reenactment solution into an accessible high-fidelity VR telepresence system, where any person can instantly build a personalized neural head avatar from any photo and bring it to life using the headset. We demonstrate state-of-the-art performance in terms of expressiveness and likeness preservation on a large set of diverse subjects and capture conditions.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2405.07933.pdf' target='_blank'>https://arxiv.org/pdf/2405.07933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyeongsik Moon, Weipeng Xu, Rohan Joshi, Chenglei Wu, Takaaki Shiratori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07933">Authentic Hand Avatar from a Phone Scan via Universal Hand Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR. In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar. For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately. The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage. On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline. We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much. Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2404.12888.pdf' target='_blank'>https://arxiv.org/pdf/2404.12888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12888">Learn2Talk: 3D Talking Face Learns from 2D Talking Face</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2404.07991.pdf' target='_blank'>https://arxiv.org/pdf/2404.07991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07991">GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2403.11700.pdf' target='_blank'>https://arxiv.org/pdf/2403.11700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Can Liu, Di Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11700">Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread popularity of internet celebrity marketing all over the world, short video production has gradually become a popular way of presenting products information. However, the traditional video production industry usually includes series of procedures as script writing, video filming in a professional studio, video clipping, special effects rendering, customized post-processing, and so forth. Not to mention that multilingual videos is not accessible for those who could not speak multilingual languages. These complicated procedures usually needs a professional team to complete, and this made short video production costly in both time and money. This paper presents an intelligent system that supports the automatic generation of talking avatar videos, namely Virbo. With simply a user-specified script, Virbo could use a deep generative model to generate a target talking videos. Meanwhile, the system also supports multimodal inputs to customize the video with specified face, specified voice and special effects. This system also integrated a multilingual customization module that supports generate multilingual talking avatar videos in a batch with hundreds of delicate templates and creative special effects. Through a series of user studies and demo tests, we found that Virbo can generate talking avatar videos that maintained a high quality of videos as those from a professional team while reducing the entire production costs significantly. This intelligent system will effectively promote the video production industry and facilitate the internet marketing neglecting of language barriers and cost challenges.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2403.08363.pdf' target='_blank'>https://arxiv.org/pdf/2403.08363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthikeya Puttur Venkatraj, Wo Meijer, Monica PerusquÃ­a-HernÃ¡ndez, Gijs Huisman, Abdallah El Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08363">ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2403.07122.pdf' target='_blank'>https://arxiv.org/pdf/2403.07122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Mal, Nina DÃ¶llinger, Erik Wolf, Stephan Wenninger, Mario Botsch, Carolin Wienrich, Marc Erich Latoschik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07122">Am I the Odd One? Exploring (In)Congruencies in the Realism of Avatars and Virtual Others in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual humans play a pivotal role in social virtual environments, shaping users' VR experiences. The diversity in available options and users' preferences can result in a heterogeneous mix of appearances among a group of virtual humans. The resulting variety in higher-order anthropomorphic and realistic cues introduces multiple (in)congruencies, eventually impacting the plausibility of the experience. In this work, we consider the impact of (in)congruencies in the realism of a group of virtual humans, including co-located others and one's self-avatar. In a 2 x 3 mixed design, participants embodied either (1) a personalized realistic or (2) a customized stylized self-avatar across three consecutive VR exposures in which they were accompanied by a group of virtual others being either (1) all realistic, (2) all stylized, or (3) mixed. Our results indicate groups of virtual others of higher realism, i.e., potentially more congruent with participants' real-world experiences and expectations, were considered more human-like, increasing the feeling of co-presence and the impression of interaction possibilities. (In)congruencies concerning the homogeneity of the group did not cause considerable effects. Furthermore, our results indicate that a self-avatar's congruence with the participant's real-world experiences concerning their own physical body yielded notable benefits for virtual body ownership and self-identification for realistic personalized avatars. Notably, the incongruence between a stylized self-avatar and a group of realistic virtual others resulted in diminished ratings of self-location and self-identification. We conclude on the implications of our findings and discuss our results within current theories of VR experiences, considering (in)congruent visual cues and their impact on the perception of virtual others, self-representation, and spatial presence.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2401.11078.pdf' target='_blank'>https://arxiv.org/pdf/2401.11078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11078">UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2312.13091.pdf' target='_blank'>https://arxiv.org/pdf/2312.13091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andre Carbonneau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13091">MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrinsic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: https://ubisoft-laforge.github.io/character/mosar/
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2312.12877.pdf' target='_blank'>https://arxiv.org/pdf/2312.12877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, Feng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12877">Relightable and Animatable Neural Avatars from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lightweight creation of 3D digital avatars is a highly desirable but challenging task. With only sparse videos of a person under unknown illumination, we propose a method to create relightable and animatable neural avatars, which can be used to synthesize photorealistic images of humans under novel viewpoints, body poses, and lighting. The key challenge here is to disentangle the geometry, material of the clothed body, and lighting, which becomes more difficult due to the complex geometry and shadow changes caused by body motions. To solve this ill-posed problem, we propose novel techniques to better model the geometry and shadow changes. For geometry change modeling, we propose an invertible deformation field, which helps to solve the inverse skinning problem and leads to better geometry quality. To model the spatial and temporal varying shading cues, we propose a pose-aware part-wise light visibility network to estimate light occlusion. Extensive experiments on synthetic and real datasets show that our approach reconstructs high-quality geometry and generates realistic shadows under different body poses. Code and data are available at \url{https://wenbin-lin.github.io/RelightableAvatar-page/}.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2312.11289.pdf' target='_blank'>https://arxiv.org/pdf/2312.11289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiran Li, Haoran Zhang, Wenjing Li, Dou Huang, Jinyu Chen, Junxiang Zhang, Xuan Song, Pengjun Zhao, Shibasaki Ryosuke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11289">Learning to Generate Pseudo Personal Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The importance of personal mobility data is widely recognized in various fields. However, the utilization of real personal mobility data raises privacy concerns. Therefore, it is crucial to generate pseudo personal mobility data that accurately reflects real-world mobility patterns while safeguarding user privacy. Nevertheless, existing methods for generating pseudo mobility data, such as mechanism-based and deep-learning-based approaches, have limitations in capturing sufficient individual heterogeneity. To address these gaps, taking pseudo-person(avatar) as ground-zero, a novel individual-based human mobility generator called GeoAvatar has been proposed - which considers individual heterogeneity in spatial and temporal decision-making, incorporates demographic characteristics, and provides interpretability. Our method utilizes a deep generative model to simulate heterogeneous individual life patterns, a reliable labeler for inferring individual demographic characteristics, and a Bayesian approach for generating spatial choices. Through our method, we have achieved the generation of heterogeneous individual human mobility data without accessing individual-level personal information, with good quality - we evaluated the proposed method based on physical features, activity patterns, and spatial-temporal characteristics, demonstrating its good performance, compared to mechanism-based modeling and black-box deep learning approaches. Furthermore, this method maintains extensibility for broader applications, making it a promising paradigm for generating human mobility data.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2312.05311.pdf' target='_blank'>https://arxiv.org/pdf/2312.05311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jalees Nehvi, Berna Kabadayi, Julien Valentin, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05311">360Â° Volumetric Portrait Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose 360Â° Volumetric Portrait (3VP) Avatar, a novel method for reconstructing 360Â° photo-realistic portrait avatars of human subjects solely based on monocular video inputs. State-of-the-art monocular avatar reconstruction methods rely on stable facial performance capturing. However, the common usage of 3DMM-based facial tracking has its limits; side-views can hardly be captured and it fails, especially, for back-views, as required inputs like facial landmarks or human parsing masks are missing. This results in incomplete avatar reconstructions that only cover the frontal hemisphere. In contrast to this, we propose a template-based tracking of the torso, head and facial expressions which allows us to cover the appearance of a human subject from all sides. Thus, given a sequence of a subject that is rotating in front of a single camera, we train a neural volumetric representation based on neural radiance fields. A key challenge to construct this representation is the modeling of appearance changes, especially, in the mouth region (i.e., lips and teeth). We, therefore, propose a deformation-field-based blend basis which allows us to interpolate between different appearance states. We evaluate our approach on captured real-world data and compare against state-of-the-art monocular reconstruction methods. In contrast to those, our method is the first monocular technique that reconstructs an entire 360Â° avatar.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2310.05917.pdf' target='_blank'>https://arxiv.org/pdf/2310.05917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, Timur Bagautdinov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05917">Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic Clothing Driven by Sparse RGB-D Input</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clothing is an important part of human appearance but challenging to model in photorealistic avatars. In this work we present avatars with dynamically moving loose clothing that can be faithfully driven by sparse RGB-D inputs as well as body and face motion. We propose a Neural Iterative Closest Point (N-ICP) algorithm that can efficiently track the coarse garment shape given sparse depth input. Given the coarse tracking results, the input RGB-D images are then remapped to texel-aligned features, which are fed into the drivable avatar models to faithfully reconstruct appearance details. We evaluate our method against recent image-driven synthesis baselines, and conduct a comprehensive analysis of the N-ICP algorithm. We demonstrate that our method can generalize to a novel testing environment, while preserving the ability to produce high-fidelity and faithful clothing dynamics and appearance.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2309.09314.pdf' target='_blank'>https://arxiv.org/pdf/2309.09314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Taeil Jin, Sung-Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09314">MOVIN: Real-time Motion Capture using a Single LiDAR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in technology have brought forth new forms of interactive applications, such as the social metaverse, where end users interact with each other through their virtual avatars. In such applications, precise full-body tracking is essential for an immersive experience and a sense of embodiment with the virtual avatar. However, current motion capture systems are not easily accessible to end users due to their high cost, the requirement for special skills to operate them, or the discomfort associated with wearable devices. In this paper, we present MOVIN, the data-driven generative method for real-time motion capture with global tracking, using a single LiDAR sensor. Our autoregressive conditional variational autoencoder (CVAE) model learns the distribution of pose variations conditioned on the given 3D point cloud from LiDAR.As a central factor for high-accuracy motion capture, we propose a novel feature encoder to learn the correlation between the historical 3D point cloud data and global, local pose features, resulting in effective learning of the pose prior. Global pose features include root translation, rotation, and foot contacts, while local features comprise joint positions and rotations. Subsequently, a pose generator takes into account the sampled latent variable along with the features from the previous frame to generate a plausible current pose. Our framework accurately predicts the performer's 3D global information and local joint details while effectively considering temporally coherent movements across frames. We demonstrate the effectiveness of our architecture through quantitative and qualitative evaluations, comparing it against state-of-the-art methods. Additionally, we implement a real-time application to showcase our method in real-world scenarios. MOVIN dataset is available at \url{https://movin3d.github.io/movin_pg2023/}.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2308.14404.pdf' target='_blank'>https://arxiv.org/pdf/2308.14404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Forouzan Farzinnejad, Javad Rasti, Navid Khezrian, Jens Grubert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14404">The Effect of an Exergame on the Shadow Play Skill Based on Muscle Memory for Young Female Participants: The Case of Forehand Drive in Table Tennis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning and practicing table tennis with traditional methods is a long, tedious process and may even lead to the internalization of incorrect techniques if not supervised by a coach. To overcome these issues, the presented study proposes an exergame with the aim of enhancing young female novice players' performance by boosting muscle memory, making practice more interesting, and decreasing the probability of faulty training. Specifically, we propose an exergame based on skeleton tracking and a virtual avatar to support correct shadow practice to learn forehand drive technique without the presence of a coach. We recruited 44 schoolgirls aged between 8 and 12 years without a background in playing table tennis and divided them into control and experimental groups. We examined their stroke skills (via the Mott-Lockhart test) and the error coefficient of their forehand drives (using a ball machine) in the pretest, post-test, and follow-up tests (10 days after the post-test). Our results showed that the experimental group had progress in the short and long term, while the control group had an improvement only in the short term. Further, the scale of improvement in the experimental group was significantly higher than in the control group. Given that the early stages of learning, particularly in girls children, are important in the internalization of individual skills in would-be athletes, this method could support promoting correct training for young females.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2306.15213.pdf' target='_blank'>https://arxiv.org/pdf/2306.15213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurtis Haut, Caleb Wohn, Benjamin Kane, Tom Carroll, Catherine Guigno, Varun Kumar, Ron Epstein, Lenhart Schubert, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15213">Validating a virtual human and automated feedback system for training doctor-patient communication skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication between a clinician and their patient is critical for delivering healthcare maximizing outcomes. Unfortunately, traditional communication training approaches that use human standardized patients and expert coaches are difficult to scale. Here, we present the development and validation of a scalable, easily accessible, digital tool known as the Standardized Online Patient for Health Interaction Education (SOPHIE) for practicing and receiving feedback on doctor-patient communication skills. SOPHIE was validated by conducting an experiment with 30 participants. We found that participants who underwent SOPHIE performed significantly better than the control in overall communication, aggregate scores, empowering the patient, and showing empathy ($p < 0.05$ in all cases). One day, we hope that SOPHIE will help make communication training resources more accessible by providing a scalable option to supplement existing resources.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2306.12188.pdf' target='_blank'>https://arxiv.org/pdf/2306.12188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ariel Larey, Omri Asraf, Adam Kelder, Itzik Wilf, Ofer Kruzel, Nati Daniel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12188">Facial Expression Re-targeting from a Single Character</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video retargeting for digital face animation is used in virtual reality, social media, gaming, movies, and video conference, aiming to animate avatars' facial expressions based on videos of human faces. The standard method to represent facial expressions for 3D characters is by blendshapes, a vector of weights representing the avatar's neutral shape and its variations under facial expressions, e.g., smile, puff, blinking. Datasets of paired frames with blendshape vectors are rare, and labeling can be laborious, time-consuming, and subjective. In this work, we developed an approach that handles the lack of appropriate datasets. Instead, we used a synthetic dataset of only one character. To generalize various characters, we re-represented each frame to face landmarks. We developed a unique deep-learning architecture that groups landmarks for each facial organ and connects them to relevant blendshape weights. Additionally, we incorporated complementary methods for facial expressions that landmarks did not represent well and gave special attention to eye expressions. We have demonstrated the superiority of our approach to previous research in qualitative and quantitative metrics. Our approach achieved a higher MOS of 68% and a lower MSE of 44.2% when tested on videos with various users and expressions.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2306.09532.pdf' target='_blank'>https://arxiv.org/pdf/2306.09532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel van de Panne, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09532">Hierarchical Planning and Control for Box Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans perform everyday tasks using a combination of locomotion and manipulation skills. Building a system that can handle both skills is essential to creating virtual humans. We present a physically-simulated human capable of solving box rearrangement tasks, which requires a combination of both skills. We propose a hierarchical control architecture, where each level solves the task at a different level of abstraction, and the result is a physics-based simulated virtual human capable of rearranging boxes in a cluttered environment. The control architecture integrates a planner, diffusion models, and physics-based motion imitation of sparse motion clips using deep reinforcement learning. Boxes can vary in size, weight, shape, and placement height. Code and trained control policies are provided.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2306.03642.pdf' target='_blank'>https://arxiv.org/pdf/2306.03642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Korosteleva, Olga Sorkine-Hornung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03642">GarmentCode: Programming Parametric Sewing Patterns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Garment modeling is an essential task of the global apparel industry and a core part of digital human modeling. Realistic representation of garments with valid sewing patterns is key to their accurate digital simulation and eventual fabrication. However, little-to-no computational tools provide support for bridging the gap between high-level construction goals and low-level editing of pattern geometry, e.g., combining or switching garment elements, semantic editing, or design exploration that maintains the validity of a sewing pattern. We suggest the first DSL for garment modeling -- GarmentCode -- that applies principles of object-oriented programming to garment construction and allows designing sewing patterns in a hierarchical, component-oriented manner. The programming-based paradigm naturally provides unique advantages of component abstraction, algorithmic manipulation, and free-form design parametrization. We additionally support the construction process by automating typical low-level tasks like placing a dart at a desired location. In our prototype garment configurator, users can manipulate meaningful design parameters and body measurements, while the construction of pattern geometry is handled by garment programs implemented with GarmentCode. Our configurator enables the free exploration of rich design spaces and the creation of garments using interchangeable, parameterized components. We showcase our approach by producing a variety of garment designs and retargeting them to different body shapes using our configurator.
  Project page: https://igl.ethz.ch/projects/garmentcode/
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2305.16411.pdf' target='_blank'>https://arxiv.org/pdf/2305.16411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhen Weng, Zeyu Wang, Serena Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16411">ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in text-to-image generation have enabled significant progress in zero-shot 3D shape generation. This is achieved by score distillation, a methodology that uses pre-trained text-to-image diffusion models to optimize the parameters of a 3D neural presentation, e.g. Neural Radiance Field (NeRF). While showing promising results, existing methods are often not able to preserve the geometry of complex shapes, such as human bodies. To address this challenge, we present ZeroAvatar, a method that introduces the explicit 3D human body prior to the optimization process. Specifically, we first estimate and refine the parameters of a parametric human body from a single image. Then during optimization, we use the posed parametric body as additional geometry constraint to regularize the diffusion model as well as the underlying density field. Lastly, we propose a UV-guided texture regularization term to further guide the completion of texture on invisible body parts. We show that ZeroAvatar significantly enhances the robustness and 3D consistency of optimization-based image-to-3D avatar generation, outperforming existing zero-shot image-to-3D methods.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2305.14345.pdf' target='_blank'>https://arxiv.org/pdf/2305.14345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeksoo Kim, Shunsuke Saito, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14345">NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models have been recently extended to synthesizing 3D digital humans. However, previous approaches treat clothed humans as a single chunk of geometry without considering the compositionality of clothing and accessories. As a result, individual items cannot be naturally composed into novel identities, leading to limited expressiveness and controllability of generative 3D avatars. While several methods attempt to address this by leveraging synthetic data, the interaction between humans and objects is not authentic due to the domain gap, and manual asset creation is difficult to scale for a wide variety of objects. In this work, we present a novel framework for learning a compositional generative model of humans and objects (backpacks, coats, scarves, and more) from real-world 3D scans. Our compositional model is interaction-aware, meaning the spatial relationship between humans and objects, and the mutual shape change by physical contact is fully incorporated. The key challenge is that, since humans and objects are in contact, their 3D scans are merged into a single piece. To decompose them without manual annotations, we propose to leverage two sets of 3D scans of a single person with and without objects. Our approach learns to decompose objects and naturally compose them back into a generative human model in an unsupervised manner. Despite our simple setup requiring only the capture of a single subject with objects, our experiments demonstrate the strong generalization of our model by enabling the natural composition of objects to diverse identities in various poses and the composition of multiple objects, which is unseen in training data. https://taeksuu.github.io/ncho/
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2304.12483.pdf' target='_blank'>https://arxiv.org/pdf/2304.12483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aashish Rai, Hiresh Gupta, Ayush Pandey, Francisco Vicente Carrasco, Shingo Jason Takagi, Amaury Aubel, Daeil Kim, Aayush Prakash, Fernando de la Torre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12483">Towards Realistic Generative 3D Face Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face model to produce high-quality albedo and precise 3D shape leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision. Moreover, this approach outperforms the state-of-the-art (SOTA) methods in the well-known NoW benchmark for shape reconstruction. It also outperforms the SOTA reconstruction models in recovering rendered faces' identities across novel poses by an average of 10%. Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2304.11113.pdf' target='_blank'>https://arxiv.org/pdf/2304.11113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhan Chen, Matthew O'Toole, Gaurav Bharaj, Pablo Garrido
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11113">Implicit Neural Head Synthesis via Controllable Local Deformation Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality reconstruction of controllable 3D head avatars from 2D videos is highly desirable for virtual human applications in movies, games, and telepresence. Neural implicit fields provide a powerful representation to model 3D head avatars with personalized shape, expressions, and facial parts, e.g., hair and mouth interior, that go beyond the linear 3D morphable model (3DMM). However, existing methods do not model faces with fine-scale facial features, or local control of facial parts that extrapolate asymmetric expressions from monocular videos. Further, most condition only on 3DMM parameters with poor(er) locality, and resolve local features with a global neural field. We build on part-based implicit shape models that decompose a global deformation field into local ones. Our novel formulation models multiple implicit deformation fields with local semantic rig-like control via 3DMM-based parameters, and representative facial landmarks. Further, we propose a local control loss and attention mask mechanism that promote sparsity of each learned deformation field. Our formulation renders sharper locally controllable nonlinear deformations than previous implicit monocular approaches, especially mouth interior, asymmetric expressions, and facial details.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2304.07410.pdf' target='_blank'>https://arxiv.org/pdf/2304.07410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samaneh Azadi, Thomas Hayes, Akbar Shah, Guan Pang, Devi Parikh, Sonal Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07410">Text-Conditional Contextualized Avatars For Zero-Shot Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large-scale text-to-image generation models have made significant improvements in the quality, realism, and diversity of the synthesized images and enable users to control the created content through language. However, the personalization aspect of these generative models is still challenging and under-explored. In this work, we propose a pipeline that enables personalization of image generation with avatars capturing a user's identity in a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic, and does not require training on the avatar at all - it is scalable to millions of users who can generate a scene with their avatar. To render the avatar in a pose faithful to the given text prompt, we propose a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-the-wild human poses improving the performance of the SOTA text-to-motion models significantly. We show, for the first time, how to leverage large-scale image datasets to learn human 3D pose parameters and overcome the limitations of motion capture datasets.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2303.02930.pdf' target='_blank'>https://arxiv.org/pdf/2303.02930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gido Kato, Yoshihiro Fukuhara, Mariko Isogawa, Hideki Tsunashima, Hirokatsu Kataoka, Shigeo Morishima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02930">Scapegoat Generation for Privacy Protection from Deepfake</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To protect privacy and prevent malicious use of deepfake, current studies propose methods that interfere with the generation process, such as detection and destruction approaches. However, these methods suffer from sub-optimal generalization performance to unseen models and add undesirable noise to the original image. To address these problems, we propose a new problem formulation for deepfake prevention: generating a ``scapegoat image'' by modifying the style of the original input in a way that is recognizable as an avatar by the user, but impossible to reconstruct the real face. Even in the case of malicious deepfake, the privacy of the users is still protected. To achieve this, we introduce an optimization-based editing method that utilizes GAN inversion to discourage deepfake models from generating similar scapegoats. We validate the effectiveness of our proposed method through quantitative and user studies.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2303.02546.pdf' target='_blank'>https://arxiv.org/pdf/2303.02546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Tejwani, Chengyuan Ma, Paolo Bonato, H. Harry Asada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02546">An Avatar Robot Overlaid with the 3D Human Model of a Remote Operator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although telepresence assistive robots have made significant progress, they still lack the sense of realism and physical presence of the remote operator. This results in a lack of trust and adoption of such robots. In this paper, we introduce an Avatar Robot System which is a mixed real/virtual robotic system that physically interacts with a person in proximity of the robot. The robot structure is overlaid with the 3D model of the remote caregiver and visualized through Augmented Reality (AR). In this way, the person receives haptic feedback as the robot touches him/her. We further present an Optimal Non-Iterative Alignment solver that solves for the optimally aligned pose of 3D Human model to the robot (shoulder to the wrist non-iteratively). The proposed alignment solver is stateless, achieves optimal alignment and faster than the baseline solvers (demonstrated in our evaluations). We also propose an evaluation framework that quantifies the alignment quality of the solvers through multifaceted metrics. We show that our solver can consistently produce poses with similar or superior alignments as IK-based baselines without their potential drawbacks.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2302.07909.pdf' target='_blank'>https://arxiv.org/pdf/2302.07909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catarina G. Fidalgo, MaurÃ­cio Sousa, Daniel Mendes, Rafael Kuffner dos Anjos, Daniel Medeiros, Karan Singh, Joaquim Jorge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07909">MAGIC: Manipulating Avatars and Gestures to Improve Remote Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote collaborative work has become pervasive in many settings, from engineering to medical professions. Users are immersed in virtual environments and communicate through life-sized avatars that enable face-to-face collaboration. Within this context, users often collaboratively view and interact with virtual 3D models, for example, to assist in designing new devices such as customized prosthetics, vehicles, or buildings. However, discussing shared 3D content face-to-face has various challenges, such as ambiguities, occlusions, and different viewpoints that all decrease mutual awareness, leading to decreased task performance and increased errors. To address this challenge, we introduce MAGIC, a novel approach for understanding pointing gestures in a face-to-face shared 3D space, improving mutual understanding and awareness. Our approach distorts the remote userÅ gestures to correctly reflect them in the local userÅ reference space when face-to-face. We introduce a novel metric called pointing agreement to measure what two users perceive in common when using pointing gestures in a shared 3D space. Results from a user study suggest that MAGIC significantly improves pointing agreement in face-to-face collaboration settings, improving co-presence and awareness of interactions performed in the shared space. We believe that MAGIC improves remote collaboration by enabling simpler communication mechanisms and better mutual awareness.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2302.02601.pdf' target='_blank'>https://arxiv.org/pdf/2302.02601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanyoung Chung, Joyce Jiyoung Whang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02601">Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level triplets, with additional consideration of the augmented triplets. We propose two new tasks: triplet prediction and conditional link prediction. Given a triplet $T_1$ and a higher-level relation, the triplet prediction predicts a triplet that is likely to be connected to $T_1$ by the higher-level relation, e.g., $\langle T_1$, PrerequisiteFor, ?$\rangle$. The conditional link prediction predicts a missing entity in a triplet conditioned on another triplet, e.g., $\langle T_1$, PrerequisiteFor, (Avatar, Wins, ?)$\rangle$. Experimental results show that BiVE significantly outperforms all other methods in the two new tasks and the typical base-level link prediction in real-world bi-level knowledge graphs.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2210.00527.pdf' target='_blank'>https://arxiv.org/pdf/2210.00527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Schell, Andreas Hotho, Marc Erich Latoschik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.00527">Comparison of Data Representations and Machine Learning Architectures for User Identification on Arbitrary Motion Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable and robust user identification and authentication are important and often necessary requirements for many digital services. It becomes paramount in social virtual reality (VR) to ensure trust, specifically in digital encounters with lifelike realistic-looking avatars as faithful replications of real persons. Recent research has shown that the movements of users in extended reality (XR) systems carry user-specific information and can thus be used to verify their identities. This article compares three different potential encodings of the motion data from head and hands (scene-relative, body-relative, and body-relative velocities), and the performances of five different machine learning architectures (random forest, multi-layer perceptron, fully recurrent neural network, long-short term memory, gated recurrent unit). We use the publicly available dataset "Talking with Hands" and publish all code to allow reproducibility and to provide baselines for future work. After hyperparameter optimization, the combination of a long-short term memory architecture and body-relative data outperformed competing combinations: the model correctly identifies any of the 34 subjects with an accuracy of 100% within 150 seconds. Altogether, our approach provides an effective foundation for behaviometric-based identification and authentication to guide researchers and practitioners. Data and code are published under https://go.uniwue.de/58w1r.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2206.09542.pdf' target='_blank'>https://arxiv.org/pdf/2206.09542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongseok Yang, Jiho Kang, Taehei Kim, Sung-Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.09542">Visual Guidance for User Placement in Avatar-Mediated Telepresence between Dissimilar Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in technology gradually realize immersive mixed-reality (MR) telepresence between distant spaces. This paper presents a novel visual guidance system for avatar-mediated telepresence, directing users to optimal placements that facilitate the clear transfer of gaze and pointing contexts through remote avatars in dissimilar spaces, where the spatial relationship between the remote avatar and the interaction targets may differ from that of the local user. Representing the spatial relationship between the user/avatar and interaction targets with angle-based interaction features, we assign recommendation scores of sampled local placements as their maximum feature similarity with remote placements. These scores are visualized as color-coded 2D sectors to inform the users of better placements for interaction with selected targets. In addition, virtual objects of the remote space are overlapped with the local space for the user to better understand the recommendations. We examine whether the proposed score measure agrees with the actual user perception of the partner's interaction context and find a score threshold for recommendation through user experiments in virtual reality (VR). A subsequent user study in VR investigates the effectiveness and perceptual overload of different combinations of visualizations. Finally, we conduct a user study in an MR telepresence scenario to evaluate the effectiveness of our method in real-world applications.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/1701.07601.pdf' target='_blank'>https://arxiv.org/pdf/1701.07601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dusko Pavlovic, Peter-Michael Seidel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1701.07601">Quotients in monadic programming: Projective algebras are equivalent to coalgebras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In monadic programming, datatypes are presented as free algebras, generated by data values, and by the algebraic operations and equations capturing some computational effects. These algebras are free in the sense that they satisfy just the equations imposed by their algebraic theory, and remain free of any additional equations. The consequence is that they do not admit quotient types. This is, of course, often inconvenient. Whenever a computation involves data with multiple representatives, and they need to be identified according to some equations that are not satisfied by all data, the monadic programmer has to leave the universe of free algebras, and resort to explicit destructors. We characterize the situation when these destructors are preserved under all operations, and the resulting quotients of free algebras are also their subalgebras. Such quotients are called *projective*. Although popular in universal algebra, projective algebras did not attract much attention in the monadic setting, where they turn out to have a surprising avatar: for any given monad, a suitable category of projective algebras is equivalent with the category of coalgebras for the comonad induced by any monad resolution. For a monadic programmer, this equivalence provides a convenient way to implement polymorphic quotients as coalgebras. The dual correspondence of injective coalgebras and all algebras leads to a different family of quotient types, which seems to have a different family of applications. Both equivalences also entail several general corollaries concerning monadicity and comonadicity.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2509.13013.pdf' target='_blank'>https://arxiv.org/pdf/2509.13013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaofeng Liu, Hengsen Li, Ruoyu Gao, Xuetong Li, Zhiyuan Ma, Tao Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13013">Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2509.07774.pdf' target='_blank'>https://arxiv.org/pdf/2509.07774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yimin Pan, Matthias Nießner, Tobias Kirschstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07774">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision. While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour. The project page can be found at: https://yimin-pan.github.io/hair-gs/
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2509.05582.pdf' target='_blank'>https://arxiv.org/pdf/2509.05582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiling Ye, Cong Zhou, Xiubao Zhang, Haifeng Shen, Weihong Deng, Quan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05582">Reconstruction and Reenactment Separated Method for Realistic Gaussian Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore a reconstruction and reenactment separated framework for 3D Gaussians head, which requires only a single portrait image as input to generate controllable avatar. Specifically, we developed a large-scale one-shot gaussian head generator built upon WebSSL and employed a two-stage training approach that significantly enhances the capabilities of generalization and high-frequency texture reconstruction. During inference, an ultra-lightweight gaussian avatar driven by control signals enables high frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further demonstrate that the proposed framework follows the scaling law, whereby increasing the parameter scale of the reconstruction module leads to improved performance. Moreover, thanks to the separation design, driving efficiency remains unaffected. Finally, extensive quantitative and qualitative experiments validate that our approach outperforms current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2509.05023.pdf' target='_blank'>https://arxiv.org/pdf/2509.05023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eneko Atxa Landa, Elena Lazkano, Igor Rodriguez, Itsaso Rodríguez-Moreno, Itziar Irigoien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05023">Evaluating Idle Animation Believability: a User Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating realistic avatars requires using high quality animations for every possible state the avatar can be in. This includes actions like walking or running, but also subtle movements that convey emotions and personality. Idle animations, such as standing, breathing or looking around, are crucial for realism and believability. In games and virtual applications, these are often handcrafted or recorded with actors, but this is costly. Furthermore, recording realistic idle animations can be very complex, because the actor must not know they are being recorded in order to make genuine movements. For this reasons idle animation datasets are not widely available. Nevertheless, this paper concludes that both acted and genuine idle animations are perceived as real, and that users are not able to distinguish between them. It also states that handmade and recorded idle animations are perceived differently. These two conclusions mean that recording idle animations should be easier than it is thought to be, meaning that actors can be specifically told to act the movements, significantly simplifying the recording process. These conclusions should help future efforts to record idle animation datasets. Finally, we also publish ReActIdle, a 3 dimensional idle animation dataset containing both real and acted idle motions.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2509.04041.pdf' target='_blank'>https://arxiv.org/pdf/2509.04041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Raggi, Gem Stapleton, Mateja Jamnik, Aaron Stockdill, Grecia Garcia Garcia, Peter C-H. Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04041">Oruga: An Avatar of Representational Systems Theory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans use representations flexibly. We draw diagrams, change representations and exploit creative analogies across different domains. We want to harness this kind of power and endow machines with it to make them more compatible with human use. Previously we developed Representational Systems Theory (RST) to study the structure and transformations of representations. In this paper we present Oruga (caterpillar in Spanish; a symbol of transformation), an implementation of various aspects of RST. Oruga consists of a core of data structures corresponding to concepts in RST, a language for communicating with the core, and an engine for producing transformations using a method we call structure transfer. In this paper we present an overview of the core and language of Oruga, with a brief example of the kind of transformation that structure transfer can execute.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2508.19518.pdf' target='_blank'>https://arxiv.org/pdf/2508.19518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hail Song, Seokhwan Yang, Woontack Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19518">Fast Texture Transfer for XR Avatars via Barycentric UV Conversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a fast and efficient method for transferring facial textures onto SMPL-X-based full-body avatars. Unlike conventional affine-transform methods that are slow and prone to visual artifacts, our method utilizes a barycentric UV conversion technique. Our approach precomputes the entire UV mapping into a single transformation matrix, enabling texture transfer in a single operation. This results in a speedup of over 7000x compared to the baseline, while also significantly improving the final texture quality by eliminating boundary artifacts. Through quantitative and qualitative evaluations, we demonstrate that our method offers a practical solution for personalization in immersive XR applications. The code is available online.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2508.14920.pdf' target='_blank'>https://arxiv.org/pdf/2508.14920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilya Fedorov, Dmitry Korobchenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14920">Human Feedback Driven Dynamic Speech Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes to explore a new area of dynamic speech emotion recognition. Unlike traditional methods, we assume that each audio track is associated with a sequence of emotions active at different moments in time. The study particularly focuses on the animation of emotional 3D avatars. We propose a multi-stage method that includes the training of a classical speech emotion recognition model, synthetic generation of emotional sequences, and further model improvement based on human feedback. Additionally, we introduce a novel approach to modeling emotional mixtures based on the Dirichlet distribution. The models are evaluated based on ground-truth emotions extracted from a dataset of 3D facial animations. We compare our models against the sliding window approach. Our experimental results show the effectiveness of Dirichlet-based approach in modeling emotional mixtures. Incorporating human feedback further improves the model quality while providing a simplified annotation procedure.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2508.08050.pdf' target='_blank'>https://arxiv.org/pdf/2508.08050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrizio Nunnari, Cristina Luna JimÃ©nez, Rosalee Wolfe, John C. McDonald, Michael Filhol, Eleni Efthimiou, Evita Fotinea, Thomas Hanke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08050">9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Sign Language Translation and Avatar Technology (SLTAT) workshops continue a series of gatherings to share recent advances in improving deaf / human communication through non-invasive means. This 2025 edition, the 9th since its first appearance in 2011, is hosted by the International Conference on Intelligent Virtual Agents (IVA), giving the opportunity for contamination between two research communities, using digital humans as either virtual interpreters or as interactive conversational agents. As presented in this summary paper, SLTAT sees contributions beyond avatar technologies, with a consistent number of submissions on sign language recognition, and other work on data collection, data analysis, tools, ethics, usability, and affective computing.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2508.07937.pdf' target='_blank'>https://arxiv.org/pdf/2508.07937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John C. McDonald, Rosalee Wolfe, Fabrizio Nunnari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07937">Challenges and opportunities in portraying emotion in generated sign language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Non-manual signals in sign languages continue to be a challenge for signing avatars. More specifically, emotional content has been difficult to incorporate because of a lack of a standard method of specifying the avatar's emotional state. This paper explores the application of an intuitive two-parameter representation for emotive non-manual signals to the Paula signing avatar that shows promise for facilitating the linguistic specification of emotional facial expressions in a more coherent manner than previous methods. Users can apply these parameters to control Paula's emotional expressions through a textual representation called the EASIER notation. The representation can allow avatars to express more nuanced emotional states using two numerical parameters. It also has the potential to enable more consistent specification of emotional non-manual signals in linguistic annotations which drive signing avatars.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2508.05358.pdf' target='_blank'>https://arxiv.org/pdf/2508.05358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fenya Wasserroth, Eleftherios Avramidis, Vera Czehmann, Tanja Kojic, Fabrizio Nunnari, Sebastian MÃ¶ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05358">Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an investigation into the impact of adding adjustment features to an existing sign language (SL) avatar on a Microsoft Hololens 2 device. Through a detailed analysis of interactions of expert German Sign Language (DGS) users with both adjustable and non-adjustable avatars in a specific use case, this study identifies the key factors influencing the comprehensibility, the user experience (UX), and the acceptability of such a system. Despite user preference for adjustable settings, no significant improvements in UX or comprehensibility were observed, which remained at low levels, amid missing SL elements (mouthings and facial expressions) and implementation issues (indistinct hand shapes, lack of feedback and menu positioning). Hedonic quality was rated higher than pragmatic quality, indicating that users found the system more emotionally or aesthetically pleasing than functionally useful. Stress levels were higher for the adjustable avatar, reflecting lower performance, greater effort and more frustration. Additionally, concerns were raised about whether the Hololens adjustment gestures are intuitive and easy to familiarise oneself with. While acceptability of the concept of adjustability was generally positive, it was strongly dependent on usability and animation quality. This study highlights that personalisation alone is insufficient, and that SL avatars must be comprehensible by default. Key recommendations include enhancing mouthing and facial animation, improving interaction interfaces, and applying participatory design.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2507.23597.pdf' target='_blank'>https://arxiv.org/pdf/2507.23597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Dong, Longteng Duan, Jie Song, Michael J. Black, Andreas Geiger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23597">MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see https://zj-dong.github.io/MoGA/.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2507.18155.pdf' target='_blank'>https://arxiv.org/pdf/2507.18155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SeungJun Moon, Hah Min Lew, Seungeun Lee, Ji-Su Kang, Gyeong-Moon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18155">GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2507.16562.pdf' target='_blank'>https://arxiv.org/pdf/2507.16562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megha Quamara, Viktor Schmuck, Cristina Iani, Axel Primavesi, Alexander Plaum, Luca Vigano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16562">Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the findings of a user study that evaluated the social acceptance of eXtended Reality (XR) agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves user interaction with a virtual avatar, enabled by a modular toolkit. The interactions are designed to provide tailored training for journalists in digital-remote settings, especially for sensitive or dangerous scenarios, without requiring specialized end-user equipment like headsets. Our research adapts and extends the Almere model, representing social acceptance through existing attributes such as perceived ease of use and perceived usefulness, along with added ones like dependability and security in the user-agent interaction. The XR agent was tested through a controlled experiment in a real-world setting, with data collected on users' perceptions. Our findings, based on quantitative and qualitative measurements involving questionnaires, contribute to the understanding of user perceptions and acceptance of XR agent solutions within a specific social context, while also identifying areas for the improvement of XR systems.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2507.14083.pdf' target='_blank'>https://arxiv.org/pdf/2507.14083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara Abdulaziz, Egor Bondarev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14083">Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in deep learning have improved anomaly detection in surveillance videos, yet they raise urgent privacy concerns due to the collection of sensitive human data. In this paper, we present a comprehensive analysis of anomaly detection performance under four human anonymization techniques, including blurring, masking, encryption, and avatar replacement, applied to the UCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU, BN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method responds to different obfuscation techniques. Experimental results demonstrate that anomaly detection remains viable under anonymized data and is dependent on the algorithmic design and the learning strategy. For instance, under certain anonymization patterns, such as encryption and masking, some models inadvertently achieve higher AUC performance compared to raw data, due to the strong responsiveness of their algorithmic components to these noise patterns. These results highlight the algorithm-specific sensitivities to anonymization and emphasize the trade-off between preserving privacy and maintaining detection utility. Furthermore, we compare these conventional anonymization techniques with the emerging privacy-by-design solutions, highlighting an often overlooked trade-off between robust privacy protection and utility flexibility. Through comprehensive experiments and analyses, this study provides a compelling benchmark and insights into balancing human privacy with the demands of anomaly detection.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2507.08624.pdf' target='_blank'>https://arxiv.org/pdf/2507.08624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>GÃ¡bor Baranyi, Zsolt Csibi, Kristian Fenech, Ãron FÃ³thi, ZsÃ³fia GaÃ¡l, Joul Skaf, AndrÃ¡s LÅrincz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08624">Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS) framework, an advanced artificial intelligence-based solution tailored for home rehabilitation environments. AIRS integrates cutting-edge technologies, including Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and large Vision-Language Models (VLMs), to create a comprehensive system for machine-guided physical rehabilitation. The general AIRS framework is demonstrated in rehabilitation scenarios following total knee replacement (TKR), utilizing a database of 263 video recordings for evaluation. A smartphone is employed within AIRS to perform RT-3DR of living spaces and has a body-matched avatar to provide visual feedback about the excercise. This avatar is necessary in (a) optimizing exercise configurations, including camera placement, patient positioning, and initial poses, and (b) addressing privacy concerns and promoting compliance with the AI Act. The system guides users through the recording process to ensure the collection of properly recorded videos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling direct comparisons between prerecorded clinical exercises and patient home recordings and (ii) VLM-generated feedback, providing detailed explanations and corrections for exercise errors. The framework also supports people with visual and hearing impairments. It also features a modular design that can be adapted to broader rehabilitation contexts. AIRS software components are available for further use and customization.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2506.23739.pdf' target='_blank'>https://arxiv.org/pdf/2506.23739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lisa Marie Otto, Michael Kaiser, Daniel Seebacher, Steffen MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23739">Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe and realistic interactions between automated driving systems and vulnerable road users (VRUs) in urban environments requires advanced testing methodologies. This paper presents a test environment that combines a Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the feasibility of cyber-physical (CP) testing of vehicle-pedestrian and vehicle-cyclist interactions. Building upon previous work focused on pedestrian localization, we further validate a human pose estimation (HPE) approach through a comparative analysis of real-world (RW) and virtual representations of VRUs. The study examines the perception of full-body motion using a commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is generated in Unreal Engine 5, where VRUs are animated in real time and projected onto a screen to stimulate the camera. The proposed stimulation technique ensures the correct perspective, enabling realistic vehicle perception. To assess the accuracy and consistency of HPE across RW and CP domains, we analyze the reliability of detections as well as variations in movement trajectories and joint estimation stability. The validation includes dynamic test scenarios where human avatars, both walking and cycling, are monitored under controlled conditions. Our results show a strong alignment in HPE between RW and CP test conditions for stable motion patterns, while notable inaccuracies persist under dynamic movements and occlusions, particularly for complex cyclist postures. These findings contribute to refining CP testing approaches for evaluating next-generation AI-based vehicle perception and to enhancing interaction models of automated vehicles and VRUs in CP environments.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2506.21780.pdf' target='_blank'>https://arxiv.org/pdf/2506.21780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anya Osborne, Sabrina Fielder, Lee Taber, Tara Lamb, Joshua McVeigh-Schultz, Katherine Isbister
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21780">Avatars and Environments for Meetings in Social VR: What Styles and Choices Matter to People in Group Creativity Tasks?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the COVID-19 pandemic, many professional entities shifted toward remote collaboration and video conferencing (VC) tools. Social virtual reality (VR) platforms present an alternative to VC for meetings and collaborative activities. Well-crafted social VR environments could enhance feelings of co-presence and togetherness at meetings, helping reduce the need for carbon-intensive travel to face-to-face meetings. This research contributes to creating meeting tools in VR by exploring the effects of avatar styles and virtual environments on groups creative performance using the Mozilla Hubs platform. We present the results of two sequential studies. Study One surveys avatar and environment preferences in various VR meeting contexts (N=87). Study Two applies these findings to the design of a between-subjects and within-subjects research where participants (N=40) perform creativity tasks in pairs as embodied avatars in different virtual settings using VR headsets. We discuss the design implications of avatar appearances and meeting settings on teamwork.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2506.09411.pdf' target='_blank'>https://arxiv.org/pdf/2506.09411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaclav Knapp, Matyas Bohacek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09411">Synthetic Human Action Video Data Generation with Pose Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In video understanding tasks, particularly those involving human motion, synthetic data generation often suffers from uncanny features, diminishing its effectiveness for training. Tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving have thus been unable to exploit the full potential of synthetic data. This paper proposes a method for generating synthetic human action video data using pose transfer (specifically, controllable 3D Gaussian avatar models). We evaluate this method on the Toyota Smarthome and NTU RGB+D datasets and show that it improves performance in action recognition tasks. Moreover, we demonstrate that the method can effectively scale few-shot datasets, making up for groups underrepresented in the real training data and adding diverse backgrounds. We open-source the method along with RANDOM People, a dataset with videos and avatars of novel human identities for pose transfer crowd-sourced from the internet.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2506.06271.pdf' target='_blank'>https://arxiv.org/pdf/2506.06271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Schmidt, Simon Giebenhain, Matthias Niessner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06271">BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce BecomingLit, a novel method for reconstructing relightable, high-resolution head avatars that can be rendered from novel viewpoints at interactive rates. Therefore, we propose a new low-cost light stage capture setup, tailored specifically towards capturing faces. Using this setup, we collect a novel dataset consisting of diverse multi-view sequences of numerous subjects under varying illumination conditions and facial expressions. By leveraging our new dataset, we introduce a new relightable avatar representation based on 3D Gaussian primitives that we animate with a parametric head model and an expression-dependent dynamics module. We propose a new hybrid neural shading approach, combining a neural diffuse BRDF with an analytical specular term. Our method reconstructs disentangled materials from our dynamic light stage recordings and enables all-frequency relighting of our avatars with both point lights and environment maps. In addition, our avatars can easily be animated and controlled from monocular videos. We validate our approach in extensive experiments on our dataset, where we consistently outperform existing state-of-the-art methods in relighting and reenactment by a significant margin.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2506.03099.pdf' target='_blank'>https://arxiv.org/pdf/2506.03099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chetwin Low, Weimin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03099">TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2505.05679.pdf' target='_blank'>https://arxiv.org/pdf/2505.05679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>QiHong Chen, Lianghao Jiang, Iftekhar Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05679">From Bias To Improved Prompts: A Case Study of Bias Mitigation of Clone Detection Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The issue of clone code has persisted in software engineering, primarily because developers often copy and paste code segments. This common practice has elevated the importance of clone code detection, garnering attention from both software engineering researchers and industry professionals. Their collective concern arises from the potential negative impacts that clone code can have on software quality. The emergence of powerful Generative Large Language Models (LLMs) like ChatGPT has exacerbated the clone code problem. These advanced models possess code generation capabilities that can inadvertently create code clones. As a result, the need to detect clone code has become more critical than ever before. In this study, we assess the suitability of LLMs for clone code detection. Our results demonstrate that the Palm model achieved a high F1 score of 89.30 for the avatar dataset and 86.41 for the poolC dataset. A known issue with LLMs is their susceptibility to prompt bias, where the performance of these models fluctuates based on the input prompt provided. In our research, we delve deeper into the reasons behind these fluctuations and propose a framework to mitigate prompt bias for clone detection. Our analysis identifies eight distinct categories of prompt bias, and our devised approach leveraging these biases yields a significant improvement of up to 10.81% in the F1 score. These findings underscore the substantial impact of prompt bias on the performance of LLMs and highlight the potential for leveraging model errors to alleviate this bias.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2505.00287.pdf' target='_blank'>https://arxiv.org/pdf/2505.00287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masanori Takano, Kenji Yokotani, Takahiro Kato, Nobuhito Abe, Fumiaki Taka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00287">Avatar Communication Provides More Efficient Online Social Support Than Text Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online communication via avatars provides a richer online social experience than text communication. This reinforces the importance of online social support. Online social support is effective for people who lack social resources because of the anonymity of online communities. We aimed to understand online social support via avatars and their social relationships to provide better social support to avatar users. Therefore, we administered a questionnaire to three avatar communication service users (Second Life, ZEPETO, and Pigg Party) and three text communication service users (Facebook, X, and Instagram) (N=8,947). There was no duplication of users for each service. By comparing avatar and text communication users, we examined the amount of online social support, stability of online relationships, and the relationships between online social support and offline social resources (e.g., offline social support). We observed that avatar communication service users received more online social support, had more stable relationships, and had fewer offline social resources than text communication service users. However, the positive association between online and offline social support for avatar communication users was more substantial than for text communication users. These findings highlight the significance of realistic online communication experiences through avatars, including nonverbal and real-time interactions with co-presence. The findings also highlighted avatar communication service users' problems in the physical world, such as the lack of offline social resources. This study suggests that enhancing online social support through avatars can address these issues. This could help resolve social resource problems, both online and offline in future metaverse societies.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2504.20403.pdf' target='_blank'>https://arxiv.org/pdf/2504.20403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Liu, Yifang Men, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20403">Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2504.12999.pdf' target='_blank'>https://arxiv.org/pdf/2504.12999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rendong Zhang, Alexandra Watkins, Nilanjan Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12999">GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation with Unity Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic avatars have become essential for immersive applications in virtual reality (VR) and augmented reality (AR), enabling lifelike interactions in areas such as training simulations, telemedicine, and virtual collaboration. These avatars bridge the gap between the physical and digital worlds, improving the user experience through realistic human representation. However, existing avatar creation techniques face significant challenges, including high costs, long creation times, and limited utility in virtual applications. Manual methods, such as MetaHuman, require extensive time and expertise, while automatic approaches, such as NeRF-based pipelines often lack efficiency, detailed facial expression fidelity, and are unable to be rendered at a speed sufficent for real-time applications. By involving several cutting-edge modern techniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar creation pipeline that leverages monocular video input to create a scalable and efficient photorealistic avatar directly compatible with the Unity game engine. Our pipeline incorporates a novel Gaussian splatting technique with customized preprocessing that enables the user of "in the wild" monocular video capture, detailed facial expression reconstruction and embedding within a fully rigged avatar model. Additionally, we present a Unity-integrated Gaussian Splatting Avatar Editor, offering a user-friendly environment for VR/AR application development. Experimental results validate the effectiveness of our preprocessing pipeline in standardizing custom data for 3DGS training and demonstrate the versatility of Gaussian avatars in Unity, highlighting the scalability and practicality of our approach.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2504.08296.pdf' target='_blank'>https://arxiv.org/pdf/2504.08296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, Praagya Bahuguna, Mark Chan, Khushi Hora, Lijian Yang, Yongqi Liang, Runhe Bian, Yunlei Liu, Isabela Campillo Valencia, Patricia Morales Tredinick, Ilia Kozlov, Sijia Jiang, Peiwen Huang, Na Chen, Xuanxuan Liu, Anyi Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08296">Generative AI for Film Creation: A Survey of Recent Advances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI (GenAI) is transforming filmmaking, equipping artists with tools like text-to-image and image-to-video diffusion, neural radiance fields, avatar generation, and 3D synthesis. This paper examines the adoption of these technologies in filmmaking, analyzing workflows from recent AI-driven films to understand how GenAI contributes to character creation, aesthetic styling, and narration. We explore key strategies for maintaining character consistency, achieving stylistic coherence, and ensuring motion continuity. Additionally, we highlight emerging trends such as the growing use of 3D generation and the integration of real footage with AI-generated elements.
  Beyond technical advancements, we examine how GenAI is enabling new artistic expressions, from generating hard-to-shoot footage to dreamlike diffusion-based morphing effects, abstract visuals, and unworldly objects. We also gather artists' feedback on challenges and desired improvements, including consistency, controllability, fine-grained editing, and motion refinement. Our study provides insights into the evolving intersection of AI and filmmaking, offering a roadmap for researchers and artists navigating this rapidly expanding field.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2503.18606.pdf' target='_blank'>https://arxiv.org/pdf/2503.18606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Molin Li, Xiaoyu Chang, Kexue Fu, Richard William Allen, RAY LC
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18606">"Becoming My Own Audience": How Dancers React to Avatars Unlike Themselves in Motion Capture-Supported Live Improvisational Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of motion capture in live dance performances has created an emerging discipline enabling dancers to play different avatars on the digital stage. Unlike classical workflows, avatars enable performers to act as different characters in customized narratives, but research has yet to address how movement, improvisation, and perception change when dancers act as avatars. We created five avatars representing differing genders, shapes, and body limitations, and invited 15 dancers to improvise with each in practice and performance settings. Results show that dancers used avatars to distance themselves from their own habitual movements, exploring new ways of moving through differing physical constraints. Dancers explored using gender-stereotyped movements like powerful or feminine actions, experimenting with gender identity. However, focusing on avatars can coincide with a lack of continuity in improvisation. This work shows how emerging practices with performance technology enable dancers to improvise with new constraints, stepping outside the classical stage.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2503.18408.pdf' target='_blank'>https://arxiv.org/pdf/2503.18408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Wu, Ruiqi Zhang, Jie Chen, Hui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18408">Fast and Physically-based Neural Explicit Surface for Relightable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling relightable human avatars from sparse-view videos is crucial for AR/VR applications. Current methods use neural implicit representations to capture dynamic geometry and reflectance, which incur high costs due to the need for dense sampling in volume rendering. To overcome these challenges, we introduce Physically-based Neural Explicit Surface (PhyNES), which employs compact neural material maps based on the Neural Explicit Surface (NES) representation. PhyNES organizes human models in a compact 2D space, enhancing material disentanglement efficiency. By connecting Signed Distance Fields to explicit surfaces, PhyNES enables efficient geometry inference around a parameterized human shape model. This approach models dynamic geometry, texture, and material maps as 2D neural representations, enabling efficient rasterization. PhyNES effectively captures physical surface attributes under varying illumination, enabling real-time physically-based rendering. Experiments show that PhyNES achieves relighting quality comparable to SOTA methods while significantly improving rendering speed, memory efficiency, and reconstruction quality.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2503.16432.pdf' target='_blank'>https://arxiv.org/pdf/2503.16432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young-Ho Bae, Casey C. Bennett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16432">Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game "Dont Starve Together", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2503.15100.pdf' target='_blank'>https://arxiv.org/pdf/2503.15100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristina Fiani, Pejman Saeghe, Mark McGill, Mohamed Khamis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15100">Exploring the Perspectives of Social VR-Aware Non-Parent Adults and Parents on Children's Use of Social Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social Virtual Reality (VR), where people meet in virtual spaces via 3D avatars, is used by children and adults alike. Children experience new forms of harassment in social VR where it is often inaccessible to parental oversight. To date, there is limited understanding of how parents and non-parent adults within the child social VR ecosystem perceive the appropriateness of social VR for different age groups and the measures in place to safeguard children. We present results of a mixed-methods questionnaire (N=149 adults, including 79 parents) focusing on encounters with children in social VR and perspectives towards children's use of social VR. We draw novel insights on the frequency of social VR use by children under 13 and current use of, and future aspirations for, child protection interventions. Compared to non-parent adults, parents familiar with social VR propose lower minimum ages and are more likely to allow social VR without supervision. Adult users experience immaturity from children in social VR, while children face abuse, encounter age-inappropriate behaviours and self-disclose to adults. We present directions to enhance the safety of social VR through pre-planned controls, real-time oversight, post-event insight and the need for evidence-based guidelines to support parents and platforms around age-appropriate interventions.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2503.13121.pdf' target='_blank'>https://arxiv.org/pdf/2503.13121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebin Lee, Yeonho Cho, Jungjin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13121">Concert Interaction Translation: Augmenting VR Live Concert Experience using Chat-Driven Artificial Collective Reactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-mediated concerts can be enjoyed on various devices, from desktop and mobile to VR devices, often supporting multiple devices simultaneously. However, due to the limited accessibility of VR devices, relatively small audience members tend to congregate in VR venues, resulting in diminished unique social experiences. To address this gap and enrich VR concert experiences, we present a novel approach that leverages non-VR user interaction data, specifically chat from audiences watching the same content on a live-streaming platform. Based on an analysis of audience reactions in offline concerts, we designed and prototyped a concert interaction translation system that extracts the level of engagement and emotions from chats and translates them to collective movements, cheers, and singalongs of virtual audience avatars in a VR venue. Our user study (n=48) demonstrates that our system, which combines both movement and audio reactions, significantly enhances the sense of immersion and co-presence than the previous method.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2503.00842.pdf' target='_blank'>https://arxiv.org/pdf/2503.00842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daye Kim, Sebin Lee, Yoonseo Jun, Yujin Shin, Jungjin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00842">VTuber's Atelier: The Design Space, Challenges, and Opportunities for VTubing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VTubing, the practice of live streaming using virtual avatars, has gained worldwide popularity among streamers seeking to maintain anonymity. While previous research has primarily focused on the social and cultural aspects of VTubing, there is a noticeable lack of studies examining the practical challenges VTubers face in creating and operating their avatars. To address this gap, we surveyed VTubers' equipment and expanded the live-streaming design space by introducing six new dimensions related to avatar creation and control. Additionally, we conducted interviews with 16 professional VTubers to comprehensively explore their practices, strategies, and challenges throughout the VTubing process. Our findings reveal that VTubers face significant burdens compared to real-person streamers due to fragmented tools and the multi-tasking nature of VTubing, leading to unique workarounds. Finally, we summarize these challenges and propose design opportunities to improve the effectiveness and efficiency of VTubing.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2502.20577.pdf' target='_blank'>https://arxiv.org/pdf/2502.20577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MD Wahiduzzaman Khan, Mingshan Jia, Xiaolin Zhang, En Yu, Caifeng Shan, Kaska Musial-Gabrys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20577">InstaFace: Identity-Preserving Facial Editing with Single Image Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial appearance editing is crucial for digital avatars, AR/VR, and personalized content creation, driving realistic user experiences. However, preserving identity with generative models is challenging, especially in scenarios with limited data availability. Traditional methods often require multiple images and still struggle with unnatural face shifts, inconsistent hair alignment, or excessive smoothing effects. To overcome these challenges, we introduce a novel diffusion-based framework, InstaFace, to generate realistic images while preserving identity using only a single image. Central to InstaFace, we introduce an efficient guidance network that harnesses 3D perspectives by integrating multiple 3DMM-based conditionals without introducing additional trainable parameters. Moreover, to ensure maximum identity retention as well as preservation of background, hair, and other contextual features like accessories, we introduce a novel module that utilizes feature embeddings from a facial recognition model and a pre-trained vision-language model. Quantitative evaluations demonstrate that our method outperforms several state-of-the-art approaches in terms of identity preservation, photorealism, and effective control of pose, expression, and lighting.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2502.11642.pdf' target='_blank'>https://arxiv.org/pdf/2502.11642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyumin Shim, Sangmin Lee, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11642">GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce GaussianMotion, a novel human rendering model that generates fully animatable scenes aligned with textual descriptions using Gaussian Splatting. Although existing methods achieve reasonable text-to-3D generation of human bodies using various 3D representations, they often face limitations in fidelity and efficiency, or primarily focus on static models with limited pose control. In contrast, our method generates fully animatable 3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score distillation, achieving high fidelity and efficient rendering for arbitrary poses. By densely generating diverse random poses during optimization, our deformable 3D human model learns to capture a wide range of natural motions distilled from a pose-conditioned diffusion model in an end-to-end manner. Furthermore, we propose Adaptive Score Distillation that effectively balances realistic detail and smoothness to achieve optimal 3D results. Experimental results demonstrate that our approach outperforms existing baselines by producing high-quality textures in both static and animated results, and by generating diverse 3D human models from various textual inputs.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2502.09617.pdf' target='_blank'>https://arxiv.org/pdf/2502.09617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wen, Alexander G. Schwing, Shenlong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09617">LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable rendering of an animatable human avatar from sparse inputs relies on data priors and inductive biases extracted from training on large data to avoid scene-specific optimization and to enable fast reconstruction. This raises two main challenges: First, unlike iterative gradient-based adjustment in scene-specific optimization, generalizable methods must reconstruct the human shape representation in a single pass at inference time. Second, rendering is preferably computationally efficient yet of high resolution. To address both challenges we augment the recently proposed dual shape representation, which combines the benefits of a mesh and Gaussian points, in two ways. To improve reconstruction, we propose an iterative feedback update framework, which successively improves the canonical human shape representation during reconstruction. To achieve computationally efficient yet high-resolution rendering, we study a coupled-multi-resolution Gaussians-on-Mesh representation. We evaluate the proposed approach on the challenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an animatable representation from sparse inputs in less than 1s, renders views with 95.1FPS at $1024 \times 1024$, and achieves PSNR/LPIPS*/FID of 24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in rendering quality.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2502.07844.pdf' target='_blank'>https://arxiv.org/pdf/2502.07844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fujiao Ju, Yuxuan Wang, Shuo Wang, Chengyin Wang, Yinbo Chen, Jianfeng Li, Mingjie Dong, Bin Fang, Qianyu Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07844">The establishment of static digital humans and the integration with spinal models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adolescent idiopathic scoliosis (AIS), a prevalent spinal deformity, significantly affects individuals' health and quality of life. Conventional imaging techniques, such as X - rays, computed tomography (CT), and magnetic resonance imaging (MRI), offer static views of the spine. However, they are restricted in capturing the dynamic changes of the spine and its interactions with overall body motion. Therefore, developing new techniques to address these limitations has become extremely important. Dynamic digital human modeling represents a major breakthrough in digital medicine. It enables a three - dimensional (3D) view of the spine as it changes during daily activities, assisting clinicians in detecting deformities that might be missed in static imaging. Although dynamic modeling holds great potential, constructing an accurate static digital human model is a crucial initial step for high - precision simulations. In this study, our focus is on constructing an accurate static digital human model integrating the spine, which is vital for subsequent dynamic digital human research on AIS. First, we generate human point - cloud data by combining the 3D Gaussian method with the Skinned Multi - Person Linear (SMPL) model from the patient's multi - view images. Then, we fit a standard skeletal model to the generated human model. Next, we align the real spine model reconstructed from CT images with the standard skeletal model. We validated the resulting personalized spine model using X - ray data from six AIS patients, with Cobb angles (used to measure the severity of scoliosis) as evaluation metrics. The results indicate that the model's error was within 1 degree of the actual measurements. This study presents an important method for constructing digital humans.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2501.19088.pdf' target='_blank'>https://arxiv.org/pdf/2501.19088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhoutao Sun, Xukun Shen, Yong Hu, Yuyou Zhong, Xueyang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19088">JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since hands are the primary interface in daily interactions, modeling high-quality digital human hands and rendering realistic images is a critical research problem. Furthermore, considering the requirements of interactive and rendering applications, it is essential to achieve real-time rendering and driveability of the digital model without compromising rendering quality. Thus, we propose Jointly 3D Gaussian Hand (JGHand), a novel joint-driven 3D Gaussian Splatting (3DGS)-based hand representation that renders high-fidelity hand images in real-time for various poses and characters. Distinct from existing articulated neural rendering techniques, we introduce a differentiable process for spatial transformations based on 3D key points. This process supports deformations from the canonical template to a mesh with arbitrary bone lengths and poses. Additionally, we propose a real-time shadow simulation method based on per-pixel depth to simulate self-occlusion shadows caused by finger movements. Finally, we embed the hand prior and propose an animatable 3DGS representation of the hand driven solely by 3D key points. We validate the effectiveness of each component of our approach through comprehensive ablation studies. Experimental results on public datasets demonstrate that JGHand achieves real-time rendering speeds with enhanced quality, surpassing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2501.17792.pdf' target='_blank'>https://arxiv.org/pdf/2501.17792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17792">CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through these experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2501.17085.pdf' target='_blank'>https://arxiv.org/pdf/2501.17085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17085">Evaluating CrowdSplat: Perceived Level of Detail for Gaussian Crowds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and realistic crowd rendering is an important element of many real-time graphics applications such as Virtual Reality (VR) and games. To this end, Levels of Detail (LOD) avatar representations such as polygonal meshes, image-based impostors, and point clouds have been proposed and evaluated. More recently, 3D Gaussian Splatting has been explored as a potential method for real-time crowd rendering. In this paper, we present a two-alternative forced choice (2AFC) experiment that aims to determine the perceived quality of 3D Gaussian avatars. Three factors were explored: Motion, LOD (i.e., #Gaussians), and the avatar height in Pixels (corresponding to the viewing distance). Participants viewed pairs of animated 3D Gaussian avatars and were tasked with choosing the most detailed one. Our findings can inform the optimization of LOD strategies in Gaussian-based crowd rendering, thereby helping to achieve efficient rendering while maintaining visual quality in real-time applications.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2412.10430.pdf' target='_blank'>https://arxiv.org/pdf/2412.10430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Wen, Xiang Wen, Hao Jiang, Siqi Yang, Bingfeng Han, Tianlei Hu, Gang Chen, Shuang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10430">Unsupervised Cross-Domain Regression for Fine-grained 3D Game Character Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of the ``metaverse'' and the rapid development of games, it has become more and more critical to reconstruct characters in the virtual world faithfully. The immersive experience is one of the most central themes of the ``metaverse'', while the reducibility of the avatar is the crucial point. Meanwhile, the game is the carrier of the metaverse, in which players can freely edit the facial appearance of the game character. In this paper, we propose a simple but powerful cross-domain framework that can reconstruct fine-grained 3D game characters from single-view images in an end-to-end manner. Different from the previous methods, which do not resolve the cross-domain gap, we propose an effective regressor that can greatly reduce the discrepancy between the real-world domain and the game domain. To figure out the drawbacks of no ground truth, our unsupervised framework has accomplished the knowledge transfer of the target domain. Additionally, an innovative contrastive loss is proposed to solve the instance-wise disparity, which keeps the person-specific details of the reconstructed character. In contrast, an auxiliary 3D identity-aware extractor is activated to make the results of our model more impeccable. Then a large set of physically meaningful facial parameters is generated robustly and exquisitely. Experiments demonstrate that our method yields state-of-the-art performance in 3D game character reconstruction.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2412.03878.pdf' target='_blank'>https://arxiv.org/pdf/2412.03878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sudha Krishnamurthy, Vimal Bhat, Abhinav Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03878">DiffSign: AI-Assisted Generation of Customizable Sign Language Videos With Enhanced Realism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of several streaming services in recent years has now made it possible for a diverse audience across the world to view the same media content, such as movies or TV shows. While translation and dubbing services are being added to make content accessible to the local audience, the support for making content accessible to people with different abilities, such as the Deaf and Hard of Hearing (DHH) community, is still lagging. Our goal is to make media content more accessible to the DHH community by generating sign language videos with synthetic signers that are realistic and expressive. Using the same signer for a given media content that is viewed globally may have limited appeal. Hence, our approach combines parametric modeling and generative modeling to generate realistic-looking synthetic signers and customize their appearance based on user preferences. We first retarget human sign language poses to 3D sign language avatars by optimizing a parametric model. The high-fidelity poses from the rendered avatars are then used to condition the poses of synthetic signers generated using a diffusion-based generative model. The appearance of the synthetic signer is controlled by an image prompt supplied through a visual adapter. Our results show that the sign language videos generated using our approach have better temporal consistency and realism than signing videos generated by a diffusion model conditioned only on text prompts. We also support multimodal prompts to allow users to further customize the appearance of the signer to accommodate diversity (e.g. skin tone, gender). Our approach is also useful for signer anonymization.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2411.15732.pdf' target='_blank'>https://arxiv.org/pdf/2411.15732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyang Qian, Yuan Sun, Yu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15732">DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and Precise Editing with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating and editing dynamic 3D head avatars are crucial tasks in virtual reality and film production. However, existing methods often suffer from facial distortions, inaccurate head movements, and limited fine-grained editing capabilities. To address these challenges, we present DynamicAvatars, a dynamic model that generates photorealistic, moving 3D head avatars from video clips and parameters associated with facial positions and expressions. Our approach enables precise editing through a novel prompt-based editing model, which integrates user-provided prompts with guiding parameters derived from large language models (LLMs). To achieve this, we propose a dual-tracking framework based on Gaussian Splatting and introduce a prompt preprocessing module to enhance editing stability. By incorporating a specialized GAN algorithm and connecting it to our control module, which generates precise guiding parameters from LLMs, we successfully address the limitations of existing methods. Additionally, we develop a dynamic editing strategy that selectively utilizes specific training datasets to improve the efficiency and adaptability of the model for dynamic editing tasks.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2411.15583.pdf' target='_blank'>https://arxiv.org/pdf/2411.15583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yawen Zhang, Han Zhou, Zhoumingju Jiang, Zilu Tang, Tao Luo, Qinyuan Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15583">Exploring Viewing Modalities in Cinematic Virtual Reality: A Systematic Review and Meta-Analysis of Challenges in Evaluating User Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cinematic Virtual Reality (CVR) is a narrative-driven VR experience that uses head-mounted displays with a 360-degree field of view. Previous research has explored different viewing modalities to enhance viewers' CVR experience. This study conducted a systematic review and meta-analysis focusing on how different viewing modalities, including intervened rotation, avatar assistance, guidance cues, and perspective shifting, influence the CVR experience. The study has screened 3444 papers (between 01/01/2013 and 17/06/2023) and selected 45 for systematic review, 13 of which also for meta-analysis. We conducted separate random-effects meta-analysis and applied Robust Variance Estimation to examine CVR viewing modalities and user experience outcomes. Evidence from experiments was synthesized as differences between standardized mean differences (SMDs) of user experience of control group ("Swivel-Chair" CVR) and experiment groups. To our surprise, we found inconsistencies in the effect sizes across different studies, even with the same viewing modalities. Moreover, in these studies, terms such as "presence," "immersion," and "narrative engagement" were often used interchangeably. Their irregular use of questionnaires, overreliance on self-developed questionnaires, and incomplete data reporting may have led to unrigorous evaluations of CVR experiences. This study contributes to Human-Computer Interaction (HCI) research by identifying gaps in CVR research, emphasizing the need for standardization of terminologies and methodologies to enhance the reliability and comparability of future CVR research.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2410.15343.pdf' target='_blank'>https://arxiv.org/pdf/2410.15343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Tang Tsui, Yu-Rou Tuan, Jia-You Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15343">POSE: Pose estimation Of virtual Sync Exhibit system</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work is a portable MetaVerse implementation, and we use 3D pose estimation with AI to make virtual avatars do synchronized actions and interact with the environment. The motivation is that we find it inconvenient to use joysticks and sensors when playing with fitness rings. In order to replace joysticks and reduce costs, we developed a platform that can control virtual avatars through pose estimation to identify the movements of real people, and we also implemented a multi-process to achieve modularization and reduce the overall latency.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2409.18083.pdf' target='_blank'>https://arxiv.org/pdf/2409.18083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mirela Ostrek, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18083">Stable Video Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in the field of generative AI and text-to-image methods in particular have transformed the way we interact with and perceive computer-generated imagery today. In parallel, much progress has been made in 3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we present SVP, a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific fine-tuning of a general 2D stable diffusion model which we lift to a video model by providing temporal 3DMM sequences as conditioning and by introducing a temporal denoising procedure. As an output, this model generates temporally smooth imagery of a person with 3DMM-based controls, i.e., a person-specific avatar. The facial appearance of this person-specific avatar can be edited and morphed to text-defined celebrities, without any fine-tuning at test time. The method is analyzed quantitatively and qualitatively, and we show that our method outperforms state-of-the-art monocular head avatar methods.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2409.16147.pdf' target='_blank'>https://arxiv.org/pdf/2409.16147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16147">Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars, providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements, the creation of controllable 3DGS-based head avatars remains time-intensive, often requiring tens of minutes to hours. To expedite this process, we here introduce the "Gaussian Deja-vu" framework, which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing, we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians, ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods, producing the avatar in minutes.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2409.15904.pdf' target='_blank'>https://arxiv.org/pdf/2409.15904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuqiao Li, Julian Chibane, Yannan He, Naama Pearl, Andreas Geiger, Gerard Pons-moll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15904">Unimotion: Unifying 3D Human Motion Synthesis and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Unimotion, the first unified multi-task human motion model capable of both flexible motion control and frame-level motion understanding. While existing works control avatar motion with global text conditioning, or with fine-grained per frame scripts, none can do both at once. In addition, none of the existing works can output frame-level text paired with the generated poses. In contrast, Unimotion allows to control motion with global text, or local frame-level text, or both at once, providing more flexible control for users. Importantly, Unimotion is the first model which by design outputs local text paired with the generated poses, allowing users to know what motion happens and when, which is necessary for a wide range of applications. We show Unimotion opens up new applications: 1.) Hierarchical control, allowing users to specify motion at different levels of detail, 2.) Obtaining motion text descriptions for existing MoCap data or YouTube videos 3.) Allowing for editability, generating motion from text, and editing the motion via text edits. Moreover, Unimotion attains state-of-the-art results for the frame-level text-to-motion task on the established HumanML3D dataset. The pre-trained model and code are available available on our project page at https://coral79.github.io/uni-motion/.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2409.10850.pdf' target='_blank'>https://arxiv.org/pdf/2409.10850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyong Zhang, Kedi Yang, Youliang Tian, Jianfeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10850">An Anti-disguise Authentication System Using the First Impression of Avatar in Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse is a vast virtual world parallel to the physical world, where the user acts as an avatar to enjoy various services that break through the temporal and spatial limitations of the physical world. Metaverse allows users to create arbitrary digital appearances as their own avatars by which an adversary may disguise his/her avatar to fraud others. In this paper, we propose an anti-disguise authentication method that draws on the idea of the first impression from the physical world to recognize an old friend. Specifically, the first meeting scenario in the metaverse is stored and recalled to help the authentication between avatars. To prevent the adversary from replacing and forging the first impression, we construct a chameleon-based signcryption mechanism and design a ciphertext authentication protocol to ensure the public verifiability of encrypted identities. The security analysis shows that the proposed signcryption mechanism meets not only the security requirement but also the public verifiability. Besides, the ciphertext authentication protocol has the capability of defending against the replacing and forging attacks on the first impression. Extensive experiments show that the proposed avatar authentication system is able to achieve anti-disguise authentication at a low storage consumption on the blockchain.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2409.05496.pdf' target='_blank'>https://arxiv.org/pdf/2409.05496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Surya Kalvakolu, Heinrich SÃ¶bke, Jannicke Baalsrud Hauge, Eckhard Kraft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05496">Educational Virtual Field Trips based on Social VR and 360Â° Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual field trips (VFTs) have proven to be valuable learning tools. Such applications are mostly based on 360Â° technology and are to be characterized as single-user applications in technological terms. In contrast, Social VR applications are characterized by multi-user capability and user-specific avatars. From a learning perspective, the concepts of collaborative learning and embodiment have long been proposed as conducive to learning. Both concepts might be supported using Social VR. However, little is currently known about the use of Social VR for VFTs. Accordingly, the research questions are to what extent VFTs can be implemented in Social VR environments and how these Social VR-based VFTs are perceived by learners. This article presents an evaluation study on the development and evaluation of a VFT environment using the Social VR platform Mozilla Hubs. It describes the design decisions to create the environment and evaluation results from a mixed-method study (N=16) using a questionnaire and focus group discussions. The study highlighted the opportunities offered by Social VR-based VFTs but also revealed several challenges that need to be addressed to embrace the potential of Social VR-based VFTs to be utilized regularly in education.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2409.00012.pdf' target='_blank'>https://arxiv.org/pdf/2409.00012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanhyuk Park, Jungbin Cho, Junwan Kim, Seongmin Lee, Jungsu Kim, Sanghoon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00012">AVIN-Chat: An Audio-Visual Interactive Chatbot System with Emotional State Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents an audio-visual interactive chatbot (AVIN-Chat) system that allows users to have face-to-face conversations with 3D avatars in real-time. Compared to the previous chatbot services, which provide text-only or speech-only communications, the proposed AVIN-Chat can offer audio-visual communications providing users with a superior experience quality. In addition, the proposed AVIN-Chat emotionally speaks and expresses according to the user's emotional state. Thus, it enables users to establish a strong bond with the chatbot system, increasing the user's immersion. Through user subjective tests, it is demonstrated that the proposed system provides users with a higher sense of immersion than previous chatbot systems. The demonstration video is available at https://www.youtube.com/watch?v=Z74uIV9k7_k.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2408.17121.pdf' target='_blank'>https://arxiv.org/pdf/2408.17121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kedi Yang, Zhenyong Zhang, Youliang Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17121">Traceable AI-driven Avatars Using Multi-factors of Physical World and Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse allows users to delegate their AI models to an AI engine, which builds corresponding AI-driven avatars to provide immersive experience for other users. Since current authentication methods mainly focus on human-driven avatars and ignore the traceability of AI-driven avatars, attackers may delegate the AI models of a target user to an AI proxy program to perform impersonation attacks without worrying about being detected.
  In this paper, we propose an authentication method using multi-factors to guarantee the traceability of AI-driven avatars. Firstly, we construct a user's identity model combining the manipulator's iris feature and the AI proxy's public key to ensure that an AI-driven avatar is associated with its original manipulator. Secondly, we propose a chameleon proxy signature scheme that supports the original manipulator to delegate his/her signing ability to an AI proxy. Finally, we design three authentication protocols for avatars based on the identity model and the chameleon proxy signature to guarantee the virtual-to-physical traceability including both the human-driven and AI-driven avatars.
  Security analysis shows that the proposed signature scheme is unforgeability and the authentication method is able to defend against false accusation. Extensive evaluations show that the designed authentication protocols complete user login, avatar delegation, mutual authentication, and avatar tracing in about 1s, meeting the actual application needs and helping to mitigate impersonation attacks by AI-driven avatars.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2408.11769.pdf' target='_blank'>https://arxiv.org/pdf/2408.11769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Nazemi, Bara Rababah, Daniel Ramos, Tangxu Zhao, Bilal Farooq
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11769">Decoding Pedestrian Stress on Urban Streets using Electrodermal Activity Monitoring in Virtual Immersive Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pedestrian stress level is shown to significantly influence human cognitive processes and, subsequently, decision-making, e.g., the decision to select a gap and cross a street. This paper systematically studies the stress experienced by a pedestrian when crossing a street under different experimental manipulations by monitoring the ElectroDermal Activity (EDA) using the Galvanic Skin Response (GSR) sensor. To fulfil the research objectives, a dynamic and immersive virtual reality (VR) platform was used, which is suitable for eliciting and capturing pedestrian's emotional responses in conjunction with monitoring their EDA. A total of 171 individuals participated in the experiment, tasked to cross a two-way street at mid-block with no signal control. Mixed effects models were employed to compare the influence of socio-demographics, social influence, vehicle technology, environment, road design, and traffic variables on the stress levels of the participants. The results indicated that having a street median in the middle of the road operates as a refuge and significantly reduced stress. Younger participants were (18-24 years) calmer than the relatively older participants (55-65 years). Arousal levels were higher when it came to the characteristics of the avatar (virtual pedestrian) in the simulation, especially for those avatars with adventurous traits. The pedestrian location influenced stress since the stress was higher on the street while crossing than waiting on the sidewalk. Significant causes of arousal were fear of accidents and an actual accident for pedestrians. The estimated random effects show a high degree of physical and mental learning by the participants while going through the scenarios.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2408.07892.pdf' target='_blank'>https://arxiv.org/pdf/2408.07892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steven Adler, ZoÃ« Hitzig, Shrey Jain, Catherine Brewer, Wayne Chang, RenÃ©e DiResta, Eddy Lazzarin, Sean McGregor, Wendy Seltzer, Divya Siddarth, Nouran Soliman, Tobin South, Connor Spelliscy, Manu Sporny, Varya Srivastava, John Bailey, Brian Christian, Andrew Critch, Ronnie Falcon, Heather Flanagan, Kim Hamilton Duffy, Eric Ho, Claire R. Leibowicz, Srikanth Nadhamuni, Alan Z. Rozenshtein, David Schnurr, Evan Shapiro, Lacey Strahm, Andrew Trask, Zoe Weinberg, Cedric Whitney, Tom Zick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07892">Personhood credentials: Artificial intelligence and the value of privacy-preserving tools to distinguish who is real online</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anonymity is an important principle online. However, malicious actors have long used misleading identities to conduct fraud, spread disinformation, and carry out other deceptive schemes. With the advent of increasingly capable AI, bad actors can amplify the potential scale and effectiveness of their operations, intensifying the challenge of balancing anonymity and trustworthiness online. In this paper, we analyze the value of a new tool to address this challenge: "personhood credentials" (PHCs), digital credentials that empower users to demonstrate that they are real people -- not AIs -- to online services, without disclosing any personal information. Such credentials can be issued by a range of trusted institutions -- governments or otherwise. A PHC system, according to our definition, could be local or global, and does not need to be biometrics-based. Two trends in AI contribute to the urgency of the challenge: AI's increasing indistinguishability from people online (i.e., lifelike content and avatars, agentic activity), and AI's increasing scalability (i.e., cost-effectiveness, accessibility). Drawing on a long history of research into anonymous credentials and "proof-of-personhood" systems, personhood credentials give people a way to signal their trustworthiness on online platforms, and offer service providers new tools for reducing misuse by bad actors. In contrast, existing countermeasures to automated deception -- such as CAPTCHAs -- are inadequate against sophisticated AI, while stringent identity verification solutions are insufficiently private for many use-cases. After surveying the benefits of personhood credentials, we also examine deployment risks and design challenges. We conclude with actionable next steps for policymakers, technologists, and standards bodies to consider in consultation with the public.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2407.21047.pdf' target='_blank'>https://arxiv.org/pdf/2407.21047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akin Caliskan, Berkay Kicanaoglu, Hyeongwoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21047">PAV: Personalized Head Avatar from Unstructured Video Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose PAV, Personalized Head Avatar for the synthesis of human faces under arbitrary viewpoints and facial expressions. PAV introduces a method that learns a dynamic deformable neural radiance field (NeRF), in particular from a collection of monocular talking face videos of the same character under various appearance and shape changes. Unlike existing head NeRF methods that are limited to modeling such input videos on a per-appearance basis, our method allows for learning multi-appearance NeRFs, introducing appearance embedding for each input video via learnable latent neural features attached to the underlying geometry. Furthermore, the proposed appearance-conditioned density formulation facilitates the shape variation of the character, such as facial hair and soft tissues, in the radiance field prediction. To the best of our knowledge, our approach is the first dynamic deformable NeRF framework to model appearance and shape variations in a single unified network for multi-appearances of the same subject. We demonstrate experimentally that PAV outperforms the baseline method in terms of visual rendering quality in our quantitative and qualitative studies on various subjects.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2407.15625.pdf' target='_blank'>https://arxiv.org/pdf/2407.15625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amaury Trujillo, Clara Bacciu, Matteo Abrate
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15625">Dressed to Gamble: How Poker Drives the Dynamics of Wearables and Visits on Decentraland's Social Virtual World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentraland is a blockchain-based social virtual world where users can publish and sell wearables to customize avatars. In it, the third-party Decentral Games (DG) allows players of its flagship game ICE Poker to earn cryptocurrency only if they possess certain wearables. Herein, we present a comprehensive study on how DG and its game influence the dynamics of wearables and in-world visits in Decentraland. To this end, we analyzed 5.9 million wearable transfers made on the Polygon blockchain (and related sales) over a two-year period, and 677 million log events of in-world user positions in an overlapping 10-month period. We found that these activities are disproportionately related to DG, with its ICE Poker casinos (less than 0.1% of the world map) representing a remarkable average share of daily unique visitors (33%) and time spent in the virtual world (20%). Despite several alternative initiatives within Decentraland, ICE Poker appears to drive user activity on the platform. Our work thus contributes to the understanding of how play-to-earn games influence user behavior in social virtual worlds, and it is among the first to study the emerging phenomenon of virtual blockchain-based gambling.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2407.11998.pdf' target='_blank'>https://arxiv.org/pdf/2407.11998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Chen, Heng Wang, Sainan Sun, Zhiyuan Chen, Zhenkun Liu, Shuhua Cao, Li Yang, Minghui Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11998">Custom Cloth Creation and Virtual Try-on for Everyone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This demo showcases a simple tool that utilizes AIGC technology, enabling both professional designers and regular users to easily customize clothing for their digital avatars. Customization options include changing clothing colors, textures, logos, and patterns. Compared with traditional 3D modeling processes, our approach significantly enhances efficiency and interactivity and reduces production costs.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2406.15074.pdf' target='_blank'>https://arxiv.org/pdf/2406.15074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suvadeep Mukherjee, Verena Distler, Gabriele Lenzini, Pedro Cardoso-Leite
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15074">Balancing The Perception of Cheating Detection, Privacy and Fairness: A Mixed-Methods Study of Visual Data Obfuscation in Remote Proctoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote proctoring technology, a cheating-preventive measure, often raises privacy and fairness concerns that may affect test-takers' experiences and the validity of test results. Our study explores how selectively obfuscating information in video recordings can protect test-takers' privacy while ensuring effective and fair cheating detection. Interviews with experts (N=9) identified four key video regions indicative of potential cheating behaviors: the test-taker's face, body, background and the presence of individuals in the background. Experts recommended specific obfuscation methods for each region based on privacy significance and cheating behavior frequency, ranging from conventional blurring to advanced methods like replacement with deepfake, 3D avatars and silhouetting. We then conducted a vignette experiment with potential test-takers (N=259, non-experts) to evaluate their perceptions of cheating detection, visual privacy and fairness, using descriptions and examples of still images for each expert-recommended combination of video regions and obfuscation methods. Our results indicate that the effectiveness of obfuscation methods varies by region. Tailoring remote proctoring with region-specific advanced obfuscation methods can improve the perceptions of privacy and fairness compared to the conventional methods, though it may decrease perceived information sufficiency for detecting cheating. However, non-experts preferred conventional blurring for videos they were more willing to share, highlighting a gap between the perceived effectiveness of the advanced obfuscation methods and their practical acceptance. This study contributes to the field of user-centered privacy by suggesting promising directions to address current remote proctoring challenges and guiding future research.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2406.13093.pdf' target='_blank'>https://arxiv.org/pdf/2406.13093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wuxinlin Cheng, Cheng Wan, Yupeng Cao, Sihan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13093">RITA: A Real-time Interactive Talking Avatars Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RITA presents a high-quality real-time interactive framework built upon generative models, designed with practical applications in mind. Our framework enables the transformation of user-uploaded photos into digital avatars that can engage in real-time dialogue interactions. By leveraging the latest advancements in generative modeling, we have developed a versatile platform that not only enhances the user experience through dynamic conversational avatars but also opens new avenues for applications in virtual reality, online education, and interactive gaming. This work showcases the potential of integrating computer vision and natural language processing technologies to create immersive and interactive digital personas, pushing the boundaries of how we interact with digital content.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2405.02672.pdf' target='_blank'>https://arxiv.org/pdf/2405.02672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Kuffner dos Anjos, JoÃ£o Madeiras Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02672">Effects of Realism and Representation on Self-Embodied Avatars in Immersive Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) has recently gained traction with many new and ever more affordable devices being released. The increase in popularity of this paradigm of interaction has given birth to new applications and has attracted casual consumers to experience VR. Providing a self-embodied representation (avatar) of users' full bodies inside shared virtual spaces can improve the VR experience and make it more engaging to both new and experienced users . This is especially important in fully immersive systems, where the equipment completely occludes the real world making self awareness problematic. Indeed, the feeling of presence of the user is highly influenced by their virtual representations, even though small flaws could lead to uncanny valley side-effects. Following previous research, we would like to assess whether using a third-person perspective could also benefit the VR experience, via an improved spatial awareness of the user's virtual surroundings. In this paper we investigate realism and perspective of self-embodied representation in VR setups in natural tasks, such as walking and avoiding obstacles. We compare both First and Third-Person perspectives with three different levels of realism in avatar representation. These range from a stylized abstract avatar, to a "realistic" mesh-based humanoid representation and a point-cloud rendering. The latter uses data captured via depth-sensors and mapped into a virtual self inside the Virtual Environment. We present a throughout evaluation and comparison of these different representations, describing a series of guidelines for self-embodied VR applications. The effects of the uncanny valley are also discussed in the context of navigation and reflex-based tasks.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2405.01858.pdf' target='_blank'>https://arxiv.org/pdf/2405.01858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salam Michael Singh, Shubhmoy Kumar Garg, Amitesh Misra, Aaditeshwar Seth, Tanmoy Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01858">SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for Sexual Education in Rural India</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sexual education aims to foster a healthy lifestyle in terms of emotional, mental and social well-being. In countries like India, where adolescents form the largest demographic group, they face significant vulnerabilities concerning sexual health. Unfortunately, sexual education is often stigmatized, creating barriers to providing essential counseling and information to this at-risk population. Consequently, issues such as early pregnancy, unsafe abortions, sexually transmitted infections, and sexual violence become prevalent. Our current proposal aims to provide a safe and trustworthy platform for sexual education to the vulnerable rural Indian population, thereby fostering the healthy and overall growth of the nation. In this regard, we strive towards designing SUKHSANDESH, a multi-staged AI-based Question Answering platform for sexual education tailored to rural India, adhering to safety guardrails and regional language support. By utilizing information retrieval techniques and large language models, SUKHSANDESH will deliver effective responses to user queries. We also propose to anonymise the dataset to mitigate safety measures and set AI guardrails against any harmful or unwanted response generation. Moreover, an innovative feature of our proposal involves integrating ``avatar therapy'' with SUKHSANDESH. This feature will convert AI-generated responses into real-time audio delivered by an animated avatar speaking regional Indian languages. This approach aims to foster empathy and connection, which is particularly beneficial for individuals with limited literacy skills. Partnering with Gram Vaani, an industry leader, we will deploy SUKHSANDESH to address sexual education needs in rural India.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2404.03071.pdf' target='_blank'>https://arxiv.org/pdf/2404.03071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kishore Vasan, Marton Karsai, Albert-Laszlo Barabasi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03071">Human Mobility in the Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The metaverse promises a shift in the way humans interact with each other, and with their digital and physical environments. The lack of geographical boundaries and travel costs in the metaverse prompts us to ask if the fundamental laws that govern human mobility in the physical world apply. We collected data on avatar movements, along with their network mobility extracted from NFT purchases. We find that despite the absence of commuting costs, an individuals inclination to explore new locations diminishes over time, limiting movement to a small fraction of the metaverse. We also find a lack of correlation between land prices and visitation, a deviation from the patterns characterizing the physical world. Finally, we identify the scaling laws that characterize meta mobility and show that we need to add preferential selection to the existing models to explain quantitative patterns of metaverse mobility. Our ability to predict the characteristics of the emerging meta mobility network implies that the laws governing human mobility are rooted in fundamental patterns of human dynamics, rather than the nature of space and cost of movement.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2403.18040.pdf' target='_blank'>https://arxiv.org/pdf/2403.18040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanz Cuevas-Velasquez, Alejandro GalÃ¡n-Cuenca, Antonio Javier Gallego, Marcelo Saval-Calvo, Robert B. Fisher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18040">Global Point Cloud Registration Network for Large Transformations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional data registration is an established yet challenging problem that is key in many different applications, such as mapping the environment for autonomous vehicles, and modeling objects and people for avatar creation, among many others. Registration refers to the process of mapping multiple data into the same coordinate system by means of matching correspondences and transformation estimation. Novel proposals exploit the benefits of deep learning architectures for this purpose, as they learn the best features for the data, providing better matches and hence results. However, the state of the art is usually focused on cases of relatively small transformations, although in certain applications and in a real and practical environment, large transformations are very common. In this paper, we present ReLaTo (Registration for Large Transformations), an architecture that faces the cases where large transformations happen while maintaining good performance for local transformations. This proposal uses a novel Softmax pooling layer to find correspondences in a bilateral consensus manner between two point sets, sampling the most confident matches. These matches are used to estimate a coarse and global registration using weighted Singular Value Decomposition (SVD). A target-guided denoising step is then applied to both the obtained matches and latent features, estimating the final fine registration considering the local geometry. All these steps are carried out following an end-to-end approach, which has been shown to improve 10 state-of-the-art registration methods in two datasets commonly used for this task (ModelNet40 and KITTI), especially in the case of large transformations.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2402.08217.pdf' target='_blank'>https://arxiv.org/pdf/2402.08217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kassie Povinelli, Yuhang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08217">Springboard, Roadblock or "Crutch"?: How Transgender Users Leverage Voice Changers for Gender Presentation in Social Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social virtual reality (VR) serves as a vital platform for transgender individuals to explore their identities through avatars and foster personal connections within online communities. However, it presents a challenge: the disconnect between avatar embodiment and voice representation, often leading to misgendering and harassment. Prior research acknowledges this issue but overlooks the potential solution of voice changers. We interviewed 13 transgender and gender-nonconforming users of social VR platforms, focusing on their experiences with and without voice changers. We found that using a voice changer not only reduces voice-related harassment, but also allows them to experience gender euphoria through both hearing their modified voice and the reactions of others to their modified voice, motivating them to pursue voice training and medication to achieve desired voices. Furthermore, we identified the technical barriers to current voice changer technology and potential improvements to alleviate the problems that transgender and gender-nonconforming users face.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2401.03761.pdf' target='_blank'>https://arxiv.org/pdf/2401.03761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, Anastasiia Ternova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03761">AKN_Regie: bridging digital and performing arts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AvatarStaging framework consists in directing avatars on a mixed theatrical stage, enabling a co-presence between the materiality of the physical actor and the virtuality of avatars controlled in real time by motion capture or specific animation players. It led to the implementation of the AKN_Regie authoring tool, programmed with the Blueprint visual language as a plugin for the Unreal Engine (UE) video game engine. The paper describes AKN_Regie main functionalities as a tool for non-programmer theatrical people. It gives insights of its implementation in the Blueprint visual language specific to UE. It details how the tool evolved along with its use in around ten theater productions. A circulation process between a nonprogramming point of view on AKN_Regie called Plugin Perspective and a programming acculturation to its development called Blueprint Perspective is discussed. Finally, a C++ Perspective is suggested to enhance the cultural appropriation of technological issues, bridging the gap between performing arts deeply involved in human materiality and avatars inviting to discover new worlds.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2401.03398.pdf' target='_blank'>https://arxiv.org/pdf/2401.03398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Li, Kang Li, Dewei Han, Jian Xu, Zhaoyuan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03398">Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI and robotics technologies have witnessed remarkable advancements in the past decade, revolutionizing work patterns and opportunities in various domains. The application of these technologies has propelled society towards an era of symbiosis between humans and machines. To facilitate efficient communication between humans and intelligent robots, we propose the "Avatar" system, an immersive low-latency panoramic human-robot interaction platform. We have designed and tested a prototype of a rugged mobile platform integrated with edge computing units, panoramic video capture devices, power batteries, robot arms, and network communication equipment. Under favorable network conditions, we achieved a low-latency high-definition panoramic visual experience with a delay of 357ms. Operators can utilize VR headsets and controllers for real-time immersive control of robots and devices. The system enables remote control over vast physical distances, spanning campuses, provinces, countries, and even continents (New York to Shenzhen). Additionally, the system incorporates visual SLAM technology for map and trajectory recording, providing autonomous navigation capabilities. We believe that this intuitive system platform can enhance efficiency and situational experience in human-robot collaboration, and with further advancements in related technologies, it will become a versatile tool for efficient and symbiotic cooperation between AI and humans.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2401.03079.pdf' target='_blank'>https://arxiv.org/pdf/2401.03079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Naughton, James Seungbum Nam, Andrew Stratton, Kris Hauser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03079">Integrating Open-World Shared Control in Immersive Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperated avatar robots allow people to transport their manipulation skills to environments that may be difficult or dangerous to work in. Current systems are able to give operators direct control of many components of the robot to immerse them in the remote environment, but operators still struggle to complete tasks as competently as they could in person. We present a framework for incorporating open-world shared control into avatar robots to combine the benefits of direct and shared control. This framework preserves the fluency of our avatar interface by minimizing obstructions to the operator's view and using the same interface for direct, shared, and fully autonomous control. In a human subjects study (N=19), we find that operators using this framework complete a range of tasks significantly more quickly and reliably than those that do not.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2401.01832.pdf' target='_blank'>https://arxiv.org/pdf/2401.01832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iuliia Zhurakovskaia, Jeanne Vezien, Patrick Bourdot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01832">Teaching with a companion: the case of gravity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) has repeatedly proven its effectiveness in student learning. However, despite its benefits, the student equipped with a personal headset remains isolated from the real world while immersed in a virtual space and the classic student-teacher model of learning is difficult to transpose in such a situation. This study aims to bring the teacher back into the learning process when students use a VR headset. We describe the benefits of using a companion for educational purposes, taking as a test case the concept of gravity. We present an experimental setup designed to compare three different teaching contexts: with a physically present real teacher, using a live video of the teacher, and with a VR avatar of the teacher. We designed and evaluated three scenarios to teach the concept of gravity: an introduction to the concept of free fall, a parabolic trajectory workshop and a final exercise combining both approaches. Due to sanitary conditions, only pre-tests are reported. The results showed that the effectiveness of using the VR simulations for learning and the self-confidence level of the students increased as well. The interviews show that the students ranked the teaching modes in this order: VR companion mode, video communication and real teacher.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2312.12634.pdf' target='_blank'>https://arxiv.org/pdf/2312.12634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Payam Jome Yazdian, Rachel Lagasse, Hamid Mohammadi, Eric Liu, Li Cheng, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12634">MotionScript: Natural Language Descriptions for Expressive 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotionScript, a novel framework for generating highly detailed, natural language descriptions of 3D human motions. Unlike existing motion datasets that rely on broad action labels or generic captions, MotionScript provides fine-grained, structured descriptions that capture the full complexity of human movement including expressive actions (e.g., emotions, stylistic walking) and interactions beyond standard motion capture datasets. MotionScript serves as both a descriptive tool and a training resource for text-to-motion models, enabling the synthesis of highly realistic and diverse human motions from text. By augmenting motion datasets with MotionScript captions, we demonstrate significant improvements in out-of-distribution motion generation, allowing large language models (LLMs) to generate motions that extend beyond existing data. Additionally, MotionScript opens new applications in animation, virtual human simulation, and robotics, providing an interpretable bridge between intuitive descriptions and motion synthesis. To the best of our knowledge, this is the first attempt to systematically translate 3D motion into structured natural language without requiring training data.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2311.07502.pdf' target='_blank'>https://arxiv.org/pdf/2311.07502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pubudu Wijesooriya, Aaron Likens, Nick Stergiou, Spyridon Mastorakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07502">ARWalker: A Virtual Walking Companion Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended Reality (XR) technologies, including Augmented Reality (AR), have attracted significant attention over the past few years and have been utilized in several fields, including education, healthcare, and manufacturing. In this paper, we aim to explore the use of AR in the field of biomechanics and human movement through the development of ARWalker, which is an AR application that features virtual walking companions (avatars). Research participants walk in close synchrony with the virtual companions, whose gait exhibits properties found in the gait of young and healthy adults. As a result, research participants can train their gait to the gait of the avatar, thus regaining the healthy properties of their gait and reducing the risk of falls. ARWalker can especially help older adults and individuals with diseases, who exhibit pathological gait thus being more prone to falls. We implement a prototype of ARWalker and evaluate its systems performance while running on a Microsoft Hololens 2 headset.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2310.19555.pdf' target='_blank'>https://arxiv.org/pdf/2310.19555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ginga Kato, Yoshihiro Kuroda, Kiyoshi Kiyokawa, Haruo Takemura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19555">Force Rendering and Its Evaluation of a Friction-based Walking Sensation Display for a Seated User</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing locomotion devices that represent the sensation of walking target a user who is actually performing a walking motion. Here, we attempted to represent the walking sensation, especially a kinesthetic sensation and advancing feeling (the sense of moving forward) while the user remains seated. To represent the walking sensation using a relatively simple device, we focused on the force rendering and its evaluation of the longitudinal friction force applied on the sole during walking. Based on the measurement of the friction force applied on the sole during actual walking, we developed a novel friction force display that can present the friction force without the influence of body weight. Using performance evaluation testing, we found that the proposed method can stably and rapidly display friction force. Also, we developed a virtual reality (VR) walk-through system that is able to present the friction force through the proposed device according to the avatar's walking motion in a virtual world. By evaluating the realism, we found that the proposed device can represent a more realistic advancing feeling than vibration feedback.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2310.13482.pdf' target='_blank'>https://arxiv.org/pdf/2310.13482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyan Yu, Shihuan Wang, Dong zhang, Yingying Zhang, Chaoqun Cen, Zhixiang you, Xiaobing zou, Hongzhu Deng, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13482">HSVRS: A Virtual Reality System of the Hide-and-Seek Game to Enhance Gaze Fixation Ability for Autistic Children</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous children diagnosed with Autism Spectrum Disorder (ASD) exhibit abnormal eye gaze pattern in communication and social interaction. Due to the high cost of ASD interventions and a shortage of professional therapists, researchers have explored the use of virtual reality (VR) systems as a supplementary intervention for autistic children. This paper presents the design of a novel VR-based system called the Hide and Seek Virtual Reality System (HSVRS). The HSVRS allows children with ASD to enhance their ocular gaze abilities while engaging in a hide-and-seek game with a virtual avatar. By employing face and voice manipulation technology, the HSVRS provides the option to customize the appearance and voice of the avatar, making it resemble someone familiar to the child, such as their parents. We conducted a pilot study at the Third Affiliated Hospital of Sun Yat-sen University, China, to evaluate the feasibility of HSVRS as an auxiliary intervention for children with autism (N=24). Through the analysis of subjective questionnaires completed by the participants' parents and objective eye gaze data, we observed that children in the VR-assisted intervention group demonstrated better performance compared to those in the control group. Furthermore, our findings indicate that the utilization of face and voice manipulation techniques to personalize avatars in hide-and-seek games can enhance the efficiency and effectiveness of the system.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2310.02753.pdf' target='_blank'>https://arxiv.org/pdf/2310.02753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Debayan Deb, Suvidha Tripathi, Pranit Puri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02753">MUNCH: Modelling Unique 'N Controllable Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automated generation of 3D human heads has been an intriguing and challenging task for computer vision researchers. Prevailing methods synthesize realistic avatars but with limited control over the diversity and quality of rendered outputs and suffer from limited correlation between shape and texture of the character. We propose a method that offers quality, diversity, control, and realism along with explainable network design, all desirable features to game-design artists in the domain. First, our proposed Geometry Generator identifies disentangled latent directions and generate novel and diverse samples. A Render Map Generator then learns to synthesize multiply high-fidelty physically-based render maps including Albedo, Glossiness, Specular, and Normals. For artists preferring fine-grained control over the output, we introduce a novel Color Transformer Model that allows semantic color control over generated maps. We also introduce quantifiable metrics called Uniqueness and Novelty and a combined metric to test the overall performance of our model. Demo for both shapes and textures can be found: https://munch-seven.vercel.app/. We will release our model along with the synthetic dataset.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2309.14634.pdf' target='_blank'>https://arxiv.org/pdf/2309.14634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong-Hao Hu, Kenichiro Ito, Ayumi Igarashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14634">Synchronizing Full-Body Avatar Transforms with WebRTC DataChannel on Educational Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Full-body avatars are suggested to be beneficial for communication in virtual environments, and consistency between users' voices and gestures is considered essential to ensure communication quality. This paper propose extending the functionality of a web-based VR platform to support the use of full-body avatars and delegated avatar transforms synchronization to WebRTC DataChannel to enhance the consistency between voices and gestures. Finally, we conducted a preliminary validation to confirm the consistency.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2308.07878.pdf' target='_blank'>https://arxiv.org/pdf/2308.07878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Behnke, Julie A. Adams, David Locke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07878">The $10 Million ANA Avatar XPRIZE Competition Advanced Immersive Telepresence Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The $10M ANA Avatar XPRIZE aimed to create avatar systems that can transport human presence to remote locations in real time. The participants of this multi-year competition developed robotic systems that allow operators to see, hear, and interact with a remote environment in a way that feels as if they are truly there. On the other hand, people in the remote environment were given the impression that the operator was present inside the avatar robot. At the competition finals, held in November 2022 in Long Beach, CA, USA, the avatar systems were evaluated on their support for remotely interacting with humans, exploring new environments, and employing specialized skills. This article describes the competition stages with tasks and evaluation procedures, reports the results, presents the winning teams' approaches, and discusses lessons learned.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2308.04868.pdf' target='_blank'>https://arxiv.org/pdf/2308.04868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Canela, Pol Caselles, Ibrar Malik, Eduard Ramon, Jaime GarcÃ­a, Jordi SÃ¡nchez-Riera, Gil Triginer, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04868">InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in full-head reconstruction have been obtained by optimizing a neural field through differentiable surface or volume rendering to represent a single scene. While these techniques achieve an unprecedented accuracy, they take several minutes, or even hours, due to the expensive optimization process required. In this work, we introduce InstantAvatar, a method that recovers full-head avatars from few images (down to just one) in a few seconds on commodity hardware. In order to speed up the reconstruction process, we propose a system that combines, for the first time, a voxel-grid neural field representation with a surface renderer. Notably, a naive combination of these two techniques leads to unstable optimizations that do not converge to valid solutions. In order to overcome this limitation, we present a novel statistical model that learns a prior distribution over 3D head signed distance functions using a voxel-grid based architecture. The use of this prior model, in combination with other design choices, results into a system that achieves 3D head reconstructions with comparable accuracy as the state-of-the-art with a 100x speed-up.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2307.01322.pdf' target='_blank'>https://arxiv.org/pdf/2307.01322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amaury Trujillo, Clara Bacciu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01322">Toward Blockchain-based Fashion Wearables in the Metaverse: the Case of Decentraland</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Among the earliest projects to combine the Metaverse and non-fungible tokens (NFTs) we find Decentraland, a blockchain-based virtual world that touts itself as the first to be owned by its users. In particular, the platform's virtual wearables (which allow avatar appearance customization) have attracted much attention from users, content creators, and the fashion industry. In this work, we present the first study to quantitatively characterize Decentraland's wearables, their publication, minting, and sales on the platform's marketplace. Our results indicate that wearables are mostly given away to promote and increase engagement on other cryptoasset or Metaverse projects, and only a small fraction is sold on the platform's marketplace, where the price is mainly driven by the preset wearable's rarity. Hence, platforms that offer virtual wearable NFTs should pay particular attention to the economics around this kind of assets beyond their mere sale.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2306.14277.pdf' target='_blank'>https://arxiv.org/pdf/2306.14277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbo Zhao, Zhetao Wang, Yaojun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14277">Effects of Different Levels of Self-Representation on Spatial Awareness, Self-Presence and Spatial Presence during Virtual Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been growing interest in investigating the effects of self-representation on user experience and perception in virtual environments. However, few studies investigated the effects of levels of body representation (full-body, lower-body and viewpoint) on locomotion experience in terms of spatial awareness, self-presence and spatial presence during virtual locomotion. Understanding such effects is essential for building new virtual locomotion systems with better locomotion experience. In the present study, we first built a walking-in-place (WIP) virtual locomotion system that can represent users using avatars at three levels (full-body, lower-body and viewpoint) and is capable of rendering walking animations during in-place walking of a user. We then conducted a virtual locomotion experiment using three levels of representation to investigate the effects of body representation on spatial awareness, self-presence and spatial presence during virtual locomotion. Experimental results showed that the full-body representation provided better virtual locomotion experience in these three factors compared to that of the lower-body representation and the viewpoint representation. The lower-body representation also provided better experience than the viewpoint representation. These results suggest that self-representation of users in virtual environments using a full-body avatar is critical for providing better locomotion experience. Using full-body avatars for self-representation of users should be considered when building new virtual locomotion systems and applications.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2306.10656.pdf' target='_blank'>https://arxiv.org/pdf/2306.10656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kenta Oono, Nontawat Charoenphakdee, Kotatsu Bito, Zhengyan Gao, Hideyoshi Igata, Masashi Yoshikawa, Yoshiaki Ota, Hiroki Okui, Kei Akita, Shoichiro Yamaguchi, Yohei Sugawara, Shin-ichi Maeda, Kunihiko Miyoshi, Yuki Saito, Koki Tsuda, Hiroshi Maruyama, Kohei Hayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10656">Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying the relationship between healthcare attributes, lifestyles, and personality is vital for understanding and improving physical and mental well-being. Machine learning approaches are promising for modeling their relationships and offering actionable suggestions. In this paper, we propose the Virtual Human Generative Model (VHGM), a novel deep generative model capable of estimating over 2,000 attributes across healthcare, lifestyle, and personality domains. VHGM leverages masked modeling to learn the joint distribution of attributes, enabling accurate predictions and robust conditional sampling. We deploy VHGM as a web service, showcasing its versatility in driving diverse healthcare applications aimed at improving user well-being. Through extensive quantitative evaluations, we demonstrate VHGM's superior performance in attribute imputation and high-quality sample generation compared to existing baselines. This work highlights VHGM as a powerful tool for personalized healthcare and lifestyle management, with broad implications for data-driven health solutions.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2306.10350.pdf' target='_blank'>https://arxiv.org/pdf/2306.10350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Xiang Zhou, Yukang Cao, Wensen Feng, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10350">MA-NeRF: Motion-Assisted Neural Radiance Fields for Face Synthesis from Sparse Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of photorealistic 3D face avatar synthesis from sparse images. Existing Parametric models for face avatar reconstruction struggle to generate details that originate from inputs. Meanwhile, although current NeRF-based avatar methods provide promising results for novel view synthesis, they fail to generalize well for unseen expressions. We improve from NeRF and propose a novel framework that, by leveraging the parametric 3DMM models, can reconstruct a high-fidelity drivable face avatar and successfully handle the unseen expressions. At the core of our implementation are structured displacement feature and semantic-aware learning module. Our structured displacement feature will introduce the motion prior as an additional constraints and help perform better for unseen expressions, by constructing displacement volume. Besides, the semantic-aware learning incorporates multi-level prior, e.g., semantic embedding, learnable latent code, to lift the performance to a higher level. Thorough experiments have been doen both quantitatively and qualitatively to demonstrate the design of our framework, and our method achieves much better results than the current state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2304.04897.pdf' target='_blank'>https://arxiv.org/pdf/2304.04897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04897">Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a method that enables synthesizing novel views and novel poses of arbitrary human performers from sparse multi-view images. A key ingredient of our method is a hybrid appearance blending module that combines the advantages of the implicit body NeRF representation and image-based rendering. Existing generalizable human NeRF methods that are conditioned on the body model have shown robustness against the geometric variation of arbitrary human performers. Yet they often exhibit blurry results when generalized onto unseen identities. Meanwhile, image-based rendering shows high-quality results when sufficient observations are available, whereas it suffers artifacts in sparse-view settings. We propose Neural Image-based Avatars (NIA) that exploits the best of those two methods: to maintain robustness under new articulations and self-occlusions while directly leveraging the available (sparse) source view colors to preserve appearance details of new subject identities. Our hybrid design outperforms recent methods on both in-domain identity generalization as well as challenging cross-dataset generalization settings. Also, in terms of the pose generalization, our method outperforms even the per-subject optimized animatable NeRF methods. The video results are available at https://youngjoongunc.github.io/nia
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2303.17212.pdf' target='_blank'>https://arxiv.org/pdf/2303.17212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arbish Akram, Nazar Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17212">SARGAN: Spatial Attention-based Residuals for Facial Expression Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Encoder-decoder based architecture has been widely used in the generator of generative adversarial networks for facial manipulation. However, we observe that the current architecture fails to recover the input image color, rich facial details such as skin color or texture and introduces artifacts as well. In this paper, we present a novel method named SARGAN that addresses the above-mentioned limitations from three perspectives. First, we employed spatial attention-based residual block instead of vanilla residual blocks to properly capture the expression-related features to be changed while keeping the other features unchanged. Second, we exploited a symmetric encoder-decoder network to attend facial features at multiple scales. Third, we proposed to train the complete network with a residual connection which relieves the generator of pressure to generate the input face image thereby producing the desired expression by directly feeding the input image towards the end of the generator. Both qualitative and quantitative experimental results show that our proposed model performs significantly better than state-of-the-art methods. In addition, existing models require much larger datasets for training but their performance degrades on out-of-distribution images. In contrast, SARGAN can be trained on smaller facial expressions datasets, which generalizes well on out-of-distribution images including human photographs, portraits, avatars and statues.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2303.15187.pdf' target='_blank'>https://arxiv.org/pdf/2303.15187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Villani, Giovanni Cortigiani, Bernardo Brogi, Nicole D'Aurizio, Tommaso Lisini Baldi, Domenico Prattichizzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15187">Avatarm: an Avatar With Manipulation Capabilities for the Physical Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse is an immersive shared space that remote users can access through virtual and augmented reality interfaces, enabling their avatars to interact with each other and the surrounding. Although digital objects can be manipulated, physical objects cannot be touched, grasped, or moved within the metaverse due to the lack of a suitable interface. This work proposes a solution to overcome this limitation by introducing the concept of a Physical Metaverse enabled by a new interface named "Avatarm". The Avatarm consists in an avatar enhanced with a robotic arm that performs physical manipulation tasks while remaining entirely hidden in the metaverse. The users have the illusion that the avatar is directly manipulating objects without the mediation by a robot. The Avatarm is the first step towards a new metaverse, the "Physical Metaverse", where users can physically interact each other and with the environment.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2303.14071.pdf' target='_blank'>https://arxiv.org/pdf/2303.14071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong-Hao Hu, Kenichiro Ito, Ayumi Igarashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14071">Improving Real-time Communication for Educational Metaverse by Alternative WebRTC SFU and Delegating Transmission of Avatar Transform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining real-time communication quality in metaverse has always been a challenge, especially when the number of participants increase. We introduce a proprietary WebRTC SFU service to an open-source web-based VR platform, to realize a more stable and reliable platform suitable for educational communication of audio, video, and avatar transform. We developed the web-based VR platform and conducted a preliminary validation on the implementation for proof of concept, and high performance in both server and client sides are confirmed, which may indicates better user experience in communication and imply a solution to realize educational metaverse.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2303.10275.pdf' target='_blank'>https://arxiv.org/pdf/2303.10275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renat Bashirov, Alexey Larionov, Evgeniya Ustinova, Mikhail Sidorenko, David Svitov, Ilya Zakharkin, Victor Lempitsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10275">MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRF avatars are rendered in real-time on mobile devices, learned from monocular videos, and have high realism. We use SMPL-X as a proxy geometry and render it with DNR (neural texture and image-2-image network). We improve on prior work, by overfitting per-frame warping fields in the neural texture space, allowing to better align the training signal between different frames. We also refine SMPL-X mesh fitting procedure to improve the overall avatar quality. In the comparisons to other monocular video-based avatar systems, MoRF avatars achieve higher image sharpness and temporal consistency. Participants of our user study also preferred avatars generated by MoRF.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2303.09375.pdf' target='_blank'>https://arxiv.org/pdf/2303.09375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Svitov, Dmitrii Gudkov, Renat Bashirov, Victor Lempitsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09375">DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DINAR, an approach for creating realistic rigged fullbody avatars from single RGB images. Similarly to previous works, our method uses neural textures combined with the SMPL-X body model to achieve photo-realistic quality of avatars while keeping them easy to animate and fast to infer. To restore the texture, we use a latent diffusion model and show how such model can be trained in the neural texture space. The use of the diffusion model allows us to realistically reconstruct large unseen regions such as the back of a person given the frontal view. The models in our pipeline are trained using 2D images and videos only. In the experiments, our approach achieves state-of-the-art rendering quality and good generalization to new poses and viewpoints. In particular, the approach improves state-of-the-art on the SnapshotPeople public benchmark.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2303.06981.pdf' target='_blank'>https://arxiv.org/pdf/2303.06981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©, Anastasiia Ternova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06981">CAstelet in Virtual reality for shadOw AVatars (CAVOAV)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>After an overview of the use of digital shadows in computing science research projects with cultural and social impacts and a focus on recent researches and insights on virtual theaters, this paper introduces a research mixing the manipulation of shadow avatars and the building of a virtual theater setup inspired by traditional shadow theater (or ``castelet'' in french) in a mixed reality environment. It describes the virtual 3D setup, the nature of the shadow avatars and the issues of directing believable interactions between virtual avatars and physical performers on stage. Two modalities of shadow avatars direction are exposed. Some results of the research are illustrated in two use cases: the development of theatrical creativity in mixed reality through pedagogical workshops; and an artistic achievement in ''The Shadow'' performance, after H. C. Andersen.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2303.05995.pdf' target='_blank'>https://arxiv.org/pdf/2303.05995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amador DurÃ¡n, Pablo FernÃ¡ndez, Beatriz BernÃ¡rdez, Nathaniel Weinman, AslÄ±han AkalÄ±n, Armando Fox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05995">Exploring Gender Bias in Remote Pair Programming among Software Engineering Students: The twincode Original Study and First External Replication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context. Software Engineering (SE) has low female representation due to gender bias that men are better at programming. Pair programming (PP) is common in industry and can increase student interest in SE, especially women; but if gender bias affects PP, it may discourage women from joining the field.
  Objective. We explore gender bias in PP. In a remote setting where students cannot see their peers' gender, we study how perceived productivity, technical competency and collaboration/interaction behaviors of SE students vary by perceived gender of their remote partner.
  Method. We developed an online PP platform (twincode) with a collaborative editing window and a chat pane. Control group had no gender information about their partner, while treatment group saw a gendered avatar as a man or woman. Avatar gender was swapped between tasks to analyze 45 variables on collaborative coding behavior, chat utterances and questionnaire responses of 46 pairs in original study at the University of Seville and 23 pairs in the replication at the University of California, Berkeley.
  Results. No significant effect of gender bias treatment or interaction between perceived partner's gender and subject's gender in any variable in original study. In replication, significant effects with moderate to large sizes in four variables within experimental group comparing subjects' actions when partner was male vs female.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2302.05172.pdf' target='_blank'>https://arxiv.org/pdf/2302.05172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdallah El Ali, Sueyoon Lee, Pablo Cesar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05172">Social Virtual Reality Avatar Biosignal Animations as Availability Status Indicators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this position paper, we outline our research challenges in Affective Interactive Systems, and present recent work on visualizing avatar biosignals for social VR entertainment. We highlight considerations for how biosignals animations in social VR spaces can (falsely) indicate users' availability status.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2301.09702.pdf' target='_blank'>https://arxiv.org/pdf/2301.09702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Guo, Amy R. Reibman, Edward J. Delp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09702">Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2207.07621.pdf' target='_blank'>https://arxiv.org/pdf/2207.07621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, Egor Zakharov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.07621">MegaPortraits: One-shot Megapixel Neural Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we advance the neural head avatar technology to the megapixel resolution while focusing on the particularly challenging task of cross-driving synthesis, i.e., when the appearance of the driving image is substantially different from the animated source image. We propose a set of new neural architectures and training methods that can leverage both medium-resolution video data and high-resolution image data to achieve the desired levels of rendered image quality and generalization to novel views and motion. We demonstrate that suggested architectures and methods produce convincing high-resolution neural avatars, outperforming the competitors in the cross-driving scenario. Lastly, we show how a trained high-resolution neural avatar model can be distilled into a lightweight student model which runs in real-time and locks the identities of neural avatars to several dozens of pre-defined source images. Real-time operation and identity lock are essential for many practical applications head avatar systems.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2509.15372.pdf' target='_blank'>https://arxiv.org/pdf/2509.15372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yudong Huang, Avneet Singh, Mark Roman Miller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15372">Experience Level Influences User's Criteria for Avatar Animation Realism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The sense of realism in avatar animation is a widely pursued goal in social VR applications. A common approach to enhancing realism is improving the match between avatar motion and real-world human movement. However, experience with existing VR platforms may reshape users' expectations, suggesting that matching reality is not the only path to enhancing the sense of realism. This study examines how different levels of experience with a social VR platform influence users' criteria for evaluating the realism of avatar animation. Participants were shown a set of animations varying in the degree they reflected real-world motion and motion seen on the social VR platform VRChat. Results showed that users with no VRChat experience found animations recorded on VRChat unnatural and unrealistic, but experienced users in fact rated these animations as more likely to come from a real person than the motion-capture animations. Additionally, highly experienced users recognized the intent to imitate VRChat's style and noted the differences from genuine in-platform animations. All these results suggest users' expectations of and criteria for realistic animation were shaped by their experience level. The findings support the idea that realism in avatar animation does not solely depend on mimicking real-world movement. Experience with VR platforms can shape how users expect, perceive, and evaluate animation realism. This insight can inform the design of more immersive VR environments and virtual humans in the future.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2509.14132.pdf' target='_blank'>https://arxiv.org/pdf/2509.14132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julia S. Dollis, Iago A. Brito, Fernanda B. Färber, Pedro S. F. B. Ribeiro, Rafael T. Sousa, Arlindo R. Galvão Filho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14132">When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While virtual reality (VR) excels at simulating physical environments, its effectiveness for training complex interpersonal skills is limited by a lack of psychologically plausible virtual humans. This is a critical gap in high-stakes domains like medical education, where communication is a core competency. This paper introduces a framework that integrates large language models (LLMs) into immersive VR to create medically coherent virtual patients with distinct, consistent personalities, built on a modular architecture that decouples personality from clinical data. We evaluated our system in a mixed-method, within-subjects study with licensed physicians who engaged in simulated consultations. Results demonstrate that the approach is not only feasible but is also perceived by physicians as a highly rewarding and effective training enhancement. Furthermore, our analysis uncovers critical design principles, including a ``realism-verbosity paradox" where less communicative agents can seem more artificial, and the need for challenges to be perceived as authentic to be instructive. This work provides a validated framework and key insights for developing the next generation of socially intelligent VR training environments.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2509.12525.pdf' target='_blank'>https://arxiv.org/pdf/2509.12525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>T. James Brandt, Cecilia Xi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12525">The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI powers a growing wave of companion chatbots, yet principles for fostering genuine connection remain unsettled. We test two routes: visible user authorship versus covert language-style mimicry. In a preregistered 3x2 experiment (N = 162), we manipulated user-controlled avatar generation (none, premade, user-generated) and Language Style Matching (LSM) (static vs. adaptive). Generating an avatar boosted rapport ($ω^2$ = .040, p = .013), whereas adaptive LSM underperformed static style on personalization and satisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t = 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony erodes connection when perceived as incoherent, destabilizing persona. To explain, we propose a stability-and-legibility account: visible authorship fosters natural interaction, while covert mimicry risks incoherence. Our findings suggest designers should prioritize legible, user-driven personalization and limit stylistic shifts rather than rely on opaque mimicry.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2509.09815.pdf' target='_blank'>https://arxiv.org/pdf/2509.09815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan He, Brendan Rooney, Rachel McDonnell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09815">Merging Bodies, Dividing Conflict: Body-Swapping in Mixed Reality Increases Closeness Yet Weakens the Joint Simon Effect</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality (MR) presents novel opportunities to investigate how individuals perceive themselves and others during shared, augmented experiences within a common physical environment. Previous research has demonstrated that users can embody avatars in MR, temporarily extending their sense of self. However, there has been limited exploration of body-swapping, a condition in which two individuals simultaneously inhabit each other's avatars, and its potential effects on social interaction in immersive environments. To address this gap, we adapted the Joint Simon Task (JST), a well-established implicit paradigm, to examine how body-swapping influences the cognitive and perceptual boundaries between self and other. Our results indicate that body-swapping led participants to experience themselves and their partner as functioning like a single, unified system, as in two bodies operating as one agent. This suggests possible cognitive and perceptual changes that go beyond simple collaboration. Our findings have significant implications for the design of MR systems intended to support collaboration, empathy, social learning, and therapeutic interventions through shared embodiment.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2508.15500.pdf' target='_blank'>https://arxiv.org/pdf/2508.15500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fulden Ece UÄur, Rafael Redondo, Albert Barreiro, Stefan Hristov, Roger MarÃ­
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15500">MExECON: Multi-view Extended Explicit Clothed humans Optimized via Normal integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents MExECON, a novel pipeline for 3D reconstruction of clothed human avatars from sparse multi-view RGB images. Building on the single-view method ECON, MExECON extends its capabilities to leverage multiple viewpoints, improving geometry and body pose estimation. At the core of the pipeline is the proposed Joint Multi-view Body Optimization (JMBO) algorithm, which fits a single SMPL-X body model jointly across all input views, enforcing multi-view consistency. The optimized body model serves as a low-frequency prior that guides the subsequent surface reconstruction, where geometric details are added via normal map integration. MExECON integrates normal maps from both front and back views to accurately capture fine-grained surface details such as clothing folds and hairstyles. All multi-view gains are achieved without requiring any network re-training. Experimental results show that MExECON consistently improves fidelity over the single-view baseline and achieves competitive performance compared to modern few-shot 3D reconstruction methods.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2508.10586.pdf' target='_blank'>https://arxiv.org/pdf/2508.10586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Birgit Nierula, Mustafa Tevfik Lafci, Anna Melnik, Mert AkgÃ¼l, Farelle Toumaleu Siewe, Sebastian Bosse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10586">Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proxemics, the study of spatial behavior, is fundamental to social interaction and increasingly relevant for virtual reality (VR) applications. While previous research has established that users respond to personal space violations in VR similarly as in real-world settings, phase-specific physiological responses and the modulating effects of facial expressions remain understudied. We investigated physiological and subjective responses to personal space violations by virtual avatars, to understand how threatening facial expressions and interaction phases (approach vs. standing) influence these responses. Sixteen participants experienced a 2x2 factorial design manipulating Personal Space (intrusion vs. respect) and Facial Expression (neutral vs. angry) while we recorded skin conductance response (SCR), heart rate variability (HRV), and discomfort ratings. Personal space boundaries were individually calibrated using a stop-distance procedure. Results show that SCR responses are significantly higher during the standing phase compared to the approach phase when personal space was violated, indicating that prolonged proximity within personal space boundaries is more physiologically arousing than the approach itself. Angry facial expressions significantly reduced HRV, reflecting decreased parasympathetic activity, and increased discomfort ratings, but did not amplify SCR responses. These findings demonstrate that different physiological modalities capture distinct aspects of proxemic responses: SCR primarily reflects spatial boundary violations, while HRV responds to facial threat cues. Our results provide insights for developing comprehensive multi-modal assessments of social behavior in virtual environments and inform the design of more realistic avatar interactions.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2508.09973.pdf' target='_blank'>https://arxiv.org/pdf/2508.09973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geonhee Sim, Gyeongsik Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09973">PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two major approaches exist for creating animatable human avatars. The first, a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a single person, achieving personalization through a disentangled identity representation. However, modeling pose-driven deformations, such as non-rigid cloth deformations, requires numerous pose-rich videos, which are costly and impractical to capture in daily life. The second, a diffusion-based approach, learns pose-driven deformations from large-scale in-the-wild videos but struggles with identity preservation and pose-dependent identity entanglement. We present PERSONA, a framework that combines the strengths of both approaches to obtain a personalized 3D human avatar with pose-driven deformations from a single image. PERSONA leverages a diffusion-based approach to generate pose-rich videos from the input image and optimizes a 3D avatar based on them. To ensure high authenticity and sharp renderings across diverse poses, we introduce balanced sampling and geometry-weighted optimization. Balanced sampling oversamples the input image to mitigate identity shifts in diffusion-generated training videos. Geometry-weighted optimization prioritizes geometry constraints over image loss, preserving rendering quality in diverse poses.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2508.05531.pdf' target='_blank'>https://arxiv.org/pdf/2508.05531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Garavaso, Federico Masi, Pietro Musoni, Umberto Castellani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05531">Point cloud segmentation for 3D Clothed Human Layering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Cloth modeling and simulation is essential for avatars creation in several fields, such as fashion, entertainment, and animation. Achieving high-quality results is challenging due to the large variability of clothed body especially in the generation of realistic wrinkles. 3D scan acquisitions provide more accuracy in the representation of real-world objects but lack semantic information that can be inferred with a reliable semantic reconstruction pipeline. To this aim, shape segmentation plays a crucial role in identifying the semantic shape parts. However, current 3D shape segmentation methods are designed for scene understanding and interpretation and only few work is devoted to modeling. In the context of clothed body modeling the segmentation is a preliminary step for fully semantic shape parts reconstruction namely the underlying body and the involved garments. These parts represent several layers with strong overlap in contrast with standard segmentation methods that provide disjoint sets. In this work we propose a new 3D point cloud segmentation paradigm where each 3D point can be simultaneously associated to different layers. In this fashion we can estimate the underlying body parts and the unseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer above. We name this segmentation paradigm clothed human layering. We create a new synthetic dataset that simulates very realistic 3D scans with the ground truth of the involved clothing layers. We propose and evaluate different neural network settings to deal with 3D clothing layering. We considered both coarse and fine grained per-layer garment identification. Our experiments demonstrates the benefit in introducing proper strategies for the segmentation on the garment domain on both the synthetic and real-world scan datasets.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2508.04904.pdf' target='_blank'>https://arxiv.org/pdf/2508.04904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Hu, Qiwen Xiong, Zhenzhen Qin, Brandon Watanabe, Yujing Wang, Mirjana Prpa, Ilmi Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04904">Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Root Cause Analysis (RCA) is a critical tool for investigating adverse events in healthcare and improving patient safety. However, existing RCA training programs are often limited by high resource demands, leading to insufficient training and inconsistent implementation. To address this challenge, we present an AI-powered 3D simulation game that helps healthcare professionals develop RCA skills through interactive, immersive simulations. This approach offers a cost-effective, scalable, and accessible alternative to traditional training. The prototype simulates an RCA investigation following a death in the ICU, where learners interview five virtual avatars representing ICU team members to investigate the incident and complete a written report. The system enables natural, life-like interactions with avatars via large language models (LLMs), emotional text-to-speech, and AI-powered animations. An additional LLM component provides formative and summative feedback to support continual improvement. We conclude by outlining plans to empirically evaluate the system's efficacy.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2508.01743.pdf' target='_blank'>https://arxiv.org/pdf/2508.01743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyao Zhang, Omar Faruk, Robert Porzel, Dennis KÃ¼ster, Tanja Schultz, Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01743">Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An increasing number of online interaction settings now provide the possibility to visually represent oneself via an animated avatar instead of a video stream. Benefits include protecting the communicator's privacy while still providing a means to express their individuality. In consequence, there has been a surge in means for avatar-based personalization, ranging from classic human representations to animals, food items, and more. However, using avatars also has drawbacks. Depending on the human-likeness of the avatar and the corresponding disparities between the avatar and the original expresser, avatars may elicit discomfort or even hinder effective nonverbal communication by distorting emotion perception. This study examines the relationship between the human-likeness of virtual avatars and emotion perception for Ekman's six "basic emotions". Research reveals that avatars with varying degrees of human-likeness have distinct effects on emotion perception. High human-likeness avatars, such as human avatars, tend to elicit more negative emotional responses from users, a phenomenon that is consistent with the concept of Uncanny Valley in aesthetics, which suggests that closely resembling humans can provoke negative emotional responses. Conversely, a raccoon avatar and a shark avatar, known as cuteness, which exhibit moderate human similarity in this study, demonstrate a positive influence on emotion perception. Our initial results suggest that the human-likeness of avatars is an important factor for emotion perception. The results from the follow-up study further suggest that the cuteness of avatars and their natural facial status may also play a significant role in emotion perception and elicitation. We discuss practical implications for strategically conveying specific human behavioral messages through avatars in multiple applications, such as business and counseling.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2508.01381.pdf' target='_blank'>https://arxiv.org/pdf/2508.01381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Onat Vuran, Hsuan-I Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01381">ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reconstruction of multi-layer 3D garments typically requires expensive multi-view capture setups and specialized 3D editing efforts. To support the creation of life-like clothed human avatars, we introduce ReMu for reconstructing multi-layer clothed humans in a new setup, Image Layers, which captures a subject wearing different layers of clothing with a single RGB camera. To reconstruct physically plausible multi-layer 3D garments, a unified 3D representation is necessary to model these garments in a layered manner. Thus, we first reconstruct and align each garment layer in a shared coordinate system defined by the canonical body pose. Afterwards, we introduce a collision-aware optimization process to address interpenetration and further refine the garment boundaries leveraging implicit neural fields. It is worth noting that our method is template-free and category-agnostic, which enables the reconstruction of 3D garments in diverse clothing styles. Through our experiments, we show that our method reconstructs nearly penetration-free 3D clothed humans and achieves competitive performance compared to category-specific methods. Project page: https://eth-ait.github.io/ReMu/
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2507.21079.pdf' target='_blank'>https://arxiv.org/pdf/2507.21079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joe Hasei, Yosuke Matsumoto, Hiroki Kawai, Yuko Okahisa, Manabu Takaki, Toshifumi Ozaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21079">Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study assessed metaverse-based support groups designed to reduce social isolation and suicide risk among LGBTQ+ youths. Using the Cluster platform, enhanced anonymity, avatar-based self-expression, and accessibility were provided. Key findings showed that 79.2% chose avatars matching their gender identity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean: 1.79/5). Social confidence significantly improved in virtual spaces compared to real-world interactions (p<0.001), particularly among participants with initially low confidence, averaging an increase of 2.08 points. About half of the first-time participants were 16 or younger, highlighting potential for early intervention. The metaverse scored higher than real-world environments for safety/privacy (3.94/5), self-expression (4.02/5), and accessibility (4.21/5). Additionally, 73.6% reported feeling more accepted virtually. However, some highly confident individuals offline experienced mild adaptation challenges, averaging a confidence decrease of 0.58 points, indicating virtual support complements rather than replaces in-person services. These findings suggest metaverse-based support effectively lowers psychological barriers and provides affirming spaces, potentially reducing severe outcomes such as suicidal ideation. Future studies should focus on integrating virtual support with existing community and clinical frameworks to enhance long-term impacts.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2507.15072.pdf' target='_blank'>https://arxiv.org/pdf/2507.15072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maisha Maimuna, Minhaz Bin Farukee, Sama Nikanfar, Mahfuza Siddiqua, Ayon Roy, Fillia Makedon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15072">NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial warehouses are congested with moving forklifts, shelves and personnel, making robot teleoperation particularly risky and demanding for blind and low-vision (BLV) operators. Although accessible teleoperation plays a key role in inclusive workforce participation, systematic research on its use in industrial environments is limited, and few existing studies barely address multimodal guidance designed for BLV users. We present a novel multimodal guidance simulator that enables BLV users to control a mobile robot through a high-fidelity warehouse environment while simultaneously receiving synchronized visual, auditory, and haptic feedback. The system combines a navigation mesh with regular re-planning so routes remain accurate avoiding collisions as forklifts and human avatars move around the warehouse. Users with low vision are guided with a visible path line towards destination; navigational voice cues with clockwise directions announce upcoming turns, and finally proximity-based haptic feedback notifies the users of static and moving obstacles in the path. This real-time, closed-loop system offers a repeatable testbed and algorithmic reference for accessible teleoperation research. The simulator's design principles can be easily adapted to real robots due to the alignment of its navigation, speech, and haptic modules with commercial hardware, supporting rapid feasibility studies and deployment of inclusive telerobotic tools in actual warehouses.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2507.09230.pdf' target='_blank'>https://arxiv.org/pdf/2507.09230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>G. Kutay TÃ¼rkoglu, Julian Tanke, Iheb Belgacem, Lev Markhasin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09230">EgoAnimate: Generating Human Animations from Egocentric top-down Views</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An ideal digital telepresence experience requires accurate replication of a person's body, clothing, and movements. To capture and transfer these movements into virtual reality, the egocentric (first-person) perspective can be adopted, which enables the use of a portable and cost-effective device without front-view cameras. However, this viewpoint introduces challenges such as occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views, and none use a generative prior-based approach. Some methods create avatars from a single egocentric image during inference, but still rely on multi-view datasets during training. To our knowledge, this is the first study using a generative backbone to reconstruct animatable avatars from egocentric inputs. Based on Stable Diffusion, our method reduces training burden and improves generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree reconstruction from a frontal image, we introduce a pipeline that generates realistic frontal views from occluded top-down images using ControlNet and a Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic frontal representation and feed it into an image-to-motion model. This enables generation of avatar motions from minimal input, paving the way for more accessible and generalizable telepresence systems.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2507.08917.pdf' target='_blank'>https://arxiv.org/pdf/2507.08917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin D. Norman, Hany Farid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08917">Detecting Deepfake Talking Heads from Facial Biometric Anomalies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The combination of highly realistic voice cloning, along with visually compelling avatar, face-swap, or lip-sync deepfake video generation, makes it relatively easy to create a video of anyone saying anything. Today, such deepfake impersonations are often used to power frauds, scams, and political disinformation. We propose a novel forensic machine learning technique for the detection of deepfake video impersonations that leverages unnatural patterns in facial biometrics. We evaluate this technique across a large dataset of deepfake techniques and impersonations, as well as assess its reliability to video laundering and its generalization to previously unseen video deepfake generators.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2507.08744.pdf' target='_blank'>https://arxiv.org/pdf/2507.08744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clarice Hilton, Kat Hawkins, Phill Tew, Freddie Collins, Seb Madgwick, Dominic Potts, Tom Mitchell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08744">EqualMotion: Accessible Motion Capture for the Creative Industries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion capture technologies are increasingly used in creative and performance contexts but often exclude disabled practitioners due to normative assumptions in body modeling, calibration, and avatar representation. EqualMotion introduces a body-agnostic, wearable motion capture system designed through a disability-centred co-design approach. By enabling personalised calibration, integrating mobility aids, and adopting an inclusive visual language, EqualMotion supports diverse body types and movement styles. The system is developed collaboratively with disabled researchers and creatives, aiming to foster equitable participation in digital performance and prototyping. This paper outlines the system's design principles and highlights ongoing case studies in dance and music to evaluate accessibility in real-world creative workflows.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2507.06060.pdf' target='_blank'>https://arxiv.org/pdf/2507.06060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Symeonidis-Herzig, Ãzge MercanoÄlu Sincan, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06060">VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2506.16159.pdf' target='_blank'>https://arxiv.org/pdf/2506.16159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taisei Omine, Naoyuki Kawabata, Fuminori Homma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16159">Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of conversational AI, research on bodily expressions, including gestures and facial expressions, has also progressed. However, many existing studies focus on photorealistic avatars, making them unsuitable for non-photorealistic characters, such as those found in anime. This study proposes methods for expressing emotions, including exaggerated expressions unique to non-photorealistic characters, by utilizing expression data extracted from comics and dialogue-specific semantic gestures. A user study demonstrated significant improvements across multiple aspects when compared to existing research.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2505.10072.pdf' target='_blank'>https://arxiv.org/pdf/2505.10072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui-Yang Ju, Sheng-Yen Huang, Yi-Ping Hung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10072">ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of 3D Gaussian blendshapes has enabled the real-time reconstruction of animatable head avatars from monocular video. Toonify, a StyleGAN-based method, has become widely used for facial image stylization. To extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1 (stylized video generation), we adopt an improved StyleGAN to generate the stylized video from the input video frames, which overcomes the limitation of cropping aligned faces at a fixed resolution as preprocessing for normal StyleGAN. This process provides a more stable stylized video, which enables Gaussian blendshapes to better capture the high-frequency details of the video frames, facilitating the synthesis of high-quality animations in the next stage. In Stage 2 (Gaussian blendshapes synthesis), our method learns a stylized neutral head model and a set of expression blendshapes from the generated stylized video. By combining the neutral head model with expression blendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary expressions. We validate the effectiveness of ToonifyGB on benchmark datasets using two representative styles: Arcane and Pixar.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2505.09376.pdf' target='_blank'>https://arxiv.org/pdf/2505.09376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunyoung Han, Jongwon Jang, Kitaeg Shim, Sang Ho Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09376">AfforDance: Personalized AR Dance Learning System with Visual Affordance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose AfforDance, an augmented reality (AR)-based dance learning system that generates personalized learning content and enhances learning through visual affordances. Our system converts user-selected dance videos into interactive learning experiences by integrating 3D reference avatars, audio synchronization, and adaptive visual cues that guide movement execution. This work contributes to personalized dance education by offering an adaptable, user-centered learning interface.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2505.03603.pdf' target='_blank'>https://arxiv.org/pdf/2505.03603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Z. Zhou, Y. B. Wang, J. F. Wu, T. Hu, J. N. Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03603">A Unit Enhancement and Guidance Framework for Audio-Driven Avatar Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose Parts-aware Audio-driven Human Animation, PAHA, a unit enhancement and guidance framework for audio-driven upper-body animation. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2505.01319.pdf' target='_blank'>https://arxiv.org/pdf/2505.01319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Pan, Karan Singh, Luiz Gustavo Hafemann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01319">Model See Model Do: Speech-Driven Facial Animation with Style Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven 3D facial animation plays a key role in applications such as virtual avatars, gaming, and digital content creation. While existing methods have made significant progress in achieving accurate lip synchronization and generating basic emotional expressions, they often struggle to capture and effectively transfer nuanced performance styles. We propose a novel example-based generation framework that conditions a latent diffusion model on a reference style clip to produce highly expressive and temporally coherent facial animations. To address the challenge of accurately adhering to the style reference, we introduce a novel conditioning mechanism called style basis, which extracts key poses from the reference and additively guides the diffusion generation process to fit the style without compromising lip synchronization quality. This approach enables the model to capture subtle stylistic cues while ensuring that the generated animations align closely with the input speech. Extensive qualitative, quantitative, and perceptual evaluations demonstrate the effectiveness of our method in faithfully reproducing the desired style while achieving superior lip synchronization across various speech scenarios.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2505.00293.pdf' target='_blank'>https://arxiv.org/pdf/2505.00293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masanori Takano, Mao Nishiguchi, Fujio Toriumi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00293">Reducing Sexual Predation and Victimization Through Warnings and Awareness among High-Risk Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online sexual predators target children by building trust, creating dependency, and arranging meetings for sexual purposes. This poses a significant challenge for online communication platforms that strive to monitor and remove such content and terminate predators' accounts. However, these platforms can only take such actions if sexual predators explicitly violate the terms of service, not during the initial stages of relationship-building. This study designed and evaluated a strategy to prevent sexual predation and victimization by delivering warnings and raising awareness among high-risk individuals based on the routine activity theory in criminal psychology. We identified high-risk users as those with a high probability of committing or being subjected to violations, using a machine learning model that analyzed social networks and monitoring data from the platform. We conducted a randomized controlled trial on a Japanese avatar-based communication application, Pigg Party. High-risk players in the intervention group received warnings and awareness-building messages, while those in the control group did not receive the messages, regardless of their risk level. The trial involved 12,842 high-risk players in the intervention group and 12,844 in the control group for 138 days. The intervention successfully reduced violations and being violated among women for 12 weeks, although the impact on men was limited. These findings contribute to efforts to combat online sexual abuse and advance understanding of criminal psychology.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2504.12943.pdf' target='_blank'>https://arxiv.org/pdf/2504.12943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Zheng, Zhuoyang Li, Xinning Gui, Yuhan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12943">Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized support is essential to fulfill individuals' emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2504.10010.pdf' target='_blank'>https://arxiv.org/pdf/2504.10010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Kocur, Niels Henze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10010">Investigating Environments' and Avatars' Effects on Thermal Perception in Virtual Reality to Reduce Energy Consumption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding thermal regulation and subjective perception of temperature is crucial for improving thermal comfort and human energy consumption in times of global warming. Previous work shows that an environment's color temperature affects the experienced temperature. As virtual reality (VR) enables visual immersion, recent work suggests that a VR scene's color temperature also affects experienced temperature. In addition, virtual avatars representing thermal cues influence users' thermal perception and even the body temperature. As immersive technology becomes increasingly prevalent in daily life, leveraging thermal cues to enhance thermal comfort - without relying on actual thermal energy - presents a promising opportunity. Understanding these effects is crucial for optimizing virtual experiences and promoting sustainable energy practices. Therefore, we propose three controlled experiments to learn more about thermal effects caused by virtual worlds and avatars.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2504.09296.pdf' target='_blank'>https://arxiv.org/pdf/2504.09296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Qing, Rekimoto Jun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09296">Look and Talk: Seamless AI Assistant Interaction with Gaze-Triggered Activation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Engaging with AI assistants to gather essential information in a timely manner is becoming increasingly common. Traditional activation methods, like wake words such as Hey Siri, Ok Google, and Hey Alexa, are constrained by technical challenges such as false activations, recognition errors, and discomfort in public settings. Similarly, activating AI systems via physical buttons imposes strict interactive limitations as it demands particular physical actions, which hinders fluid and spontaneous communication with AI. Our approach employs eye-tracking technology within AR glasses to discern a user's intention to engage with the AI assistant. By sustaining eye contact on a virtual AI avatar for a specific time, users can initiate an interaction silently and without using their hands. Preliminary user feedback suggests that this technique is relatively intuitive, natural, and less obtrusive, highlighting its potential for integrating AI assistants fluidly into everyday interactions.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2503.15489.pdf' target='_blank'>https://arxiv.org/pdf/2503.15489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elvis Kimara, Kunle S. Oguntoye, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15489">PersonaAI: Leveraging Retrieval-Augmented Generation and Personalized Context for AI-Driven Digital Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces PersonaAI, a cutting-edge application that leverages Retrieval-Augmented Generation (RAG) and the LLAMA model to create highly personalized digital avatars capable of accurately mimicking individual personalities. Designed as a cloud-based mobile application, PersonaAI captures user data seamlessly, storing it in a secure database for retrieval and analysis. The result is a system that provides context-aware, accurate responses to user queries, enhancing the potential of AI-driven personalization.
  Why should you care? PersonaAI combines the scalability of RAG with the efficiency of prompt-engineered LLAMA3, offering a lightweight, sustainable alternative to traditional large language model (LLM) training methods. The system's novel approach to data collection, utilizing real-time user interactions via a mobile app, ensures enhanced context relevance while maintaining user privacy. By open-sourcing our implementation, we aim to foster adaptability and community-driven development.
  PersonaAI demonstrates how AI can transform interactions by merging efficiency, scalability, and personalization, making it a significant step forward in the future of digital avatars and personalized AI.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2503.15225.pdf' target='_blank'>https://arxiv.org/pdf/2503.15225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelo Di Porzio, Marco Coraggio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15225">A Personalized Data-Driven Generative Model of Human Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of autonomous virtual avatars (in extended reality) and robots in human group activities - such as rehabilitation therapy, sports, and manufacturing - is expected to increase as these technologies become more pervasive. Designing cognitive architectures and control strategies to drive these agents requires realistic models of human motion. However, existing models only provide simplified descriptions of human motor behavior. In this work, we propose a fully data-driven approach, based on Long Short-Term Memory neural networks, to generate original motion that captures the unique characteristics of specific individuals. We validate the architecture using real data of scalar oscillatory motion. Extensive analyses show that our model effectively replicates the velocity distribution and amplitude envelopes of the individual it was trained on, remaining different from other individuals, and outperforming state-of-the-art models in terms of similarity to human data.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2503.14943.pdf' target='_blank'>https://arxiv.org/pdf/2503.14943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wang, Ivan Molodetskikh, Ondrej Texler, Dimitar Dinev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14943">3D Engine-ready Photorealistic Avatars via Dynamic Textures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the digital and physical worlds become more intertwined, there has been a lot of interest in digital avatars that closely resemble their real-world counterparts. Current digitization methods used in 3D production pipelines require costly capture setups, making them impractical for mass usage among common consumers. Recent academic literature has found success in reconstructing humans from limited data using implicit representations (e.g., voxels used in NeRFs), which are able to produce impressive videos. However, these methods are incompatible with traditional rendering pipelines, making it difficult to use them in applications such as games. In this work, we propose an end-to-end pipeline that builds explicitly-represented photorealistic 3D avatars using standard 3D assets. Our key idea is the use of dynamically-generated textures to enhance the realism and visually mask deficiencies in the underlying mesh geometry. This allows for seamless integration with current graphics pipelines while achieving comparable visual quality to state-of-the-art 3D avatar generation methods.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2503.14408.pdf' target='_blank'>https://arxiv.org/pdf/2503.14408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parisa Ghanad Torshizi, Laura B. Hensel, Ari Shapiro, Stacy C. Marsella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14408">Large Language Models for Virtual Human Gesture Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gestures convey a wide variety of meanings and play an important role in face-to-face human interactions. These gestures significantly influence the addressee's engagement, recall, comprehension, and attitudes toward the speaker. Similarly, they impact interactions between humans and embodied virtual agents. The process of selecting and animating meaningful gestures has thus become a key focus in the design of these agents. However, automating this gesture selection process poses a significant challenge. Prior gesture generation techniques have varied from fully automated, data-driven methods, which often struggle to produce contextually meaningful gestures, to more manual approaches that require crafting specific gesture expertise and are time-consuming and lack generalizability. In this paper, we leverage the semantic capabilities of Large Language Models to develop a gesture selection approach that suggests meaningful, appropriate co-speech gestures. We first describe how information on gestures is encoded into GPT-4. Then, we conduct a study to evaluate alternative prompting approaches for their ability to select meaningful, contextually relevant gestures and to align them appropriately with the co-speech utterance. Finally, we detail and demonstrate how this approach has been implemented within a virtual agent system, automating the selection and subsequent animation of the selected gestures for enhanced human-agent interactions.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2502.15956.pdf' target='_blank'>https://arxiv.org/pdf/2502.15956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canxuan Gang, Yiran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15956">Human Motion Prediction, Reconstruction, and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2502.02468.pdf' target='_blank'>https://arxiv.org/pdf/2502.02468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akash Haridas, Imran N. Junejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02468">High-Fidelity Human Avatars from Laptop Webcams using Edge Compute</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applications of generating photo-realistic human avatars are many, however, high-fidelity avatar generation traditionally required expensive professional camera rigs and artistic labor, but recent research has enabled constructing them automatically from smartphones with RGB and IR sensors. However, these new methods still rely on the presence of high-resolution cameras on modern smartphones and often require offloading the processing to powerful servers with GPUs. Modern applications such as video conferencing call for the ability to generate these avatars from consumer-grade laptop webcams using limited compute available on-device. In this work, we develop a novel method based on 3D morphable models, landmark detection, photo-realistic texture GANs, and differentiable rendering to tackle the problem of low webcam image quality and edge computation. We build an automatic system to generate high-fidelity animatable avatars under these limitations, leveraging the neural compute capabilities of mobile chips.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2501.16341.pdf' target='_blank'>https://arxiv.org/pdf/2501.16341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>D. Griol, A. Sanchis, J. M. Molina, Z. Callejas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16341">Developing Enhanced Conversational Agents for Social Virtual Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a methodology for the development of embodied conversational agents for social virtual worlds. The agents provide multimodal communication with their users in which speech interaction is included. Our proposal combines different techniques related to Artificial Intelligence, Natural Language Processing, Affective Computing, and User Modeling. Firstly, the developed conversational agents. A statistical methodology has been developed to model the system conversational behavior, which is learned from an initial corpus and improved with the knowledge acquired from the successive interactions. In addition, the selection of the next system response is adapted considering information stored into users profiles and also the emotional contents detected in the users utterances. Our proposal has been evaluated with the successful development of an embodied conversational agent which has been placed in the Second Life social virtual world. The avatar includes the different models and interacts with the users who inhabit the virtual world in order to provide academic information. The experimental results show that the agents conversational behavior adapts successfully to the specific characteristics of users interacting in such environments.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2501.12587.pdf' target='_blank'>https://arxiv.org/pdf/2501.12587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dekun Wang, Hongwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12587">How Collective Intelligence Emerges in a Crowd of People Through Learned Division of Labor: A Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the factors fostering collective intelligence (CI) through a case study of *LinYi's Experiment, where over 2000 human players collectively controll an avatar car. By conducting theoretical analysis and replicating observed behaviors through numerical simulations, we demonstrate how self-organized division of labor (DOL) among individuals fosters the emergence of CI and identify two essential conditions fostering CI by formulating this problem into a stability problem of a Markov Jump Linear System (MJLS). These conditions, independent of external stimulus, emphasize the importance of both elite and common players in fostering CI. Additionally, we propose an index for emergence of CI and a distributed method for estimating joint actions, enabling individuals to learn their optimal social roles without global action information of the whole crowd.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2412.16151.pdf' target='_blank'>https://arxiv.org/pdf/2412.16151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bharat Vyas, Carol O'Sullivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16151">Shape Shifters: Does Body Shape Change the Perception of Small-Scale Crowd Motions?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The animation of realistic virtual avatars in crowd scenarios is an important element of immersive virtual environments. However, achieving this realism requires attention to multiple factors, such as their visual appearance and motion cues. We investigated how body shape diversity influences the perception of motion clones in virtual crowds. A physics-based model was used to simulate virtual avatars in a small-scale crowd of size twelve. Participants viewed side-by-side video clips of these virtual crowds: one featuring all unique motions (Baseline) and the other containing motion clones (i.e., the same motion used to animate two or more avatars in the crowd). We also varied the levels of body shape and motion diversity. Our findings revealed that body shape diversity did not influence participants' ratings of motion clone detection, and motion variety had a greater impact on their perception of the crowd. Further research is needed to investigate how other visual factors interact with motion in order to enhance the perception of virtual crowd realism.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2412.07912.pdf' target='_blank'>https://arxiv.org/pdf/2412.07912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bora Tarlan, Nisa Erdal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07912">How Can I Assist You Today?: A Comparative Analysis of a Humanoid Robot and a Virtual Human Avatar in Human Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores human perceptions of intelligent agents by comparing interactions with a humanoid robot and a virtual human avatar, both utilizing GPT-3 for response generation. The study aims to understand how physical and virtual embodiments influence perceptions of anthropomorphism, animacy, likeability, and perceived intelligence. The uncanny valley effect was also investigated in the scope of this study based on the two agents' human-likeness and affinity. Conducted with ten participants from Sabanci University, the experiment involved tasks that sought advice, followed by assessments using the Godspeed Questionnaire Series and structured interviews. Results revealed no significant difference in anthropomorphism between the humanoid robot and the virtual human avatar, but the humanoid robot was perceived as more likable and slightly more intelligent, highlighting the importance of physical presence and interactive gestures. These findings suggest that while virtual avatars can achieve high human-likeness, physical embodiment enhances likeability and perceived intelligence. However, the study's scope was insufficient to claim the existence of the uncanny valley effect in the participants' interactions. The study offers practical insights for designing future intelligent assistants, emphasizing the need for integrating physical elements and sophisticated communicative behaviors to improve user experience and acceptance.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2412.04433.pdf' target='_blank'>https://arxiv.org/pdf/2412.04433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shota Sasaki, Jane Wu, Ko Nishino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04433">PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel clothed human model that can be learned from multiview RGB videos, with a particular emphasis on recovering physically accurate body and cloth movements. Our method, Position Based Dynamic Gaussians (PBDyG), realizes ``movement-dependent'' cloth deformation via physical simulation, rather than merely relying on ``pose-dependent'' rigid transformations. We model the clothed human holistically but with two distinct physical entities in contact: clothing modeled as 3D Gaussians, which are attached to a skinned SMPL body that follows the movement of the person in the input videos. The articulation of the SMPL body also drives physically-based simulation of the clothes' Gaussians to transform the avatar to novel poses. In order to run position based dynamics simulation, physical properties including mass and material stiffness are estimated from the RGB videos through Dynamic 3D Gaussian Splatting. Experiments demonstrate that our method not only accurately reproduces appearance but also enables the reconstruction of avatars wearing highly deformable garments, such as skirts or coats, which have been challenging to reconstruct using existing methods.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2411.18994.pdf' target='_blank'>https://arxiv.org/pdf/2411.18994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Asadchy, Maximilian Schich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18994">Descriptions of women are longer than that of men: An analysis of gender portrayal prompts in Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI for image creation emerges as a staple in the toolkit of digital artists, visual designers, and the general public. Social media users have many tools to shape their visual representation: image editing tools, filters, face masks, face swaps, avatars, and AI-generated images. The importance of the right profile image can not be understated: It is crucial for creating the right first impression, sustains trust, and enables communication. Conventionally correct representation of individuals, groups, and collectives may help foster inclusivity, understanding, and respect in society, ensuring that diverse perspectives are acknowledged and valued. While previous research revealed the biases in large image datasets such as ImageNet and inherited biases in the AI systems trained on it, within this work, we look at the prejudices and stereotypes as they emerge from textual prompts used for generating images on Discord using the StableDiffusion model. We analyze over 1.8 million prompts depicting men and women and use statistical methods to uncover how prompts describing men and women are constructed and what words constitute the portrayals of respective genders. We show that the median male description length is systematically shorter than the median female description length, while our findings also suggest a shared practice of prompting regarding the word length distribution. The topic analysis suggests the existence of classic stereotypes in which men are described using dominant qualities such as "strong" and "rugged". In contrast, women are represented with concepts related to body and submission: "beautiful", "pretty", etc. These results highlight the importance of the original intent of the prompting and suggest that cultural practices on platforms such as Discord should be considered when designing interfaces that promote exploration and fair representation.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2411.18949.pdf' target='_blank'>https://arxiv.org/pdf/2411.18949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Zhou, Ding Ding, Shengyu Wang, Chuhan Shi, Xiangyu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18949">Study on the Influence of Embodied Avatars on Gait Parameters in Virtual Environments and Real World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we compare the virtual and real gait parameters to investigate the effect of appearances of embodied avatars and virtual reality experience on gait in physical and virtual environments. We developed a virtual environment simulation and gait detection system for analyzing gait. The system transfers real-life scenarios into a realistic presentation in the virtual environment and provides look-alike same-age and old-age avatars for participants. We conducted an empirical study and used subjective questionnaires to evaluate participants' feelings about the virtual reality experience. Also, the paired sample t-test and neural network were implemented to analyze gait differences. The results suggest that there are disparities in gait between virtual and real environments. Also, the appearance of embodied avatars could influence the gait parameters in the virtual environment. Moreover, the experience of embodying old-age avatars affects the gait in the real world.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2411.13592.pdf' target='_blank'>https://arxiv.org/pdf/2411.13592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lamia Berriche, Maha Driss, Areej Ahmed Almuntashri, Asma Mufreh Lghabi, Heba Saleh Almudhi, Munerah Abdul-Aziz Almansour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13592">A Novel Speech Analysis and Correction Tool for Arabic-Speaking Children</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new application named ArPA for Arabic kids who have trouble with pronunciation. Our application comprises two key components: the diagnostic module and the therapeutic module. The diagnostic process involves capturing the child's speech signal, preprocessing, and analyzing it using different machine learning classifiers like K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Decision Trees as well as deep neural network classifiers like ResNet18. The therapeutic module offers eye-catching gamified interfaces in which each correctly spoken letter earns a higher avatar level, providing positive reinforcement for the child's pronunciation improvement. Two datasets were used for experimental evaluation: one from a childcare centre and the other including Arabic alphabet pronunciation recordings. Our work uses a novel technique for speech recognition using Melspectrogram and MFCC images. The results show that the ResNet18 classifier on speech-to-image converted data effectively identifies mispronunciations in Arabic speech with an accuracy of 99.015\% with Mel-Spectrogram images outperforming ResNet18 with MFCC images.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2411.10940.pdf' target='_blank'>https://arxiv.org/pdf/2411.10940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Hsiang Lien, Benedictus Kent Chandra, Robin Fischer, Ya-Hui Tang, Shiann-Jang Wang, Wei-En Hsu, Li-Chen Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10940">A Monocular SLAM-based Multi-User Positioning System with Image Occlusion in Augmented Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, with the rapid development of augmented reality (AR) technology, there is an increasing demand for multi-user collaborative experiences. Unlike for single-user experiences, ensuring the spatial localization of every user and maintaining synchronization and consistency of positioning and orientation across multiple users is a significant challenge. In this paper, we propose a multi-user localization system based on ORB-SLAM2 using monocular RGB images as a development platform based on the Unity 3D game engine. This system not only performs user localization but also places a common virtual object on a planar surface (such as table) in the environment so that every user holds a proper perspective view of the object. These generated virtual objects serve as reference points for multi-user position synchronization. The positioning information is passed among every user's AR devices via a central server, based on which the relative position and movement of other users in the space of a specific user are presented via virtual avatars all with respect to these virtual objects. In addition, we use deep learning techniques to estimate the depth map of an image from a single RGB image to solve occlusion problems in AR applications, making virtual objects appear more natural in AR scenes.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2411.08642.pdf' target='_blank'>https://arxiv.org/pdf/2411.08642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengdong Dong, Vijayakumar Bhagavatula, Zhenyu Zhou, Ajay Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08642">Towards More Accurate Fake Detection on Images Generated from Advanced Generative and Neural Rendering Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable progress in neural-network-driven visual data generation, especially with neural rendering techniques like Neural Radiance Fields and 3D Gaussian splatting, offers a powerful alternative to GANs and diffusion models. These methods can produce high-fidelity images and lifelike avatars, highlighting the need for robust detection methods. In response, an unsupervised training technique is proposed that enables the model to extract comprehensive features from the Fourier spectrum magnitude, thereby overcoming the challenges of reconstructing the spectrum due to its centrosymmetric properties. By leveraging the spectral domain and dynamically combining it with spatial domain information, we create a robust multimodal detector that demonstrates superior generalization capabilities in identifying challenging synthetic images generated by the latest image synthesis techniques. To address the absence of a 3D neural rendering-based fake image database, we develop a comprehensive database that includes images generated by diverse neural rendering techniques, providing a robust foundation for evaluating and advancing detection methods.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2411.05827.pdf' target='_blank'>https://arxiv.org/pdf/2411.05827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathieu Lamarre, Patrick Anderson, Ãtienne Danvoye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05827">A Theory of Stabilization by Skull Carving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate stabilization of facial motion is essential for applications in photoreal avatar construction for 3D games, virtual reality, movies, and training data collection. For the latter, stabilization must work automatically for the general population with people of varying morphology. Distinguishing rigid skull motion from facial expressions is critical since misalignment between skull motion and facial expressions can lead to animation models that are hard to control and can not fit natural motion. Existing methods struggle to work with sparse sets of very different expressions, such as when combining multiple units from the Facial Action Coding System (FACS). Certain approaches are not robust enough, some depend on motion data to find stable points, while others make one-for-all invalid physiological assumptions. In this paper, we leverage recent advances in neural signed distance fields and differentiable isosurface meshing to compute skull stabilization rigid transforms directly on unstructured triangle meshes or point clouds, significantly enhancing accuracy and robustness. We introduce the concept of a stable hull as the surface of the boolean intersection of stabilized scans, analogous to the visual hull in shape-from-silhouette and the photo hull from space carving. This hull resembles a skull overlaid with minimal soft tissue thickness, upper teeth are automatically included. Our skull carving algorithm simultaneously optimizes the stable hull shape and rigid transforms to get accurate stabilization of complex expressions for large diverse sets of people, outperforming existing methods.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2411.05653.pdf' target='_blank'>https://arxiv.org/pdf/2411.05653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leon O. H. Kroczek, Alexander May, Selina Hettenkofer, Andreas Ruider, Bernd Ludwig, Andreas MÃ¼hlberger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05653">The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities in conversational tasks. Embodying an LLM as a virtual human allows users to engage in face-to-face social interactions in Virtual Reality. However, the influence of person- and task-related factors in social interactions with LLM-controlled agents remains unclear. In this study, forty-six participants interacted with a virtual agent whose persona was manipulated as extravert or introvert in three different conversational tasks (small talk, knowledge test, convincing). Social-evaluation, emotional experience, and realism were assessed using ratings. Interactive engagement was measured by quantifying participants' words and conversational turns. Finally, we measured participants' willingness to ask the agent for help during the knowledge test. Our findings show that the extraverted agent was more positively evaluated, elicited a more pleasant experience and greater engagement, and was assessed as more realistic compared to the introverted agent. Whereas persona did not affect the tendency to ask for help, participants were generally more confident in the answer when they had help of the LLM. Variation of personality traits of LLM-controlled embodied virtual agents, therefore, affects social-emotional processing and behavior in virtual interactions. Embodied virtual agents allow the presentation of naturalistic social encounters in a virtual environment.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2411.01512.pdf' target='_blank'>https://arxiv.org/pdf/2411.01512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alvaro Budria, Adrian Lopez-Rodriguez, Oscar Lorente, Francesc Moreno-Noguer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01512">InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present InstantGeoAvatar, a method for efficient and effective learning from monocular video of detailed 3D geometry and appearance of animatable implicit human avatars. Our key observation is that the optimization of a hash grid encoding to represent a signed distance function (SDF) of the human subject is fraught with instabilities and bad local minima. We thus propose a principled geometry-aware SDF regularization scheme that seamlessly fits into the volume rendering pipeline and adds negligible computational overhead. Our regularization scheme significantly outperforms previous approaches for training SDFs on hash grids. We obtain competitive results in geometry reconstruction and novel view synthesis in as little as five minutes of training time, a significant reduction from the several hours required by previous work. InstantGeoAvatar represents a significant leap forward towards achieving interactive reconstruction of virtual avatars.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2410.21894.pdf' target='_blank'>https://arxiv.org/pdf/2410.21894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enes Yigitbas, Christian Kaltschmidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21894">Effects of Human Avatar Representation in Virtual Reality on Inter-Brain Connection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Increasing advances in affordable consumer hardware and accessible software frameworks are now bringing Virtual Reality (VR) to the masses. Especially collaborative VR applications where different people can work together are gaining momentum. In this context, human avatars and their representations are a crucial aspect of collaborative VR applications as they represent a digital twin of the end-users and determine how one is perceived in a virtual environment. When it comes to the effect of avatar representation on the end-users of collaborative VR applications, so far mostly questionnaires have been used to assess the quality of avatar representations. A promising alternative to objectively measure the effect of avatar representation is the investigation of inter-brain connections during the usage of a collaborative VR application. However, the combination of immersive VR applications and inter-brain connections has not been fully researched yet. Thus, our work investigates how different human avatar representations (real (RL), full-body (FB), and head-hand (HH)) affect inter-brain connections. For this purpose, we have designed and conducted a hyperscanning study with eight pairs. The main results of our hyperscanning study show that the number of significant sensor pairs was the highest in the RL, medium in the FB, and lowest in the HH condition indicating that an avatar that looks more like a real human enables more significant sensor pairs to appear in an EEG analysis.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2410.17741.pdf' target='_blank'>https://arxiv.org/pdf/2410.17741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zexu Huang, Sarah Monazam Erfani, Siying Lu, Mingming Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17741">Efficient Neural Implicit Representation for 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence, AR/VR, 3D graphics, and the rapidly evolving metaverse. Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110X faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 seconds of training.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2410.01840.pdf' target='_blank'>https://arxiv.org/pdf/2410.01840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanquan Shao, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01840">Target Pose Guided Whole-body Grasping Motion Generation for Digital Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grasping manipulation is a fundamental mode for human interaction with daily life objects. The synthesis of grasping motion is also greatly demanded in many applications such as animation and robotics. In objects grasping research field, most works focus on generating the last static grasping pose with a parallel gripper or dexterous hand. Grasping motion generation for the full arm especially for the full humanlike intelligent agent is still under-explored. In this work, we propose a grasping motion generation framework for digital human which is an anthropomorphic intelligent agent with high degrees of freedom in virtual world. Given an object known initial pose in 3D space, we first generate a target pose for whole-body digital human based on off-the-shelf target grasping pose generation methods. With an initial pose and this generated target pose, a transformer-based neural network is used to generate the whole grasping trajectory, which connects initial pose and target pose smoothly and naturally. Additionally, two post optimization components are designed to mitigates foot-skating issue and hand-object interpenetration separately. Experiments are conducted on GRAB dataset to demonstrate effectiveness of this proposed method for whole-body grasping motion generation with randomly placed unknown objects.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2409.16404.pdf' target='_blank'>https://arxiv.org/pdf/2409.16404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixin Guo, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16404">FastTalker: Jointly Generating Speech and Conversational Gestures from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human gestures and speech from a text script is critical for creating realistic talking avatars. One solution is to leverage separate pipelines for text-to-speech (TTS) and speech-to-gesture (STG), but this approach suffers from poor alignment of speech and gestures and slow inference times. In this paper, we introduce FastTalker, an efficient and effective framework that simultaneously generates high-quality speech audio and 3D human gestures at high inference speeds. Our key insight is reusing the intermediate features from speech synthesis for gesture generation, as these features contain more precise rhythmic information than features re-extracted from generated speech. Specifically, 1) we propose an end-to-end framework that concurrently generates speech waveforms and full-body gestures, using intermediate speech features such as pitch, onset, energy, and duration directly for gesture decoding; 2) we redesign the causal network architecture to eliminate dependencies on future inputs for real applications; 3) we employ Reinforcement Learning-based Neural Architecture Search (NAS) to enhance both performance and inference speed by optimizing our network architecture. Experimental results on the BEAT2 dataset demonstrate that FastTalker achieves state-of-the-art performance in both speech synthesis and gesture generation, processing speech and gestures in 0.17 seconds per second on an NVIDIA 3090.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2409.14430.pdf' target='_blank'>https://arxiv.org/pdf/2409.14430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzu-Chieh Liu, Chih-Ting Liu, Shao-Yi Chien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14430">Pomo3D: 3D-Aware Portrait Accessorizing and More</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Pomo3D, a 3D portrait manipulation framework that allows free accessorizing by decomposing and recomposing portraits and accessories. It enables the avatars to attain out-of-distribution (OOD) appearances of simultaneously wearing multiple accessories. Existing methods still struggle to offer such explicit and fine-grained editing; they either fail to generate additional objects on given portraits or cause alterations to portraits (e.g., identity shift) when generating accessories. This restriction presents a noteworthy obstacle as people typically seek to create charming appearances with diverse and fashionable accessories in the virtual universe. Our approach provides an effective solution to this less-addressed issue. We further introduce the Scribble2Accessories module, enabling Pomo3D to create 3D accessories from user-drawn accessory scribble maps. Moreover, we design a bias-conscious mapper to mitigate biased associations present in real-world datasets. In addition to object-level manipulation above, Pomo3D also offers extensive editing options on portraits, including global or local editing of geometry and texture and avatar stylization, elevating 3D editing of neural portraits to a more comprehensive level.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2409.09326.pdf' target='_blank'>https://arxiv.org/pdf/2409.09326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deng Junli, Luo Yihao, Yang Xueting, Li Siyou, Wang Wei, Guo Jinyang, Shi Ping
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09326">LawDNet: Enhanced Audio-Driven Lip Synthesis via Local Affine Warping Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of photorealistic avatar generation, the fidelity of audio-driven lip motion synthesis is essential for realistic virtual interactions. Existing methods face two key challenges: a lack of vivacity due to limited diversity in generated lip poses and noticeable anamorphose motions caused by poor temporal coherence. To address these issues, we propose LawDNet, a novel deep-learning architecture enhancing lip synthesis through a Local Affine Warping Deformation mechanism. This mechanism models the intricate lip movements in response to the audio input by controllable non-linear warping fields. These fields consist of local affine transformations focused on abstract keypoints within deep feature maps, offering a novel universal paradigm for feature warping in networks. Additionally, LawDNet incorporates a dual-stream discriminator for improved frame-to-frame continuity and employs face normalization techniques to handle pose and scene variations. Extensive evaluations demonstrate LawDNet's superior robustness and lip movement dynamism performance compared to previous methods. The advancements presented in this paper, including the methodologies, training data, source codes, and pre-trained models, will be made accessible to the research community.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2409.08577.pdf' target='_blank'>https://arxiv.org/pdf/2409.08577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Genki Sasaki, Hiroshi Igarashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08577">Exploring Remote Collaborative Tasks: The Impact of Avatar Representation on Dyadic Haptic Interactions in Shared Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study is the first to explore the interplay between haptic interaction and avatar representation in Shared Virtual Environments (SVEs). Specifically, how these factors shape users' sense of social presence during dyadic collaborations, while assessing potential effects on task performance. In a series of experiments, participants performed the collaborative task with haptic interaction under four avatar representation conditions: avatars of both participant and partner were displayed, only the participant's avatar was displayed, only the partner's avatar was displayed, and no avatars were displayed. The study finds that avatar representation, especially of the partner, significantly enhances the perception of social presence, which haptic interaction alone does not fully achieve. However, neither the presence nor the type of avatar representation impacts the task performance or participants' force effort of the task, suggesting that haptic interaction provides sufficient interaction cues for the execution of the task. These results underscore the significance of integrating both visual and haptic modalities to optimize remote collaboration experiences in virtual environments, ensuring effective communication and a strong sense of social presence.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2409.07984.pdf' target='_blank'>https://arxiv.org/pdf/2409.07984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kelian Baert, Shrisha Bharadwaj, Fabien Castan, Benoit Maujean, Marc Christie, Victoria Abrevaya, Adnane Boukhayma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07984">SPARK: Self-supervised Personalized Real-time Monocular Face Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feedforward monocular face capture methods seek to reconstruct posed faces from a single image of a person. Current state of the art approaches have the ability to regress parametric 3D face models in real-time across a wide range of identities, lighting conditions and poses by leveraging large image datasets of human faces. These methods however suffer from clear limitations in that the underlying parametric face model only provides a coarse estimation of the face shape, thereby limiting their practical applicability in tasks that require precise 3D reconstruction (aging, face swapping, digital make-up, ...). In this paper, we propose a method for high-precision 3D face capture taking advantage of a collection of unconstrained videos of a subject as prior information. Our proposal builds on a two stage approach. We start with the reconstruction of a detailed 3D face avatar of the person, capturing both precise geometry and appearance from a collection of videos. We then use the encoder from a pre-trained monocular face reconstruction method, substituting its decoder with our personalized model, and proceed with transfer learning on the video collection. Using our pre-estimated image formation model, we obtain a more precise self-supervision objective, enabling improved expression and pose alignment. This results in a trained encoder capable of efficiently regressing pose and expression parameters in real-time from previously unseen images, which combined with our personalized geometry model yields more accurate and high fidelity mesh inference. Through extensive qualitative and quantitative evaluation, we showcase the superiority of our final model as compared to state-of-the-art baselines, and demonstrate its generalization ability to unseen pose, expression and lighting.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2407.21521.pdf' target='_blank'>https://arxiv.org/pdf/2407.21521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boele Visser, Peter van der Putten, Amirhossein Zohrehvand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21521">The Impacts of AI Avatar Appearance and Disclosure on User Motivation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study examines the influence of perceived AI features on user motivation in virtual interactions. AI avatars, being disclosed as being an AI, or embodying specific genders, could be used in user-AI interactions. Leveraging insights from AI and avatar research, we explore how AI disclosure and gender affect user motivation. We conducted a game-based experiment involving over 72,500 participants who solved search problems alone or with an AI companion. Different groups experienced varying AI appearances and disclosures. We measured play intensity. Results revealed that the presence of another avatar led to less intense play compared to solo play. Disclosure of the avatar as AI heightened effort intensity compared to non-disclosed AI companions. Additionally, a masculine AI appearance reduced effort intensity.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2407.18934.pdf' target='_blank'>https://arxiv.org/pdf/2407.18934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senem Tanberk, Dilek Bilgin Tukel, Kadir Acar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18934">The Design of a 3D Character Animation System for Digital Twins in the Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of Industry 4.0, digital twin technology has emerged with rapid advancements as a powerful tool for visualizing and analyzing industrial assets. This technology has attracted considerable interest from researchers across diverse domains such as manufacturing, security, transportation, and gaming. The metaverse has emerged as a significant enabler in these domains, facilitating the integration of various technologies to create virtual replicas of physical assets. The utilization of 3D character animation, often referred to as avatars, is crucial for implementing the metaverse. Traditionally, costly motion capture technologies are employed for creating a realistic avatar system. To meet the needs of this evolving landscape, we have developed a modular framework tailored for asset digital twins as a more affordable alternative. This framework offers flexibility for the independent customization of individual system components. To validate our approach, we employ the English peg solitaire game as a use case, generating a solution tree using the breadth-first search algorithm. The results encompass both qualitative and quantitative findings of a data-driven 3D animation system utilizing motion primitives. The presented methodologies and infrastructure are adaptable and modular, making them applicable to asset digital twins across diverse business contexts. This case study lays the groundwork for pilot applications and can be tailored for education, health, or Industry 4.0 material development.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2407.15956.pdf' target='_blank'>https://arxiv.org/pdf/2407.15956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Alizadeh, Dave Randall, Peter Tolmie, Minha Lee, Yuhui Xu, Sarah Mennicken, MikoÅaj P. WoÅºniak, Dennis Paul, Dominik Pins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15956">Future of Home-living: Designing Smart Spaces for Modern Domestic Life</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of smart home technologies, particularly agentic ones such as conversational agents, robots, and virtual avatars, is reshaping our understanding of home and domestic life. This shift highlights the complexities of modern domestic life, with the household landscape now featuring diverse cohabiting units like co-housing and communal living arrangements. These agentic technologies present specific design challenges and opportunities as they become integrated into everyday routines and activities. Our workshop envisions smart homes as dynamic, user-shaped spaces, focusing on the integration of these technologies into daily life. We aim to explore how these technologies transform household dynamics, especially through boundary fluidity, by uniting researchers and practitioners from fields such as design, sociology, and ethnography. Together, we will develop a taxonomy of challenges and opportunities, providing a structured perspective on the integration of agentic technologies and their impact on contemporary living arrangements.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2407.04610.pdf' target='_blank'>https://arxiv.org/pdf/2407.04610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fred Atilla, Marie Postma, Maryam Alimardani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04610">Gamification of Motor Imagery Brain-Computer Interface Training Protocols: a systematic review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Motor Imagery Brain-Computer Interfaces (MI-BCI) require a lengthy and monotonous training procedure to train both the system and the user. Considering many users struggle with effective control of MI-BCI systems, a more user-centered approach to training might help motivate users and facilitate learning, alleviating inefficiency of the BCI system. With the increase of BCI-controlled games, researchers have suggested using game principles for BCI training, as games are naturally centered on the player. This review identifies and evaluates the application of game design elements to MI-BCI training, a process known as gamification. Through a systematic literature search, we examined how MI-BCI training protocols have been gamified and how specific game elements impacted the training outcomes. We identified 86 studies that employed gamified MI-BCI protocols in the past decade. The prevalence and reported effects of individual game elements on user experience and performance were extracted and synthesized. Results reveal that MI-BCI training protocols are most often gamified by having users move an avatar in a virtual environment that provides visual feedback. Furthermore, in these virtual environments, users were provided with goals that guided their actions. Using gamification, the reviewed protocols allowed users to reach effective MI-BCI control, with studies reporting positive effects of four individual elements on user performance and experience, namely: feedback, avatars, assistance, and social interaction. Based on these elements, this review makes current and future recommendations for effective gamification, such as the use of virtual reality and adaptation of game difficulty to user skill level.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2406.01223.pdf' target='_blank'>https://arxiv.org/pdf/2406.01223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Liu, Ke Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01223">Report on Methods and Applications for Crafting 3D Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an in-depth exploration of 3D human model and avatar generation technology, propelled by the rapid advancements in large-scale models and artificial intelligence. The paper reviews the comprehensive process of 3D human model generation, from scanning to rendering, and highlights the pivotal role these models play in entertainment, VR, AR, healthcare, and education. We underscore the significance of diffusion models in generating high-fidelity images and videos. It emphasizes the indispensable nature of 3D human models in enhancing user experiences and functionalities across various fields. Furthermore, this paper anticipates the potential of integrating large-scale models with deep learning to revolutionize 3D content generation, offering insights into the future prospects of the technology. It concludes by emphasizing the importance of continuous innovation in the field, suggesting that ongoing advancements will significantly expand the capabilities and applications of 3D human models and avatars.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2405.19941.pdf' target='_blank'>https://arxiv.org/pdf/2405.19941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon N. Chu, Alex J. Goodell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19941">Synthetic Patients: Simulating Difficult Conversations with Multimodal Generative AI for Medical Education</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Problem: Effective patient-centered communication is a core competency for physicians. However, both seasoned providers and medical trainees report decreased confidence in leading conversations on sensitive topics such as goals of care or end-of-life discussions. The significant administrative burden and the resources required to provide dedicated training in leading difficult conversations has been a long-standing problem in medical education.
  Approach: In this work, we present a novel educational tool designed to facilitate interactive, real-time simulations of difficult conversations in a video-based format through the use of multimodal generative artificial intelligence (AI). Leveraging recent advances in language modeling, computer vision, and generative audio, this tool creates realistic, interactive scenarios with avatars, or "synthetic patients." These synthetic patients interact with users throughout various stages of medical care using a custom-built video chat application, offering learners the chance to practice conversations with patients from diverse belief systems, personalities, and ethnic backgrounds.
  Outcomes: While the development of this platform demanded substantial upfront investment in labor, it offers a highly-realistic simulation experience with minimal financial investment. For medical trainees, this educational tool can be implemented within programs to simulate patient-provider conversations and can be incorporated into existing palliative care curriculum to provide a scalable, high-fidelity simulation environment for mastering difficult conversations.
  Next Steps: Future developments will explore enhancing the authenticity of these encounters by working with patients to incorporate their histories and personalities, as well as employing the use of AI-generated evaluations to offer immediate, constructive feedback to learners post-simulation.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2405.13924.pdf' target='_blank'>https://arxiv.org/pdf/2405.13924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunday David Ubur, Denis Gracanin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13924">Narrative Review of Emotional Expression Support in XR: Psychophysiology of Speech-to-Text Interfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This narrative review examines recent advancements, limitations, and research gaps in integrating emotional expression into speech-to-text (STT) interfaces within extended reality (XR) environments. Drawing from 37 peer-reviewed studies published between 2020 and 2024, we synthesized literature across multiple domains, including affective computing, psychophysiology, captioning innovation, and immersive human-computer interaction. Thematic categories include communication enhancement technologies for Deaf and Hard of Hearing (DHH) users, emotive captioning strategies, visual and affective augmentation in AR/VR, speech emotion recognition, and the development of empathic systems. Despite the growing accessibility of real-time STT tools, such systems largely fail to convey affective nuance, limiting the richness of communication for DHH users and other caption consumers. This review highlights emerging approaches such as animated captions, emojilization, color-coded overlays, and avatar-based emotion visualization, but finds a persistent gap in real-time emotion-aware captioning within immersive XR contexts. We identify key research opportunities at the intersection of accessibility, XR, and emotional expression, and propose future directions for the development of affect-responsive, user-centered captioning interfaces.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2405.07418.pdf' target='_blank'>https://arxiv.org/pdf/2405.07418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Guillermo FeijÃ³o-GarcÃ­a, Chase Wrenn, Alexandre Gomes de Siqueira, Rashi Ghosh, Jacob Stuart, Heng Yao, Benjamin Lok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07418">Exploring the Effects of User-Agent and User-Designer Similarity in Virtual Human Design to Promote Mental Health Intentions for College Students</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual humans (i.e., embodied conversational agents) have the potential to support college students' mental health, particularly in Science, Technology, Engineering, and Mathematics (STEM) fields where students are at a heightened risk of mental disorders such as anxiety and depression. A comprehensive understanding of students, considering their cultural characteristics, experiences, and expectations, is crucial for creating timely and effective virtual human interventions. To this end, we conducted a user study with 481 computer science students from a major university in North America, exploring how they co-designed virtual humans to support mental health conversations for students similar to them. Our findings suggest that computer science students who engage in co-design processes of virtual humans tend to create agents that closely resemble them demographically--agent-designer demographic similarity. Key factors influencing virtual human design included age, gender, ethnicity, and the matching between appearance and voice. We also observed that the demographic characteristics of virtual human designers, especially ethnicity and gender, tend to be associated with those of the virtual humans they designed. Finally, we provide insights concerning the impact of user-designer demographic similarity in virtual humans' effectiveness in promoting mental health conversations when designers' characteristics are shared explicitly or implicitly. Understanding how virtual humans' characteristics serve users' experiences in mental wellness conversations and the similarity-attraction effects between agents, users, and designers may help tailor virtual humans' design to enhance their acceptance and increase their counseling effectiveness.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2405.05933.pdf' target='_blank'>https://arxiv.org/pdf/2405.05933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakki O. Bailey, Xinyue, You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05933">Considering Avatar Crossing as Harm or Help for Adolescents in Social VR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People leverage avatars to communicate nonverbal behaviors in immersive virtual reality (VR), like interpersonal distance [2, 6] and virtual touch [5]. However, violations of appropriate physical distancing and unsolicited intimate touching behavior in social virtual worlds represent potential social and psychological virtual harm to older adolescent users [4, 8]. Obtaining peer acceptance and social rewards, while avoiding social rejection can drive older adolescent behavior even in simulated virtual spaces [1, 3], and while "the beginning of adolescence is largely defined by a biological event, [...] the end of adolescence is often defined socially" [3] (p.912). Avatar crossing, the phenomenon of avatars walking through each other in virtual environments, is a unique capability of virtual embodiment, and others intriguing possibilities and ethical concerns for older adolescents experiencing social virtual spaces. For example, the ability to cross through and share positions with other avatars in a virtual classroom helps students concentrate on accessing and comprehending information without concerns about blocking others when navigating for better viewpoints [10]. However, the ability to cross through others in virtual spaces has been associated with a reduction in perceived presence and avatar realism, coupled with a greater level of discomfort and intimidation in comparison to avatar collisions [12]. In this article, we consider the potential benefits and harms of utilizing avatar crossing with adolescent users.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2405.03844.pdf' target='_blank'>https://arxiv.org/pdf/2405.03844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pitch Sinlapanuntakul, Mark Zachry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03844">Perception in Pixels: Effects of Avatar Representation in Video-Mediated Collaborative Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive collaborative video is now a common part of remote work. Despite its prevalence, traditional video conferencing can be challenging, sometimes causing social discomforts that undermine process and outcomes. Avatars on 2D displays offer a promising alternative for enhancing self-representation, bridging the gap between virtual reality (VR) and traditional non-immersive video. However, the use of such avatars in activity-oriented group settings remains underexplored. To address this gap, we conducted a mixed-methods, within-subject study investigating the impacts of avatar-mediated versus traditional video representations on collaboration satisfaction and self-esteem. 32 participants (8 groups of 4 with pre-established relationships) engaged in goal-directed activities, followed by group interviews. Results indicate that avatars significantly enhance self-esteem and collaboration satisfaction, while qualitative insights reveal the dynamic perceptions and experiences of avatars, including benefits, challenges, and factors influencing adoption likelihood. Our study contributes to understanding and implications of avatars as a camera-driven representation in video-mediated collaborative interactions.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2404.12317.pdf' target='_blank'>https://arxiv.org/pdf/2404.12317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbo Yu, Graeme McKinley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12317">Synthetic Participatory Planning of Shard Automated Electric Mobility Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unleashing the synergies among rapidly evolving mobility technologies in a multi-stakeholder setting presents unique challenges and opportunities for addressing urban transportation problems. This paper introduces a novel synthetic participatory method that critically leverages large language models (LLMs) to create digital avatars representing diverse stakeholders to plan shared automated electric mobility systems (SAEMS). These calibratable agents collaboratively identify objectives, envision and evaluate SAEMS alternatives, and strategize implementation under risks and constraints. The results of a Montreal case study indicate that a structured and parameterized workflow provides outputs with higher controllability and comprehensiveness on an SAEMS plan than that generated using a single LLM-enabled expert agent. Consequently, this approach provides a promising avenue for cost-efficiently improving the inclusivity and interpretability of multi-objective transportation planning, suggesting a paradigm shift in how we envision and strategize for sustainable transportation systems.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2402.15340.pdf' target='_blank'>https://arxiv.org/pdf/2402.15340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aitor Toichoa Eyam, Jose L. Martinez Lastra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15340">MetaStates: An Approach for Representing Human Workers' Psychophysiological States in the Industrial Metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photo-realistic avatar is a modern term referring to the digital asset that represents a human in computer graphic advanced systems such as video games and simulation tools. These avatars utilize the advances in graphic technologies in both software and hardware aspects. While photo-realistic avatars are increasingly used in industrial simulations, representing human factors such as human workers psychophysiological states, remains a challenge. This article contributes to resolving this issue by introducing the novel concept of MetaStates which are the digitization and representation of the psychophysiological states of a human worker in the digital world. The MetaStates influence the physical representation and performance of a digital human worker while performing a task. To demonstrate this concept, this study presents the development of a photo-realistic avatar enhanced with multi-level graphical representations of psychophysiological states relevant to Industry 5.0. This approach represents a major step forward in the use of digital humans for industrial simulations, allowing companies to better leverage the benefits of the Industrial Metaverse in their daily operations and simulations while keeping human workers at the center of the system.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2402.05854.pdf' target='_blank'>https://arxiv.org/pdf/2402.05854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>LÃª ThÃ nh DÅ©ng NguyÃªn, Gabriele Vanoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05854">Slightly Non-Linear Higher-Order Tree Transducers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the tree-to-tree functions computed by "affine $Î»$-transducers": tree automata whose memory consists of an affine $Î»$-term instead of a finite state. They can be seen as variations on Gallot, Lemay and Salvati's Linear High-Order Deterministic Tree Transducers.
  When the memory is almost purely affine ($\textit{Ã  la}$ Kanazawa), we show that these machines can be translated to tree-walking transducers (and with a purely affine memory, we get a reversible tree-walking transducer). This leads to a proof of an inexpressivity conjecture of NguyÃªn and Pradic on "implicit automata" in an affine $Î»$-calculus. We also prove that a more powerful variant, extended with preprocessing by an MSO relabeling and allowing a limited amount of non-linearity, is equivalent in expressive power to Engelfriet, Hoogeboom and Samwel's invisible pebble tree transducers.
  The key technical tool in our proofs is the Interaction Abstract Machine (IAM), an operational avatar of Girard's geometry of interaction, a semantics of linear logic. We work with ad-hoc specializations to $Î»$-terms of low exponential depth of a tree-generating version of the IAM.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2401.16534.pdf' target='_blank'>https://arxiv.org/pdf/2401.16534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Zhu, Dalton Omens, Haodi He, Ron Fedkiw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16534">Democratizing the Creation of Animatable Facial Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In high-end visual effects pipelines, a customized (and expensive) light stage system is (typically) used to scan an actor in order to acquire both geometry and texture for various expressions. Aiming towards democratization, we propose a novel pipeline for obtaining geometry and texture as well as enough expression information to build a customized person-specific animation rig without using a light stage or any other high-end hardware (or manual cleanup). A key novel idea consists of warping real-world images to align with the geometry of a template avatar and subsequently projecting the warped image into the template avatar's texture; importantly, this allows us to leverage baked-in real-world lighting/texture information in order to create surrogate facial features (and bridge the domain gap) for the sake of geometry reconstruction. Not only can our method be used to obtain a neutral expression geometry and de-lit texture, but it can also be used to improve avatars after they have been imported into an animation system (noting that such imports tend to be lossy, while also hallucinating various features). Since a default animation rig will contain template expressions that do not correctly correspond to those of a particular individual, we use a Simon Says approach to capture various expressions and build a person-specific animation rig (that moves like they do). Our aforementioned warping/projection method has high enough efficacy to reconstruct geometry corresponding to each expressions.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2401.12981.pdf' target='_blank'>https://arxiv.org/pdf/2401.12981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Yan, Gil Alterovitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12981">A General-purpose AI Avatar in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in machine learning and natural language processing have led to the rapid development of artificial intelligence (AI) as a valuable tool in the healthcare industry. Using large language models (LLMs) as conversational agents or chatbots has the potential to assist doctors in diagnosing patients, detecting early symptoms of diseases, and providing health advice to patients. This paper focuses on the role of chatbots in healthcare and explores the use of avatars to make AI interactions more appealing to patients. A framework of a general-purpose AI avatar application is demonstrated by using a three-category prompt dictionary and prompt improvement mechanism. A two-phase approach is suggested to fine-tune a general-purpose AI language model and create different AI avatars to discuss medical issues with users. Prompt engineering enhances the chatbot's conversational abilities and personality traits, fostering a more human-like interaction with patients. Ultimately, the injection of personality into the chatbot could potentially increase patient engagement. Future directions for research include investigating ways to improve chatbots' understanding of context and ensuring the accuracy of their outputs through fine-tuning with specialized medical data sets.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2401.06957.pdf' target='_blank'>https://arxiv.org/pdf/2401.06957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Nadeem, Raza Imam, Rouqaiah Al-Refai, Meriem Chkir, Mohamad Hoda, Abdulmotaleb El Saddik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06957">EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has grown. Addressing this demand, we introduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation (EVOKE), a lightweight emotion recognition framework designed for the seamless integration of emotion recognition into 3D avatars within virtual environments. Our approach leverages knowledge distillation involving multi-label classification on the publicly available DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Remarkably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results, boasting an accuracy of 87% while demanding far less computational resources. This equilibrium between performance and deployability positions our framework as an ideal choice for virtual environment systems. Furthermore, the multi-label classification outcomes are utilized to map emotions onto custom-designed 3D avatars.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2401.02171.pdf' target='_blank'>https://arxiv.org/pdf/2401.02171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanyu Wang, Weizhan Zhang, Christian Sandor, Hongbo Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02171">Real-and-Present: Investigating the Use of Life-Size 2D Video Avatars in HMD-Based AR Teleconferencing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented Reality (AR) teleconferencing allows separately located users to interact with each other in 3D through agents in their own physical environments. Existing methods leveraging volumetric capturing and reconstruction can provide a high-fidelity experience but are often too complex and expensive for everyday usage. Other solutions target mobile and effortless-to-setup teleconferencing on AR Head Mounted Displays (HMD). They directly transplant the conventional video conferencing onto an AR-HMD platform or use avatars to represent remote participants. However, they can only support either a high fidelity or a high level of co-presence. Moreover, the limited Field of View (FoV) of HMDs could further influence users' immersive experience. To achieve a balance between fidelity and co-presence, we explore using life-size 2D video-based avatars (video avatars for short) in AR teleconferencing. Specifically, with the potential effect of FoV on users' perception of proximity, we first conduct a pilot study to explore the local-user-centered optimal placement of video avatars in small-group AR conversations. With the placement results, we then implement a proof-of-concept prototype of video-avatar-based teleconferencing. We conduct user evaluations with the prototype to verify its effectiveness in balancing fidelity and co-presence. Following the indication in the pilot study, we further quantitatively explore the effect of FoV size on the video avatar's optimal placement through a user study involving more FoV conditions in a VR-simulated environment. We regress placement models to serve as references for computationally determining video avatar placements in such teleconferencing applications on various existing AR HMDs and future ones with bigger FoVs.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2401.00289.pdf' target='_blank'>https://arxiv.org/pdf/2401.00289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Shahinur Alam, Jason Lamberton, Jianye Wang, Carly Leannah, Sarah Miller, Joseph Palagano, Myles de Bastion, Heather L. Smith, Melissa Malzkuhn, Lorna C. Quandt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00289">ASL Champ!: A Virtual Reality Game with Deep-Learning Driven Sign Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We developed an American Sign Language (ASL) learning platform in a Virtual Reality (VR) environment to facilitate immersive interaction and real-time feedback for ASL learners. We describe the first game to use an interactive teaching style in which users learn from a fluent signing avatar and the first implementation of ASL sign recognition using deep learning within the VR environment. Advanced motion-capture technology powers an expressive ASL teaching avatar within an immersive three-dimensional environment. The teacher demonstrates an ASL sign for an object, prompting the user to copy the sign. Upon the user's signing, a third-party plugin executes the sign recognition process alongside a deep learning model. Depending on the accuracy of a user's sign production, the avatar repeats the sign or introduces a new one. We gathered a 3D VR ASL dataset from fifteen diverse participants to power the sign recognition model. The proposed deep learning model's training, validation, and test accuracy are 90.12%, 89.37%, and 86.66%, respectively. The functional prototype can teach sign language vocabulary and be successfully adapted as an interactive ASL learning platform in VR.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2312.06310.pdf' target='_blank'>https://arxiv.org/pdf/2312.06310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mizuki Nakajima, Kaoruko Shinkawa, Yoshihiro Nakata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06310">Development of the Lifelike Head Unit for a Humanoid Cybernetic Avatar `Yui' and Its Operation Interface</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of avatar-mediated communication, it is crucial for the face-to-face interlocutor to sense the operator's presence and emotions via the avatar. Although androids resembling humans have been developed to convey presence through appearance and movement, few studies have prioritized deepening the communication experience for both operator and interlocutor using android robot as an avatar. Addressing this gap, we introduce the ``Cybernetic Avatar `Yui','' featuring a human-like head unit with 28 degrees of freedom, capable of expressing gaze, facial emotions, and speech-related mouth movements. Through an eye-tracking unit in a Head-Mounted Display (HMD) and degrees of freedom on both eyes of Yui, operators can control the avatar's gaze naturally. Additionally, microphones embedded in Yui's ears allow operators to hear surrounding sounds in three dimensions, enabling them to discern the direction of calls based solely on auditory information. An HMD's face-tracking unit synchronizes the avatar's facial movements with those of the operator. This immersive interface, coupled with Yui's human-like appearance, enables real-time emotion transmission and communication, enhancing the sense of presence for both parties. Our experiments demonstrate Yui's facial expression capabilities, and validate the system's efficacy through teleoperation trials, suggesting potential advancements in avatar technology.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2312.02897.pdf' target='_blank'>https://arxiv.org/pdf/2312.02897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Long, Swati Pandita, Andrea Stevenson Won
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02897">Perspectives from Naive Participants and Experienced Social Science Researchers on Addressing Embodiment in a Virtual Cyberball Task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe the design of an immersive virtual Cyberball task that included avatar customization, and user feedback on this design. We first created a prototype of an avatar customization template and added it to a Cyberball prototype built in the Unity3D game engine. Then, we conducted in-depth user testing and feedback sessions with 15 Cyberball stakeholders: five naive participants with no prior knowledge of Cyberball and ten experienced researchers with extensive experience using the Cyberball paradigm. We report the divergent perspectives of the two groups on the following design insights; designing for intuitive use, inclusivity, and realistic experiences versus minimalism. Participant responses shed light on how system design problems may contribute to or perpetuate negative experiences when customizing avatars. They also demonstrate the value of considering multiple stakeholders' feedback in the design process for virtual reality, presenting a more comprehensive view in designing future Cyberball prototypes and interactive systems for social science research.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2312.02128.pdf' target='_blank'>https://arxiv.org/pdf/2312.02128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vitor Miguel Xavier Peres, Greice Pinho Dal Molin, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02128">Can we truly transfer an actor's genuine happiness to avatars? An investigation into virtual, real, posed and spontaneous faces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A look is worth a thousand words is a popular phrase. And why is a simple look enough to portray our feelings about something or someone? Behind this question are the theoretical foundations of the field of psychology regarding social cognition and the studies of psychologist Paul Ekman. Facial expressions, as a form of non-verbal communication, are the primary way to transmit emotions between human beings. The set of movements and expressions of facial muscles that convey some emotional state of the individual to their observers are targets of studies in many areas. Our research aims to evaluate Ekman's action units in datasets of real human faces, posed and spontaneous, and virtual human faces resulting from transferring real faces into Computer Graphics faces. In addition, we also conducted a case study with specific movie characters, such as SheHulk and Genius. We intend to find differences and similarities in facial expressions between real and CG datasets, posed and spontaneous faces, and also to consider the actors' genders in the videos. This investigation can help several areas of knowledge, whether using real or virtual human beings, in education, health, entertainment, games, security, and even legal matters. Our results indicate that AU intensities are greater for posed than spontaneous datasets, regardless of gender. Furthermore, there is a smoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a real face is transformed into CG.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2311.15251.pdf' target='_blank'>https://arxiv.org/pdf/2311.15251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Misirlis, Yiannis Nikolaidis, Anna Sabidussi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15251">Should I use metaverse or not? An investigation of university students behavioral intention to use MetaEducation technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse, a burgeoning technological trend that combines virtual and augmented reality, provides users with a fully digital environment where they can assume a virtual identity through a digital avatar and interact with others as they were in the real world. Its applications span diverse domains such as economy (with its entry into the cryptocurrency field), finance, social life, working environment, healthcare, real estate, and education. During the COVID-19 and post-COVID-19 era, universities have rapidly adopted e-learning technologies to provide students with online access to learning content and platforms, rendering previous considerations on integrating such technologies or preparing institutional infrastructures virtually obsolete. In light of this context, the present study proposes a framework for analyzing university students' acceptance and intention to use metaverse technologies in education, drawing upon the Technology Acceptance Model (TAM). The study aims to investigate the relationship between students' intention to use metaverse technologies in education, hereafter referred to as MetaEducation, and selected TAM constructs, including Attitude, Perceived Usefulness, Perceived Ease of Use, Self-efficacy of metaverse technologies in education, and Subjective Norm. Notably, Self-efficacy and Subjective Norm have a positive influence on Attitude and Perceived Usefulness, whereas Perceived Ease of Use does not exhibit a strong correlation with Attitude or Perceived Usefulness. The authors postulate that the weak associations between the study's constructs may be attributed to limited knowledge regarding MetaEducation and its potential benefits. Further investigation and analysis of the study's proposed model are warranted to comprehensively understand the complex dynamics involved in the acceptance and utilization of MetaEducation technologies in the realm of higher education
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2311.04427.pdf' target='_blank'>https://arxiv.org/pdf/2311.04427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shuo Lin, Ching-Yi Tsai, Lung-Pan Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04427">Clonemator: Composing Spatiotemporal Clones to Create Interactive Automators in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clonemator is a virtual reality (VR) system allowing users to create their avatar clones and configure them spatially and temporally, forming automators to accomplish complex tasks. In particular, clones can (1) freeze at a user's body pose as static objects, (2) synchronously mimic the user's movement, and (3) replay a sequence of the user's actions in a period of time later. Combined with traditional techniques such as scaling, positional rearrangement, group selection, and duplication, Clonemator enables users to iteratively develop customized and reusable solutions by breaking down complex tasks into a sequence of collaborations with clones. This bypasses implementing dedicated interaction techniques or scripts while allowing flexible interactions in VR applications. We demonstrate the flexibility of Clonemator with several examples and validate its usability and effectiveness through a preliminary user study. Finally, we discuss the potential of Clonemator in VR applications such as gaming mechanisms, spatial interaction techniques, and multi-robot control and provide our insights for future research.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2311.02082.pdf' target='_blank'>https://arxiv.org/pdf/2311.02082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel AP Oliveira, Stephane Manara, Bruno MolÃ©, Thomas Muller, AurÃ©lien Guillouche, Lysann Hesske, Bruce Jordan, Gilles Hubert, Chinmay Kulkarni, Pralipta Jagdev, Cedric R. Berger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02082">Semantic Modelling of Organizational Knowledge as a Basis for Enterprise Data Governance 4.0 -- Application to a Unified Clinical Data Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Individuals and organizations cope with an always-growing amount of data, which is heterogeneous in its contents and formats. An adequate data management process yielding data quality and control over its lifecycle is a prerequisite to getting value out of this data and minimizing inherent risks related to multiple usages. Common data governance frameworks rely on people, policies, and processes that fall short of the overwhelming complexity of data. Yet, harnessing this complexity is necessary to achieve high-quality standards. The latter will condition any downstream data usage outcome, including generative artificial intelligence trained on this data. In this paper, we report our concrete experience establishing a simple, cost-efficient framework that enables metadata-driven, agile and (semi-)automated data governance (i.e. Data Governance 4.0). We explain how we implement and use this framework to integrate 25 years of clinical study data at an enterprise scale in a fully productive environment. The framework encompasses both methodologies and technologies leveraging semantic web principles. We built a knowledge graph describing avatars of data assets in their business context, including governance principles. Multiple ontologies articulated by an enterprise upper ontology enable key governance actions such as FAIRification, lifecycle management, definition of roles and responsibilities, lineage across transformations and provenance from source systems. This metadata model is the keystone to data governance 4.0: a semi-automatised data management process that considers the business context in an agile manner to adapt governance constraints to each use case and dynamically tune it based on business changes.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2310.15447.pdf' target='_blank'>https://arxiv.org/pdf/2310.15447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyun-Song Kwon, Sung-Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15447">DeepIron: Predicting Unwarped Garment Texture from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic reconstruction of 3D clothing from an image has wide applications, such as avatar creation and virtual try-on. This paper presents a novel framework that reconstructs the texture map for 3D garments from a single image with pose. Assuming that 3D garments are modeled by stitching 2D garment sewing patterns, our specific goal is to generate a texture image for the sewing patterns. A key component of our framework, the Texture Unwarper, infers the original texture image from the input clothing image, which exhibits warping and occlusion of texture due to the user's body shape and pose. The Texture Unwarper effectively transforms between the input and output images by mapping the latent spaces of the two images. By inferring the unwarped original texture of the input garment, our method helps reconstruct 3D garment models that can show high-quality texture images realistically deformed for new poses. We validate the effectiveness of our approach through a comparison with other methods and ablation studies.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2310.02739.pdf' target='_blank'>https://arxiv.org/pdf/2310.02739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hussam Azzuni, Sharim Jamal, Abdulmotaleb Elsaddik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02739">uTalk: Bridging the Gap Between Humans and AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have revolutionized various industries by harnessing their power to improve productivity and facilitate learning across different fields. One intriguing application involves combining LLMs with visual models to create a novel approach to Human-Computer Interaction. The core idea of this system is to create a user-friendly platform that enables people to utilize ChatGPT's features in their everyday lives. uTalk is comprised of technologies like Whisper, ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking head system SadTalker. Users can engage in human-like conversation with a digital twin and receive answers to any questions. Also, uTalk could generate content by submitting an image and input (text or audio). This system is hosted on Streamlit, where users will be prompted to provide an image to serve as their AI assistant. Then, as the input (text or audio) is provided, a set of operations will produce a video of the avatar with the precise response. This paper outlines how SadTalker's run-time has been optimized by 27.69% based on 25 frames per second (FPS) generated videos and 38.38% compared to our 20FPS generated videos. Furthermore, the integration and parallelization of SadTalker and Streamlit have resulted in a 9.8% improvement compared to the initial performance of the system.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2308.10207.pdf' target='_blank'>https://arxiv.org/pdf/2308.10207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Lu, Bo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10207">Affective Digital Twins for Digital Human: Bridging the Gap in Human-Machine Affective Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, metaverse and digital humans have become important research and industry areas of focus. However, existing digital humans still lack realistic affective traits, making emotional interaction with humans difficult. Grounded in the developments of artificial intelligence, human-computer interaction, virtual reality, and affective computing, this paper proposes the concept and technical framework of "Affective Digital Twins for Digital Human" based on the philosophy of digital twin technology. The paper discusses several key technical issues including affective modeling, affective perception, affective encoding, and affective expression. Based on this, the paper conducts a preliminary imagination of the future application prospects of affective digital twins for digital human, while considering potential problems that may need to be addressed.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2308.02139.pdf' target='_blank'>https://arxiv.org/pdf/2308.02139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Berthault, Takuma Kato, Akihiko Shirai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02139">Avatar Fusion Karaoke: Research and development on multi-user music play VR experience in the metaverse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper contributes to building a standard process of research and development (R&D) for new user experiences (UX) in metaverse services. We tested this R&D process on a new UX proof of concept (PoC) for Meta Quest head-mounted display (HMDs) consisting of a school-life karaoke experience with the hypothesis that it is possible to design the avatars with only the necessary functions and rendering costs. The school life metaverse is a relevant subject for discovering issues and problems in this type of simultaneous connection. To qualitatively evaluate the potential of a multi-person metaverse experience, this study investigated subjects where each avatar requires expressive skills. While avatar play experiences feature artistic expressions, such as dancing, playing musical instruments, and drawing, and these can be used to evaluate their operability and expressive capabilities qualitatively, the Quest's tracking capabilities are insufficient for full-body performance and graphical art expression. Considering such hardware limitations, this study evaluated the Quest, focusing primarily on UX simplicity using AI Fusion techniques and expressiveness in instrumental scenes played by approximately four avatars. This research reported methods for multiuser metaverse communication and its supporting technologies, such as head-mounted devices and their graphics performance, special interaction techniques, and complementary tools and the importance of PoC development, its evaluation, and its iterations. The result is remarkable for further research; these expressive technologies in a multi-user context are directly related to the quality of communication within the metaverse and the value of the user-generated content (UGC) produced there.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2308.01889.pdf' target='_blank'>https://arxiv.org/pdf/2308.01889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bavo Van Kerrebroeck, Kristel CrombÃ©, StÃ©phanie Wilain, Marc Leman, Pieter-Jan Maes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01889">The virtual drum circle: polyrhythmic music interactions in extended reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging technologies in the domain of extended reality offer rich, new possibilities for the study and practice of joint music performance. Apart from the technological challenges, bringing music players together in extended reality raises important questions on their performance and embodied coordination. In this study, we designed an extended reality platform to assess a remote, bidirectional polyrhythmic interaction between two players, mediated in real time by their three-dimensional embodied avatars and a shared, virtual drum circle. We leveraged a multi-layered analysis framework to assess their performance quality, embodied co-regulation and first-person interaction experience, using statistical techniques for time-series analysis and mixed-effect regression and focusing on contrasts of visual coupling (not seeing / seeing as avatars / seeing as real) and auditory context (metronome / music). Results reveal that an auditory context with music improved the performance output as measured by a prediction error, increased movement energy and levels of experienced agency. Visual coupling impacted experiential qualities and induced prosocial effects with increased levels of partner realism resulting in increased levels of shared agency and self-other merging. Embodied co-regulation between players was impacted by auditory context and visual coupling, suggesting prediction-based compensatory mechanisms to deal with the novelty, difficulty, and expressivity in the musical interaction. This study contributes to the understanding of music performance in extended reality by using a methodological approach to demonstrate how co-regulation between players is impacted by visual coupling and auditory context and provides a basis and future directions for further action-oriented research.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2308.01492.pdf' target='_blank'>https://arxiv.org/pdf/2308.01492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Blooma John, Ramanathan Subramanian, Jayan Chirayath Kurian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01492">A Virtual Reality Game to Improve Physical and Cognitive Acuity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Virtual Human Benchmark (VHB) game to evaluate and improve physical and cognitive acuity. VHB simulates in 3D the BATAK lightboard game, which is designed to improve physical reaction and hand-eye coordination, on the \textit{Oculus Rift} and \textit{Quest} headsets. The game comprises the \textit{reaction}, \textit{accumulator} and \textit{sequence} modes; \bj{along} with the \textit{reaction} and \textit{accumulator} modes which mimic BATAK functionalities, the \textit{sequence} mode involves the user repeating a sequence of illuminated targets with increasing complexity to train visual memory and cognitive processing. A first version of the game (VHB v1) was evaluated against the real-world BATAK by 20 users, and their feedback was utilized to improve game design and obtain a second version (VHB v2). Another study to evaluate VHB v2 was conducted with 20 users, whose results confirmed that the deign improvements enhanced game usability and user experience in multiple respects. Also, logging and visualization of performance data such as \textit{reaction time}, \textit{speed between targets} and \textit{completed sequence patterns} provides useful data for coaches/therapists monitoring sports/rehabilitation regimens.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2307.12345.pdf' target='_blank'>https://arxiv.org/pdf/2307.12345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Stock, Stephan SchlÃ¶gl, Aleksander Groth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12345">Tell me, what are you most afraid of? Exploring the Effects of Agent Representation on Information Disclosure in Human-Chatbot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-disclosure counts as a key factor influencing successful health treatment, particularly when it comes to building a functioning patient-therapist-connection. To this end, the use of chatbots may be considered a promising puzzle piece that helps foster respective information provision. Several studies have shown that people disclose more information when they are interacting with a chatbot than when they are interacting with another human being. If and how the chatbot is embodied, however, seems to play an important role influencing the extent to which information is disclosed. Here, research shows that people disclose less if the chatbot is embodied with a human avatar in comparison to a chatbot without embodiment. Still, there is only little information available as to whether it is the embodiment with a human face that inhibits disclosure, or whether any type of face will reduce the amount of shared information. The study presented in this paper thus aims to investigate how the type of chatbot embodiment influences self-disclosure in human-chatbot-interaction. We conducted a quasi-experimental study in which $n=178$ participants were asked to interact with one of three settings of a chatbot app. In each setting, the humanness of the chatbot embodiment was different (i.e., human vs. robot vs. disembodied). A subsequent discourse analysis explored difference in the breadth and depth of self-disclosure. Results show that non-human embodiment seems to have little effect on self-disclosure. Yet, our data also shows, that, contradicting to previous work, human embodiment may have a positive effect on the breadth and depth of self-disclosure.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2307.06124.pdf' target='_blank'>https://arxiv.org/pdf/2307.06124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>InÃªs Lacerda, Hugo Nicolau, Luisa Coheur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06124">Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current signing avatars are often described as unnatural as they cannot accurately reproduce all the subtleties of synchronized body behaviors of a human signer. In this paper, we propose a new dynamic approach for transitions between signs, focusing on mouthing animations for Portuguese Sign Language. Although native signers preferred animations with dynamic transitions, we did not find significant differences in comprehension and perceived naturalness scores. On the other hand, we show that including mouthing behaviors improved comprehension and perceived naturalness for novice sign language learners. Results have implications in computational linguistics, human-computer interaction, and synthetic animation of signing avatars.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2307.04247.pdf' target='_blank'>https://arxiv.org/pdf/2307.04247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jieun Kim, Hauke Sandhaus, Susan R. Fussell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04247">VR Job Interview Using a Gender-Swapped Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality (VR) has emerged as a potential solution for mitigating bias in a job interview by hiding the applicants' demographic features. The current study examines the use of a gender-swapped avatar in a virtual job interview that affects the applicants' perceptions and their performance evaluated by recruiters. With a mixed-method approach, we first conducted a lab experiment (N=8) exploring how using a gender-swapped avatar in a virtual job interview impacts perceived anxiety, confidence, competence, and ability to perform. Then, a semi-structured interview investigated the participants' VR interview experiences using an avatar. Our findings suggest that using gender-swapped avatars may reduce the anxiety that job applicants will experience during the interview. Also, the affinity diagram produced seven key themes highlighting the advantages and limitations of VR as an interview platform. These findings contribute to the emerging field of VR-based recruitment and have practical implications for promoting diversity and inclusion in the hiring process.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2307.02103.pdf' target='_blank'>https://arxiv.org/pdf/2307.02103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelhadi Soudi, Manal El Hakkaoui, Kristof Van Laerhoven
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02103">Do predictability factors towards signing avatars hold across cultures?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Avatar technology can offer accessibility possibilities and improve the Deaf-and-Hard of Hearing sign language users access to communication, education and services, such as the healthcare system. However, sign language users acceptance of signing avatars as well as their attitudes towards them vary and depend on many factors. Furthermore, research on avatar technology is mostly done by researchers who are not Deaf. The study examines the extent to which intrinsic or extrinsic factors contribute to predict the attitude towards avatars across cultures. Intrinsic factors include the characteristics of the avatar, such as appearance, movements and facial expressions. Extrinsic factors include users technology experience, their hearing status, age and their sign language fluency. This work attempts to answer questions such as, if lower attitude ratings are related to poor technology experience with ASL users, for example, is that also true for Moroccan Sign Language (MSL) users? For the purposes of the study, we designed a questionnaire to understand MSL users attitude towards avatars. Three groups of participants were surveyed: Deaf (57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then compared with those reported in other relevant studies.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2307.01468.pdf' target='_blank'>https://arxiv.org/pdf/2307.01468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanyu Pan, Guowei Yang, Taijiang Mu, Yu-Kun Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01468">Generating Animatable 3D Cartoon Faces from Single Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the booming of virtual reality (VR) technology, there is a growing need for customized 3D avatars. However, traditional methods for 3D avatar modeling are either time-consuming or fail to retain similarity to the person being modeled. We present a novel framework to generate animatable 3D cartoon faces from a single portrait image. We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN. Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture, which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision. Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer. Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria. Furthermore, we demonstrate the capability of real-time facial animation of our 3D model.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2306.14754.pdf' target='_blank'>https://arxiv.org/pdf/2306.14754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Filhol, Thomas von Ascheberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14754">ReprÃ©sentation graphique de la langue des signes franÃ§aise et Ã©dition logicielle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cet article propose une mÃ©thode pour dÃ©finir une forme graphique Ã©ditable standardisÃ©e pour les langues des signes, ainsi qu'une proposition "AZVD" et un Ã©diteur logiciel associÃ©. InspirÃ©e d'une part par les rÃ©gularitÃ©s observÃ©es dans les pratiques spontanÃ©es de locuteurs pratiquant la schÃ©matisation, la dÃ©marche tente garantir un systÃ¨me qualifiÃ© d'adoptable. LiÃ©e d'autre part au modÃ¨le formel de reprÃ©sentation AZee, elle vise Ã©galement Ã  spÃ©cifier un systÃ¨me dont toutes les productions ont une lecture dÃ©terminÃ©e au point oÃ¹ elles sont automatiquement synthÃ©tisables par un avatar.
  --
  This paper proposes a definition method for an editable standard graphical form of Sign Language discourse representation. It also puts forward a tentative system "AZVD", and presents an associated software editor. The system is inspired by the regularities observed in spontaneous diagrams produced by some language users, in order to make it as adoptable as possible. Moreover, it is built upon the formal representation model AZee, so that any graphical instance produced by the system determines its own read-out form, to the point that they can be automatically synthesised by an avatar.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2305.06747.pdf' target='_blank'>https://arxiv.org/pdf/2305.06747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zina Kamal, Hossein Hassani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06747">The First Parallel Corpora for Kurdish Sign Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kurdish Sign Language (KuSL) is the natural language of the Kurdish Deaf people. We work on automatic translation between spoken Kurdish and KuSL. Sign languages evolve rapidly and follow grammatical rules that differ from spoken languages. Consequently,those differences should be considered during any translation. We proposed an avatar-based automatic translation of Kurdish texts in the Sorani (Central Kurdish) dialect into the Kurdish Sign language. We developed the first parallel corpora for that pair that we use to train a Statistical Machine Translation (SMT) engine. We tested the outcome understandability and evaluated it using the Bilingual Evaluation Understudy (BLEU). Results showed 53.8% accuracy. Compared to the previous experiments in the field, the result is considerably high. We suspect the reason to be the similarity between the structure of the two pairs. We plan to make the resources publicly available under CC BY-NC-SA 4.0 license on the Kurdish-BLARK (https://kurdishblark.github.io/).
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2305.01665.pdf' target='_blank'>https://arxiv.org/pdf/2305.01665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takuma Miyaguchi, Hideyoshi Yanagisawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01665">Modeling the sense of presence of remote participants in hybrid communication and its application to the design of avatar robot behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formulated the sense of the presence of a remote participant in hybrid communication using a Bayesian framework. We also applied the knowledge gained from the simulation with the Bayesian model to the avatar robot's intervention behavior and encouraged the local participants to speak by intervening in the remote participant's behavior using an avatar robot. We then modeled the influence of the avatar robot's behavior on the local participants' statements using an active inference framework that included the presence of a remote participant as a latent variable. Based on the simulation results, we designed the gaze behavior of an avatar robot. Finally, we examined the effectiveness of the designed gaze behavior of the avatar robot. The gaze behavior expressed more of the remote participant's attention and interest in local participants, but local participants expressed fewer opinions in the meeting tasks. The results suggest that gaze behavior increased the presence of the remote participant and discouraged the local participant from speaking in the context of the experimental task. We believe that presence has a sufficiently large influence on whether participants want to express an opinion. It is worth investigating the influence of presence and its control methods using Bayesian models.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2305.00082.pdf' target='_blank'>https://arxiv.org/pdf/2305.00082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Kataoka, Hyunsoo Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00082">AVATAR: Adversarial self-superVised domain Adaptation network for TARget domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an unsupervised domain adaptation (UDA) method for predicting unlabeled target domain data, specific to complex UDA tasks where the domain gap is significant. Mainstream UDA models aim to learn from both domains and improve target discrimination by utilizing labeled source domain data. However, the performance boost may be limited when the discrepancy between the source and target domains is large or the target domain contains outliers. To explicitly address this issue, we propose the Adversarial self-superVised domain Adaptation network for the TARget domain (AVATAR) algorithm. It outperforms state-of-the-art UDA models by concurrently reducing domain discrepancy while enhancing discrimination through domain adversarial learning, self-supervised learning, and sample selection strategy for the target domain, all guided by deep clustering. Our proposed model significantly outperforms state-of-the-art methods on three UDA benchmarks, and extensive ablation studies and experiments demonstrate the effectiveness of our approach for addressing complex UDA tasks.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2304.06678.pdf' target='_blank'>https://arxiv.org/pdf/2304.06678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Viceconti, Maarten De Vos, Sabato Mellone, Liesbet Geris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06678">From the digital twins in healthcare to the Virtual Human Twin: a moon-shot project for digital health research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The idea of a systematic digital representation of the entire known human pathophysiology, which we could call the Virtual Human Twin, has been around for decades. To date, most research groups focused instead on developing highly specialised, highly focused patient-specific models able to predict specific quantities of clinical relevance. While it has facilitated harvesting the low-hanging fruits, this narrow focus is, in the long run, leaving some significant challenges that slow the adoption of digital twins in healthcare. This position paper lays the conceptual foundations for developing the Virtual Human Twin (VHT). The VHT is intended as a distributed and collaborative infrastructure, a collection of technologies and resources (data, models) that enable it, and a collection of Standard Operating Procedures (SOP) that regulate its use. The VHT infrastructure aims to facilitate academic researchers, public organisations, and the biomedical industry in developing and validating new digital twins in healthcare solutions with the possibility of integrating multiple resources if required by the specific context of use. Healthcare professionals and patients can also use the VHT infrastructure for clinical decision support or personalised health forecasting. As the European Commission launched the EDITH coordination and support action to develop a roadmap for the development of the Virtual Human Twin, this position paper is intended as a starting point for the consensus process and a call to arms for all stakeholders.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2304.02309.pdf' target='_blank'>https://arxiv.org/pdf/2304.02309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Stettler, Alexander Lappe, Nick Taubert, Martin Giese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02309">Multi-Domain Norm-referenced Encoding Enables Data Efficient Transfer Learning of Facial Expression Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>People can innately recognize human facial expressions in unnatural forms, such as when depicted on the unusual faces drawn in cartoons or when applied to an animal's features. However, current machine learning algorithms struggle with out-of-domain transfer in facial expression recognition (FER). We propose a biologically-inspired mechanism for such transfer learning, which is based on norm-referenced encoding, where patterns are encoded in terms of difference vectors relative to a domain-specific reference vector. By incorporating domain-specific reference frames, we demonstrate high data efficiency in transfer learning across multiple domains. Our proposed architecture provides an explanation for how the human brain might innately recognize facial expressions on varying head shapes (humans, monkeys, and cartoon avatars) without extensive training. Norm-referenced encoding also allows the intensity of the expression to be read out directly from neural unit activity, similar to face-selective neurons in the brain. Our model achieves a classification accuracy of 92.15\% on the FERG dataset with extreme data efficiency. We train our proposed mechanism with only 12 images, including a single image of each class (facial expression) and one image per domain (avatar). In comparison, the authors of the FERG dataset achieved a classification accuracy of 89.02\% with their FaceExpr model, which was trained on 43,000 images.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2304.01405.pdf' target='_blank'>https://arxiv.org/pdf/2304.01405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vrushank Phadnis, Kristin Moore, Mar Gonzalez Franco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01405">The Work Avatar Face-Off: Knowledge Worker Preferences for Realism in Meetings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While avatars have grown in popularity in social settings, their use in the workplace is still debatable. We conducted a large-scale survey to evaluate knowledge worker sentiment towards avatars, particularly the effects of realism on their acceptability for work meetings. Our survey of 2509 knowledge workers from multiple countries rated five avatar styles for use by managers, known colleagues and unknown colleagues.
  In all scenarios, participants favored higher realism, but fully realistic avatars were sometimes perceived as uncanny. Less realistic avatars were rated worse when interacting with an unknown colleague or manager, as compared to a known colleague. Avatar acceptability varied by country, with participants from the United States and South Korea rating avatars more favorably. We supplemented our quantitative findings with a thematic analysis of open-ended responses to provide a comprehensive understanding of factors influencing work avatar choices.
  In conclusion, our results show that realism had a significant positive correlation with acceptability. Non-realistic avatars were seen as fun and playful, but only suitable for occasional use.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2303.09275.pdf' target='_blank'>https://arxiv.org/pdf/2303.09275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johan F. Hoorn, Ivy S. Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09275">The Media Inequality, Uncanny Mountain, and the Singularity is Far from Near: Iwaa and Sophia Robot versus a Real Human Being</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Design of Artificial Intelligence and robotics habitually assumes that adding more humanlike features improves the user experience, mainly kept in check by suspicion of uncanny effects. Three strands of theorizing are brought together for the first time and empirically put to the test: Media Equation (and in its wake, Computers Are Social Actors), Uncanny Valley theory, and as an extreme of human-likeness assumptions, the Singularity. We measured the user experience of real-life visitors of a number of seminars who were checked in either by Smart Dynamics' Iwaa, Hanson's Sophia robot, Sophia's on-screen avatar, or a human assistant. Results showed that human-likeness was not in appearance or behavior but in attributed qualities of being alive. Media Equation, Singularity, and Uncanny hypotheses were not confirmed. We discuss the imprecision in theorizing about human-likeness and rather opt for machines that 'function adequately.'
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2303.04932.pdf' target='_blank'>https://arxiv.org/pdf/2303.04932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Luo, Chunpeng Wang, Colin Keil, David Nguyen, Henry Mayne, Stephen Alt, Eric Schwarm, Evelyn Mendoza, TaÅkÄ±n PadÄ±r, John Peter Whitney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04932">Team Northeastern's Approach to ANA XPRIZE Avatar Final Testing: A Holistic Approach to Telepresence and Lessons Learned</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports on Team Northeastern's Avatar system for telepresence, and our holistic approach to meet the ANA Avatar XPRIZE Final testing task requirements. The system features a dual-arm configuration with hydraulically actuated glove-gripper pair for haptic force feedback. Our proposed Avatar system was evaluated in the ANA Avatar XPRIZE Finals and completed all 10 tasks, scored 14.5 points out of 15.0, and received the 3rd Place Award. We provide the details of improvements over our first generation Avatar, covering manipulation, perception, locomotion, power, network, and controller design. We also extensively discuss the major lessons learned during our participation in the competition.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2301.08719.pdf' target='_blank'>https://arxiv.org/pdf/2301.08719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A Badano, M Lago, E Sizikova, JG Delfino, S Guan, MA Anastasio, B Sahiner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08719">The stochastic digital human is now enrolling for in silico imaging trials -- Methods and tools for generating digital cohorts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Randomized clinical trials, while often viewed as the highest evidentiary bar by which to judge the quality of a medical intervention, are far from perfect. In silico imaging trials are computational studies that seek to ascertain the performance of a medical device by collecting this information entirely via computer simulations. The benefits of in silico trials for evaluating new technology include significant resource and time savings, minimization of subject risk, the ability to study devices that are not achievable in the physical world, allow for the rapid and effective investigation of new technologies and ensure representation from all relevant subgroups. To conduct in silico trials, digital representations of humans are needed. We review the latest developments in methods and tools for obtaining digital humans for in silico imaging studies. First, we introduce terminology and a classification of digital human models. Second, we survey available methodologies for generating digital humans with healthy and diseased status and examine briefly the role of augmentation methods. Finally, we discuss the trade-offs of four approaches for sampling digital cohorts and the associated potential for study bias with selecting specific patient distributions.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2301.02527.pdf' target='_blank'>https://arxiv.org/pdf/2301.02527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bianca Marques, Rui NÃ³brega, Carmen Morgado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02527">Avatar-centred AR Collaborative Mobile Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interaction with the physical environment and different users is essential to foster a collaborative experience. For this, we propose an interaction based on a central point represented by an Augmented Reality marker in which several users can capture the attention and interact with a virtual avatar. The interface provides different game modes, with various challenges, supporting a collaborative mobile interaction. The system fosters various group interactions with a virtual avatar and enables various tasks with playful and didactic components.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2212.13063.pdf' target='_blank'>https://arxiv.org/pdf/2212.13063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Marx, Jonas Rieskamp, Milad Mirbabaie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.13063">"Just a Normal Day in the Metaverse" -- Distraction Conflicts of Knowledge Work in Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The changing nature of knowledge work creates demands for emerging technologies as enablers for workplace innovation. One emerging technology to potentially remedy drawbacks of remote work arrangements are meta-verses that merge physical reality with digital virtuality. In the literature, such innovations in the knowledge work sector have been primarily examined against the backdrop of collaboration as a dependent variable. In this paper, however, we investigate knowledge work in metaverses from a distraction-conflict perspective because independent, uninterrupted activities are as much characteristic of knowledge work as collaboration. Preliminary findings show that knowledge workers in meta-verses experience arousal from the 1) presence, appearance, and behaviour of other avatars, 2) realism, novelty, and affordances of the virtual environment, and 3) technological friction and navigation. This work has the theoretical implication that distraction-conflict theory must be extended to incorporate additional sources of arousal when applied to the context of knowledge work in metaverses.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2210.04366.pdf' target='_blank'>https://arxiv.org/pdf/2210.04366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Perrine, Trevor Kirkby
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04366">KP-RNN: A Deep Learning Pipeline for Human Motion Prediction and Synthesis of Performance Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digitally synthesizing human motion is an inherently complex process, which can create obstacles in application areas such as virtual reality. We offer a new approach for predicting human motion, KP-RNN, a neural network which can integrate easily with existing image processing and generation pipelines. We utilize a new human motion dataset of performance art, Take The Lead, as well as the motion generation pipeline, the Everybody Dance Now system, to demonstrate the effectiveness of KP-RNN's motion predictions. We have found that our neural network can predict human dance movements effectively, which serves as a baseline result for future works using the Take The Lead dataset. Since KP-RNN can work alongside a system such as Everybody Dance Now, we argue that our approach could inspire new methods for rendering human avatar animation. This work also serves to benefit the visualization of performance art in digital platforms by utilizing accessible neural networks.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2209.01320.pdf' target='_blank'>https://arxiv.org/pdf/2209.01320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddarth Ravichandran, OndÅej Texler, Dimitar Dinev, Hyun Jae Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01320">Synthesizing Photorealistic Virtual Humans Through Cross-modal Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last few decades, many aspects of human life have been enhanced with virtual domains, from the advent of digital assistants such as Amazon's Alexa and Apple's Siri to the latest metaverse efforts of the rebranded Meta. These trends underscore the importance of generating photorealistic visual depictions of humans. This has led to the rapid growth of so-called deepfake and talking-head generation methods in recent years. Despite their impressive results and popularity, they usually lack certain qualitative aspects such as texture quality, lips synchronization, or resolution, and practical aspects such as the ability to run in real-time. To allow for virtual human avatars to be used in practical scenarios, we propose an end-to-end framework for synthesizing high-quality virtual human faces capable of speaking with accurate lip motion with a special emphasis on performance. We introduce a novel network utilizing visemes as an intermediate audio representation and a novel data augmentation strategy employing a hierarchical image synthesis approach that allows disentanglement of the different modalities used to control the global head motion. Our method runs in real-time, and is able to deliver superior results compared to the current state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2509.17508.pdf' target='_blank'>https://arxiv.org/pdf/2509.17508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Filiol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17508">Community Covert Communication - Dynamic Mass Covert Communication Through Social Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the early 2010s, social network-based influence technologies have grown almost exponentially. Initiated by the U.S. Army's early OEV system in 2011, a number of companies specializing in this field have emerged. The most (in)famous cases are Bell Pottinger, Cambridge Analytica, Aggregate-IQ and, more recently, Team Jorge. In this paper, we consider the use-case of sock puppet master activities, which consist in creating hundreds or even thousands of avatars, in organizing them into communities and implement influence operations. On-purpose software is used to automate these operations (e.g. Ripon software, AIMS) and organize these avatar populations into communities. The aim is to organize targeted and directed influence communication to rather large communities (influence targets). The goal of the present research work is to show how these community management techniques (social networks) can also be used to communicate/disseminate relatively large volumes (up to a few tens of Mb) of multi-level encrypted information to a limited number of actors. To a certain extent, this can be compared to a Dark Post-type function, with a number of much more powerful potentialities. As a consequence, the concept of communication has been totally redefined and disrupted, so that eavesdropping, interception and jamming operations no longer make sense.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2509.03212.pdf' target='_blank'>https://arxiv.org/pdf/2509.03212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03212">AIVA: An AI-based Virtual Companion for Emotion-aware Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have significantly improved natural language understanding and generation, enhancing Human-Computer Interaction (HCI). However, LLMs are limited to unimodal text processing and lack the ability to interpret emotional cues from non-verbal signals, hindering more immersive and empathetic interactions. This work explores integrating multimodal sentiment perception into LLMs to create emotion-aware agents. We propose \ours, an AI-based virtual companion that captures multimodal sentiment cues, enabling emotionally aligned and animated HCI. \ours introduces a Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion transformer and supervised contrastive learning to provide emotional cues. Additionally, we develop an emotion-aware prompt engineering strategy for generating empathetic responses and integrate a Text-to-Speech (TTS) system and animated avatar module for expressive interactions. \ours provides a framework for emotion-aware agents with applications in companion robotics, social care, mental health, and human-centered AI.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2508.11872.pdf' target='_blank'>https://arxiv.org/pdf/2508.11872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinxing Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11872">Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical teaching, we observe that few students thoroughly read or fully comprehend the information provided in traditional, text-based course syllabi. As a result, essential details, such as course policies and learning outcomes, are frequently overlooked. To address this challenge, in this paper, we propose a novel approach leveraging AI-generated singing and virtual avatars to present syllabi in a format that is more visually appealing, engaging, and memorable. Especially, we leveraged the open-source tool, HeyGem, to transform textual syllabi into audiovisual presentations, in which digital avatars perform the syllabus content as songs. The proposed approach aims to stimulate students' curiosity, foster emotional connection, and enhance retention of critical course information. Student feedback indicated that AI-sung syllabi significantly improved awareness and recall of key course information.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2506.11890.pdf' target='_blank'>https://arxiv.org/pdf/2506.11890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Judson Leroy Dean Haynes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11890">Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality simulators offer a powerful tool for teacher training, yet the integration of AI-powered student avatars presents a critical challenge: determining the optimal level of avatar realism for effective pedagogy. This literature review examines the evolution of avatar realism in VR teacher training, synthesizes its theoretical implications, and proposes a new pedagogical framework to guide future design. Through a systematic review, this paper traces the progression from human-controlled avatars to generative AI prototypes. Applying learning theories like Cognitive Load Theory, we argue that hyper-realism is not always optimal, as high-fidelity avatars can impose excessive extraneous cognitive load on novices, a stance supported by recent empirical findings. A significant gap exists between the technological drive for photorealism and the pedagogical need for scaffolded learning. To address this gap, we propose Graduated Realism, a framework advocating for starting trainees with lower-fidelity avatars and progressively increasing behavioral complexity as skills develop. To make this computationally feasible, we outline a novel single-call architecture, Crazy Slots, which uses a probabilistic engine and a Retrieval-Augmented Generation database to generate authentic, real-time responses without the latency and cost of multi-step reasoning models. This review provides evidence-based principles for designing the next generation of AI simulators, arguing that a pedagogically grounded approach to realism is essential for creating scalable and effective teacher education tools.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2505.05475.pdf' target='_blank'>https://arxiv.org/pdf/2505.05475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonwoo Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05475">SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2411.18047.pdf' target='_blank'>https://arxiv.org/pdf/2411.18047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jennifer Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18047">The Trusted Caregiver: The Influence of Eye and Mouth Design Incorporating the Baby Schema Effect in Virtual Humanoid Agents on Older Adults Users' Perception of Trustworthiness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing proportion of the older adult population has made the smart home care industry one of the critical markets for virtual human-like agents. It is crucial to effectively promote a trustworthy human-computer partnership with older adults, enhancing service acceptance and effectiveness. However, few studies have focused on the facial features of the agents themselves, where the "baby schema" effect plays a vital role in enhancing trustworthiness. The eyes and mouth, in particular, attract most of the audience's attention and are especially significant. This study explores the impact of eye and mouth design on users' perception of trustworthiness. Specifically, a virtual humanoid agents model was developed, and based on this, 729 virtual facial images of children were designed. Participants (N=162) were asked to evaluate the impact of variations in the size and positioning of the eyes and mouth regions on the perceived credibility of these virtual agents. The results revealed that when the facial aspect ratio (width and height denoted as W and H, respectively) aligned with the "baby schema" effect (eye size at 0.25W, mouth size at 0.27W, eye height at 0.64H, eye distance at 0.43W, mouth height at 0.74H, and smile arc at 0.043H), the virtual agents achieved the highest facial credibility. This study proposes a design paradigm for the main facial features of virtual humanoid agents, which can increase the trust of older adults during interactions and significantly contribute to the research on the trustworthiness of virtual humanoid agents.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2411.10452.pdf' target='_blank'>https://arxiv.org/pdf/2411.10452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10452">Directing Avatars in Live Performances -- an Autonomy Simulacrum of Virtual Entities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose to review the main stages in the computer history of virtual actors, with a view to the exploration of virtual reality and discussion on different approaches to human simulation. The notion of autonomy emerges as a key issue for the virtual entities. We then explore one way of building elements of autonomy and conclude with an example of avatar stage direction leading to a simulacrum of autonomy in a live performance.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2410.12148.pdf' target='_blank'>https://arxiv.org/pdf/2410.12148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wells Lucas Santo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12148">Facing Identity: The Formation and Performance of Identity via Face-Based Artificial Intelligence Technologies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How is identity constructed and performed in the digital via face-based artificial intelligence technologies? While questions of identity on the textual Internet have been thoroughly explored, the Internet has progressed to a multimedia form that not only centers the visual, but specifically the face. At the same time, a wealth of scholarship has and continues to center the topics of surveillance and control through facial recognition technologies (FRTs), which have extended the logics of the racist pseudoscience of physiognomy. Much less work has been devoted to understanding how such face-based artificial intelligence technologies have influenced the formation and performance of identity. This literature review considers how such technologies interact with faciality, which entails the construction of what a face may represent or signify, along axes of identity such as race, gender, and sexuality. In grappling with recent advances in AI such as image generation and deepfakes, I propose that we are now in an era of "post-facial" technologies that build off our existing culture of facility while eschewing the analog face, complicating our relationship with identity vis-a-vis the face. Drawing from previous frameworks of identity play in the digital, as well as trans practices that have historically played with or transgressed the boundaries of identity classification, we can develop concepts adequate for analyzing digital faciality and identity given the current landscape of post-facial artificial intelligence technologies that allow users to interface with the digital in an entirely novel manner. To ground this framework of transgression, I conclude by proposing an interview study with VTubers -- online streamers who perform using motion-captured avatars instead of their real-life faces -- to gain qualitative insight on how these sociotechnical experiences.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2408.10040.pdf' target='_blank'>https://arxiv.org/pdf/2408.10040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moshe BenBassat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10040">The Practimum-Optimum Algorithm for Manufacturing Scheduling: A Paradigm Shift Leading to Breakthroughs in Scale and Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Practimum-Optimum (P-O) algorithm represents a paradigm shift in developing automatic optimization products for complex real-life business problems such as large-scale manufacturing scheduling. It leverages deep business domain expertise to create a group of virtual human expert (VHE) agents with different "schools of thought" on how to create high-quality schedules. By computerizing them into algorithms, P-O generates many valid schedules at far higher speeds than human schedulers are capable of. Initially, these schedules can also be local optimum peaks far away from high-quality schedules. By submitting these schedules to a reinforced machine learning algorithm (RL), P-O learns the weaknesses and strengths of each VHE schedule, and accordingly derives reward and punishment changes in the Demand Set that will modify the relative priorities for time and resource allocation that jobs received in the prior iteration that led to the current state of the schedule. These cause the core logic of the VHE algorithms to explore, in the subsequent iteration, substantially different parts of the schedules universe and potentially find higher-quality schedules. Using the hill climbing analogy, this may be viewed as a big jump, shifting from a given local peak to a faraway promising start point equipped with knowledge embedded in the demand set for future iterations. This is a fundamental difference from most contemporary algorithms, which spend considerable time on local micro-steps restricted to the neighbourhoods of local peaks they visit. This difference enables a breakthrough in scale and performance for fully automatic manufacturing scheduling in complex organizations. The P-O algorithm is at the heart of Plataine Scheduler that, in one click, routinely schedules 30,000-50,000 tasks for real-life complex manufacturing operations.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2407.09483.pdf' target='_blank'>https://arxiv.org/pdf/2407.09483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09483">The Shadow: Coevolution Processes Between a Director, Actors and Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Andersen's tale The Shadow offers a theatrical situation confronting a Scholar to his Shadow. I program specific creatures that I called shadow avatar to stage the story with five of them and a physical narrator. Echoing Edmond Couchot's ideas about virtual people helping human beings to adapt to technological evolutions, I describe dynamics of coevolution characterizing the relationship between a director, actors, and shadow avatars during the process of staging The Shadow.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2406.01056.pdf' target='_blank'>https://arxiv.org/pdf/2406.01056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Mandava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01056">Virtual avatar generation models as world navigators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SABR-CLIMB, a novel video model simulating human movement in rock climbing environments using a virtual avatar. Our diffusion transformer predicts the sample instead of noise in each diffusion step and ingests entire videos to output complete motion sequences. By leveraging a large proprietary dataset, NAV-22M, and substantial computational resources, we showcase a proof of concept for a system to train general-purpose virtual avatars for complex tasks in robotics, sports, and healthcare.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2405.09574.pdf' target='_blank'>https://arxiv.org/pdf/2405.09574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georges GagnerÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09574">AKN_Regie: a bridge between digital and performing arts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In parallel with the dissemination of information technology, we note the persistence of frontiers within creative practices, in particular between the digital arts and the performing arts. Crossings of these frontiers brought to light the need for a common appropriation of digital issues. As a result of this appropriation, the AvatarStaging platform and its software dimension AKN_Regie will be described in their use to direct avatars on a mixed theatre stage. Developed with the Blueprint visual language within Epic Games' Unreal Engine, AKN_Regie offers a user interface accessible to non-programming artists. This feature will be used to describe two perspectives of appropriation of the tool: the Plugin perspective for these users and the Blueprint perspective for programming artists who want to improve the tool. These two perspectives are then completed by a C++ perspective that aligns AKN_Regie with the language with which the engine itself is programmed. The circulations between these three perspectives are finally studied by drawing on work on the ecology of collective intelligence.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2403.17631.pdf' target='_blank'>https://arxiv.org/pdf/2403.17631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoxu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17631">AniArtAvatar: Animatable 3D Art Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements. Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D diffusion model to synthesize multi-view images from a single art portrait with a neutral expression. With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface. For avatar animation, we extract control points, transfer the motion with these points, and deform the implicit canonical space. Firstly, we render the front image of the avatar, extract the 2D landmarks, and project them to the 3D space using a trained SDF network. We extract 3D driving landmarks using 3DMM and transfer the motion to the avatar landmarks. To animate the avatar pose, we manually set the body height and bound the head and torso of an avatar with two cages. The head and torso can be animated by transforming the two cages. Our approach is a one-shot pipeline that can be applied to various styles. Experiments demonstrate that our method can generate high-quality 3D art avatars with desired control over different motions.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2403.00153.pdf' target='_blank'>https://arxiv.org/pdf/2403.00153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karan Ahuja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00153">Practical and Rich User Digitization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A long-standing vision in computer science has been to evolve computing devices into proactive assistants that enhance our productivity, health and wellness, and many other facets of our lives. User digitization is crucial in achieving this vision as it allows computers to intimately understand their users, capturing activity, pose, routine, and behavior. Today's consumer devices - like smartphones and smartwatches provide a glimpse of this potential, offering coarse digital representations of users with metrics such as step count, heart rate, and a handful of human activities like running and biking. Even these very low-dimensional representations are already bringing value to millions of people's lives, but there is significant potential for improvement. On the other end, professional, high-fidelity comprehensive user digitization systems exist. For example, motion capture suits and multi-camera rigs that digitize our full body and appearance, and scanning machines such as MRI capture our detailed anatomy. However, these carry significant user practicality burdens, such as financial, privacy, ergonomic, aesthetic, and instrumentation considerations, that preclude consumer use. In general, the higher the fidelity of capture, the lower the user's practicality. Most conventional approaches strike a balance between user practicality and digitization fidelity.
  My research aims to break this trend, developing sensing systems that increase user digitization fidelity to create new and powerful computing experiences while retaining or even improving user practicality and accessibility, allowing such technologies to have a societal impact. Armed with such knowledge, our future devices could offer longitudinal health tracking, more productive work environments, full body avatars in extended reality, and embodied telepresence experiences, to name just a few domains.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2311.17409.pdf' target='_blank'>https://arxiv.org/pdf/2311.17409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pramook Khungurn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17409">Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of creating a character model that can be controlled in real time from a single image of an anime character. A solution to this problem would greatly reduce the cost of creating avatars, computer games, and other interactive applications.
  Talking Head Anime 3 (THA3) is an open source project that attempts to directly address the problem. It takes as input (1) an image of an anime character's upper body and (2) a 45-dimensional pose vector and outputs a new image of the same character taking the specified pose. The range of possible movements is expressive enough for personal avatars and certain types of game characters. However, the system is too slow to generate animations in real time on common PCs, and its image quality can be improved.
  In this paper, we improve THA3 in two ways. First, we propose new architectures for constituent networks that rotate the character's head and body based on U-Nets with attention that are widely used in modern generative models. The new architectures consistently yield better image quality than the THA3 baseline. Nevertheless, they also make the whole system much slower: it takes up to 150 milliseconds to generate a frame. Second, we propose a technique to distill the system into a small network (less than 2 MB) that can generate 512x512 animation frames in real time (under 30 FPS) using consumer gaming GPUs while keeping the image quality close to that of the full system. This improvement makes the whole system practical for real-time applications.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2311.09226.pdf' target='_blank'>https://arxiv.org/pdf/2311.09226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Araz Zirar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09226">The value creation potential of digital humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>'Digital humans' are digital reproductions of humans powered by artificial intelligence (AI) and capable of communicating and forming emotional bonds. The value creation potential of digital humans is overlooked due to the limitations of digital human technologies. This article explores the value creation potential and the value realisation limitations of digital humans. The analysis is based on a review of 62 articles retrieved from the Web of Science database. The analysis suggests that digital humans have the potential to alleviate labour and skill shortages, reduce the natural human element in high-risk tasks, avoid design errors, improve the ergonomics of products and workplaces, and provide guidance and emotional support, all of which will benefit natural humans in the workplace. However, technical limits, evolving understanding of digital humans, the social significance and acceptance of digital humans, ethical considerations, and the adjustment of legal tradition limit the value realisation. This review suggests that digital humans' perceived usefulness and ease of development determine organisations' willingness to utilise this technology. Overcoming the limitations, which still involve engineering challenges and a change in how they are perceived, will positively affect realising the value potential of digital humans in organisations.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2310.05064.pdf' target='_blank'>https://arxiv.org/pdf/2310.05064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Moryossef
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05064">sign.mt: Real-Time Multilingual Sign Language Translation Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This demo paper presents sign.mt, an open-source application pioneering real-time multilingual bi-directional translation between spoken and signed languages. Harnessing state-of-the-art open-source models, this tool aims to address the communication divide between the hearing and the deaf, facilitating seamless translation in both spoken-to-signed and signed-to-spoken translation directions.
  Promising reliable and unrestricted communication, sign.mt offers offline functionality, crucial in areas with limited internet connectivity. It further enhances user engagement by offering customizable photo-realistic sign language avatars, thereby encouraging a more personalized and authentic user experience.
  Licensed under CC BY-NC-SA 4.0, sign.mt signifies an important stride towards open, inclusive communication. The app can be used, and modified for personal and academic uses, and even supports a translation API, fostering integration into a wider range of applications. However, it is by no means a finished product.
  We invite the NLP community to contribute towards the evolution of sign.mt. Whether it be the integration of more refined models, the development of innovative pipelines, or user experience improvements, your contributions can propel this project to new heights. Available at https://sign.mt, it stands as a testament to what we can achieve together, as we strive to make communication accessible to all.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2306.08718.pdf' target='_blank'>https://arxiv.org/pdf/2306.08718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brendon Rhoades
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08718">Increasing subsequences, matrix loci, and Viennot shadows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Let $\mathbf{x}_{n \times n}$ be an $n \times n$ matrix of variables and let $\mathbb{F}[\mathbf{x}_{n \times n}]$ be the polynomial ring in these variables over a field $\mathbb{F}$. We study the ideal $I_n \subseteq \mathbb{F}[\mathbf{x}_{n \times n}]$ generated by all row and column variable sums and all products of two variables drawn from the same row or column. We show that the quotient $\mathbb{F}[\mathbf{x}_{n \times n}]/I_n$ admits a standard monomial basis determined by Viennot's shadow line avatar of the Schensted correspondence. As a corollary, the Hilbert series of $\mathbb{F}[\mathbf{x}_{n \times n}]/I_n$ is the generating function of permutations in $\mathfrak{S}_n$ by the length of their longest increasing subsequence. Along the way, we describe a `shadow junta' basis of the vector space of $k$-local permutation statistics. We also calculate the structure of $\mathbb{F}[\mathbf{x}_{n \times n}]/I_n$ as a graded $\mathfrak{S}_n \times \mathfrak{S}_n$-module.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2305.08326.pdf' target='_blank'>https://arxiv.org/pdf/2305.08326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun-Cheng Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08326">Learner-Centered Analysis in Educational Metaverse Environments: Exploring Value Exchange Systems through Natural Interaction and Text Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the potential developments of self-directed learning in the metaverse in response to Education 4.0 and the Fourth Industrial Revolution. It highlights the importance of education keeping up with technological advancements and adopting learner-centered approaches. Additionally, it focuses on exploring value exchange systems through natural interaction, text mining, and analysis. The metaverse concept extends beyond extended reality (XR) technologies, encompassing digital avatars and shared ecological value. The role of educators in exploring new technologies and leveraging text-mining techniques to enhance learning efficiency is emphasized. The metaverse is presented as a platform for value exchange, necessitating meaningful and valuable content to attract users. Integrating virtual and real-world experiences within the metaverse offers practical applications and contributes to its essence. This paper sheds light on the metaverse's potential to create a learner-centered educational environment and adapt to the evolving landscape of Education 4.0. Its findings, supported by text mining analysis, contribute to understanding the metaverse's role in shaping education in the Fourth Industrial Revolution.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2304.01443.pdf' target='_blank'>https://arxiv.org/pdf/2304.01443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01443">Virtual Avatar Stream: a cost-down approach to the Metaverse experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Metaverse through VR headsets is a rapidly growing concept, but the high cost of entry currently limits access for many users. This project aims to provide an accessible entry point to the immersive Metaverse experience by leveraging web technologies. The platform developed allows users to engage with rendered avatars using only a web browser, microphone, and webcam. By employing the WebGL and MediaPipe face tracking AI model from Google, the application generates real-time 3D face meshes for users. It uses a client-to-client streaming cluster to establish a connection, and clients negotiate SRTP protocol through WebRTC for direct data streaming. Additionally, the project addresses backend challenges through an architecture that is serverless, distributive, auto-scaling, highly resilient, and secure. The platform offers a scalable, hardware-free solution for users to experience a near-immersive Metaverse, with the potential for future integration with game server clusters. This project provides an important step toward a more inclusive Metaverse accessible to a wider audience.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2210.04047.pdf' target='_blank'>https://arxiv.org/pdf/2210.04047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M Seetha Ramaiah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04047">Motion Planning on Visual Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this thesis, we propose an alternative characterization of the notion of Configuration Space, which we call Visual Configuration Space (VCS). This new characterization allows an embodied agent (e.g., a robot) to discover its own body structure and plan obstacle-free motions in its peripersonal space using a set of its own images in random poses. Here, we do not assume any knowledge of geometry of the agent, obstacles or the environment. We demonstrate the usefulness of VCS in (a) building and working with geometry-free models for robot motion planning, (b) explaining how a human baby might learn to reach objects in its peripersonal space through motor babbling, and (c) automatically generating natural looking head motion animations for digital avatars in virtual environments. This work is based on the formalism of manifolds and manifold learning using the agent's images and hence we call it Motion Planning on Visual Manifolds.
