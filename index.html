<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2507.13648.pdf' target='_blank'>https://arxiv.org/pdf/2507.13648.pdf</a></span>   <span><a href='https://github.com/seungjun-moon/epsilon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjun Moon, Sangjoon Yu, Gyeong-Moon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13648">EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of neural radiance fields (NeRF) has paved the way to generate animatable human avatars from a monocular video. However, the sole usage of NeRF suffers from a lack of details, which results in the emergence of hybrid representation that utilizes SMPL-based mesh together with NeRF representation. While hybrid-based models show photo-realistic human avatar generation qualities, they suffer from extremely slow inference due to their deformation scheme: to be aligned with the mesh, hybrid-based models use the deformation based on SMPL skinning weights, which needs high computational costs on each sampled point. We observe that since most of the sampled points are located in empty space, they do not affect the generation quality but result in inference latency with deformation. In light of this observation, we propose EPSilon, a hybrid-based 3D avatar generation scheme with novel efficient point sampling strategies that boost both training and inference. In EPSilon, we propose two methods to omit empty points at rendering; empty ray omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that progress through the empty space. Then, EIO narrows down the sampling interval on the ray, which wipes out the region not occupied by either clothes or mesh. The delicate sampling scheme of EPSilon enables not only great computational cost reduction during deformation but also the designation of the important regions to be sampled, which enables a single-stage NeRF structure without hierarchical sampling. Compared to existing methods, EPSilon maintains the generation quality while using only 3.9% of sampled points and achieves around 20 times faster inference, together with 4 times faster training convergence. We provide video results on https://github.com/seungjun-moon/epsilon.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2507.00792.pdf' target='_blank'>https://arxiv.org/pdf/2507.00792.pdf</a></span>   <span><a href='https://github.com/hvoss-techfak/JAX-IK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hendric Voss, Stefan Kopp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00792">JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/JAX-IK
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2503.18665.pdf' target='_blank'>https://arxiv.org/pdf/2503.18665.pdf</a></span>   <span><a href='https://github.com/antgroup/Similar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18665">Boosting Virtual Agent Learning and Reasoning: A Step-Wise, Multi-Dimensional, and Generalist Reward Model with Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of Generalist Virtual Agents (GVAs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The project is available at https://github.com/antgroup/Similar.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2503.06154.pdf' target='_blank'>https://arxiv.org/pdf/2503.06154.pdf</a></span>   <span><a href='https://github.com/wang-zidu/SRM-Hair' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zidu Wang, Jiankuo Zhao, Miao Xu, Xiangyu Zhu, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06154">SRM-Hair: Single Image Head Mesh Reconstruction via 3D Morphable Hair</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Morphable Models (3DMMs) have played a pivotal role as a fundamental representation or initialization for 3D avatar animation and reconstruction. However, extending 3DMMs to hair remains challenging due to the difficulty of enforcing vertex-level consistent semantic meaning across hair shapes. This paper introduces a novel method, Semantic-consistent Ray Modeling of Hair (SRM-Hair), for making 3D hair morphable and controlled by coefficients. The key contribution lies in semantic-consistent ray modeling, which extracts ordered hair surface vertices and exhibits notable properties such as additivity for hairstyle fusion, adaptability, flipping, and thickness modification. We collect a dataset of over 250 high-fidelity real hair scans paired with 3D face data to serve as a prior for the 3D morphable hair. Based on this, SRM-Hair can reconstruct a hair mesh combined with a 3D head from a single image. Note that SRM-Hair produces an independent hair mesh, facilitating applications in virtual avatar creation, realistic animation, and high-fidelity hair rendering. Both quantitative and qualitative experiments demonstrate that SRM-Hair achieves state-of-the-art performance in 3D mesh reconstruction. Our project is available at https://github.com/wang-zidu/SRM-Hair
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2411.04249.pdf' target='_blank'>https://arxiv.org/pdf/2411.04249.pdf</a></span>   <span><a href='https://github.com/sidsunny/pocoloco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharth Seth, Rishabh Dabral, Diogo Luvizon, Marc Habermann, Ming-Hsuan Yang, Christian Theobalt, Adam Kortylewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04249">PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling a human avatar that can plausibly deform to articulations is an active area of research. We present PocoLoco -- the first template-free, point-based, pose-conditioned generative model for 3D humans in loose clothing. We motivate our work by noting that most methods require a parametric model of the human body to ground pose-dependent deformations. Consequently, they are restricted to modeling clothing that is topologically similar to the naked body and do not extend well to loose clothing. The few methods that attempt to model loose clothing typically require either canonicalization or a UV-parameterization and need to address the challenging problem of explicitly estimating correspondences for the deforming clothes. In this work, we formulate avatar clothing deformation as a conditional point-cloud generation task within the denoising diffusion framework. Crucially, our framework operates directly on unordered point clouds, eliminating the need for a parametric model or a clothing template. This also enables a variety of practical applications, such as point-cloud completion and pose-based editing -- important features for virtual human animation. As current datasets for human avatars in loose clothing are far too small for training diffusion models, we release a dataset of two subjects performing various poses in loose clothing with a total of 75K point clouds. By contributing towards tackling the challenging task of effectively modeling loose clothing and expanding the available data for training these models, we aim to set the stage for further innovation in digital humans. The source code is available at https://github.com/sidsunny/pocoloco .
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2409.01502.pdf' target='_blank'>https://arxiv.org/pdf/2409.01502.pdf</a></span>   <span><a href='https://github.com/zshyang/amg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangsihao Yang, Mengyi Shan, Mohammad Farazi, Wenhui Zhu, Yanxi Chen, Xuanzhao Dong, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01502">AMG: Avatar Motion Guided Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human video generation task has gained significant attention with the advancement of deep generative models. Generating realistic videos with human movements is challenging in nature, due to the intricacies of human body topology and sensitivity to visual artifacts. The extensively studied 2D media generation methods take advantage of massive human media datasets, but struggle with 3D-aware control; whereas 3D avatar-based approaches, while offering more freedom in control, lack photorealism and cannot be harmonized seamlessly with background scene. We propose AMG, a method that combines the 2D photorealism and 3D controllability by conditioning video diffusion models on controlled rendering of 3D avatars. We additionally introduce a novel data processing pipeline that reconstructs and renders human avatar movements from dynamic camera videos. AMG is the first method that enables multi-person diffusion video generation with precise control over camera positions, human motions, and background style. We also demonstrate through extensive evaluation that it outperforms existing human video generation methods conditioned on pose sequences or driving videos in terms of realism and adaptability.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2405.14869.pdf' target='_blank'>https://arxiv.org/pdf/2405.14869.pdf</a></span>   <span><a href='https://github.com/YuliangXiu/PuzzleAvatar,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, Michael J. Black
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14869">PuzzleAvatar: Assembling 3D Avatars from Personal Albums</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating personalized 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if a user could just upload their personal "OOTD" (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel "Album2Human" task by developing PuzzleAvatar, a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, while bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into (separate) learned tokens and instilling these cues into the VLM. In effect, we exploit the learned tokens as "puzzle pieces" from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total of nearly 1K OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and strong robustness. Our code and data are publicly available for research purpose at https://puzzleavatar.is.tue.mpg.de/
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2403.05087.pdf' target='_blank'>https://arxiv.org/pdf/2403.05087.pdf</a></span>   <span><a href='https://github.com/initialneil/SplattingAvatar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05087">SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2402.06149.pdf' target='_blank'>https://arxiv.org/pdf/2402.06149.pdf</a></span>   <span><a href='https://github.com/ZhenglinZhou/HeadStudio' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06149">HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising results achieved with 2D diffusion priors, current methods struggle to create high-quality and consistent animated avatars efficiently. Previous animatable head models like FLAME have difficulty in accurately representing detailed texture and geometry. Additionally, high-quality 3D static representations face challenges in semantically driving with dynamic priors. In this paper, we introduce \textbf{HeadStudio}, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animatable avatars from text prompts. Firstly, we associate 3D Gaussians with animatable head prior model, facilitating semantic animation on high-quality 3D representations. To ensure consistent animation, we further enhance the optimization from initialization, distillation, and regularization to jointly learn the shape, texture, and animation. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. Moreover, These avatars can be smoothly driven by real-world speech and video. We hope that HeadStudio can enhance digital avatar creation and gain popularity in the community. Code is at: https://github.com/ZhenglinZhou/HeadStudio.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2401.12133.pdf' target='_blank'>https://arxiv.org/pdf/2401.12133.pdf</a></span>   <span><a href='https://github.com/KindOPSTAR/VRMN-bD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>He Zhang, Xinyang Li, Yuanxi Sun, Xinyi Fu, Christine Qiu, John M. Carroll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12133">VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2311.12804.pdf' target='_blank'>https://arxiv.org/pdf/2311.12804.pdf</a></span>   <span><a href='https://github.com/aldelb/non_verbal_facial_animation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alice Delbosc, Magalie Ochs, Nicolas Sabouret, Brian Ravenet, StÃ©phane Ayache
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12804">Towards the generation of synchronized and believable non-verbal facial behaviors of a talking virtual agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a new model to generate rhythmically relevant non-verbal facial behaviors for virtual agents while they speak. The model demonstrates perceived performance comparable to behaviors directly extracted from the data and replayed on a virtual agent, in terms of synchronization with speech and believability. Interestingly, we found that training the model with two different sets of data, instead of one, did not necessarily improve its performance. The expressiveness of the people in the dataset and the shooting conditions are key elements. We also show that employing an adversarial model, in which fabricated fake examples are introduced during the training phase, increases the perception of synchronization with speech. A collection of videos demonstrating the results and code can be accessed at: https://github.com/aldelb/non_verbal_facial_animation.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2307.02609.pdf' target='_blank'>https://arxiv.org/pdf/2307.02609.pdf</a></span>   <span><a href='https://github.com/SSYSteve/MRecGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Xu, Cheng Luo, Weicheng Xie, Linlin Shen, Xiaofeng Liu, Lu Liu, Hatice Gunes, Siyang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02609">MRecGen: Multimodal Appropriate Reaction Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verbal and non-verbal human reaction generation is a challenging task, as different reactions could be appropriate for responding to the same behaviour. This paper proposes the first multiple and multimodal (verbal and nonverbal) appropriate human reaction generation framework that can generate appropriate and realistic human-style reactions (displayed in the form of synchronised text, audio and video streams) in response to an input user behaviour. This novel technique can be applied to various human-computer interaction scenarios by generating appropriate virtual agent/robot behaviours. Our demo is available at \url{https://github.com/SSYSteve/MRecGen}.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2305.19012.pdf' target='_blank'>https://arxiv.org/pdf/2305.19012.pdf</a></span>   <span><a href='https://github.com/icoz69/StyleAvatar3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19012">StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the diversity of the generated avatars. Additionally, we develop a latent diffusion model within the style space of StyleGAN to enable the generation of avatars based on image inputs. Our approach demonstrates superior performance over current state-of-the-art methods in terms of visual quality and diversity of the produced avatars.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2303.01765.pdf' target='_blank'>https://arxiv.org/pdf/2303.01765.pdf</a></span>   <span><a href='https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Chen Liu, Muyi Sun, Lincheng Li, Changjie Fan, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01765">Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting natural and diverse 3D hand gestures from the upper body dynamics is a practical yet challenging task in virtual avatar creation. Previous works usually overlook the asymmetric motions between two hands and generate two hands in a holistic manner, leading to unnatural results. In this work, we introduce a novel bilateral hand disentanglement based two-stage 3D hand generation method to achieve natural and diverse 3D hand prediction from body dynamics. In the first stage, we intend to generate natural hand gestures by two hand-disentanglement branches. Considering the asymmetric gestures and motions of two hands, we introduce a Spatial-Residual Memory (SRM) module to model spatial interaction between the body and each hand by residual learning. To enhance the coordination of two hand motions wrt. body dynamics holistically, we then present a Temporal-Motion Memory (TMM) module. TMM can effectively model the temporal association between body dynamics and two hand motions. The second stage is built upon the insight that 3D hand predictions should be non-deterministic given the sequential body postures. Thus, we further diversify our 3D hand predictions based on the initial output from the stage one. Concretely, we propose a Prototypical-Memory Sampling Strategy (PSS) to generate the non-deterministic hand gestures by gradient-based Markov Chain Monte Carlo (MCMC) sampling. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on the B2H dataset and our newly collected TED Hands dataset. The dataset and code are available at https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2309.01426.pdf' target='_blank'>https://arxiv.org/pdf/2309.01426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Deepu Rajan, Shiwen Mao, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01426">A Unified Framework for Guiding Generative AI with Wireless Perception in Resource Constrained Mobile Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the significant advancements in artificial intelligence (AI) technologies and powerful computational capabilities, generative AI (GAI) has become a pivotal digital content generation technique for offering superior digital services. However, directing GAI towards desired outputs still suffer the inherent instability of the AI model. In this paper, we design a novel framework that utilizes wireless perception to guide GAI (WiPe-GAI) for providing digital content generation service, i.e., AI-generated content (AIGC), in resource-constrained mobile edge networks. Specifically, we first propose a new sequential multi-scale perception (SMSP) algorithm to predict user skeleton based on the channel state information (CSI) extracted from wireless signals. This prediction then guides GAI to provide users with AIGC, such as virtual character generation. To ensure the efficient operation of the proposed framework in resource constrained networks, we further design a pricing-based incentive mechanism and introduce a diffusion model based approach to generate an optimal pricing strategy for the service provisioning. The strategy maximizes the user's utility while enhancing the participation of the virtual service provider (VSP) in AIGC provision. The experimental results demonstrate the effectiveness of the designed framework in terms of skeleton prediction and optimal pricing strategy generation comparing with other existing solutions.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2310.08529.pdf' target='_blank'>https://arxiv.org/pdf/2310.08529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08529">GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2406.11208.pdf' target='_blank'>https://arxiv.org/pdf/2406.11208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Su, Xiaofeng Luo, Zhenmou Liu, Jiawen Kang, Min Hao, Zehui Xiong, Zhaohui Yang, Chongwen Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11208">Privacy-preserving Pseudonym Schemes for Personalized 3D Avatars in Mobile Social Metaverses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of mobile social metaverses, a novel paradigm bridging physical and virtual realms, has led to the widespread adoption of avatars as digital representations for Social Metaverse Users (SMUs) within virtual spaces. Equipped with immersive devices, SMUs leverage Edge Servers (ESs) to deploy their avatars and engage with other SMUs in virtual spaces. To enhance immersion, SMUs incline to opt for 3D avatars for social interactions. However, existing 3D avatars are typically generated through scanning the real faces of SMUs, which can raise concerns regarding information privacy and security, such as profile identity leakages. To tackle this, we introduce a new framework for personalized 3D avatar construction, leveraging a two-layer network model that provides SMUs with the option to customize their personal avatars for privacy preservation. Specifically, our approach introduces avatar pseudonyms to jointly safeguard the profile and digital identity privacy of the generated avatars. Then, we design a novel metric named Privacy of Personalized Avatars (PoPA), to evaluate effectiveness of the avatar pseudonyms. To optimize pseudonym resource, we model the pseudonym distribution process as a Stackelberg game and employ Deep Reinforcement Learning (DRL) to learn equilibrium strategies under incomplete information. Simulation results validate the efficacy and feasibility of our proposed schemes for mobile social metaverses.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2405.00954.pdf' target='_blank'>https://arxiv.org/pdf/2405.00954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00954">X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry->Texture->Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: https://xmu-xiaoma666.github.io/Projects/X-Oscar/.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2506.08933.pdf' target='_blank'>https://arxiv.org/pdf/2506.08933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08933">What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at https://omni-bench.github.io/.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2411.10943.pdf' target='_blank'>https://arxiv.org/pdf/2411.10943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghe Gao, Wendong Bu, Bingchen Miao, Yang Wu, Yunfei Li, Juncheng Li, Siliang Tang, Qi Wu, Yueting Zhuang, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10943">Generalist Virtual Agents: A Survey on Autonomous Agents Across Digital Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Generalist Virtual Agent (GVA), an autonomous entity engineered to function across diverse digital platforms and environments, assisting users by executing a variety of tasks. This survey delves into the evolution of GVAs, tracing their progress from early intelligent assistants to contemporary implementations that incorporate large-scale models. We explore both the philosophical underpinnings and practical foundations of GVAs, addressing their developmental challenges and the methodologies currently employed in their design and operation. By presenting a detailed taxonomy of GVA environments, tasks, and capabilities, this paper aims to bridge the theoretical and practical aspects of GVAs, concluding those that operate in environments closely mirroring the real world are more likely to demonstrate human-like intelligence. We discuss potential future directions for GVA research, highlighting the necessity for realistic evaluation metrics and the enhancement of long-sequence decision-making capabilities to advance the field toward more systematic or embodied applications. This work not only synthesizes the existing body of literature but also proposes frameworks for future investigations, contributing significantly to the ongoing development of intelligent systems.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2407.17438.pdf' target='_blank'>https://arxiv.org/pdf/2407.17438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17438">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2405.16874.pdf' target='_blank'>https://arxiv.org/pdf/2405.16874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Hengyuan Zhang, Yatian Wang, Jiahao Pan, Chen Liu, Peng Li, Xiaowei Chi, Mengfei Li, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16874">CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2311.17532.pdf' target='_blank'>https://arxiv.org/pdf/2311.17532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17532">Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating vivid and emotional 3D co-speech gestures is crucial for virtual avatar animation in human-machine interaction applications. While the existing methods enable generating the gestures to follow a single emotion label, they overlook that long gesture sequence modeling with emotion transition is more practical in real scenes. In addition, the lack of large-scale available datasets with emotional transition speech and corresponding 3D human gestures also limits the addressing of this task. To fulfill this goal, we first incorporate the ChatGPT-4 and an audio inpainting approach to construct the high-fidelity emotion transition human speeches. Considering obtaining the realistic 3D pose annotations corresponding to the dynamically inpainted emotion transition audio is extremely difficult, we propose a novel weakly supervised training strategy to encourage authority gesture transitions. Specifically, to enhance the coordination of transition gestures w.r.t different emotional ones, we model the temporal association representation between two different emotional gesture sequences as style guidance and infuse it into the transition generation. We further devise an emotion mixture mechanism that provides weak supervision based on a learnable mixed emotion label for transition gestures. Last, we present a keyframe sampler to supply effective initial posture cues in long sequences, enabling us to generate diverse gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models constructed by adapting single emotion-conditioned counterparts on our newly defined emotion transition task and datasets. Our code and dataset will be released on the project page: https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2401.08503.pdf' target='_blank'>https://arxiv.org/pdf/2401.08503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08503">Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples and source code are available at https://real3dportrait.github.io .
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2312.04547.pdf' target='_blank'>https://arxiv.org/pdf/2312.04547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04547">Digital Life Project: Autonomous 3D Characters with Social Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: https://digital-life-project.com/
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2312.05295.pdf' target='_blank'>https://arxiv.org/pdf/2312.05295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Cheng Lin, Xin Li, Wenping Wang, Rong Xie, Li Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05295">Disentangled Clothed Avatar Generation from Text Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel text-to-avatar generation method that separately generates the human body and the clothes and allows high-quality animation on the generated avatar. While recent advancements in text-to-avatar generation have yielded diverse human avatars from text prompts, these methods typically combine all elements-clothes, hair, and body-into a single 3D representation. Such an entangled approach poses challenges for downstream tasks like editing or animation. To overcome these limitations, we propose a novel disentangled 3D avatar representation named Sequentially Offset-SMPL (SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body and clothes with two separate meshes but associates them with offsets to ensure the physical alignment between the body and the clothes. Then, we design a Score Distillation Sampling (SDS)-based distillation framework to generate the proposed SO-SMPL representation from text prompts. Our approach not only achieves higher texture and geometry quality and better semantic alignment with text prompts, but also significantly improves the visual quality of character animation, virtual try-on, and avatar editing. Project page: https://shanemankiw.github.io/SO-SMPL/.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2312.11587.pdf' target='_blank'>https://arxiv.org/pdf/2312.11587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diogo Luvizon, Vladislav Golyanik, Adam Kortylewski, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11587">Relightable Neural Actor with Intrinsic Decomposition and Pose Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a controllable and relightable digital avatar from multi-view video with fixed illumination is a very challenging problem since humans are highly articulated, creating pose-dependent appearance effects, and skin as well as clothing require space-varying BRDF modeling. Existing works on creating animatible avatars either do not focus on relighting at all, require controlled illumination setups, or try to recover a relightable avatar from very low cost setups, i.e. a single RGB video, at the cost of severely limited result quality, e.g. shadows not even being modeled. To address this, we propose Relightable Neural Actor, a new video-based method for learning a pose-driven neural human model that can be relighted, allows appearance editing, and models pose-dependent effects such as wrinkles and self-shadows. Importantly, for training, our method solely requires a multi-view recording of the human under a known, but static lighting condition. To tackle this challenging problem, we leverage an implicit geometry representation of the actor with a drivable density field that models pose-dependent deformations and derive a dynamic mapping between 3D and UV spaces, where normal, visibility, and materials are effectively encoded. To evaluate our approach in real-world scenarios, we collect a new dataset with four identities recorded under different light conditions, indoors and outdoors, providing the first benchmark of its kind for human relighting, and demonstrating state-of-the-art relighting results for novel human poses.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2310.08583.pdf' target='_blank'>https://arxiv.org/pdf/2310.08583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noshaba Cheema, Rui Xu, Nam Hee Kim, Perttu HÃ¤mÃ¤lÃ¤inen, Vladislav Golyanik, Marc Habermann, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08583">Discovering Fatigued Movements for Virtual Character Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual character animation and movement synthesis have advanced rapidly during recent years, especially through a combination of extensive motion capture datasets and machine learning. A remaining challenge is interactively simulating characters that fatigue when performing extended motions, which is indispensable for the realism of generated animations. However, capturing such movements is problematic, as performing movements like backflips with fatigued variations up to exhaustion raises capture cost and risk of injury. Surprisingly, little research has been done on faithful fatigue modeling. To address this, we propose a deep reinforcement learning-based approach, which -- for the first time in literature -- generates control policies for full-body physically simulated agents aware of cumulative fatigue. For this, we first leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert policy for the skill; Second, we learn a fatigue policy by limiting the generated constant torque bounds based on endurance time to non-linear, state- and time-dependent limits in the joint-actuation space using a Three-Compartment Controller (3CC) model. Our results demonstrate that agents can adapt to different fatigue and rest rates interactively, and discover realistic recovery strategies without the need for any captured data of fatigued movement.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2309.01848.pdf' target='_blank'>https://arxiv.org/pdf/2309.01848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Howe Yuan Zhu, Nguyen Quang Hieu, Dinh Thai Hoang, Diep N. Nguyen, Chin-Teng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01848">A Human-Centric Metaverse Enabled by Brain-Computer Interface: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing interest in the Metaverse has generated momentum for members of academia and industry to innovate toward realizing the Metaverse world. The Metaverse is a unique, continuous, and shared virtual world where humans embody a digital form within an online platform. Through a digital avatar, Metaverse users should have a perceptual presence within the environment and can interact and control the virtual world around them. Thus, a human-centric design is a crucial element of the Metaverse. The human users are not only the central entity but also the source of multi-sensory data that can be used to enrich the Metaverse ecosystem. In this survey, we study the potential applications of Brain-Computer Interface (BCI) technologies that can enhance the experience of Metaverse users. By directly communicating with the human brain, the most complex organ in the human body, BCI technologies hold the potential for the most intuitive human-machine system operating at the speed of thought. BCI technologies can enable various innovative applications for the Metaverse through this neural pathway, such as user cognitive state monitoring, digital avatar control, virtual interactions, and imagined speech communications. This survey first outlines the fundamental background of the Metaverse and BCI technologies. We then discuss the current challenges of the Metaverse that can potentially be addressed by BCI, such as motion sickness when users experience virtual environments or the negative emotional states of users in immersive virtual applications. After that, we propose and discuss a new research direction called Human Digital Twin, in which digital twins can create an intelligent and interactable avatar from the user's brain signals. We also present the challenges and potential solutions in synchronizing and communicating between virtual and physical entities in the Metaverse.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2308.12969.pdf' target='_blank'>https://arxiv.org/pdf/2308.12969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanyue Zhang, Rishabh Dabral, Thomas LeimkÃ¼hler, Vladislav Golyanik, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12969">ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing automatic approaches for 3D virtual character motion synthesis supporting scene interactions do not generalise well to new objects outside training distributions, even when trained on extensive motion capture datasets with diverse objects and annotated interactions. This paper addresses this limitation and shows that robustness and generalisation to novel scene objects in 3D object-aware character synthesis can be achieved by training a motion model with as few as one reference object. We leverage an implicit feature representation trained on object-only datasets, which encodes an SE(3)-equivariant descriptor field around the object. Given an unseen object and a reference pose-object pair, we optimise for the object-aware pose that is closest in the feature space to the reference pose. Finally, we use l-NSM, i.e., our motion generation model that is trained to seamlessly transition from locomotion to object interaction with the proposed bidirectional pose blending scheme. Through comprehensive numerical comparisons to state-of-the-art methods and in a user study, we demonstrate substantial improvements in 3D virtual character motion and interaction quality and robustness to scenarios with unseen objects. Our project page is available at https://vcai.mpi-inf.mpg.de/projects/ROAM/.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2212.07555.pdf' target='_blank'>https://arxiv.org/pdf/2212.07555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.07555">IMos: Intent-Driven Full-Body Motion Synthesis for Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we make virtual characters in a scene interact with their surrounding objects through simple instructions? Is it possible to synthesize such motion plausibly with a diverse set of objects and instructions? Inspired by these questions, we present the first framework to synthesize the full-body motion of virtual human characters performing specified actions with 3D objects placed within their reach. Our system takes textual instructions specifying the objects and the associated intentions of the virtual characters as input and outputs diverse sequences of full-body motions. This contrasts existing works, where full-body action synthesis methods generally do not consider object interactions, and human-object interaction methods focus mainly on synthesizing hand or finger movements for grasping objects. We accomplish our objective by designing an intent-driven fullbody motion generator, which uses a pair of decoupled conditional variational auto-regressors to learn the motion of the body parts in an autoregressive manner. We also optimize the 6-DoF pose of the objects such that they plausibly fit within the hands of the synthesized characters. We compare our proposed method with the existing methods of motion synthesis and establish a new and stronger state-of-the-art for the task of intent-driven motion synthesis.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2212.04495.pdf' target='_blank'>https://arxiv.org/pdf/2212.04495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04495">MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional methods for human motion synthesis are either deterministic or struggle with the trade-off between motion diversity and motion quality. In response to these limitations, we introduce MoFusion, i.e., a new denoising-diffusion-based framework for high-quality conditional human motion synthesis that can generate long, temporally plausible, and semantically accurate motions based on a range of conditioning contexts (such as music and text). We also present ways to introduce well-known kinematic losses for motion plausibility within the motion diffusion framework through our scheduled weighting strategy. The learned latent space can be used for several interactive motion editing applications -- like inbetweening, seed conditioning, and text-based editing -- thus, providing crucial abilities for virtual character animation and robotics. Through comprehensive quantitative evaluations and a perceptual user study, we demonstrate the effectiveness of MoFusion compared to the state of the art on established benchmarks in the literature. We urge the reader to watch our supplementary video and visit https://vcai.mpi-inf.mpg.de/projects/MoFusion.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2312.15059.pdf' target='_blank'>https://arxiv.org/pdf/2312.15059.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15059">Deformable 3D Gaussian Splatting for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2410.01835.pdf' target='_blank'>https://arxiv.org/pdf/2410.01835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianchun Chen, Jian Wang, Yinda Zhang, Rohit Pandey, Thabo Beeler, Marc Habermann, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01835">EgoAvatar: Egocentric View-Driven and Photorealistic Full-body Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immersive VR telepresence ideally means being able to interact and communicate with digital avatars that are indistinguishable from and precisely reflect the behaviour of their real counterparts. The core technical challenge is two fold: Creating a digital double that faithfully reflects the real human and tracking the real human solely from egocentric sensing devices that are lightweight and have a low energy consumption, e.g. a single RGB camera. Up to date, no unified solution to this problem exists as recent works solely focus on egocentric motion capture, only model the head, or build avatars from multi-view captures. In this work, we, for the first time in literature, propose a person-specific egocentric telepresence approach, which jointly models the photoreal digital avatar while also driving it from a single egocentric video. We first present a character model that is animatible, i.e. can be solely driven by skeletal motion, while being capable of modeling geometry and appearance. Then, we introduce a personalized egocentric motion capture component, which recovers full-body motion from an egocentric video. Finally, we apply the recovered pose to our character model and perform a test-time mesh refinement such that the geometry faithfully projects onto the egocentric view. To validate our design choices, we propose a new and challenging benchmark, which provides paired egocentric and dense multi-view videos of real humans performing various motions. Our experiments demonstrate a clear step towards egocentric and photoreal telepresence as our method outperforms baselines as well as competing methods. For more details, code, and data, we refer to our project page.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2306.03576.pdf' target='_blank'>https://arxiv.org/pdf/2306.03576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Sun, Dingkang Yang, Dongliang Kou, Yang Jiang, Weihua Shan, Zhe Yan, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03576">Human 3D Avatar Modeling with Implicit Neural Representation: A Brief Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A human 3D avatar is one of the important elements in the metaverse, and the modeling effect directly affects people's visual experience. However, the human body has a complex topology and diverse details, so it is often expensive, time-consuming, and laborious to build a satisfactory model. Recent studies have proposed a novel method, implicit neural representation, which is a continuous representation method and can describe objects with arbitrary topology at arbitrary resolution. Researchers have applied implicit neural representation to human 3D avatar modeling and obtained more excellent results than traditional methods. This paper comprehensively reviews the application of implicit neural representation in human body modeling. First, we introduce three implicit representations of occupancy field, SDF, and NeRF, and make a classification of the literature investigated in this paper. Then the application of implicit modeling methods in the body, hand, and head are compared and analyzed respectively. Finally, we point out the shortcomings of current work and provide available suggestions for researchers.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2204.11184.pdf' target='_blank'>https://arxiv.org/pdf/2204.11184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Zhu, Tingting Liao, Jiangjing Lyu, Xiang Yan, Yunfeng Wang, Kan Guo, Qiong Cao, Stan Z. Li, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.11184">MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider a novel problem of reconstructing a 3D human avatar from multiple unconstrained frames, independent of assumptions on camera calibration, capture space, and constrained actions. The problem should be addressed by a framework that takes multiple unconstrained images as inputs, and generates a shape-with-skinning avatar in the canonical space, finished in one feed-forward pass. To this end, we present 3D Avatar Reconstruction in the wild (ARwild), which first reconstructs the implicit skinning fields in a multi-level manner, by which the image features from multiple images are aligned and integrated to estimate a pixel-aligned implicit function that represents the clothed shape. To enable the training and testing of the new framework, we contribute a large-scale dataset, MVP-Human (Multi-View and multi-Pose 3D Human), which contains 400 subjects, each of which has 15 scans in different poses and 8-view images for each pose, providing 6,000 3D scans and 48,000 images in total. Overall, benefits from the specific network architecture and the diverse data, the trained model enables 3D avatar reconstruction from unconstrained frames and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2308.05925.pdf' target='_blank'>https://arxiv.org/pdf/2308.05925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqi Su, Liangxiao Hu, Siyou Lin, Hongwen Zhang, Shengping Zhang, Justus Thies, Yebin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05925">CaPhy: Capturing Physical Properties for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and physical properties of the clothing from real observations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing. To this end, we combine unsupervised training with physics-based losses and 3D-supervised training using scanned data to reconstruct a dynamic model of clothing that is physically realistic and conforms to the human scans. We also optimize the physical parameters of the underlying physical model from the scans by introducing gradient constraints of the physics-based losses. In contrast to previous work on 3D avatar reconstruction, our method is able to generalize to novel poses with realistic dynamic cloth deformations. Experiments on several subjects demonstrate that our method can estimate the physical properties of the garments, resulting in superior quantitative and qualitative results compared with previous methods.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2312.08889.pdf' target='_blank'>https://arxiv.org/pdf/2312.08889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyou Xu, Zongxin Yang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08889">SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: https://yoxu515.github.io/SEEAvatar/.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2309.04946.pdf' target='_blank'>https://arxiv.org/pdf/2309.04946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04946">Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking-head synthesis is a popular research topic for virtual human-related applications. However, the inflexibility and inefficiency of existing methods, which necessitate expensive end-to-end training to transfer emotions from guidance videos to talking-head predictions, are significant limitations. In this work, we propose the Emotional Adaptation for Audio-driven Talking-head (EAT) method, which transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adaptations. Our approach utilizes a pretrained emotion-agnostic talking-head transformer and introduces three lightweight adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and Emotional Adaptation Module) from different perspectives to enable precise and realistic emotion controls. Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including LRW and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable generalization ability, even in scenarios where emotional training videos are scarce or nonexistent. Project website: https://yuangan.github.io/eat/
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2401.00711.pdf' target='_blank'>https://arxiv.org/pdf/2401.00711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00711">Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating 3D human models directly from text helps reduce the cost and time of character modeling. However, achieving multi-attribute controllable and realistic 3D human avatar generation is still challenging due to feature coupling and the scarcity of realistic 3D human avatar datasets. To address these issues, we propose Text2Avatar, which can generate realistic-style 3D avatars based on the coupled text prompts. Text2Avatar leverages a discrete codebook as an intermediate feature to establish a connection between text and avatars, enabling the disentanglement of features. Furthermore, to alleviate the scarcity of realistic style 3D human avatar data, we utilize a pre-trained unconditional 3D human avatar generation model to obtain a large amount of 3D avatar pseudo data, which allows Text2Avatar to achieve realistic style generation. Experimental results demonstrate that our method can generate realistic 3D avatars from coupled textual data, which is challenging for other existing methods in this field.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2509.02466.pdf' target='_blank'>https://arxiv.org/pdf/2509.02466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwen Wang, Yiyu Zhuang, Jiawei Zhang, Li Wang, Yifei Zeng, Xun Cao, Xinxin Zuo, Hao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02466">TeRA: Rethinking Text-guided Realistic 3D Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative models. Our approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human representation. Experiments have proven our approach's superiority over previous text-to-avatar generative models in subjective and objective evaluation.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2504.08581.pdf' target='_blank'>https://arxiv.org/pdf/2504.08581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08581">FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2405.19203.pdf' target='_blank'>https://arxiv.org/pdf/2405.19203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19203">$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing. Our project page is https://olivia23333.github.io/E3Gen.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2306.09864.pdf' target='_blank'>https://arxiv.org/pdf/2306.09864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09864">AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AvatarBooth, a novel method for generating high-quality 3D avatars using text prompts or specific images. Unlike previous approaches that can only synthesize avatars based on simple text descriptions, our method enables the creation of personalized avatars from casually captured face or body images, while still supporting text-based model generation and editing. Our key contribution is the precise avatar generation control by using dual fine-tuned diffusion models separately for the human face and body. This enables us to capture intricate details of facial appearance, clothing, and accessories, resulting in highly realistic avatar generations. Furthermore, we introduce pose-consistent constraint to the optimization process to enhance the multi-view consistency of synthesized head images from the diffusion model and thus eliminate interference from uncontrolled human poses. In addition, we present a multi-resolution rendering strategy that facilitates coarse-to-fine supervision of 3D avatar generation, thereby enhancing the performance of the proposed system. The resulting avatar model can be further edited using additional text descriptions and driven by motion sequences. Experiments show that AvatarBooth outperforms previous text-to-3D methods in terms of rendering and geometric quality from either text prompts or specific images. Please check our project website at https://zeng-yifei.github.io/avatarbooth_page/.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2203.07931.pdf' target='_blank'>https://arxiv.org/pdf/2203.07931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.07931">DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversation is an essential component of virtual avatar activities in the metaverse. With the development of natural language processing, textual and vocal conversation generation has achieved a significant breakthrough. However, face-to-face conversations account for the vast majority of daily conversations, while most existing methods focused on single-person talking head generation. In this work, we take a step further and consider generating realistic face-to-face conversation videos. Conversation generation is more challenging than single-person talking head generation, since it not only requires generating photo-realistic individual talking heads but also demands the listener to respond to the speaker. In this paper, we propose a novel unified framework based on neural radiance field (NeRF) to address this task. Specifically, we model both the speaker and listener with a NeRF framework, with different conditions to control individual expressions. The speaker is driven by the audio signal, while the response of the listener depends on both visual and acoustic information. In this way, face-to-face conversation videos are generated between human avatars, with all the interlocutors modeled within the same network. Moreover, to facilitate future research on this task, we collect a new human conversation dataset containing 34 clips of videos. Quantitative and qualitative experiments evaluate our method in different aspects, e.g., image quality, pose sequence trend, and naturalness of the rendering videos. Experimental results demonstrate that the avatars in the resulting videos are able to perform a realistic conversation, and maintain individual styles. All the code, data, and models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2507.13052.pdf' target='_blank'>https://arxiv.org/pdf/2507.13052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13052">Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2308.09705.pdf' target='_blank'>https://arxiv.org/pdf/2308.09705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09705">Guide3D: Create 3D Avatars from Text and Image Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text-to-image generation has exhibited remarkable advancements, with the ability to produce visually impressive results. In contrast, text-to-3D generation has not yet reached a comparable level of quality. Existing methods primarily rely on text-guided score distillation sampling (SDS), and they encounter difficulties in transferring 2D attributes of the generated images to 3D content. In this work, we aim to develop an effective 3D generative model capable of synthesizing high-resolution textured meshes by leveraging both textual and image information. To this end, we introduce Guide3D, a zero-shot text-and-image-guided generative model for 3D avatar generation based on diffusion models. Our model involves (1) generating sparse-view images of a text-consistent character using diffusion models, and (2) jointly optimizing multi-resolution differentiable marching tetrahedral grids with pixel-aligned image features. We further propose a similarity-aware feature fusion strategy for efficiently integrating features from different views. Moreover, we introduce two novel training objectives as an alternative to calculating SDS, significantly enhancing the optimization process. We thoroughly evaluate the performance and components of our framework, which outperforms the current state-of-the-art in producing topologically and structurally correct geometry and high-resolution textures. Guide3D enables the direct transfer of 2D-generated images to the 3D space. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2408.09126.pdf' target='_blank'>https://arxiv.org/pdf/2408.09126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaokun Sun, Zhenyu Zhang, Ying Tai, Hao Tang, Zili Yi, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09126">Barbie: Text to Barbie-Style 3D Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To integrate digital humans into everyday life, there is a strong demand for generating high-quality, fine-grained disentangled 3D avatars that support expressive animation and simulation capabilities, ideally from low-cost textual inputs. Although text-driven 3D avatar generation has made significant progress by leveraging 2D generative priors, existing methods still struggle to fulfill all these requirements simultaneously. To address this challenge, we propose Barbie, a novel text-driven framework for generating animatable 3D avatars with separable shoes, accessories, and simulation-ready garments, truly capturing the iconic ``Barbie doll'' aesthetic. The core of our framework lies in an expressive 3D representation combined with appropriate modeling constraints. Unlike previous methods, we innovatively employ G-Shell to uniformly model both watertight components (e.g., bodies, shoes, and accessories) and non-watertight garments compatible with simulation. Furthermore, we introduce a well-designed initialization and a hole regularization loss to ensure clean open surface modeling. These disentangled 3D representations are then optimized by specialized expert diffusion models tailored to each domain, ensuring high-fidelity outputs. To mitigate geometric artifacts and texture conflicts when combining different expert models, we further propose several effective geometric losses and strategies. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation. Our framework further enables diverse applications, including apparel combination, editing, expressive animation, and physical simulation. Our project page is: https://xiaokunsun.github.io/Barbie.github.io
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2409.17145.pdf' target='_blank'>https://arxiv.org/pdf/2409.17145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17145">DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2308.15016.pdf' target='_blank'>https://arxiv.org/pdf/2308.15016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longbin Ji, Pengfei Wei, Yi Ren, Jinglin Liu, Chen Zhang, Xiang Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15016">C2G2: Controllable Co-speech Gesture Generation with Latent Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gesture generation is crucial for automatic digital avatar animation. However, existing methods suffer from issues such as unstable training and temporal inconsistency, particularly in generating high-fidelity and comprehensive gestures. Additionally, these methods lack effective control over speaker identity and temporal editing of the generated gestures. Focusing on capturing temporal latent information and applying practical controlling, we propose a Controllable Co-speech Gesture Generation framework, named C2G2. Specifically, we propose a two-stage temporal dependency enhancement strategy motivated by latent diffusion models. We further introduce two key features to C2G2, namely a speaker-specific decoder to generate speaker-related real-length skeletons and a repainting strategy for flexible gesture generation/editing. Extensive experiments on benchmark gesture datasets verify the effectiveness of our proposed C2G2 compared with several state-of-the-art baselines. The link of the project demo page can be found at https://c2g2-gesture.github.io/c2_gesture
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2305.12529.pdf' target='_blank'>https://arxiv.org/pdf/2305.12529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12529">DreamWaltz: Make a Scene with Complex 3D Animatable Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable 3D avatar representation from abundant image priors of diffusion model conditioned on various poses, which could animate complex non-rigged avatars given arbitrary poses without retraining. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The proposed framework further enables the creation of complex scenes with diverse compositions, including avatar-avatar, avatar-object and avatar-scene interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and animation results.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2405.11286.pdf' target='_blank'>https://arxiv.org/pdf/2405.11286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11286">Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant interest in creating 3D avatars and motions, driven by their diverse applications in areas like film-making, video games, AR/VR, and human-robot interaction. However, current efforts primarily concentrate on either generating the 3D avatar mesh alone or producing motion sequences, with integrating these two aspects proving to be a persistent challenge. Additionally, while avatar and motion generation predominantly target humans, extending these techniques to animals remains a significant challenge due to inadequate training data and methods. To bridge these gaps, our paper presents three key contributions. Firstly, we proposed a novel agent-based approach named Motion Avatar, which allows for the automatic generation of high-quality customizable human and animal avatars with motions through text queries. The method significantly advanced the progress in dynamic 3D character generation. Secondly, we introduced a LLM planner that coordinates both motion and avatar generation, which transforms a discriminative planning into a customizable Q&A fashion. Lastly, we presented an animal motion dataset named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65 animal categories and its building pipeline ZooGen, which serves as a valuable resource for the community. See project website https://steve-zeyu-zhang.github.io/MotionAvatar/
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2404.04421.pdf' target='_blank'>https://arxiv.org/pdf/2404.04421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zheng, Qingqing Zhao, Guandao Yang, Wang Yifan, Donglai Xiang, Florian Dubost, Dmitry Lagun, Thabo Beeler, Federico Tombari, Leonidas Guibas, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04421">PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling and rendering photorealistic avatars is of crucial importance in many applications. Existing methods that build a 3D avatar from visual observations, however, struggle to reconstruct clothed humans. We introduce PhysAvatar, a novel framework that combines inverse rendering with inverse physics to automatically estimate the shape and appearance of a human from multi-view video data along with the physical parameters of the fabric of their clothes. For this purpose, we adopt a mesh-aligned 4D Gaussian technique for spatio-temporal mesh tracking as well as a physically based inverse renderer to estimate the intrinsic material properties. PhysAvatar integrates a physics simulator to estimate the physical parameters of the garments using gradient-based optimization in a principled manner. These novel capabilities enable PhysAvatar to create high-quality novel-view renderings of avatars dressed in loose-fitting clothes under motions and lighting conditions not seen in the training data. This marks a significant advancement towards modeling photorealistic digital humans using physically based inverse rendering with physics in the loop. Our project website is at: https://qingqing-zhao.github.io/PhysAvatar
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2311.17917.pdf' target='_blank'>https://arxiv.org/pdf/2311.17917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfeng Zhang, Xuanmeng Zhang, Huichao Zhang, Jun Hao Liew, Chenxu Zhang, Yi Yang, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17917">AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of creating high-fidelity and animatable 3D avatars from only textual descriptions. Existing text-to-avatar methods are either limited to static avatars which cannot be animated or struggle to generate animatable avatars with promising quality and precise pose control. To address these limitations, we propose AvatarStudio, a coarse-to-fine generative model that generates explicit textured 3D meshes for animatable human avatars. Specifically, AvatarStudio begins with a low-resolution NeRF-based representation for coarse generation, followed by incorporating SMPL-guided articulation into the explicit mesh representation to support avatar animation and high resolution rendering. To ensure view consistency and pose controllability of the resulting avatars, we introduce a 2D diffusion model conditioned on DensePose for Score Distillation Sampling supervision. By effectively leveraging the synergy between the articulated mesh representation and the DensePose-conditional diffusion model, AvatarStudio can create high-quality avatars from text that are ready for animation, significantly outperforming previous methods. Moreover, it is competent for many applications, e.g., multimodal avatar animations and style-guided avatar creation. For more results, please refer to our project page: http://jeff95.me/projects/avatarstudio.html
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2408.13995.pdf' target='_blank'>https://arxiv.org/pdf/2408.13995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Geng Foo, Yixuan He, Ajmal Saeed Mian, Hossein Rahmani, Jun Liu, Christian Theobalt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13995">Avatar Concept Slider: Controllable Editing of Concepts in 3D Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-based editing of 3D human avatars to precisely match user requirements is challenging due to the inherent ambiguity and limited expressiveness of natural language. To overcome this, we propose the Avatar Concept Slider (ACS), a 3D avatar editing method that allows precise editing of semantic concepts in human avatars towards a specified intermediate point between two extremes of concepts, akin to moving a knob along a slider track. To achieve this, our ACS has three designs: Firstly, a Concept Sliding Loss based on linear discriminant analysis to pinpoint the concept-specific axes for precise editing. Secondly, an Attribute Preserving Loss based on principal component analysis for improved preservation of avatar identity during editing. We further propose a 3D Gaussian Splatting primitive selection mechanism based on concept-sensitivity, which updates only the primitives that are the most sensitive to our target concept, to improve efficiency. Results demonstrate that our ACS enables controllable 3D avatar editing, without compromising the avatar quality or its identifying attributes.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2405.12663.pdf' target='_blank'>https://arxiv.org/pdf/2405.12663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Gong, Shenyu Ji, Lin Geng Foo, Kang Chen, Hossein Rahmani, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12663">LAGA: Layered 3D Avatar Generation and Customization via Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating and customizing a 3D clothed avatar from textual descriptions is a critical and challenging task. Traditional methods often treat the human body and clothing as inseparable, limiting users' ability to freely mix and match garments. In response to this limitation, we present LAyered Gaussian Avatar (LAGA), a carefully designed framework enabling the creation of high-fidelity decomposable avatars with diverse garments. By decoupling garments from avatar, our framework empowers users to conviniently edit avatars at the garment level. Our approach begins by modeling the avatar using a set of Gaussian points organized in a layered structure, where each layer corresponds to a specific garment or the human body itself. To generate high-quality garments for each layer, we introduce a coarse-to-fine strategy for diverse garment generation and a novel dual-SDS loss function to maintain coherence between the generated garments and avatar components, including the human body and other garments. Moreover, we introduce three regularization losses to guide the movement of Gaussians for garment transfer, allowing garments to be freely transferred to various avatars. Extensive experimentation demonstrates that our approach surpasses existing methods in the generation of 3D clothed humans.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2311.17053.pdf' target='_blank'>https://arxiv.org/pdf/2311.17053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsun-Hsuan Wang, Juntian Zheng, Pingchuan Ma, Yilun Du, Byungchul Kim, Andrew Spielberg, Joshua Tenenbaum, Chuang Gan, Daniela Rus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17053">DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy. Co-optimization of artificial creatures' morphology and control in silico shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure. In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks. DiffuseBot bridges the gap between virtually generated content and physical utility by (i) augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and (ii) introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation. We showcase a range of simulated and fabricated robots along with their capabilities. Check our website at https://diffusebot.github.io/
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2410.18975.pdf' target='_blank'>https://arxiv.org/pdf/2410.18975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Li, Yuanzhen Li, Neal Wadhwa, Yael Pritch, David E. Jacobs, Michael Rubinstein, Mohit Bansal, Nataniel Ruiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18975">Unbounded: A Generative Infinite Game of Character Life Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2302.11566.pdf' target='_blank'>https://arxiv.org/pdf/2302.11566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11566">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human geometry reconstructions. We evaluate our methods on publicly available datasets and show improvements over prior art.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2507.17029.pdf' target='_blank'>https://arxiv.org/pdf/2507.17029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17029">StreamME: Simplify 3D Gaussian Avatar within Live Stream</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2410.15536.pdf' target='_blank'>https://arxiv.org/pdf/2410.15536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Zook, Fan-Yun Sun, Josef Spjut, Valts Blukis, Stan Birchfield, Jonathan Tremblay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15536">GRS: Generating Robotic Simulation Tasks from Real-World Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GRS (Generating Robotic Simulation tasks), a system addressing real-to-sim for robotic simulations. GRS creates digital twin simulations from single RGB-D observations with solvable tasks for virtual agent training. Using vision-language models (VLMs), our pipeline operates in three stages: 1) scene comprehension with SAM2 for segmentation and object description, 2) matching objects with simulation-ready assets, and 3) generating appropriate tasks. We ensure simulation-task alignment through generated test suites and introduce a router that iteratively refines both simulation and test code. Experiments demonstrate our system's effectiveness in object correspondence and task environment generation through our novel router mechanism.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2312.13578.pdf' target='_blank'>https://arxiv.org/pdf/2312.13578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13578">DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of emotional talking faces from a single portrait image remains a significant challenge. The simultaneous achievement of expressive emotional talking and accurate lip-sync is particularly difficult, as expressiveness is often compromised for the accuracy of lip-sync. As widely adopted by many prior works, the LSTM network often fails to capture the subtleties and variations of emotional expressions. To address these challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven framework, tailored for generating diverse expressions and accurate lip-sync concurrently. In the first stage, we propose EmoDiff, a novel diffusion module that generates diverse highly dynamic emotional expressions and head poses in accordance with the audio and the referenced emotion style. Given the strong correlation between lip motion and audio, we then refine the dynamics with enhanced lip-sync accuracy using audio features and emotion style. To this end, we deploy a video-to-video rendering module to transfer the expressions and lip motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of expressiveness, lip-sync accuracy and perceptual quality.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2312.02963.pdf' target='_blank'>https://arxiv.org/pdf/2312.02963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02963">MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset. Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data. To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation. Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2508.14357.pdf' target='_blank'>https://arxiv.org/pdf/2508.14357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rihao Chang, He Jiao, Weizhi Nie, Honglin Guo, Keliang Xie, Zhenhua Wu, Lina Zhao, Yunpeng Bai, Yongtao Ma, Lanjun Wang, Yuting Su, Xi Gao, Weijie Wang, Nicu Sebe, Bruno Lepri, Bingwei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14357">Organ-Agents: Virtual Human Physiology Simulator via LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have enabled new possibilities in simulating complex physiological systems. We introduce Organ-Agents, a multi-agent framework that simulates human physiology via LLM-driven agents. Each Simulator models a specific system (e.g., cardiovascular, renal, immune). Training consists of supervised fine-tuning on system-specific time-series data, followed by reinforcement-guided coordination using dynamic reference selection and error correction. We curated data from 7,134 sepsis patients and 7,895 controls, generating high-resolution trajectories across 9 systems and 125 variables. Organ-Agents achieved high simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and robustness across SOFA-based severity strata. External validation on 22,689 ICU patients from two hospitals showed moderate degradation under distribution shifts with stable simulation. Organ-Agents faithfully reproduces critical multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with coherent timing and phase progression. Evaluation by 15 critical care physicians confirmed realism and physiological plausibility (mean Likert ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations under alternative sepsis treatment strategies, generating trajectories and APACHE II scores aligned with matched real-world patients. In downstream early warning tasks, classifiers trained on synthetic data showed minimal AUROC drops (<0.04), indicating preserved decision-relevant patterns. These results position Organ-Agents as a credible, interpretable, and generalizable digital twin for precision diagnosis, treatment simulation, and hypothesis testing in critical care.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2503.22249.pdf' target='_blank'>https://arxiv.org/pdf/2503.22249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22249">FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2407.06938.pdf' target='_blank'>https://arxiv.org/pdf/2407.06938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Yiji Cheng, Chunyu Wang, Ting Zhang, Jiaolong Yang, Yansong Tang, Feng Zhao, Dong Chen, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06938">RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present RodinHD, which can generate high-fidelity 3D avatars from a portrait image. Existing methods fail to capture intricate details such as hairstyles which we tackle in this paper. We first identify an overlooked problem of catastrophic forgetting that arises when fitting triplanes sequentially on many avatars, caused by the MLP decoder sharing scheme. To overcome this issue, we raise a novel data scheduling strategy and a weight consolidation regularization term, which improves the decoder's capability of rendering sharper details. Additionally, we optimize the guiding effect of the portrait image by computing a finer-grained hierarchical representation that captures rich 2D texture cues, and injecting them to the 3D diffusion model at multiple layers via cross-attention. When trained on 46K avatars with a noise schedule optimized for triplanes, the resulting model can generate 3D avatars with notably better details than previous methods and can generalize to in-the-wild portrait input.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2404.14463.pdf' target='_blank'>https://arxiv.org/pdf/2404.14463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Burdisso, Ernesto Reyes-RamÃ­rez, EsaÃº Villatoro-Tello, Fernando SÃ¡nchez-Vega, Pastor LÃ³pez-Monroy, Petr Motlicek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14463">DAIC-WOZ: On the Validity of Using the Therapist's prompts in Automatic Depression Detection from Clinical Interviews</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic depression detection from conversational data has gained significant interest in recent years. The DAIC-WOZ dataset, interviews conducted by a human-controlled virtual agent, has been widely used for this task. Recent studies have reported enhanced performance when incorporating interviewer's prompts into the model. In this work, we hypothesize that this improvement might be mainly due to a bias present in these prompts, rather than the proposed architectures and methods. Through ablation experiments and qualitative analysis, we discover that models using interviewer's prompts learn to focus on a specific region of the interviews, where questions about past experiences with mental health issues are asked, and use them as discriminative shortcuts to detect depressed participants. In contrast, models using participant responses gather evidence from across the entire interview. Finally, to highlight the magnitude of this bias, we achieve a 0.90 F1 score by intentionally exploiting it, the highest result reported to date on this dataset using only textual information. Our findings underline the need for caution when incorporating interviewers' prompts into models, as they may inadvertently learn to exploit targeted prompts, rather than learning to characterize the language and behavior that are genuinely indicative of the patient's mental health condition.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2403.19655.pdf' target='_blank'>https://arxiv.org/pdf/2403.19655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19655">GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling. Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods. We derive GaussianCube by first using a novel densification-constrained Gaussian fitting algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians, and then rearranging these Gaussians into a predefined voxel grid via Optimal Transport. Since GaussianCube is a structured grid representation, it allows us to use standard 3D U-Net as our backbone in diffusion modeling without elaborate designs. More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude. The compactness of GaussianCube greatly eases the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling. Project page: https://gaussiancube.github.io/.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2311.12817.pdf' target='_blank'>https://arxiv.org/pdf/2311.12817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binzhe Li, Bolin Chen, Zhao Wang, Shiqi Wang, Yan Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12817">Semantic Face Compression for Metaverse: A Compact 3D Descriptor Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this letter, we envision a new metaverse communication paradigm for virtual avatar faces, and develop the semantic face compression with compact 3D facial descriptors. The fundamental principle is that the communication of virtual avatar faces primarily emphasizes the conveyance of semantic information. In light of this, the proposed scheme offers the advantages of being highly flexible, efficient and semantically meaningful. The semantic face compression, which allows the communication of the descriptors for artificial intelligence based understanding, could facilitate numerous applications without the involvement of humans in metaverse. The promise of the proposed paradigm is also demonstrated by performance comparisons with the state-of-the-art video coding standard, Versatile Video Coding. A significant improvement in terms of rate-accuracy performance has been achieved. The proposed scheme is expected to enable numerous applications, such as digital human communication based on machine analysis, and to form the cornerstone of interaction and communication in the metaverse.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2509.11342.pdf' target='_blank'>https://arxiv.org/pdf/2509.11342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyun Han, Siyeon Bak, So-Hui Kim, Kangsoo Kim, Sun-Jeong Kim, Isaac Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11342">What if Virtual Agents Had Scents? Users' Judgments of Virtual Agent Personality and Appeals in Encounters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating multi-sensory cues into Virtual Reality (VR) can significantly enhance user experiences, mirroring the multi-sensory interactions we encounter in the real-world. Olfaction plays a crucial role in shaping impressions when engaging with others. This study examines how non-verbal cues from virtual agents-specifically olfactory cues, emotional expressions, and gender-influence user perceptions during encounters with virtual agents. Our findings indicate that in unscented, woodsy, and floral scent conditions, participants primarily relied on visually observable cues to form their impressions of virtual agents. Positive emotional expressions, conveyed through facial expressions and gestures, contributed to more favorable impressions, with this effect being stronger for the female agent than the male agent. However, in the unpleasant scent condition, participants consistently formed negative impressions, which overpowered the influence of emotional expressions and gender, suggesting that aversive olfactory stimuli can detrimentally impact user perceptions. Our results emphasize the importance of carefully selecting olfactory stimuli when designing immersive and engaging VR interactions. Finally, we present our findings and outline future research directions for effectively integrating olfactory cues into virtual agents.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2410.05131.pdf' target='_blank'>https://arxiv.org/pdf/2410.05131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ashrafi, Francesco Vona, Carina Ringsdorf, Christian Hertel, Luca Toni, Sarina Kailer, Alice Bartels, Tanja Kojic, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05131">Enhancing Job Interview Preparation Through Immersive Experiences Using Photorealistic, AI-powered Metahuman Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study will investigate the user experience while interacting with highly photorealistic virtual job interviewer avatars in Virtual Reality (VR), Augmented Reality (AR), and on a 2D screen. Having a precise speech recognition mechanism, our virtual character performs a mock-up software engineering job interview to adequately immerse the user in a life-like scenario. To evaluate the efficiency of our system, we measure factors such as the provoked level of anxiety, social presence, self-esteem, and intrinsic motivation. This research is a work in progress with a prospective within-subject user study including approximately 40 participants. All users will engage with three job interview conditions (VR, AR, and desktop) and provide their feedback. Additionally, users' bio-physical responses will be collected using a biosensor to measure the level of anxiety during the job interview.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2408.04068.pdf' target='_blank'>https://arxiv.org/pdf/2408.04068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timothy Rupprecht, Sung-En Chang, Yushu Wu, Lei Lu, Enfu Nan, Chih-hsiang Li, Caiyue Lai, Zhimin Li, Zhijun Hu, Yumei He, David Kaeli, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04068">Digital Avatars: Framework Development and Their Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel prompting strategy for artificial intelligence driven digital avatars. To better quantify how our prompting strategy affects anthropomorphic features like humor, authenticity, and favorability we present Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a large language model (LLM) candidate over competitors answering the same or similar prompts. To visualize the responses of our LLM, and the effectiveness of our prompting strategy we propose an end-to-end framework for creating high-fidelity artificial intelligence (AI) driven digital avatars. This pipeline effectively captures an individual's essence for interaction and our streaming algorithm delivers a high-quality digital avatar with real-time audio-video streaming from server to mobile device. Both our visualization tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have state-of-the-art humor, authenticity, and favorability outperforming all competitors and baselines. In the case of our Donald Trump and Joe Biden avatars, their authenticity and favorability are rated higher than even their real-world equivalents.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2404.01700.pdf' target='_blank'>https://arxiv.org/pdf/2404.01700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang YU, Jiayuan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01700">MotionChain: Conversational Motion Controllers via Multimodal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2403.09544.pdf' target='_blank'>https://arxiv.org/pdf/2403.09544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Ashrafi, Vanessa Neuhaus, Francesco Vona, Nicolina Laura Peperkorn, Youssef Shiban, Jan-Niklas Voigt-Antons
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09544">Effect of external characteristics of a virtual human being during the use of a computer-assisted therapy tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identification within media, whether with real or fictional characters, significantly impacts users, shaping their behavior and enriching their social and emotional experiences. Immersive media, like video games, utilize virtual entities such as agents, avatars, or NPCs to connect users with virtual worlds, fostering a heightened sense of immersion and identification. However, challenges arise in visually representing these entities, with design decisions crucial for enhancing user interaction. Recent research highlights the potential of user-defined design, or customization, which goes beyond mere visual resemblance to the user. Understanding how identification with virtual avatars influences user experiences, especially in psychological interventions, is pivotal. In a study exploring this, 22 participants created virtual agents either similar or dissimilar to themselves, which then addressed their dysfunctional thoughts. Results indicate that similarity between users and virtual agents not only boosts identification but also positively impacts emotions and motivation, enhancing interest and enjoyment. This study sheds light on the significance of customization and identification, particularly in computer-assisted therapy tools, underscoring the importance of visual design for optimizing user experiences.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2402.13724.pdf' target='_blank'>https://arxiv.org/pdf/2402.13724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Bai, Peng Chen, Xiaolan Peng, Lu Liu, Hui Chen, Mike Zheng Shou, Feng Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13724">Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2508.10576.pdf' target='_blank'>https://arxiv.org/pdf/2508.10576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10576">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2505.24877.pdf' target='_blank'>https://arxiv.org/pdf/2505.24877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24877">AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2504.05046.pdf' target='_blank'>https://arxiv.org/pdf/2504.05046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghao Ren, Yi Lu, Jiayi Huang, Jiayi Zhao, He Zhang, Tao Yu, Qiu Shen, Xun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05046">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing human Motion Capture (MoCap) methods mostly focus on the visual similarity while neglecting the physical plausibility. As a result, downstream tasks such as driving virtual human in 3D scene or humanoid robots in real world suffer from issues such as timing drift and jitter, spatial problems like sliding and penetration, and poor global trajectory accuracy. In this paper, we revisit human MoCap from the perspective of interaction between human body and physical world by exploring the role of pressure. Firstly, we construct a large-scale human Motion capture dataset with Pressure, RGB and Optical sensors (named MotionPRO), which comprises 70 volunteers performing 400 types of motion, encompassing a total of 12.4M pose frames. Secondly, we examine both the necessity and effectiveness of the pressure signal through two challenging tasks: (1) pose and trajectory estimation based solely on pressure: We propose a network that incorporates a small kernel decoder and a long-short-term attention module, and proof that pressure could provide accurate global trajectory and plausible lower body pose. (2) pose and trajectory estimation by fusing pressure and RGB: We impose constraints on orthographic similarity along the camera axis and whole-body contact along the vertical axis to enhance the cross-attention strategy to fuse pressure and RGB feature maps. Experiments demonstrate that fusing pressure with RGB features not only significantly improves performance in terms of objective metrics, but also plausibly drives virtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that incorporating physical perception enables humanoid robots to perform more precise and stable actions, which is highly beneficial for the development of embodied artificial intelligence. Project page is available at: https://nju-cite-mocaphumanoid.github.io/MotionPRO/
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2411.08228.pdf' target='_blank'>https://arxiv.org/pdf/2411.08228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atieh Taheri, Purav Bhardwaj, Arthur Caetano, Alice Zhong, Misha Sra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08228">Virtual Buddy: Redefining Conversational AI Interactions for Individuals with Hand Motor Disabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence have transformed the paradigm of human-computer interaction, with the development of conversational AI systems playing a pivotal role. These systems employ technologies such as natural language processing and machine learning to simulate intelligent and human-like conversations. Driven by the personal experience of an individual with a neuromuscular disease who faces challenges with leaving home and contends with limited hand-motor control when operating digital systems, including conversational AI platforms, we propose a method aimed at enriching their interaction with conversational AI. Our prototype allows the creation of multiple agent personas based on hobbies and interests, to support topic-based conversations. In contrast with existing systems, such as Replika, that offer a 1:1 relation with a virtual agent, our design enables one-to-many relationships, easing the process of interaction for this individual by reducing the need for constant data input. We can imagine our prototype potentially helping others who are in a similar situation with reduced typing/input ability.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2309.03550.pdf' target='_blank'>https://arxiv.org/pdf/2309.03550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungwon Hwang, Junha Hyung, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03550">Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models such as ControlNet have enabled geometrically controllable, high-fidelity text-to-image generation. However, none of them addresses the question of adding such controllability to text-to-3D generation. In response, we propose Text2Control3D, a controllable text-to-3D avatar generation method whose facial expression is controllable given a monocular video casually captured with hand-held camera. Our main strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF) optimized with a set of controlled viewpoint-aware images that we generate from ControlNet, whose condition input is the depth map extracted from the input video. When generating the viewpoint-aware images, we utilize cross-reference attention to inject well-controlled, referential facial expression and appearance via cross attention. We also conduct low-pass filtering of Gaussian latent of the diffusion model in order to ameliorate the viewpoint-agnostic texture problem we observed from our empirical analysis, where the viewpoint-aware images contain identical textures on identical pixel positions that are incomprehensible in 3D. Finally, to train NeRF with the images that are viewpoint-aware yet are not strictly consistent in geometry, our approach considers per-image geometric variation as a view of deformation from a shared 3D canonical space. Consequently, we construct the 3D avatar in a canonical space of deformable NeRF by learning a set of per-image deformation via deformation field table. We demonstrate the empirical results and discuss the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2306.12570.pdf' target='_blank'>https://arxiv.org/pdf/2306.12570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12570">Local 3D Editing via 3D Distillation of CLIP Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D content manipulation is an important computer vision task with many real-world applications (e.g., product design, cartoon generation, and 3D Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic 3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of NeRF still remains a challenging problem since the visual quality tends to degrade after manipulation and suboptimal control handles such as 2D semantic maps are used for manipulations. While text-guided manipulations have shown potential in 3D editing, such approaches often lack locality. To overcome these problems, we propose Local Editing NeRF (LENeRF), which only requires text inputs for fine-grained and localized manipulation. Specifically, we present three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field Network, and the Deformation Network, which are jointly used for local manipulations of 3D features by estimating a 3D attention field. The 3D attention field is learned in an unsupervised way, by distilling the zero-shot mask generation capability of CLIP to the 3D space with multi-view guidance. We conduct diverse experiments and thorough evaluations both quantitatively and qualitatively.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2507.10542.pdf' target='_blank'>https://arxiv.org/pdf/2507.10542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias NieÃner, Derek Bradley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10542">ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2502.02372.pdf' target='_blank'>https://arxiv.org/pdf/2502.02372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02372">MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of a virtual digital avatar is a crucial research topic in the field of computer vision. Many existing works utilize Neural Radiance Fields (NeRF) to address this issue and have achieved impressive results. However, previous works assume the images of the training person are available and fixed while the appearances and poses of a subject could constantly change and increase in real-world scenarios. How to update the human avatar but also maintain the ability to render the old appearance of the person is a practical challenge. One trivial solution is to combine the existing virtual avatar models based on NeRF with continual learning methods. However, there are some critical issues in this approach: learning new appearances and poses can cause the model to forget past information, which in turn leads to a degradation in the rendering quality of past appearances, especially color bleeding issues, and incorrect human body poses. In this work, we propose a maintainable avatar (MaintaAvatar) based on neural radiance fields by continual learning, which resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose Distillation Module. Overall, our model requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting, thus achieving a maintainable virtual avatar. The experimental results validate the effectiveness of our MaintaAvatar model.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2412.08684.pdf' target='_blank'>https://arxiv.org/pdf/2412.08684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengze Wang, Xueting Li, Chao Liu, Matthew Chan, Michael Stengel, Henry Fuchs, Shalini De Mello, Koki Nagano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08684">Coherent3D: Coherent 3D Portrait Video Reconstruction via Triplane Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in single-image 3D portrait reconstruction have enabled telepresence systems to stream 3D portrait videos from a single camera in real-time, democratizing telepresence. However, per-frame 3D reconstruction exhibits temporal inconsistency and forgets the user's appearance. On the other hand, self-reenactment methods can render coherent 3D portraits by driving a 3D avatar built from a single reference image, but fail to faithfully preserve the user's per-frame appearance (e.g., instantaneous facial expression and lighting). As a result, none of these two frameworks is an ideal solution for democratized 3D telepresence. In this work, we address this dilemma and propose a novel solution that maintains both coherent identity and dynamic per-frame appearance to enable the best possible realism. To this end, we propose a new fusion-based method that takes the best of both worlds by fusing a canonical 3D prior from a reference view with dynamic appearance from per-frame input views, producing temporally stable 3D videos with faithful reconstruction of the user's per-frame appearance. Trained only using synthetic data produced by an expression-conditioned 3D GAN, our encoder-based method achieves both state-of-the-art 3D reconstruction and temporal consistency on in-studio and in-the-wild datasets. https://research.nvidia.com/labs/amri/projects/coherent3d
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2404.02686.pdf' target='_blank'>https://arxiv.org/pdf/2404.02686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiali Zheng, Rolandos Alexandros Potamias, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02686">Design2Cloth: 3D Cloth Generation from 2D Masks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a significant shift in the field of digital avatar research, towards modeling, animating and reconstructing clothed human representations, as a key step towards creating realistic avatars. However, current 3D cloth generation methods are garment specific or trained completely on synthetic data, hence lacking fine details and realism. In this work, we make a step towards automatic realistic garment design and propose Design2Cloth, a high fidelity 3D generative model trained on a real world dataset from more than 2000 subject scans. To provide vital contribution to the fashion industry, we developed a user-friendly adversarial model capable of generating diverse and detailed clothes simply by drawing a 2D cloth mask. Under a series of both qualitative and quantitative experiments, we showcase that Design2Cloth outperforms current state-of-the-art cloth generative models by a large margin. In addition to the generative properties of our network, we showcase that the proposed method can be used to achieve high quality reconstructions from single in-the-wild images and 3D scans. Dataset, code and pre-trained model will become publicly available.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2403.17213.pdf' target='_blank'>https://arxiv.org/pdf/2403.17213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Stefanos Zafeiriou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17213">AnimateMe: 4D Facial Expressions via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of photorealistic 3D avatar reconstruction and generation has garnered significant attention in recent years; however, animating such avatars remains challenging. Recent advances in diffusion models have notably enhanced the capabilities of generative models in 2D animation. In this work, we directly utilize these models within the 3D domain to achieve controllable and high-fidelity 4D facial animation. By integrating the strengths of diffusion processes and geometric deep learning, we employ Graph Neural Networks (GNNs) as denoising diffusion models in a novel approach, formulating the diffusion process directly on the mesh space and enabling the generation of 3D facial expressions. This facilitates the generation of facial deformations through a mesh-diffusion-based model. Additionally, to ensure temporal coherence in our animations, we propose a consistent noise sampling method. Under a series of both quantitative and qualitative experiments, we showcase that the proposed method outperforms prior work in 4D expression synthesis by generating high-fidelity extreme expressions. Furthermore, we applied our method to textured 4D facial expression generation, implementing a straightforward extension that involves training on a large-scale textured 4D facial expression database.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2402.05803.pdf' target='_blank'>https://arxiv.org/pdf/2402.05803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wamiq Reyaz Para, Abdelrahman Eldesokey, Zhenyu Li, Pradyumna Reddy, Jiankang Deng, Peter Wonka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05803">AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an approach for 3D head avatar generation and editing with multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and a Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatars given a single or no condition. However, it is challenging to generate samples that adhere to multiple conditions of different modalities. On the other hand, LDMs excel at learning complex conditional distributions. To this end, we propose to exploit the conditioning capabilities of LDMs to enable multi-modal control over the latent space of a pre-trained 3D GAN. Our method can generate and edit 3D head avatars given a mixture of control signals such as RGB input, segmentation masks, and global attributes. This provides better control over the generation and editing of synthetic avatars both globally and locally. Experiments show that our proposed approach outperforms a solely GAN-based approach both qualitatively and quantitatively on generation and editing tasks. To the best of our knowledge, our approach is the first to introduce multi-modal conditioning to 3D avatar generation and editing. \\href{avatarmmc-sig24.github.io}{Project Page}
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2508.16401.pdf' target='_blank'>https://arxiv.org/pdf/2508.16401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NVIDIA, :, Chaeyeon Chung, Ilya Fedorov, Michael Huang, Aleksey Karmanov, Dmitry Korobchenko, Roger Ribera, Yeongho Seol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16401">Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven facial animation presents an effective solution for animating digital avatars. In this paper, we detail the technical aspects of NVIDIA Audio2Face-3D, including data acquisition, network architecture, retargeting methodology, evaluation metrics, and use cases. Audio2Face-3D system enables real-time interaction between human users and interactive avatars, facilitating facial animation authoring for game characters. To assist digital avatar creators and game developers in generating realistic facial animations, we have open-sourced Audio2Face-3D networks, SDK, training framework, and example dataset.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2507.19481.pdf' target='_blank'>https://arxiv.org/pdf/2507.19481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byungjun Kim, Shunsuke Saito, Giljoo Nam, Tomas Simon, Jason Saragih, Hanbyul Joo, Junxuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19481">HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2412.10487.pdf' target='_blank'>https://arxiv.org/pdf/2412.10487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonello Ceravola, Frank Joublin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10487">HyperGraphOS: A Modern Meta-Operating System for the Scientific and Engineering Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents HyperGraphOS, a significant innovation in the domain of operating systems, specifically designed to address the needs of scientific and engineering domains. This platform aims to combine model-based engineering, graph modeling, data containers, and documents, along with tools for handling computational elements. HyperGraphOS functions as an Operating System offering to users an infinite workspace for creating and managing complex models represented as graphs with customizable semantics. By leveraging a web-based architecture, it requires only a modern web browser for access, allowing organization of knowledge, documents, and content into models represented in a network of workspaces. Elements of the workspace are defined in terms of domain-specific languages (DSLs). These DSLs are pivotal for navigating workspaces, generating code, triggering AI components, and organizing information and processes. The models' dual nature as both visual drawings and data structures allows dynamic modifications and inspections both interactively as well as programaticaly. We evaluated HyperGraphOS's efficiency and applicability across a large set of diverse domains, including the design and development of a virtual Avatar dialog system, a robotic task planner based on large language models (LLMs), a new meta-model for feature-based code development and many others. Our findings show that HyperGraphOS offers substantial benefits in the interaction with a computer as information system, as platoform for experiments and data analysis, as streamlined engineering processes, demonstrating enhanced flexibility in managing data, computation and documents, showing an innovative approaches to persistent desktop environments.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2406.12035.pdf' target='_blank'>https://arxiv.org/pdf/2406.12035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rhythm Arora, Pooja Prajod, Matteo Lavit Nicora, Daniele Panzeri, Giovanni Tauro, Rocco Vertechy, Matteo Malosio, Elisabeth AndrÃ©, Patrick Gebhard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12035">Socially Interactive Agents for Robotic Neurorehabilitation Training: Conceptualization and Proof-of-concept Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Individuals with diverse motor abilities often benefit from intensive and specialized rehabilitation therapies aimed at enhancing their functional recovery. Nevertheless, the challenge lies in the restricted availability of neurorehabilitation professionals, hindering the effective delivery of the necessary level of care. Robotic devices hold great potential in reducing the dependence on medical personnel during therapy but, at the same time, they generally lack the crucial human interaction and motivation that traditional in-person sessions provide. To bridge this gap, we introduce an AI-based system aimed at delivering personalized, out-of-hospital assistance during neurorehabilitation training. This system includes a rehabilitation training device, affective signal classification models, training exercises, and a socially interactive agent as the user interface. With the assistance of a professional, the envisioned system is designed to be tailored to accommodate the unique rehabilitation requirements of an individual patient. Conceptually, after a preliminary setup and instruction phase, the patient is equipped to continue their rehabilitation regimen autonomously in the comfort of their home, facilitated by a socially interactive agent functioning as a virtual coaching assistant. Our approach involves the integration of an interactive socially-aware virtual agent into a neurorehabilitation robotic framework, with the primary objective of recreating the social aspects inherent to in-person rehabilitation sessions. We also conducted a feasibility study to test the framework with healthy patients. The results of our preliminary investigation indicate that participants demonstrated a propensity to adapt to the system. Notably, the presence of the interactive agent during the proposed exercises did not act as a source of distraction; instead, it positively impacted users' engagement.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2406.09839.pdf' target='_blank'>https://arxiv.org/pdf/2406.09839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Yeza Baihaqi, Angel GarcÃ­a Contreras, Seiya Kawano, Koichiro Yoshino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09839">Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapport is known as a conversational aspect focusing on relationship building, which influences outcomes in collaborative tasks. This study aims to establish human-agent rapport through small talk by using a rapport-building strategy. We implemented this strategy for the virtual agents based on dialogue strategies by prompting a large language model (LLM). In particular, we utilized two dialogue strategies-predefined sequence and free-form-to guide the dialogue generation framework. We conducted analyses based on human evaluations, examining correlations between total turn, utterance characters, rapport score, and user experience variables: naturalness, satisfaction, interest, engagement, and usability. We investigated correlations between rapport score and naturalness, satisfaction, engagement, and conversation flow. Our experimental results also indicated that using free-form to prompt the rapport-building strategy performed the best in subjective scores.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2403.11453.pdf' target='_blank'>https://arxiv.org/pdf/2403.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongrui Cai, Yuting Xiao, Xuan Wang, Jiafei Li, Yudong Guo, Yanbo Fan, Shenghua Gao, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11453">Hybrid Explicit Representation for Ultra-Realistic Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel approach to creating ultra-realistic head avatars and rendering them in real-time (>30fps at $2048 \times 1334$ resolution). First, we propose a hybrid explicit representation that combines the advantages of two primitive-based efficient rendering techniques. UV-mapped 3D mesh is utilized to capture sharp and rich textures on smooth surfaces, while 3D Gaussian Splatting is employed to represent complex geometric structures. In the pipeline of modeling an avatar, after tracking parametric models based on captured multi-view RGB videos, our goal is to simultaneously optimize the texture and opacity map of mesh, as well as a set of 3D Gaussian splats localized and rigged onto the mesh facets. Specifically, we perform $Î±$-blending on the color and opacity values based on the merged and re-ordered z-buffer from the rasterization results of mesh and 3DGS. This process involves the mesh and 3DGS adaptively fitting the captured visual information to outline a high-fidelity digital avatar. To avoid artifacts caused by Gaussian splats crossing the mesh facets, we design a stable hybrid depth sorting strategy. Experiments illustrate that our modeled results exceed those of state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2108.05681.pdf' target='_blank'>https://arxiv.org/pdf/2108.05681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyowoon Seo, Jihong Park, Mehdi Bennis, MÃ©rouane Debbah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.05681">Semantics-Native Communication with Contextual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spurred by a huge interest in the post-Shannon communication, it has recently been shown that leveraging semantics can significantly improve the communication effectiveness across many tasks. In this article, inspired by human communication, we propose a novel stochastic model of System 1 semantics-native communication (SNC) for generic tasks, where a speaker has an intention of referring to an entity, extracts the semantics, and communicates its symbolic representation to a target listener. To further reach its full potential, we additionally infuse contextual reasoning into SNC such that the speaker locally and iteratively self-communicates with a virtual agent built on the physical listener's unique way of coding its semantics, i.e., communication context. The resultant System 2 SNC allows the speaker to extract the most effective semantics for its listener. Leveraging the proposed stochastic model, we show that the reliability of System 2 SNC increases with the number of meaningful concepts, and derive the expected semantic representation (SR) bit length which quantifies the extracted effective semantics. It is also shown that System 2 SNC significantly reduces the SR length without compromising communication reliability.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2508.19688.pdf' target='_blank'>https://arxiv.org/pdf/2508.19688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gangjian Zhang, Jian Shu, Nanjie Yao, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19688">SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular texture 3D human reconstruction aims to create a complete 3D digital avatar from just a single front-view human RGB image. However, the geometric ambiguity inherent in a single 2D image and the scarcity of 3D human training data are the main obstacles limiting progress in this field. To address these issues, current methods employ prior geometric estimation networks to derive various human geometric forms, such as the SMPL model and normal maps. However, they struggle to integrate these modalities effectively, leading to view inconsistencies, such as facial distortions. To this end, we propose a two-process 3D human reconstruction framework, SAT, which seamlessly learns various prior geometries in a unified manner and reconstructs high-quality textured 3D avatars as the final output. To further facilitate geometry learning, we introduce a Supervisor Feature Regularization module. By employing a multi-view network with the same structure to provide intermediate features as training supervision, these varied geometric priors can be better fused. To tackle data scarcity and further improve reconstruction quality, we also propose an Online Animation Augmentation module. By building a one-feed-forward animation network, we augment a massive number of samples from the original 3D human data online for model training. Extensive experiments on two benchmarks show the superiority of our approach compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2504.18215.pdf' target='_blank'>https://arxiv.org/pdf/2504.18215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nanjie Yao, Gangjian Zhang, Wenhao Shen, Jian Shu, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18215">Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : https://e2e3dgsrecon.github.io/e2e3dgsrecon/.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2502.01046.pdf' target='_blank'>https://arxiv.org/pdf/2502.01046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Ye, Boyuan Cao, Hongming Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01046">Emotional Face-to-Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How much can we infer about an emotional voice solely from an expressive face? This intriguing question holds great potential for applications such as virtual character dubbing and aiding individuals with expressive language disorders. Existing face-to-speech methods offer great promise in capturing identity characteristics but struggle to generate diverse vocal styles with emotional expression. In this paper, we explore a new task, termed emotional face-to-speech, aiming to synthesize emotional speech directly from expressive facial cues. To that end, we introduce DEmoFace, a novel generative framework that leverages a discrete diffusion transformer (DiT) with curriculum learning, built upon a multi-level neural audio codec. Specifically, we propose multimodal DiT blocks to dynamically align text and speech while tailoring vocal styles based on facial emotion and identity. To enhance training efficiency and generation quality, we further introduce a coarse-to-fine curriculum learning algorithm for multi-level token processing. In addition, we develop an enhanced predictor-free guidance to handle diverse conditioning scenarios, enabling multi-conditional generation and disentangling complex attributes effectively. Extensive experimental results demonstrate that DEmoFace generates more natural and consistent speech compared to baselines, even surpassing speech-driven methods. Demos are shown at https://demoface-ai.github.io/.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2403.08764.pdf' target='_blank'>https://arxiv.org/pdf/2403.08764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08764">VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2402.17292.pdf' target='_blank'>https://arxiv.org/pdf/2402.17292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17292">DivAvatar: Diverse 3D Avatar Generation with a Single Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Avatar generation has recently made significant strides due to advancements in diffusion models. However, most existing work remains constrained by limited diversity, producing avatars with subtle differences in appearance for a given text prompt. We design DivAvatar, a novel framework that generates diverse avatars, empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text prompt. Different from most existing work that exploits scene-specific 3D representations such as NeRF, DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss, the former producing appearances of high textual fidelity by separate fine-tuning of specific body parts and the latter improving geometry quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2401.13832.pdf' target='_blank'>https://arxiv.org/pdf/2401.13832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elizaveta Kuznetsova, Mykola Makhortykh, Maryna Sydorova, Aleksandra Urman, Ilaria Vitulano, Martha Stolze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13832">Algorithmically Curated Lies: How Search Engines Handle Misinformation about US Biolabs in Ukraine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing volume of online content prompts the need for adopting algorithmic systems of information curation. These systems range from web search engines to recommender systems and are integral for helping users stay informed about important societal developments. However, unlike journalistic editing the algorithmic information curation systems (AICSs) are known to be subject to different forms of malperformance which make them vulnerable to possible manipulation. The risk of manipulation is particularly prominent in the case when AICSs have to deal with information about false claims that underpin propaganda campaigns of authoritarian regimes. Using as a case study of the Russian disinformation campaign concerning the US biolabs in Ukraine, we investigate how one of the most commonly used forms of AICSs - i.e. web search engines - curate misinformation-related content. For this aim, we conduct virtual agent-based algorithm audits of Google, Bing, and Yandex search outputs in June 2022. Our findings highlight the troubling performance of search engines. Even though some search engines, like Google, were less likely to return misinformation results, across all languages and locations, the three search engines still mentioned or promoted a considerable share of false content (33% on Google; 44% on Bing, and 70% on Yandex). We also find significant disparities in misinformation exposure based on the language of search, with all search engines presenting a higher number of false stories in Russian. Location matters as well with users from Germany being more likely to be exposed to search results promoting false information. These observations stress the possibility of AICSs being vulnerable to manipulation, in particular in the case of the unfolding propaganda campaigns, and underline the importance of monitoring performance of these systems to prevent it.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2306.09329.pdf' target='_blank'>https://arxiv.org/pdf/2306.09329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru, Cristian Sminchisescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09329">DreamHuman: Animatable 3D Avatars from Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DreamHuman, a method to generate realistic animatable 3D human avatar models solely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than animated 3D human models, and anthropometric consistency for complex structures like people remains a challenge. DreamHuman connects large text-to-image synthesis models, neural radiance fields, and statistical human body models in a novel modeling and optimization framework. This makes it possible to generate dynamic 3D human avatars with high-quality textures and learned, instance-specific, surface deformations. We demonstrate that our method is capable to generate a wide variety of animatable, realistic 3D human models from text. Our 3D models have diverse appearance, clothing, skin tones and body shapes, and significantly outperform both generic text-to-3D approaches and previous text-based 3D avatar generators in visual fidelity. For more results and animations please check our website at https://dream-human.github.io.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2304.00247.pdf' target='_blank'>https://arxiv.org/pdf/2304.00247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takahiro Tsumura, Seiji Yamada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00247">Improving of Robotic Virtual Agent's errors that are accepted by reaction and human's preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One way to improve the relationship between humans and anthropomorphic agents is to have humans empathize with the agents. In this study, we focused on a task between an agent and a human in which the agent makes a mistake. To investigate significant factors for designing a robotic agent that can promote humans empathy, we experimentally examined the hypothesis that agent reaction and human's preference affect human empathy and acceptance of the agent's mistakes. The experiment consisted of a four-condition, three-factor mixed design with agent reaction, selected agent's body color for human's preference, and pre- and post-task as factors. The results showed that agent reaction and human's preference did not affect empathy toward the agent but did allow the agent to make mistakes. It was also shown that empathy for the agent decreased when the agent made a mistake on the task. The results of this study provide a way to control impressions of the robotic virtual agent's behaviors, which are increasingly used in society.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2505.21531.pdf' target='_blank'>https://arxiv.org/pdf/2505.21531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21531">How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore the human motion knowledge of Large Language Models (LLMs) through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations. Using 20 representative motion instructions that cover fundamental movements and balance body part usage, we conduct comprehensive evaluations, including human and automatic scoring of both high-level movement plans and generated animations, as well as automatic comparison with oracle positions in low-level planning. Our findings show that LLMs are strong at interpreting high-level body movements but struggle with precise body part positioning. While decomposing motion queries into atomic components improves planning, LLMs face challenges in multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximations for general spatial descriptions, but fall short in handling precise spatial specifications. Notably, LLMs demonstrate promise in conceptualizing creative motions and distinguishing culturally specific motion patterns.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2504.21718.pdf' target='_blank'>https://arxiv.org/pdf/2504.21718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiying Li, Xingqun Qi, Bingkun Yang, Chen Weile, Zezhao Tian, Muyi Sun, Qifeng Liu, Man Zhang, Zhenan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21718">VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating responsive listener head dynamics with nuanced emotions and expressive reactions is crucial for practical dialogue modeling in various virtual avatar animations. Previous studies mainly focus on the direct short-term production of listener behavior. They overlook the fine-grained control over motion variations and emotional intensity, especially in long-sequence modeling. Moreover, the lack of long-term and large-scale paired speaker-listener corpora including head dynamics and fine-grained multi-modality annotations (e.g., text-based expression descriptions, emotional intensity) also limits the application of dialogue modeling.Therefore, we first newly collect a large-scale multi-turn dataset of 3D dyadic conversation containing more than 1.4M valid frames for multi-modal responsive interaction, dubbed ListenerX. Additionally, we propose VividListener, a novel framework enabling fine-grained, expressive and controllable listener dynamics modeling. This framework leverages multi-modal conditions as guiding principles for fostering coherent interactions between speakers and listeners.Specifically, we design the Responsive Interaction Module (RIM) to adaptively represent the multi-modal interactive embeddings. RIM ensures the listener dynamics achieve fine-grained semantic coordination with textual descriptions and adjustments, while preserving expressive reaction with speaker behavior. Meanwhile, we design the Emotional Intensity Tags (EIT) for emotion intensity editing with multi-modal information integration, applying to both text descriptions and listener motion amplitude.Extensive experiments conducted on our newly collected ListenerX dataset demonstrate that VividListener achieves state-of-the-art performance, realizing expressive and controllable listener dynamics.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2504.06031.pdf' target='_blank'>https://arxiv.org/pdf/2504.06031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Leichert, Monique Koke, Britta Wrede, Birte Richter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06031">Virtual Agent Tutors in Sheltered Workshops: A Feasibility Study on Attention Training for Individuals with Intellectual Disabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we evaluate the feasibility of socially assistive virtual agent-based cognitive training for people with intellectual disabilities (ID) in a sheltered workshop. The Robo- Camp system, originally developed for children with Attention Deficit Hyperactivity Disorder (ADHD), is adapted based on the results of a pilot study in which we identified barriers and collected feedback from workshop staff. In a subsequent study, we investigate the aspects of usability, technical reliability, attention training capabilities and novelty effect in the feasibility of integrating the RoboCamp system.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2502.10088.pdf' target='_blank'>https://arxiv.org/pdf/2502.10088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Song, Felix Pabst, Ulrich Eck, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10088">Enhancing Patient Acceptance of Robotic Ultrasound through Conversational Virtual Agent and Immersive Visualizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic ultrasound systems can enhance medical diagnostics, but patient acceptance is a challenge. We propose a system combining an AI-powered conversational virtual agent with three mixed reality visualizations to improve trust and comfort. The virtual agent, powered by a large language model, engages in natural conversations and guides the ultrasound robot, enhancing interaction reliability. The visualizations include augmented reality, augmented virtuality, and fully immersive virtual reality, each designed to create patient-friendly experiences. A user study demonstrated significant improvements in trust and acceptance, offering valuable insights for designing mixed reality and virtual agents in autonomous medical procedures.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2409.08738.pdf' target='_blank'>https://arxiv.org/pdf/2409.08738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Gao, Haochun Huai, Sena Yildiz-Degirmenci, Maria Bannert, Enkelejda Kasneci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08738">DataliVR: Transformation of Data Literacy Education through Virtual Reality with ChatGPT-Powered Enhancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data literacy is essential in today's data-driven world, emphasizing individuals' abilities to effectively manage data and extract meaningful insights. However, traditional classroom-based educational approaches often struggle to fully address the multifaceted nature of data literacy. As education undergoes digital transformation, innovative technologies such as Virtual Reality (VR) offer promising avenues for immersive and engaging learning experiences. This paper introduces DataliVR, a pioneering VR application aimed at enhancing the data literacy skills of university students within a contextual and gamified virtual learning environment. By integrating Large Language Models (LLMs) like ChatGPT as a conversational artificial intelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides personalized learning assistance, enriching user learning experiences. Our study employed an experimental approach, with chatbot availability as the independent variable, analyzing learning experiences and outcomes as dependent variables with a sample of thirty participants. Our approach underscores the effectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering data literacy skills. Moreover, our study examines the impact of the ChatGPT-based AI chatbot on users' learning, revealing significant effects on both learning experiences and outcomes. Our study presents a robust tool for fostering data literacy skills, contributing significantly to the digital advancement of data literacy education through cutting-edge VR and AI technologies. Moreover, our research provides valuable insights and implications for future research endeavors aiming to integrate LLMs (e.g., ChatGPT) into educational VR platforms.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2403.10805.pdf' target='_blank'>https://arxiv.org/pdf/2403.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, Shunman Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10805">Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture. The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy features, represented as a unified latent feature, are fed into the AdaLN transformer. The AdaLN transformer introduces a conditional mechanism that applies a uniform function across all tokens, thereby effectively modeling the correlation between the fuzzy features and the gesture sequence. This module ensures a high level of gesture-speech synchronization while preserving naturalness. Finally, we employ the diffusion model to train and infer various gestures. Extensive subjective and objective evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's superior performance to the current state-of-the-art approaches. Persona-Gestor improves the system's usability and generalization capabilities, setting a new benchmark in speech-driven gesture synthesis and broadening the horizon for virtual human technology. Supplementary videos and code can be accessed at https://zf223669.github.io/Diffmotion-v2-website/
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2312.02214.pdf' target='_blank'>https://arxiv.org/pdf/2312.02214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02214">FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D Gaussian field embedded in the surface of a parametric face model and learn extra spatial offset to model non-surface regions and subtle facial details. While full use of geometric priors can capture high-frequency facial details and preserve exaggerated expressions, proper initialization can help reduce the number of Gaussians, thus enabling super-fast rendering speed. Extensive experimental results demonstrate that FlashAvatar outperforms existing works regarding visual quality and personalized details and is almost an order of magnitude faster in rendering speed. Project page: https://ustc3dv.github.io/FlashAvatar/
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2301.06471.pdf' target='_blank'>https://arxiv.org/pdf/2301.06471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Lavit Nicora, Sebastian Beyrodt, Dimitra Tsovaltzi, Fabrizio Nunnari, Patrick Gebhard, Matteo Malosio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06471">Towards social embodied cobots: The integration of an industrial cobot with a social virtual agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of the physical capabilities of an industrial collaborative robot with a social virtual character may represent a viable solution to enhance the workers' perception of the system as an embodied social entity and increase social engagement and well-being at the workplace. An online study was setup using prerecorded video interactions in order to pilot potential advantages of different embodied configurations of the cobot-avatar system in terms of perceptions of Social Presence, cobot-avatar Unity and Social Role of the system, and explore the relation of these. In particular, two different configurations were explored and compared: the virtual character was displayed either on a tablet strapped onto the base of the cobot or on a large TV screen positioned at the back of the workcell. The results imply that participants showed no clear preference based on the constructs, and both configurations fulfill these basic criteria. In terms of the relations between the constructs, there were strong correlations between perception of Social Presence, Unity and Social Role (Collegiality). This gives a valuable insight into the role of these constructs in the perception of cobots as embodied social entities, and towards building cobots that support well-being at the workplace.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2507.19359.pdf' target='_blank'>https://arxiv.org/pdf/2507.19359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanmiao Liu, Esam Ghaleb, AslÄ± ÃzyÃ¼rek, Zerrin Yumak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19359">SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at https://semgesture.github.io/.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2505.05376.pdf' target='_blank'>https://arxiv.org/pdf/2505.05376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rachmadio Noval Lazuardi, Artem Sevastopolsky, Egor Zakharov, Matthias Niessner, Vanessa Sklyarova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05376">GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel method that reconstructs hair strands directly from colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair strand reconstruction is a fundamental problem in computer vision and graphics that can be used for high-fidelity digital avatar synthesis, animation, and AR/VR applications. However, accurately recovering hair strands from raw scan data remains challenging due to human hair's complex and fine-grained structure. Existing methods typically rely on RGB captures, which can be sensitive to the environment and can be a challenging domain for extracting the orientation of guiding strands, especially in the case of challenging hairstyles. To reconstruct the hair purely from the observed geometry, our method finds sharp surface features directly on the scan and estimates strand orientation through a neural 2D line detector applied to the renderings of scan shading. Additionally, we incorporate a diffusion prior trained on a diverse set of synthetic hair scans, refined with an improved noise schedule, and adapted to the reconstructed contents via a scan-specific text prompt. We demonstrate that this combination of supervision signals enables accurate reconstruction of both simple and intricate hairstyles without relying on color information. To facilitate further research, we introduce Strands400, the largest publicly available dataset of hair strands with detailed surface geometry extracted from real-world data, which contains reconstructed hair strands from the scans of 400 subjects.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2504.15835.pdf' target='_blank'>https://arxiv.org/pdf/2504.15835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Wu, Malte Prinzler, Xiaogang Jin, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15835">Text-based Animatable 3D Avatars with Morphable Model Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of high-quality, animatable 3D head avatars from text has enormous potential in content creation applications such as games, movies, and embodied virtual assistants. Current text-to-3D generation methods typically combine parametric head models with 2D diffusion models using score distillation sampling to produce 3D-consistent results. However, they struggle to synthesize realistic details and suffer from misalignments between the appearance and the driving parametric model, resulting in unnatural animation results. We discovered that these limitations stem from ambiguities in the 2D diffusion predictions during 3D avatar distillation, specifically: i) the avatar's appearance and geometry is underconstrained by the text input, and ii) the semantic alignment between the predictions and the parametric head model is insufficient because the diffusion model alone cannot incorporate information from the parametric model. In this work, we propose a novel framework, AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with morphable model alignment, and introduce two key strategies to address these challenges. First, we tackle appearance and geometry ambiguities by utilizing prior information from a pretrained text-to-3D model to initialize a 3D avatar with robust appearance, geometry, and rigging relationships to the morphable model. Second, we refine the initial 3D avatar for dynamic expressions using a ControlNet that is conditioned on semantic and normal maps of the morphable model to ensure accurate alignment. As a result, our method outperforms existing approaches in terms of synthesis quality, alignment, and animation fidelity. Our experiments show that the proposed method advances the state of the art in text-based, animatable 3D head avatar generation.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2411.16729.pdf' target='_blank'>https://arxiv.org/pdf/2411.16729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Siyuan Zhao, Naye Ji, Zhaohan Wang, Jingmei Wu, Fuxing Gao, Zhenqing Ye, Leyao Yan, Lanxin Dai, Weidong Geng, Xin Lyu, Bozuo Zhao, Dingguo Yu, Hui Du, Bin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16729">DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation using transformer-based generative models represents a rapidly advancing area within virtual human creation. However, existing models face significant challenges due to their quadratic time and space complexities, limiting scalability and efficiency. To address these limitations, we introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamba-2 architecture. DiM-Gestor features a dual-component framework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping module, both built on the Mamba-2. The fuzzy feature extractor, integrated with a Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit, continuous speech features. These features are synthesized into a unified latent representation and then processed by the speech-to-gesture mapping module. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced Mamba-2 mechanism to uniformly apply transformations across all sequence tokens. This enables precise modeling of the nuanced interplay between speech features and gesture dynamics. We utilize a diffusion model to train and infer diverse gesture outputs. Extensive subjective and objective evaluations conducted on the newly released Chinese Co-Speech Gestures dataset corroborate the efficacy of our proposed model. Compared with Transformer-based architecture, the assessments reveal that our approach delivers competitive results and significantly reduces memory usage, approximately 2.4 times, and enhances inference speeds by 2 to 4 times. Additionally, we released the CCG dataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six styles across five scenarios) of 3D full-body skeleton gesture motion performed by professional Chinese TV broadcasters.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2408.00370.pdf' target='_blank'>https://arxiv.org/pdf/2408.00370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Bozuo Zhao, Jingmei Wu, Yanbing Jiang, Hui Du, Zhenqing Ye, Jiayang Zhu, WeiFan Zhong, Leyao Yan, Xiaomeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00370">DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2 framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture generation is an emerging domain within virtual human creation, where current methods predominantly utilize Transformer-based architectures that necessitate extensive memory and are characterized by slow inference speeds. In response to these limitations, we propose \textit{DiM-Gestures}, a novel end-to-end generative model crafted to create highly personalized 3D full-body gestures solely from raw speech audio, employing Mamba-based architectures. This model integrates a Mamba-based fuzzy feature extractor with a non-autoregressive Adaptive Layer Normalization (AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba framework and a WavLM pre-trained model, autonomously derives implicit, continuous fuzzy features, which are then unified into a singular latent feature. This feature is processed by the AdaLN Mamba-2, which implements a uniform conditional mechanism across all tokens to robustly model the interplay between the fuzzy features and the resultant gesture sequence. This innovative approach guarantees high fidelity in gesture-speech synchronization while maintaining the naturalness of the gestures. Employing a diffusion model for training and inference, our framework has undergone extensive subjective and objective evaluations on the ZEGGS and BEAT datasets. These assessments substantiate our model's enhanced performance relative to contemporary state-of-the-art methods, demonstrating competitive outcomes with the DiTs architecture (Persona-Gestors) while optimizing memory usage and accelerating inference speed.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2405.11993.pdf' target='_blank'>https://arxiv.org/pdf/2405.11993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11993">GGAvatar: Geometric Adjustment of Gaussian Head Avatar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2402.10636.pdf' target='_blank'>https://arxiv.org/pdf/2402.10636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunsoo Cha, Byungjun Kim, Hanbyul Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10636">PEGASUS: Personalized Generative 3D Avatars with Composable Attributes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PEGASUS, a method for constructing a personalized generative 3D face avatar from monocular video sources. Our generative 3D avatar enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) while preserving the identity. Our approach consists of two stages: synthetic database generation and constructing a personalized generative avatar. We generate a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing the attributes from monocular videos of diverse identities. Then, we build a person-specific generative 3D avatar that can modify its attributes continuously while preserving its identity. Through extensive experiments, we demonstrate that our method of generating a synthetic database and creating a 3D generative avatar is the most effective in preserving identity while achieving high realism. Subsequently, we introduce a zero-shot approach to achieve the same goal of generative modeling more efficiently by leveraging a previously constructed personalized generative model.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2401.08739.pdf' target='_blank'>https://arxiv.org/pdf/2401.08739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08739">EgoGen: An Egocentric Synthetic Data Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2312.04784.pdf' target='_blank'>https://arxiv.org/pdf/2312.04784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Rao, Eduardo Perez Pellitero, Benjamin Busam, Yiren Zhou, Jifei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04784">Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D avatar generation excel with multi-view supervision for photorealistic models. However, monocular counterparts lag in quality despite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB is a fully-differentiable pipeline that learns high-fidelity 3D human avatars from just a single RGB video. A pose-conditioned deformable NeRF is optimized to volumetrically represent a human subject in canonical T-pose. The canonical representation is then leveraged to efficiently associate neural textures using 2D-3D correspondences. This enables the separation of diffused color generation and lighting correction branches that jointly compose an RGB prediction. The design allows to control intermediate results for human pose, body shape, texture, and lighting with text prompts. An image-conditioned diffusion model thereby helps to animate appearance and pose of the 3D avatar to create video sequences with previously unseen human motion. Extensive experiments show that ReCaLaB outperforms previous monocular approaches in terms of image quality for image synthesis tasks. Moreover, natural language offers an intuitive user interface for creative manipulation of 3D human avatars.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2310.16287.pdf' target='_blank'>https://arxiv.org/pdf/2310.16287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejas S. Prabhune, Peter Wu, Bohan Yu, Gopala K. Anumanchipalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16287">Towards Streaming Speech-to-Avatar Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Streaming speech-to-avatar synthesis creates real-time animations for a virtual character from audio data. Accurate avatar representations of speech are important for the visualization of sound in linguistics, phonetics, and phonology, visual feedback to assist second language acquisition, and virtual embodiment for paralyzed patients. Previous works have highlighted the capability of deep articulatory inversion to perform high-quality avatar animation using electromagnetic articulography (EMA) features. However, these models focus on offline avatar synthesis with recordings rather than real-time audio, which is necessary for live avatar visualization or embodiment. To address this issue, we propose a method using articulatory inversion for streaming high quality facial and inner-mouth avatar animation from real-time audio. Our approach achieves 130ms average streaming latency for every 0.1 seconds of audio with a 0.792 correlation with ground truth articulations. Finally, we show generated mouth and tongue animations to demonstrate the efficacy of our methodology.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2308.05995.pdf' target='_blank'>https://arxiv.org/pdf/2308.05995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05995">Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2307.14770.pdf' target='_blank'>https://arxiv.org/pdf/2307.14770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqian Wu, Hao Xu, Xiangjun Tang, Yue Shangguan, Hongbo Fu, Xiaogang Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14770">3DPortraitGAN: Learning One-Quarter Headshot 3D GANs from a Single-View Portrait Dataset with Diverse Body Poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D-aware face generators are typically trained on 2D real-life face image datasets that primarily consist of near-frontal face data, and as such, they are unable to construct one-quarter headshot 3D portraits with complete head, neck, and shoulder geometry. Two reasons account for this issue: First, existing facial recognition methods struggle with extracting facial data captured from large camera angles or back views. Second, it is challenging to learn a distribution of 3D portraits covering the one-quarter headshot region from single-view data due to significant geometric deformation caused by diverse body poses. To this end, we first create the dataset 360Â°-Portrait-HQ (360Â°PHQ for short) which consists of high-quality single-view real portraits annotated with a variety of camera parameters (the yaw angles span the entire 360Â° range) and body poses. We then propose 3DPortraitGAN, the first 3D-aware one-quarter headshot portrait generator that learns a canonical 3D avatar distribution from the 360Â°PHQ dataset with body pose self-learning. Our model can generate view-consistent portrait images from all camera angles with a canonical one-quarter headshot 3D representation. Our experiments show that the proposed framework can accurately predict portrait body poses and generate view-consistent, realistic portrait images with complete geometry from all camera angles.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2303.06537.pdf' target='_blank'>https://arxiv.org/pdf/2303.06537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungbok Shin, Sanghyun Hong, Niklas Elmqvist
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06537">Perceptual Pat: A Virtual Human System for Iterative Visualization Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing a visualization is often a process of iterative refinement where the designer improves a chart over time by adding features, improving encodings, and fixing mistakes. However, effective design requires external critique and evaluation. Unfortunately, such critique is not always available on short notice and evaluation can be costly. To address this need, we present Perceptual Pat, an extensible suite of AI and computer vision techniques that forms a virtual human visual system for supporting iterative visualization design. The system analyzes snapshots of a visualization using an extensible set of filters - including gaze maps, text recognition, color analysis, etc - and generates a report summarizing the findings. The web-based Pat Design Lab provides a version tracking system that enables the designer to track improvements over time. We validate Perceptual Pat using a longitudinal qualitative study involving 4 professional visualization designers that used the tool over a few days to design a new visualization.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2301.10047.pdf' target='_blank'>https://arxiv.org/pdf/2301.10047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10047">DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven gesture synthesis is a field of growing interest in virtual human creation. However, a critical challenge is the inherent intricate one-to-many mapping between speech and gestures. Previous studies have explored and achieved significant progress with generative models. Notwithstanding, most synthetic gestures are still vastly less natural. This paper presents DiffMotion, a novel speech-driven gesture synthesis architecture based on diffusion models. The model comprises an autoregressive temporal encoder and a denoising diffusion probability Module. The encoder extracts the temporal context of the speech input and historical gestures. The diffusion module learns a parameterized Markov chain to gradually convert a simple distribution into a complex distribution and generates the gestures according to the accompanied speech. Compared with baselines, objective and subjective evaluations confirm that our approach can produce natural and diverse gesticulation and demonstrate the benefits of diffusion-based models on speech-driven gesture synthesis.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2509.17803.pdf' target='_blank'>https://arxiv.org/pdf/2509.17803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nabila Amadou, Kazi Injamamul Haque, Zerrin Yumak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17803">Effect of Appearance and Animation Realism on the Perception of Emotionally Expressive Virtual Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Virtual Human technology is growing with several potential applications in health, education, business and telecommunications. Investigating the perception of these virtual humans can help guide to develop better and more effective applications. Recent developments show that the appearance of the virtual humans reached to a very realistic level. However, there is not yet adequate analysis on the perception of appearance and animation realism for emotionally expressive virtual humans. In this paper, we designed a user experiment and analyzed the effect of a realistic virtual human's appearance realism and animation realism in varying emotion conditions. We found that higher appearance realism and higher animation realism leads to higher social presence and higher attractiveness ratings. We also found significant effects of animation realism on perceived realism and emotion intensity levels. Our study sheds light into how appearance and animation realism effects the perception of highly realistic virtual humans in emotionally expressive scenarios and points out to future directions.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2509.10147.pdf' target='_blank'>https://arxiv.org/pdf/2509.10147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nenad Tomasev, Matija Franklin, Joel Z. Leibo, Julian Jacobs, William A. Cunningham, Iason Gabriel, Simon Osindero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10147">Virtual Agent Economies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. We propose the "sandbox economy" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). Our current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting us with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here we discuss a number of possible design choices that may lead to safely steerable AI agent markets. In particular, we consider auction mechanisms for fair resource allocation and preference resolution, the design of AI "mission economies" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, we argue for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2507.22153.pdf' target='_blank'>https://arxiv.org/pdf/2507.22153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Wilson, Vincent Bindschaedler, Sophie JÃ¶rg, Sean Sheikholeslam, Kevin Butler, Eakta Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22153">Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photorealistic 3D avatar generation has rapidly improved in recent years, and realistic avatars that match a user's true appearance are more feasible in Mixed Reality (MR) than ever before. Yet, there are known risks to sharing one's likeness online, and photorealistic MR avatars could exacerbate these risks. If user likenesses were to be shared broadly, there are risks for cyber abuse or targeted fraud based on user appearances. We propose an alternate avatar rendering scheme for broader social MR -- synthesizing realistic avatars that preserve a user's demographic identity while being distinct enough from the individual user to protect facial biometric information. We introduce a methodology for privatizing appearance by isolating identity within the feature space of identity-encoding generative models. We develop two algorithms that then obfuscate identity: \epsmethod{} provides differential privacy guarantees and \thetamethod{} provides fine-grained control for the level of identity offset. These methods are shown to successfully generate de-identified virtual avatars across multiple generative architectures in 2D and 3D. With these techniques, it is possible to protect user privacy while largely preserving attributes related to sense of self. Employing these techniques in public settings could enable the use of photorealistic avatars broadly in MR, maintaining high realism and immersion without privacy risk.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2507.09862.pdf' target='_blank'>https://arxiv.org/pdf/2507.09862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09862">SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2507.00472.pdf' target='_blank'>https://arxiv.org/pdf/2507.00472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, Xiaoming Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00472">ARIG: Autoregressive Interactive Head Generation for Real-time Conversations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2505.06131.pdf' target='_blank'>https://arxiv.org/pdf/2505.06131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Hou, Yuting Xiao, Xiangyang Xue, Taiping Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06131">ELA-ZSON: Efficient Layout-Aware Zero-Shot Object Navigation Agent with Hierarchical Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ELA-ZSON, an efficient layout-aware zero-shot object navigation (ZSON) approach designed for complex multi-room indoor environments.
  By planning hierarchically leveraging a global topologigal map with layout information and local imperative approach with detailed scene representation memory, ELA-ZSON achieves both efficient and effective navigation.
  The process is managed by an LLM-powered agent, ensuring seamless effective planning and navigation, without the need for human interaction, complex rewards, or costly training.
  Our experimental results on the MP3D benchmark achieves 85\% object navigation success rate (SR) and 79\% success rate weighted by path length (SPL) (over 40\% point improvement in SR and 60\% improvement in SPL compared to exsisting methods). Furthermore, we validate the robustness of our approach through virtual agent and real-world robotic deployment, showcasing its capability in practical scenarios. See https://anonymous.4open.science/r/ELA-ZSON-C67E/ for details.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2503.09293.pdf' target='_blank'>https://arxiv.org/pdf/2503.09293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Moreau, Mohammed Brahimi, Richard Shaw, Athanasios Papaioannou, Thomas Tanay, Zhensong Zhang, Eduardo PÃ©rez-Pellitero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09293">Better Together: Unified Motion Capture and 3D Avatar Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Better Together, a method that simultaneously solves the human pose estimation problem while reconstructing a photorealistic 3D human avatar from multi-view videos. While prior art usually solves these problems separately, we argue that joint optimization of skeletal motion with a 3D renderable body model brings synergistic effects, i.e. yields more precise motion capture and improved visual quality of real-time rendering of avatars. To achieve this, we introduce a novel animatable avatar with 3D Gaussians rigged on a personalized mesh and propose to optimize the motion sequence with time-dependent MLPs that provide accurate and temporally consistent pose estimates. We first evaluate our method on highly challenging yoga poses and demonstrate state-of-the-art accuracy on multi-view human pose estimation, reducing error by 35% on body joints and 45% on hand joints compared to keypoint-based methods. At the same time, our method significantly boosts the visual quality of animatable avatars (+2dB PSNR on novel view synthesis) on diverse challenging subjects.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2502.03069.pdf' target='_blank'>https://arxiv.org/pdf/2502.03069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Rasch, Julia TÃ¶ws, Teresa Hirzle, Florian MÃ¼ller, Martin Schmitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03069">CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI's contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2408.01826.pdf' target='_blank'>https://arxiv.org/pdf/2408.01826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Lin, Zhaoxin Fan, Xianjia Wu, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01826">GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven talking head generation is a critical yet challenging task with applications in augmented reality and virtual human modeling. While recent approaches using autoregressive and diffusion-based models have achieved notable progress, they often suffer from modality inconsistencies, particularly misalignment between audio and mesh, leading to reduced motion diversity and lip-sync accuracy. To address this, we propose GLDiTalker, a novel speech-driven 3D facial animation model based on a Graph Latent Diffusion Transformer. GLDiTalker resolves modality misalignment by diffusing signals within a quantized spatiotemporal latent space. It employs a two-stage training pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion diversity. Together, these stages enable GLDiTalker to generate realistic, temporally stable 3D facial animations. Extensive evaluations on standard benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving superior results in both lip-sync accuracy and motion diversity.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2407.04545.pdf' target='_blank'>https://arxiv.org/pdf/2407.04545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04545">Gaussian Eigen Models for Human Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current personalized neural head avatars face a trade-off: lightweight models lack detail and realism, while high-quality, animatable avatars require significant computational resources, making them unsuitable for commodity devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars. GEM utilizes 3D Gaussian primitives for representing the appearance combined with Gaussian splatting for rendering. Building on the success of mesh-based 3D morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases for representing the head appearance of a specific subject. In particular, we construct linear bases to represent the position, scale, rotation, and opacity of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives of a specific head shape by a linear combination of the basis vectors, only requiring a low-dimensional parameter vector that contains the respective coefficients. We propose to construct these linear bases (GEM) by distilling high-quality compute-intense CNN-based Gaussian avatar models that can generate expression-dependent appearance changes like wrinkles. These high-quality models are trained on multi-view videos of a subject and are distilled using a series of principal component analyses. Once we have obtained the bases that represent the animatable appearance space of a specific human, we learn a regressor that takes a single RGB image as input and predicts the low-dimensional parameter vector that corresponds to the shown facial expression. In a series of experiments, we compare GEM's self-reenactment and cross-person reenactment results to state-of-the-art 3D avatar methods, demonstrating GEM's higher visual quality and better generalization to new expressions.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2404.14745.pdf' target='_blank'>https://arxiv.org/pdf/2404.14745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runqi Wang, Caoyuan Ma, Guopeng Li, Hanrui Xu, Yuke Li, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14745">You Think, You ACT: The New Task of Arbitrary Text to Motion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text to Motion aims to generate human motions from texts. Existing settings rely on limited Action Texts that include action labels, which limits flexibility and practicability in scenarios difficult to describe directly. This paper extends limited Action Texts to arbitrary ones. Scene texts without explicit action labels can enhance the practicality of models in complex and diverse industries such as virtual human interaction, robot behavior generation, and film production, while also supporting the exploration of potential implicit behavior patterns. However, newly introduced Scene Texts may yield multiple reasonable output results, causing significant challenges in existing data, framework, and evaluation. To address this practical issue, we first create a new dataset HUMANML3D++ by extending texts of the largest existing dataset HUMANML3D. Secondly, we propose a simple yet effective framework that extracts action instructions from arbitrary texts and subsequently generates motions. Furthermore, we also benchmark this new setting with multi-solution metrics to address the inadequacies of existing single-solution metrics. Extensive experiments indicate that Text to Motion in this realistic setting is challenging, fostering new research in this practical direction.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2404.00300.pdf' target='_blank'>https://arxiv.org/pdf/2404.00300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seoyeon Bae, Yoon Kyung Lee, Jungcheol Lee, Jaeheon Kim, Haeseong Jeon, Seung-Hwan Lim, Byung-Cheol Kim, Sowon Hahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00300">Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A growth mindset has shown promising outcomes for increasing empathy ability. However, stimulating a growth mindset in VR-based empathy interventions is under-explored. In the present study, we implemented prosocial VR content, Our Neighbor Hero, focusing on embodying a virtual character to modulate players' mindsets. The virtual body served as a stepping stone, enabling players to identify with the character and cultivate a growth mindset as they followed mission instructions. We considered several implementation factors to assist players in positioning within the VR experience, including positive feedback, content difficulty, background lighting, and multimodal feedback. We conducted an experiment to investigate the intervention's effectiveness in increasing empathy. Our findings revealed that the VR content and mindset training encouraged participants to improve their growth mindsets and empathic motives. This VR content was developed for college students to enhance their empathy and teamwork skills. It has the potential to improve collaboration in organizational and community environments.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2402.06385.pdf' target='_blank'>https://arxiv.org/pdf/2402.06385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dragos Costea, Alina Marcu, Cristina Lazar, Marius Leordeanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06385">Maia: A Real-time Non-Verbal Chat for Human-AI Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling face-to-face communication in computer vision, which focuses on recognizing and analyzing nonverbal cues and behaviors during interactions, serves as the foundation for our proposed alternative to text-based Human-AI interaction. By leveraging nonverbal visual communication, through facial expressions, head and body movements, we aim to enhance engagement and capture the user's attention through a novel improvisational element, that goes beyond mirroring gestures. Our goal is to track and analyze facial expressions, and other nonverbal cues in real-time, and use this information to build models that can predict and understand human behavior. Operating in real-time and requiring minimal computational resources, our approach signifies a major leap forward in making AI interactions more natural and accessible. We offer three different complementary approaches, based on retrieval, statistical, and deep learning techniques. A key novelty of our work is the integration of an artistic component atop an efficient human-computer interaction system, using art as a medium to transmit emotions. Our approach is not art-specific and can be adapted to various paintings, animations, and avatars. In our experiments, we compare state-of-the-art diffusion models as mediums for emotion translation in 2D, and our 3D avatar, Maia, that we introduce in this work, with not just facial movements but also body motions for a more natural and engaging experience. We demonstrate the effectiveness of our approach in translating AI-generated emotions into human-relatable expressions, through both human and automatic evaluation procedures, highlighting its potential to significantly enhance the naturalness and engagement of Human-AI interactions across various applications.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2308.11261.pdf' target='_blank'>https://arxiv.org/pdf/2308.11261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadegh Aliakbarian, Fatemeh Saleh, David Collier, Pashmina Cameron, Darren Cosker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11261">HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating both plausible and accurate full body avatar motion is the key to the quality of immersive experiences in mixed reality scenarios. Head-Mounted Devices (HMDs) typically only provide a few input signals, such as head and hands 6-DoF. Recently, different approaches achieved impressive performance in generating full body motion given only head and hands signal. However, to the best of our knowledge, all existing approaches rely on full hand visibility. While this is the case when, e.g., using motion controllers, a considerable proportion of mixed reality experiences do not involve motion controllers and instead rely on egocentric hand tracking. This introduces the challenge of partial hand visibility owing to the restricted field of view of the HMD. In this paper, we propose the first unified approach, HMD-NeMo, that addresses plausible and accurate full body motion generation even when the hands may be only partially visible. HMD-NeMo is a lightweight neural network that predicts the full body motion in an online and real-time fashion. At the heart of HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask tokens that encourage plausible motion in the absence of hand observations. We perform extensive analysis of the impact of different components in HMD-NeMo and introduce a new state-of-the-art on AMASS dataset through our evaluation.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2308.10843.pdf' target='_blank'>https://arxiv.org/pdf/2308.10843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mireille Fares, Catherine Pelachaud, Nicolas Obin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10843">TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of transferring the behavior expressivity style of a virtual agent to another one while preserving behaviors shape as they carry communicative meaning. Behavior expressivity style is viewed here as the qualitative properties of behaviors. We propose TranSTYLer, a multimodal transformer based model that synthesizes the multimodal behaviors of a source speaker with the style of a target speaker. We assume that behavior expressivity style is encoded across various modalities of communication, including text, speech, body gestures, and facial expressions. The model employs a style and content disentanglement schema to ensure that the transferred style does not interfere with the meaning conveyed by the source behaviors. Our approach eliminates the need for style labels and allows the generalization to styles that have not been seen during the training phase. We train our model on the PATS corpus, which we extended to include dialog acts and 2D facial landmarks. Objective and subjective evaluations show that our model outperforms state of the art models in style transfer for both seen and unseen styles during training. To tackle the issues of style and content leakage that may arise, we propose a methodology to assess the degree to which behavior and gestures associated with the target style are successfully transferred, while ensuring the preservation of the ones related to the source content.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2307.10533.pdf' target='_blank'>https://arxiv.org/pdf/2307.10533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillermo Colin, Joseph Byrnes, Youngwoo Sim, Patrick Wensing, Joao Ramos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10533">Whole-Body Dynamic Telelocomotion: A Step-to-Step Dynamics Approach to Human Walking Reference Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teleoperated humanoid robots hold significant potential as physical avatars for humans in hazardous and inaccessible environments, with the goal of channeling human intelligence and sensorimotor skills through these robotic counterparts. Precise coordination between humans and robots is crucial for accomplishing whole-body behaviors involving locomotion and manipulation. To progress successfully, dynamic synchronization between humans and humanoid robots must be achieved. This work enhances advancements in whole-body dynamic telelocomotion, addressing challenges in robustness. By embedding the hybrid and underactuated nature of bipedal walking into a virtual human walking interface, we achieve dynamically consistent walking gait generation. Additionally, we integrate a reactive robot controller into a whole-body dynamic telelocomotion framework. Thus, allowing the realization of telelocomotion behaviors on the full-body dynamics of a bipedal robot. Real-time telelocomotion simulation experiments validate the effectiveness of our methods, demonstrating that a trained human pilot can dynamically synchronize with a simulated bipedal robot, achieving sustained locomotion, controlling walking speeds within the range of 0.0 m/s to 0.3 m/s, and enabling backward walking for distances of up to 2.0 m. This research contributes to advancing teleoperated humanoid robots and paves the way for future developments in synchronized locomotion between humans and bipedal robots.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2211.12499.pdf' target='_blank'>https://arxiv.org/pdf/2211.12499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Zielonka, Timo Bolkart, Justus Thies
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12499">Instant Volumetric Head Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Instant Volumetric Head Avatars (INSTA), a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2209.00776.pdf' target='_blank'>https://arxiv.org/pdf/2209.00776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanhang Yan, Yu Sun, Qian Bao, Jinhui Pang, Wu Liu, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00776">WOC: A Handy Webcam-based 3D Online Chatroom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop WOC, a webcam-based 3D virtual online chatroom for multi-person interaction, which captures the 3D motion of users and drives their individual 3D virtual avatars in real-time. Compared to the existing wearable equipment-based solution, WOC offers convenient and low-cost 3D motion capture with a single camera. To promote the immersive chat experience, WOC provides high-fidelity virtual avatar manipulation, which also supports the user-defined characters. With the distributed data flow service, the system delivers highly synchronized motion and voice for all users. Deployed on the website and no installation required, users can freely experience the virtual online chat at https://yanch.cloud.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2508.20623.pdf' target='_blank'>https://arxiv.org/pdf/2508.20623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Xin, Xiaolin Zhang, Yanbin Liu, Peng Zhang, Caifeng Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20623">AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2508.02376.pdf' target='_blank'>https://arxiv.org/pdf/2508.02376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matus Krajcovic, Peter Demcak, Eduard Kuric
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02376">Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied conversational agents (ECAs) are increasingly more realistic and capable of dynamic conversations. In online surveys, anthropomorphic agents could help address issues like careless responding and satisficing, which originate from the lack of personal engagement and perceived accountability. However, there is a lack of understanding of how ECAs in user experience research may affect participant engagement, satisfaction, and the quality of responses. As a proof of concept, we propose an instrument that enables the incorporation of conversations with a virtual avatar into surveys, using on AI-driven video generation, speech recognition, and Large Language Models. In our between-subjects study, 80 participants (UK, stratified random sample of general population) either talked to a voice-based agent with an animated video avatar, or interacted with a chatbot. Across surveys based on two self-reported psychometric tests, 2,265 conversation responses were obtained. Statistical comparison of results indicates that embodied agents can contribute significantly to more informative, detailed responses, as well as higher yet more time-efficient engagement. Furthermore, qualitative analysis provides valuable insights for causes of no significant change to satisfaction, linked to personal preferences, turn-taking delays and Uncanny Valley reactions. These findings support the pursuit and development of new methods toward human-like agents for the transformation of online surveys into more natural interactions resembling in-person interviews.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2506.11829.pdf' target='_blank'>https://arxiv.org/pdf/2506.11829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana MÃ¼ller, Anja Richert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11829">The Space Between Us: A Methodological Framework for Researching Bonding and Proxemics in Situated Group-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a multimethod framework for studying spatial and social dynamics in real-world group-agent interactions with socially interactive agents. Drawing on proxemics and bonding theories, the method combines subjective self-reports and objective spatial tracking. Applied in two field studies in a museum (N = 187) with a robot and a virtual agent, the paper addresses the challenges in aligning human perception and behavior. We focus on presenting an open source, scalable, and field-tested toolkit for future studies.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2506.10462.pdf' target='_blank'>https://arxiv.org/pdf/2506.10462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana MÃ¼ller, Sabina Jeschke, Anja Richert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10462">Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the impact of a group-adaptive conversation design in two socially interactive agents (SIAs) through two real-world studies. Both SIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped with a conversational artificial intelligence (CAI) backend combining hybrid retrieval and generative models. The studies were carried out in an in-the-wild setting with a total of $N = 188$ participants who interacted with the SIAs - in dyads, triads or larger groups - at a German museum. Although the results did not reveal a significant effect of the group-sensitive conversation design on perceived satisfaction, the findings provide valuable insights into the challenges of adapting CAI for multi-party interactions and across different embodiments (robot vs.\ virtual agent), highlighting the need for multimodal strategies beyond linguistic pluralization. These insights contribute to the fields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and broader Human-Machine Interaction (HMI), providing insights for future research on effective dialogue adaptation in group settings.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2505.08293.pdf' target='_blank'>https://arxiv.org/pdf/2505.08293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhuo Yin, Yuk Hang Tsui, Pan Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08293">M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2503.21886.pdf' target='_blank'>https://arxiv.org/pdf/2503.21886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pilseo Park, Ze Zhang, Michel Sarkis, Ning Bi, Xiaoming Liu, Yiying Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21886">Refined Geometry-guided Head Avatar Reconstruction from Monocular RGB Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity reconstruction of head avatars from monocular videos is highly desirable for virtual human applications, but it remains a challenge in the fields of computer graphics and computer vision. In this paper, we propose a two-phase head avatar reconstruction network that incorporates a refined 3D mesh representation. Our approach, in contrast to existing methods that rely on coarse template-based 3D representations derived from 3DMM, aims to learn a refined mesh representation suitable for a NeRF that captures complex facial nuances. In the first phase, we train 3DMM-stored NeRF with an initial mesh to utilize geometric priors and integrate observations across frames using a consistent set of latent codes. In the second phase, we leverage a novel mesh refinement procedure based on an SDF constructed from the density field of the initial NeRF. To mitigate the typical noise in the NeRF density field without compromising the features of the 3DMM, we employ Laplace smoothing on the displacement field. Subsequently, we apply a second-phase training with these refined meshes, directing the learning process of the network towards capturing intricate facial details. Our experiments demonstrate that our method further enhances the NeRF rendering based on the initial mesh and achieves performance superior to state-of-the-art methods in reconstructing high-fidelity head avatars with such input.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2501.07104.pdf' target='_blank'>https://arxiv.org/pdf/2501.07104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07104">RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures low-frequency motion and surface deformation of the avatar. Due to the limitations of LBS formula, the human skeleton is hard to control complex non-rigid transformations. We then design a pose-related Gaussian rectification module to learn fine-detailed non-rigid deformations, further improving the realism and expressiveness of the avatar. We conduct extensive experiments on public datasets, RMAvatar shows state-of-the-art performance on both rendering quality and quantitative evaluations. Please see our project page at https://rm-avatar.github.io.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2407.00229.pdf' target='_blank'>https://arxiv.org/pdf/2407.00229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirban Mukherjee, Venkat Suprabath Bitra, Vignesh Bondugula, Tarun Reddy Tallapureddy, Dinesh Babu Jayagopi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00229">SemUV: Deep Learning based semantic manipulation over UV texture map of virtual human heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing and manipulating virtual human heads is essential across various applications, including AR, VR, gaming, human-computer interaction and VFX. Traditional graphic-based approaches require manual effort and resources to achieve accurate representation of human heads. While modern deep learning techniques can generate and edit highly photorealistic images of faces, their focus remains predominantly on 2D facial images. This limitation makes them less suitable for 3D applications. Recognizing the vital role of editing within the UV texture space as a key component in the 3D graphics pipeline, our work focuses on this aspect to benefit graphic designers by providing enhanced control and precision in appearance manipulation. Research on existing methods within the UV texture space is limited, complex, and poses challenges. In this paper, we introduce SemUV: a simple and effective approach using the FFHQ-UV dataset for semantic manipulation directly within the UV texture space. We train a StyleGAN model on the publicly available FFHQ-UV dataset, and subsequently train a boundary for interpolation and semantic feature manipulation. Through experiments comparing our method with 2D manipulation technique, we demonstrate its superior ability to preserve identity while effectively modifying semantic features such as age, gender, and facial hair. Our approach is simple, agnostic to other 3D components such as structure, lighting, and rendering, and also enables seamless integration into standard 3D graphics pipelines without demanding extensive domain expertise, time, or resources.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2406.16478.pdf' target='_blank'>https://arxiv.org/pdf/2406.16478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucie Galland, Catherine Pelachaud, Florian Pecune
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16478">EMMI -- Empathic Multimodal Motivational Interviews Dataset: Analyses and Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of multimodal interaction in therapy can yield a comprehensive understanding of therapist and patient behavior that can be used to develop a multimodal virtual agent supporting therapy. This investigation aims to uncover how therapists skillfully blend therapy's task goal (employing classical steps of Motivational Interviewing) with the social goal (building a trusting relationship and expressing empathy). Furthermore, we seek to categorize patients into various ``types'' requiring tailored therapeutic approaches. To this intent, we present multimodal annotations of a corpus consisting of simulated motivational interviewing conversations, wherein actors portray the roles of patients and therapists. We introduce EMMI, composed of two publicly available MI corpora, AnnoMI and the Motivational Interviewing Dataset, for which we add multimodal annotations. We analyze these annotations to characterize functional behavior for developing a virtual agent performing motivational interviews emphasizing social and empathic behaviors. Our analysis found three clusters of patients expressing significant differences in behavior and adaptation of the therapist's behavior to those types. This shows the importance of a therapist being able to adapt their behavior depending on the current situation within the dialog and the type of user.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2403.07314.pdf' target='_blank'>https://arxiv.org/pdf/2403.07314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megan A. Witherow, Crystal Butler, Winston J. Shields, Furkan Ilgin, Norou Diawara, Janice Keener, John W. Harrington, Khan M. Iftekharuddin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07314">Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more. However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert. To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss encourages feature correlation with AUs while discouraging correlation with subject identities for improved generalization. We train BeCoME-Net for unilateral and bilateral AU detection and compare with state-of-the-art approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty healthy adult volunteers complete expression recognition and mimicry tasks in an online feasibility study while webcam-based eye-tracking and video are collected. We test validity of multiple constructs, including face preference during recognition and AUs during mimicry.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2402.05448.pdf' target='_blank'>https://arxiv.org/pdf/2402.05448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bumsoo Kim, Sanghyun Byun, Yonghoon Jung, Wonseop Shin, Sareer UI Amin, Sanghyun Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05448">Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2309.10902.pdf' target='_blank'>https://arxiv.org/pdf/2309.10902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiffany D. Do, Steve Zelenty, Mar Gonzalez-Franco, Ryan P. McMahan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10902">VALID: A perceptually validated Virtual Avatar Library for Inclusion and Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As consumer adoption of immersive technologies grows, virtual avatars will play a prominent role in the future of social computing. However, as people begin to interact more frequently through virtual avatars, it is important to ensure that the research community has validated tools to evaluate the effects and consequences of such technologies. We present the first iteration of a new, freely available 3D avatar library called the Virtual Avatar Library for Inclusion and Diversity (VALID), which includes 210 fully rigged avatars with a focus on advancing racial diversity and inclusion. We present a detailed process for creating, iterating, and validating avatars of diversity. Through a large online study (n=132) with participants from 33 countries, we provide statistically validated labels for each avatar's perceived race and gender. Through our validation study, we also advance knowledge pertaining to the perception of an avatar's race. In particular, we found that avatars of some races were more accurately identified by participants of the same race.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2308.13551.pdf' target='_blank'>https://arxiv.org/pdf/2308.13551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyue Yao, Mingjie Sun, Bingliang Li, Fengyu Yang, Junle Wang, Ruimao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13551">Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as references for motion generation. Then, we introduce a hyper-parameter that coordinates the similarity between dancers by masking poses to prevent the generation of sequences that are over-diverse or consistent. To avoid the rigidity of movements, we design a Dance Pre-generated stage to pre-generate these masked poses instead of filling them with zeros. After that, a Dance Motion Transfer stage is adopted with leader sequences and music, in which a multi-conditional sampling formula is rewritten to transfer the pre-generated poses into a sequence with a partner style. In practice, to address the lack of multi-person datasets, we introduce AIST-M, a new dataset for partner dancer generation, which is publicly availiable. Comprehensive evaluations on our AIST-M dataset demonstrate that the proposed DanY can synthesize satisfactory partner dancer results with controllable diversity.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2308.03610.pdf' target='_blank'>https://arxiv.org/pdf/2308.03610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, Min Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03610">AvatarVerse: High-quality & Stable 3D Avatar Creation from Text and Pose</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating expressive, diverse and high-quality 3D avatars from highly customized text descriptions and pose guidance is a challenging task, due to the intricacy of modeling and texturing in 3D that ensure details and various styles (realistic, fictional, etc). We present AvatarVerse, a stable pipeline for generating expressive high-quality 3D avatars from nothing but text descriptions and pose guidance. In specific, we introduce a 2D diffusion model conditioned on DensePose signal to establish 3D pose control of avatars through 2D images, which enhances view consistency from partially observed scenarios. It addresses the infamous Janus Problem and significantly stablizes the generation process. Moreover, we propose a progressive high-resolution 3D synthesis strategy, which obtains substantial improvement over the quality of the created 3D avatars. To this end, the proposed AvatarVerse pipeline achieves zero-shot 3D modeling of 3D avatars that are not only more expressive, but also in higher quality and fidelity than previous works. Rigorous qualitative evaluations and user studies showcase AvatarVerse's superiority in synthesizing high-fidelity 3D avatars, leading to a new standard in high-quality and stable 3D avatar creation. Our project page is: https://avatarverse3d.github.io
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2307.05501.pdf' target='_blank'>https://arxiv.org/pdf/2307.05501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruslan Isaev, Radmir Gumerov, Gulzada Esenalieva, Remudin Reshid Mekuria, Ermek Doszhanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05501">HIVA: Holographic Intellectual Voice Assistant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Holographic Intellectual Voice Assistant (HIVA) aims to facilitate human computer interaction using audiovisual effects and 3D avatar. HIVA provides complete information about the university, including requests of various nature: admission, study issues, fees, departments, university structure and history, canteen, human resources, library, student life and events, information about the country and the city, etc. There are other ways for receiving the data listed above: the university's official website and other supporting apps, HEI (Higher Education Institution) official social media, directly asking the HEI staff, and other channels. However, HIVA provides the unique experience of "face-to-face" interaction with an animated 3D mascot, helping to get a sense of 'real-life' communication. The system includes many sub-modules and connects a family of applications such as mobile applications, Telegram chatbot, suggestion categorization, and entertainment services. The Voice assistant uses Russian language NLP models and tools, which are pipelined for the best user experience.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2303.11463.pdf' target='_blank'>https://arxiv.org/pdf/2303.11463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio MartÃ­n Serrano, RubÃ©n Izquierdo, IvÃ¡n GarcÃ­a Daza, Miguel Ãngel Sotelo, David FernÃ¡ndez Llorca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11463">Digital twin in virtual reality for human-vehicle interactions in the context of autonomous driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the results of tests of interactions between real humans and simulated vehicles in a virtual scenario. Human activity is inserted into the virtual world via a virtual reality interface for pedestrians. The autonomous vehicle is equipped with a virtual Human-Machine interface (HMI) and drives through the digital twin of a real crosswalk. The HMI was combined with gentle and aggressive braking maneuvers when the pedestrian intended to cross. The results of the interactions were obtained through questionnaires and measurable variables such as the distance to the vehicle when the pedestrian initiated the crossing action. The questionnaires show that pedestrians feel safer whenever HMI is activated and that varying the braking maneuver does not influence their perception of danger as much, while the measurable variables show that both HMI activation and the gentle braking maneuver cause the pedestrian to cross earlier.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2211.11903.pdf' target='_blank'>https://arxiv.org/pdf/2211.11903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Purva Tendulkar, DÃ­dac SurÃ­s, Carl Vondrick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11903">FLEX: Full-Body Grasping Without Full-Body Grasps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in AR/VR, video games and robotics. Towards this goal, we address the task of generating a virtual human -- hands and full body -- grasping everyday objects. Existing methods approach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object positions and orientations, or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasping data. Our key insight is to leverage the existence of both full-body pose and hand grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are superior to baselines both quantitatively and qualitatively. See our webpage for more details: https://flex.cs.columbia.edu/.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2508.19754.pdf' target='_blank'>https://arxiv.org/pdf/2508.19754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wu, Yufan Wu, Wen Li, Yuxi Lu, Kairui Feng, Xuanhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19754">FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in 3D avatar reconstruction, it still faces challenges such as high time complexity, sensitivity to data quality, and low data utilization. We propose FastAvatar, a feedforward 3D avatar framework capable of flexibly leveraging diverse daily recordings (e.g., a single image, multi-view observations, or monocular video) to reconstruct a high-quality 3D Gaussian Splatting (3DGS) model within seconds, using only a single unified model. FastAvatar's core is a Large Gaussian Reconstruction Transformer featuring three key designs: First, a variant VGGT-style transformer architecture aggregating multi-frame cues while injecting initial 3D prompt to predict an aggregatable canonical 3DGS representation; Second, multi-granular guidance encoding (camera pose, FLAME expression, head pose) mitigating animation-induced misalignment for variable-length inputs; Third, incremental Gaussian aggregation via landmark tracking and sliced fusion losses. Integrating these features, FastAvatar enables incremental reconstruction, i.e., improving quality with more observations, unlike prior work wasting input data. This yields a quality-speed-tunable paradigm for highly usable avatar modeling. Extensive experiments show that FastAvatar has higher quality and highly competitive speed compared to existing methods.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2508.09402.pdf' target='_blank'>https://arxiv.org/pdf/2508.09402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Von Ralph Dane Marquez Herbuela, Yukie Nagai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09402">Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many individuals especially those with autism spectrum disorder (ASD), alexithymia, or other neurodivergent profiles face challenges in recognizing, expressing, or interpreting emotions. To support more inclusive and personalized emotion technologies, we present a real-time multimodal emotion estimation system that combines neurophysiological EEG, ECG, blood volume pulse (BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial expressions, and speech) in a unified arousal-valence 2D interface to track moment-to-moment emotional states. This architecture enables interpretable, user-specific analysis and supports applications in emotion education, neuroadaptive feedback, and interaction support for neurodiverse users. Two demonstration scenarios illustrate its application: (1) passive media viewing (2D or VR videos) reveals cortical and autonomic responses to affective content, and (2) semi-scripted conversations with a facilitator or virtual agent capture real-time facial and vocal expressions. These tasks enable controlled and naturalistic emotion monitoring, making the system well-suited for personalized feedback and neurodiversity-informed interaction design.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2508.08930.pdf' target='_blank'>https://arxiv.org/pdf/2508.08930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juyeong Hwang, Seong-Eun Hon, JaeYoung Seon, Hyeongyeop Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08930">How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive Reasoning for Embodied Head Rotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural head rotation is critical for believable embodied virtual agents, yet this micro-level behavior remains largely underexplored. While head-rotation prediction algorithms could, in principle, reproduce this behavior, they typically focus on visually salient stimuli and overlook the cognitive motives that guide head rotation. This yields agents that look at conspicuous objects while overlooking obstacles or task-relevant cues, diminishing realism in a virtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning framework for Embodied Head Rotation, a data-agnostic framework that produces context-aware head movements without task-specific training or hand-tuned heuristics. A controlled VR study (N=20) identifies five motivational drivers of human head movements: Interest, Information Seeking, Safety, Social Schema, and Habit. SCORE encodes these drivers as symbolic predicates, perceives the scene with a Vision-Language Model (VLM), and plans head poses with a Large Language Model (LLM). The framework employs a hybrid workflow: the VLM-LLM reasoning is executed offline, after which a lightweight FastVLM performs online validation to suppress hallucinations while maintaining responsiveness to scene dynamics. The result is an agent that predicts not only where to look but also why, generalizing to unseen scenes and multi-agent crowds while retaining behavioral plausibility.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2507.17327.pdf' target='_blank'>https://arxiv.org/pdf/2507.17327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao He, Jianqiang Ren, Jianjing Xiang, Xiejie Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17327">CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of large foundation models, AIGC, cloud rendering, and real-time motion capture technologies, digital humans are now capable of achieving synchronized facial expressions and body movements, engaging in intelligent dialogues driven by natural language, and enabling the fast creation of personalized avatars. While current mainstream approaches to digital humans primarily focus on 3D models and 2D video-based representations, interactive 2D cartoon-style digital humans have received relatively less attention. Compared to 3D digital humans that require complex modeling and high rendering costs, and 2D video-based solutions that lack flexibility and real-time interactivity, 2D cartoon-style Live2D models offer a more efficient and expressive alternative. By simulating 3D-like motion through layered segmentation without the need for traditional 3D modeling, Live2D enables dynamic and real-time manipulation. In this technical report, we present CartoonAlive, an innovative method for generating high-quality Live2D digital humans from a single input portrait image. CartoonAlive leverages the shape basis concept commonly used in 3D face modeling to construct facial blendshapes suitable for Live2D. It then infers the corresponding blendshape weights based on facial keypoints detected from the input image. This approach allows for the rapid generation of a highly expressive and visually accurate Live2D model that closely resembles the input portrait, within less than half a minute. Our work provides a practical and scalable solution for creating interactive 2D cartoon characters, opening new possibilities in digital content creation and virtual character animation. The project homepage is https://human3daigc.github.io/CartoonAlive_webpage/.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2507.16542.pdf' target='_blank'>https://arxiv.org/pdf/2507.16542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiong Wu, Yan Dong, Zipeng Zhang, Ruochen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16542">The Effect of Scale Consistency between Real and Virtual Spaces on Immersion in Exhibition Hybrid Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In exhibition hybrid spaces, scale consistency between real and virtual spaces is crucial for user immersion. However, there is currently a lack of systematic research to determine appropriate virtual-to-real mapping ratios. This study developed an immersive interaction system based on Intel 3D Athlete Tracking body mapping technology. Two experiments investigated the impact of virtual space and virtual avatar scale on immersion. Experiment 1 investigated 30 participants' preferences for virtual space scale, while Experiment 2 tested the effect of 6 different virtual avatar sizes (25%-150%) on immersion. A 5-point Likert scale was used to assess immersion, followed by analysis of variance and Tukey HSD post-hoc tests. Experiment 1 showed that participants preferred a virtual space ratio of 130% (mean 127.29%, SD 8.55%). Experiment 2 found that virtual avatar sizes within the 75%-100% range produced optimal immersion (p < 0.05). Immersion decreased significantly when virtual avatar sizes deviated from users' actual height (below 50% or above 125%). Participants were more sensitive to size changes in the 25%-75% range, while perception was weaker for changes in the 75%-100% range. Virtual environments slightly larger than real space (130%) and virtual avatars slightly smaller than users (75%-100%) optimize user immersion. These findings have been applied in the Intel Global Trade Center exhibition hall, demonstrating actionable insights for designing hybrid spaces that enhance immersion and coherence.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2507.10469.pdf' target='_blank'>https://arxiv.org/pdf/2507.10469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikko Korkiakoski, Saeid Sheikhi, Jesper Nyman, Jussi Saariniemi, Kalle Tapio, Panos Kostakos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10469">An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in artificial intelligence (AI) have significantly enhanced the realism and interactivity of non-player characters (NPCs) in virtual reality (VR), creating more engaging and believable user experiences. This paper evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their perceived realism, usability, and system performance. The simulator features two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage participants in a scenario to determine the suspect's guilt or innocence. A user study with 18 participants assessed the system using the System Usability Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent Believability Questionnaire, alongside latency measurements for speech-to-text (STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency. Results showed an average cycle latency of 7 seconds, influenced by the increasing conversational context. Believability scored 6.67 out of 10, with high ratings in behavior, social relationships, and intelligence but moderate scores in emotion and personality. The system achieved a SUS score of 79.44, indicating good usability. These findings demonstrate the potential of large language models to improve NPC realism and interaction in VR while highlighting challenges in reducing system latency and enhancing emotional depth. This research contributes to the development of more sophisticated AI-driven NPCs, revealing the need for performance optimization to achieve increasingly immersive virtual experiences.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2506.05806.pdf' target='_blank'>https://arxiv.org/pdf/2506.05806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Yu, Zhaonian Wang, Yihan Pan, Meng Cheng, Hao Yang, Chao Wang, Tao Xie, Xiaoming Xu, Xiaoming Wei, Xunliang Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05806">LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based models have gained wide adoption in the virtual human generation due to their outstanding expressiveness. However, their substantial computational requirements have constrained their deployment in real-time interactive avatar applications, where stringent speed, latency, and duration requirements are paramount. We present a novel audio-driven portrait video generation framework based on the diffusion model to address these challenges. Firstly, we propose robust variable-length video generation to reduce the minimum time required to generate the initial video clip or state transitions, which significantly enhances the user experience. Secondly, we propose a consistency model training strategy for Audio-Image-to-Video to ensure real-time performance, enabling a fast few-step generation. Model quantization and pipeline parallelism are further employed to accelerate the inference speed. To mitigate the stability loss incurred by the diffusion process and model quantization, we introduce a new inference strategy tailored for long-duration video generation. These methods ensure real-time performance and low latency while maintaining high-fidelity output. Thirdly, we incorporate class labels as a conditional input to seamlessly switch between speaking, listening, and idle states. Lastly, we design a novel mechanism for fine-grained facial expression control to exploit our model's inherent capacity. Extensive experiments demonstrate that our approach achieves low-latency, fluid, and authentic two-way communication. On an NVIDIA RTX 4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45 FPS at a resolution of 512x512, with an initial video generation latency of 140 ms and 215 ms, respectively.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2505.23301.pdf' target='_blank'>https://arxiv.org/pdf/2505.23301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rim Rekik, Stefanie Wuhrer, Ludovic Hoyet, Katja Zibrek, Anne-HÃ©lÃ¨ne Olivier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23301">Quality assessment of 3D human animation: Subjective and objective evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual human animations have a wide range of applications in virtual and augmented reality. While automatic generation methods of animated virtual humans have been developed, assessing their quality remains challenging. Recently, approaches introducing task-oriented evaluation metrics have been proposed, leveraging neural network training. However, quality assessment measures for animated virtual humans that are not generated with parametric body models have yet to be developed. In this context, we introduce a first such quality assessment measure leveraging a novel data-driven framework. First, we generate a dataset of virtual human animations together with their corresponding subjective realism evaluation scores collected with a user study. Second, we use the resulting dataset to learn predicting perceptual evaluation scores. Results indicate that training a linear regressor on our dataset results in a correlation of 90%, which outperforms a state of the art deep learning baseline.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2505.04387.pdf' target='_blank'>https://arxiv.org/pdf/2505.04387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amin Fadaeinejad, Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amaury Depierre, Nikolaus F. Troje, Marcus A. Brubaker, Marc-AndrÃ© Carbonneau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04387">Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating realistic 3D head assets for virtual characters that match a precise artistic vision remains labor-intensive. We present a novel framework that streamlines this process by providing artists with intuitive control over generated 3D heads. Our approach uses a geometry-aware texture synthesis pipeline that learns correlations between head geometry and skin texture maps across different demographics. The framework offers three levels of artistic control: manipulation of overall head geometry, adjustment of skin tone while preserving facial characteristics, and fine-grained editing of details such as wrinkles or facial hair. Our pipeline allows artists to make edits to a single texture map using familiar tools, with our system automatically propagating these changes coherently across the remaining texture maps needed for realistic rendering. Experiments demonstrate that our method produces diverse results with clean geometries. We showcase practical applications focusing on intuitive control for artists, including skin tone adjustments and simplified editing workflows for adding age-related details or removing unwanted features from scanned models. This integrated approach aims to streamline the artistic workflow in virtual character creation.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2505.02694.pdf' target='_blank'>https://arxiv.org/pdf/2505.02694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurtis Haut, Masum Hasan, Thomas Carroll, Ronald Epstein, Taylan Sen, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02694">AI Standardized Patient Improves Human Conversations in Advanced Cancer Care</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Serious illness communication (SIC) in end-of-life care faces challenges such as emotional stress, cultural barriers, and balancing hope with honesty. Despite its importance, one of the few available ways for clinicians to practice SIC is with standardized patients, which is expensive, time-consuming, and inflexible. In this paper, we present SOPHIE, an AI-powered standardized patient simulation and automated feedback system. SOPHIE combines large language models (LLMs), a lifelike virtual avatar, and automated, personalized feedback based on clinical literature to provide remote, on-demand SIC training. In a randomized control study with healthcare students and professionals, SOPHIE users demonstrated significant improvement across three critical SIC domains: Empathize, Be Explicit, and Empower. These results suggest that AI-driven tools can enhance complex interpersonal communication skills, offering scalable, accessible solutions to address a critical gap in clinician education.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2504.09018.pdf' target='_blank'>https://arxiv.org/pdf/2504.09018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Yin, Chenxinran Shen, Robert Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09018">Entertainers Between Real and Virtual -- Investigating Viewer Interaction, Engagement, and Relationships with Avatarized Virtual Livestreamers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual YouTubers (VTubers) are avatar-based livestreamers that are voiced and played by human actors. VTubers have been popular in East Asia for years and have more recently seen widespread international growth. Despite their emergent popularity, research has been scarce into the interactions and relationships that exist between avatarized VTubers and their viewers, particularly in contrast to non-avatarized streamers. To address this gap, we performed in-depth interviews with self-reported VTuber viewers (n=21). Our findings first reveal that the avatarized nature of VTubers fosters new forms of theatrical engagement, as factors of the virtual blend with the real to create a mixture of fantasy and realism in possible livestream interactions. Avatarization furthermore results in a unique audience perception regarding the identity of VTubers - an identity which comprises a dynamic, distinct mix of the real human (the voice actor/actress) and the virtual character. Our findings suggest that each of these dual identities both individually and symbiotically affect viewer interactions and relationships with VTubers. Whereas the performer's identity mediates social factors such as intimacy, relatability, and authenticity, the virtual character's identity offers feelings of escapism, novelty in interactions, and a sense of continuity beyond the livestream. We situate our findings within existing livestreaming literature to highlight how avatarization drives unique, character-based interactions as well as reshapes the motivations and relationships that viewers form with livestreamers. Finally, we provide suggestions and recommendations for areas of future exploration to address the challenges involved in present livestreamed avatarized entertainment.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2503.17306.pdf' target='_blank'>https://arxiv.org/pdf/2503.17306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meisam Jamshidi Seikavandi, Jostein Fimland, Maria Jung Barrett, Paolo Burelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17306">Exploring the Temporal Dynamics of Facial Mimicry in Emotion Processing Using Action Units</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial mimicry - the automatic, unconscious imitation of others' expressions - is vital for emotional understanding. This study investigates how mimicry differs across emotions using Face Action Units from videos and participants' responses. Dynamic Time Warping quantified the temporal alignment between participants' and stimuli's facial expressions, revealing significant emotional variations. Post-hoc tests indicated greater mimicry for 'Fear' than 'Happy' and reduced mimicry for 'Anger' compared to 'Fear'. The mimicry correlations with personality traits like Extraversion and Agreeableness were significant, showcasing subtle yet meaningful connections. These findings suggest specific emotions evoke stronger mimicry, with personality traits playing a secondary role in emotional alignment. Notably, our results highlight how personality-linked mimicry mechanisms extend beyond interpersonal communication to affective computing applications, such as remote human-human interactions and human-virtual-agent scenarios. Insights from temporal facial mimicry - e.g., designing digital agents that adaptively mirror user expressions - enable developers to create empathetic, personalized systems, enhancing emotional resonance and user engagement.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2502.07030.pdf' target='_blank'>https://arxiv.org/pdf/2502.07030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07030">PrismAvatar: Real-time animated 3D neural head avatars on edge devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PrismAvatar: a 3D head avatar model which is designed specifically to enable real-time animation and rendering on resource-constrained edge devices, while still enjoying the benefits of neural volumetric rendering at training time. By integrating a rigged prism lattice with a 3D morphable head model, we use a hybrid rendering model to simultaneously reconstruct a mesh-based head and a deformable NeRF model for regions not represented by the 3DMM. We then distill the deformable NeRF into a rigged mesh and neural textures, which can be animated and rendered efficiently within the constraints of the traditional triangle rendering pipeline. In addition to running at 60 fps with low memory usage on mobile devices, we find that our trained models have comparable quality to state-of-the-art 3D avatar models on desktop devices.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2501.16870.pdf' target='_blank'>https://arxiv.org/pdf/2501.16870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Josep Lopez Camunas, Cristina Bustos, Yanjun Zhu, Raquel Ros, Agata Lapedriza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16870">Experimenting with Affective Computing Models in Video Interviews with Spanish-speaking Older Adults</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding emotional signals in older adults is crucial for designing virtual assistants that support their well-being. However, existing affective computing models often face significant limitations: (1) limited availability of datasets representing older adults, especially in non-English-speaking populations, and (2) poor generalization of models trained on younger or homogeneous demographics. To address these gaps, this study evaluates state-of-the-art affective computing models -- including facial expression recognition, text sentiment analysis, and smile detection -- using videos of older adults interacting with either a person or a virtual avatar. As part of this effort, we introduce a novel dataset featuring Spanish-speaking older adults engaged in human-to-human video interviews. Through three comprehensive analyses, we investigate (1) the alignment between human-annotated labels and automatic model outputs, (2) the relationships between model outputs across different modalities, and (3) individual variations in emotional signals. Using both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover limited agreement between human annotations and model predictions, weak consistency across modalities, and significant variability among individuals. These findings highlight the shortcomings of generalized emotion perception models and emphasize the need of incorporating personal variability and cultural nuances into future systems.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2501.05755.pdf' target='_blank'>https://arxiv.org/pdf/2501.05755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madhurananda Pahar, Fuxiang Tao, Bahman Mirheidari, Nathan Pevy, Rebecca Bright, Swapnil Gadgil, Lise Sproson, Dorota Braun, Caitlin Illingworth, Daniel Blackburn, Heidi Christensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05755">CognoSpeak: an automatic, remote assessment of early cognitive decline in real-world conversational speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The early signs of cognitive decline are often noticeable in conversational speech, and identifying those signs is crucial in dealing with later and more serious stages of neurodegenerative diseases. Clinical detection is costly and time-consuming and although there has been recent progress in the automatic detection of speech-based cues, those systems are trained on relatively small databases, lacking detailed metadata and demographic information. This paper presents CognoSpeak and its associated data collection efforts. CognoSpeak asks memory-probing long and short-term questions and administers standard cognitive tasks such as verbal and semantic fluency and picture description using a virtual agent on a mobile or web platform. In addition, it collects multimodal data such as audio and video along with a rich set of metadata from primary and secondary care, memory clinics and remote settings like people's homes. Here, we present results from 126 subjects whose audio was manually transcribed. Several classic classifiers, as well as large language model-based classifiers, have been investigated and evaluated across the different types of prompts. We demonstrate a high level of performance; in particular, we achieved an F1-score of 0.873 using a DistilBERT model to discriminate people with cognitive impairment (dementia and people with mild cognitive impairment (MCI)) from healthy volunteers using the memory responses, fluency tasks and cookie theft picture description. CognoSpeak is an automatic, remote, low-cost, repeatable, non-invasive and less stressful alternative to existing clinical cognitive assessments.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2412.12061.pdf' target='_blank'>https://arxiv.org/pdf/2412.12061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farnaz Nouraei, Keith Rebello, Mina Fallah, Prasanth Murali, Haley Matuszak, Valerie Jap, Andrea Parker, Michael Paasche-Orlow, Timothy Bickmore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12061">Virtual Agent-Based Communication Skills Training to Facilitate Health Persuasion Among Peers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many laypeople are motivated to improve the health behavior of their family or friends but do not know where to start, especially if the health behavior is potentially stigmatizing or controversial. We present an approach that uses virtual agents to coach community-based volunteers in health counseling techniques, such as motivational interviewing, and allows them to practice these skills in role-playing scenarios. We use this approach in a virtual agent-based system to increase COVID-19 vaccination by empowering users to influence their social network. In a between-subjects comparative design study, we test the effects of agent system interactivity and role-playing functionality on counseling outcomes, with participants evaluated by standardized patients and objective judges. We find that all versions are effective at producing peer counselors who score adequately on a standardized measure of counseling competence, and that participants were significantly more satisfied with interactive virtual agents compared to passive viewing of the training material. We discuss design implications for interpersonal skills training systems based on our findings.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2411.11102.pdf' target='_blank'>https://arxiv.org/pdf/2411.11102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Annalena Bea Aicher, Yuki Matsuda, Keichii Yasumoto, Wolfgang Minker, Elisabeth AndrÃ©, Stefan Ultes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11102">Exploring the Impact of Non-Verbal Virtual Agent Behavior on User Engagement in Argumentative Dialogues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Engaging in discussions that involve diverse perspectives and exchanging arguments on a controversial issue is a natural way for humans to form opinions. In this process, the way arguments are presented plays a crucial role in determining how engaged users are, whether the interaction takes place solely among humans or within human-agent teams. This is of great importance as user engagement plays a crucial role in determining the success or failure of cooperative argumentative discussions. One main goal is to maintain the user's motivation to participate in a reflective opinion-building process, even when addressing contradicting viewpoints. This work investigates how non-verbal agent behavior, specifically co-speech gestures, influences the user's engagement and interest during an ongoing argumentative interaction. The results of a laboratory study conducted with 56 participants demonstrate that the agent's co-speech gestures have a substantial impact on user engagement and interest and the overall perception of the system. Therefore, this research offers valuable insights for the design of future cooperative argumentative virtual agents.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2410.17262.pdf' target='_blank'>https://arxiv.org/pdf/2410.17262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqing Wang, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17262">EmoGene: Audio-Driven Emotional 3D Talking-Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking-head generation is a crucial and useful technology for virtual human interaction and film-making. While recent advances have focused on improving image fidelity and lip synchronization, generating accurate emotional expressions remains underexplored. In this paper, we introduce EmoGene, a novel framework for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Our approach employs a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks, which are concatenated with emotional embedding in a motion-to-emotion module to produce emotional landmarks. These landmarks drive a Neural Radiance Fields (NeRF)-based emotion-to-video module to render realistic emotional talking-head videos. Additionally, we propose a pose sampling method to generate natural idle-state (non-speaking) videos for silent audio inputs. Extensive experiments demonstrate that EmoGene outperforms previous methods in generating high-fidelity emotional talking-head videos.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2408.15762.pdf' target='_blank'>https://arxiv.org/pdf/2408.15762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Fonseca Silva, Paulo Ricardo Knob, Rubens Halbig Montanha, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15762">Evaluating and Comparing Crowd Simulations: Perspectives from a Crowd Authoring Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd simulation is a research area widely used in diverse fields, including gaming and security, assessing virtual agent movements through metrics like time to reach their goals, speed, trajectories, and densities. This is relevant for security applications, for instance, as different crowd configurations can determine the time people spend in environments trying to evacuate them. In this work, we extend WebCrowds, an authoring tool for crowd simulation, to allow users to build scenarios and evaluate them through a set of metrics. The aim is to provide a quantitative metric that can, based on simulation data, select the best crowd configuration in a certain environment. We conduct experiments to validate our proposed metric in multiple crowd simulation scenarios and perform a comparison with another metric found in the literature. The results show that experts in the domain of crowd scenarios agree with our proposed quantitative metric.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2407.08095.pdf' target='_blank'>https://arxiv.org/pdf/2407.08095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Steenstra, Farnaz Nouraei, Mehdi Arjmand, Timothy W. Bickmore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08095">Virtual Agents for Alcohol Use Counseling: Exploring LLM-Powered Motivational Interviewing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel application of large language models (LLMs) in developing a virtual counselor capable of conducting motivational interviewing (MI) for alcohol use counseling. Access to effective counseling remains limited, particularly for substance abuse, and virtual agents offer a promising solution by leveraging LLM capabilities to simulate nuanced communication techniques inherent in MI. Our approach combines prompt engineering and integration into a user-friendly virtual platform to facilitate realistic, empathetic interactions. We evaluate the effectiveness of our virtual agent through a series of studies focusing on replicating MI techniques and human counselor dialog. Initial findings suggest that our LLM-powered virtual agent matches human counselors' empathetic and adaptive conversational skills, presenting a significant step forward in virtual health counseling and providing insights into the design and implementation of LLM-based therapeutic interactions.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2406.16815.pdf' target='_blank'>https://arxiv.org/pdf/2406.16815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Liu, Junshu Tang, Chu Zheng, Shijie Zhang, Jinkun Hao, Junwei Zhu, Dongjin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16815">ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity 3D garment synthesis from text is desirable yet challenging for digital avatar creation. Recent diffusion-based approaches via Score Distillation Sampling (SDS) have enabled new possibilities but either intricately couple with human body or struggle to reuse. We introduce ClotheDreamer, a 3D Gaussian-based method for generating wearable, production-ready 3D garment assets from text prompts. We propose a novel representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate optimization. DCGS represents clothed avatar as one Gaussian model but freezes body Gaussian splats. To enhance quality and completeness, we incorporate bidirectional SDS to supervise clothed avatar and garment RGBD renderings respectively with pose conditions and propose a new pruning strategy for loose clothing. Our approach can also support custom clothing templates as input. Benefiting from our design, the synthetic 3D garment can be easily applied to virtual try-on and support physically accurate animation. Extensive experiments showcase our method's superior and competitive performance. Our project page is at https://ggxxii.github.io/clothedreamer.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2404.07991.pdf' target='_blank'>https://arxiv.org/pdf/2404.07991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07991">GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2403.08363.pdf' target='_blank'>https://arxiv.org/pdf/2403.08363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthikeya Puttur Venkatraj, Wo Meijer, Monica PerusquÃ­a-HernÃ¡ndez, Gijs Huisman, Abdallah El Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08363">ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2401.11078.pdf' target='_blank'>https://arxiv.org/pdf/2401.11078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11078">UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2312.13091.pdf' target='_blank'>https://arxiv.org/pdf/2312.13091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andre Carbonneau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13091">MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrinsic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: https://ubisoft-laforge.github.io/character/mosar/
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2309.09314.pdf' target='_blank'>https://arxiv.org/pdf/2309.09314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Taeil Jin, Sung-Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09314">MOVIN: Real-time Motion Capture using a Single LiDAR</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in technology have brought forth new forms of interactive applications, such as the social metaverse, where end users interact with each other through their virtual avatars. In such applications, precise full-body tracking is essential for an immersive experience and a sense of embodiment with the virtual avatar. However, current motion capture systems are not easily accessible to end users due to their high cost, the requirement for special skills to operate them, or the discomfort associated with wearable devices. In this paper, we present MOVIN, the data-driven generative method for real-time motion capture with global tracking, using a single LiDAR sensor. Our autoregressive conditional variational autoencoder (CVAE) model learns the distribution of pose variations conditioned on the given 3D point cloud from LiDAR.As a central factor for high-accuracy motion capture, we propose a novel feature encoder to learn the correlation between the historical 3D point cloud data and global, local pose features, resulting in effective learning of the pose prior. Global pose features include root translation, rotation, and foot contacts, while local features comprise joint positions and rotations. Subsequently, a pose generator takes into account the sampled latent variable along with the features from the previous frame to generate a plausible current pose. Our framework accurately predicts the performer's 3D global information and local joint details while effectively considering temporally coherent movements across frames. We demonstrate the effectiveness of our architecture through quantitative and qualitative evaluations, comparing it against state-of-the-art methods. Additionally, we implement a real-time application to showcase our method in real-world scenarios. MOVIN dataset is available at \url{https://movin3d.github.io/movin_pg2023/}.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2308.14404.pdf' target='_blank'>https://arxiv.org/pdf/2308.14404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Forouzan Farzinnejad, Javad Rasti, Navid Khezrian, Jens Grubert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14404">The Effect of an Exergame on the Shadow Play Skill Based on Muscle Memory for Young Female Participants: The Case of Forehand Drive in Table Tennis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning and practicing table tennis with traditional methods is a long, tedious process and may even lead to the internalization of incorrect techniques if not supervised by a coach. To overcome these issues, the presented study proposes an exergame with the aim of enhancing young female novice players' performance by boosting muscle memory, making practice more interesting, and decreasing the probability of faulty training. Specifically, we propose an exergame based on skeleton tracking and a virtual avatar to support correct shadow practice to learn forehand drive technique without the presence of a coach. We recruited 44 schoolgirls aged between 8 and 12 years without a background in playing table tennis and divided them into control and experimental groups. We examined their stroke skills (via the Mott-Lockhart test) and the error coefficient of their forehand drives (using a ball machine) in the pretest, post-test, and follow-up tests (10 days after the post-test). Our results showed that the experimental group had progress in the short and long term, while the control group had an improvement only in the short term. Further, the scale of improvement in the experimental group was significantly higher than in the control group. Given that the early stages of learning, particularly in girls children, are important in the internalization of individual skills in would-be athletes, this method could support promoting correct training for young females.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2308.03022.pdf' target='_blank'>https://arxiv.org/pdf/2308.03022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masum Hasan, Cengiz Ozel, Sammy Potter, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03022">SAPIEN: Affective Virtual Agents Powered by Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this demo paper, we introduce SAPIEN, a platform for high-fidelity virtual agents driven by large language models that can hold open domain conversations with users in 13 different languages, and display emotions through facial expressions and voice. The platform allows users to customize their virtual agent's personality, background, and conversation premise, thus providing a rich, immersive interaction experience. Furthermore, after the virtual meeting, the user can choose to get the conversation analyzed and receive actionable feedback on their communication skills. This paper illustrates an overview of the platform and discusses the various application domains of this technology, ranging from entertainment to mental health, communication training, language learning, education, healthcare, and beyond. Additionally, we consider the ethical implications of such realistic virtual agent representations and the potential challenges in ensuring responsible use.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2306.15213.pdf' target='_blank'>https://arxiv.org/pdf/2306.15213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurtis Haut, Caleb Wohn, Benjamin Kane, Tom Carroll, Catherine Guigno, Varun Kumar, Ron Epstein, Lenhart Schubert, Ehsan Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15213">Validating a virtual human and automated feedback system for training doctor-patient communication skills</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication between a clinician and their patient is critical for delivering healthcare maximizing outcomes. Unfortunately, traditional communication training approaches that use human standardized patients and expert coaches are difficult to scale. Here, we present the development and validation of a scalable, easily accessible, digital tool known as the Standardized Online Patient for Health Interaction Education (SOPHIE) for practicing and receiving feedback on doctor-patient communication skills. SOPHIE was validated by conducting an experiment with 30 participants. We found that participants who underwent SOPHIE performed significantly better than the control in overall communication, aggregate scores, empowering the patient, and showing empathy ($p < 0.05$ in all cases). One day, we hope that SOPHIE will help make communication training resources more accessible by providing a scalable option to supplement existing resources.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2306.09532.pdf' target='_blank'>https://arxiv.org/pdf/2306.09532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel van de Panne, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09532">Hierarchical Planning and Control for Box Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans perform everyday tasks using a combination of locomotion and manipulation skills. Building a system that can handle both skills is essential to creating virtual humans. We present a physically-simulated human capable of solving box rearrangement tasks, which requires a combination of both skills. We propose a hierarchical control architecture, where each level solves the task at a different level of abstraction, and the result is a physics-based simulated virtual human capable of rearranging boxes in a cluttered environment. The control architecture integrates a planner, diffusion models, and physics-based motion imitation of sparse motion clips using deep reinforcement learning. Boxes can vary in size, weight, shape, and placement height. Code and trained control policies are provided.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2305.16411.pdf' target='_blank'>https://arxiv.org/pdf/2305.16411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhen Weng, Zeyu Wang, Serena Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16411">ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in text-to-image generation have enabled significant progress in zero-shot 3D shape generation. This is achieved by score distillation, a methodology that uses pre-trained text-to-image diffusion models to optimize the parameters of a 3D neural presentation, e.g. Neural Radiance Field (NeRF). While showing promising results, existing methods are often not able to preserve the geometry of complex shapes, such as human bodies. To address this challenge, we present ZeroAvatar, a method that introduces the explicit 3D human body prior to the optimization process. Specifically, we first estimate and refine the parameters of a parametric human body from a single image. Then during optimization, we use the posed parametric body as additional geometry constraint to regularize the diffusion model as well as the underlying density field. Lastly, we propose a UV-guided texture regularization term to further guide the completion of texture on invisible body parts. We show that ZeroAvatar significantly enhances the robustness and 3D consistency of optimization-based image-to-3D avatar generation, outperforming existing zero-shot image-to-3D methods.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2304.11113.pdf' target='_blank'>https://arxiv.org/pdf/2304.11113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhan Chen, Matthew O'Toole, Gaurav Bharaj, Pablo Garrido
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11113">Implicit Neural Head Synthesis via Controllable Local Deformation Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality reconstruction of controllable 3D head avatars from 2D videos is highly desirable for virtual human applications in movies, games, and telepresence. Neural implicit fields provide a powerful representation to model 3D head avatars with personalized shape, expressions, and facial parts, e.g., hair and mouth interior, that go beyond the linear 3D morphable model (3DMM). However, existing methods do not model faces with fine-scale facial features, or local control of facial parts that extrapolate asymmetric expressions from monocular videos. Further, most condition only on 3DMM parameters with poor(er) locality, and resolve local features with a global neural field. We build on part-based implicit shape models that decompose a global deformation field into local ones. Our novel formulation models multiple implicit deformation fields with local semantic rig-like control via 3DMM-based parameters, and representative facial landmarks. Further, we propose a local control loss and attention mask mechanism that promote sparsity of each learned deformation field. Our formulation renders sharper locally controllable nonlinear deformations than previous implicit monocular approaches, especially mouth interior, asymmetric expressions, and facial details.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2509.13013.pdf' target='_blank'>https://arxiv.org/pdf/2509.13013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaofeng Liu, Hengsen Li, Ruoyu Gao, Xuetong Li, Zhiyuan Ma, Tao Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13013">Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2508.08429.pdf' target='_blank'>https://arxiv.org/pdf/2508.08429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dalton Omens, Allise Thurman, Jihun Yu, Ronald Fedkiw
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08429">Improving Facial Rig Semantics for Tracking and Retargeting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider retargeting a tracked facial performance to either another person or to a virtual character in a game or virtual reality (VR) environment. We remove the difficulties associated with identifying and retargeting the semantics of one rig framework to another by utilizing the same framework (3DMM, FLAME, MetaHuman, etc.) for both subjects. Although this does not constrain the choice of framework when retargeting from one person to another, it does force the tracker to use the game/VR character rig when retargeting to a game/VR character. We utilize volumetric morphing in order to fit facial rigs to both performers and targets; in addition, a carefully chosen set of Simon-Says expressions is used to calibrate each rig to the motion signatures of the relevant performer or target. Although a uniform set of Simon-Says expressions can likely be used for all person to person retargeting, we argue that person to game/VR character retargeting benefits from Simon-Says expressions that capture the distinct motion signature of the game/VR character rig. The Simon-Says calibrated rigs tend to produce the desired expressions when exercising animation controls (as expected). Unfortunately, these well-calibrated rigs still lead to undesirable controls when tracking a performance (a well-behaved function can have an arbitrarily ill-conditioned inverse), even though they typically produce acceptable geometry reconstructions. Thus, we propose a fine-tuning approach that modifies the rig used by the tracker in order to promote the output of more semantically meaningful animation controls, facilitating high efficacy retargeting. In order to better address real-world scenarios, the fine-tuning relies on implicit differentiation so that the tracker can be treated as a (potentially non-differentiable) black box.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2507.16562.pdf' target='_blank'>https://arxiv.org/pdf/2507.16562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Megha Quamara, Viktor Schmuck, Cristina Iani, Axel Primavesi, Alexander Plaum, Luca Vigano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16562">Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the findings of a user study that evaluated the social acceptance of eXtended Reality (XR) agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves user interaction with a virtual avatar, enabled by a modular toolkit. The interactions are designed to provide tailored training for journalists in digital-remote settings, especially for sensitive or dangerous scenarios, without requiring specialized end-user equipment like headsets. Our research adapts and extends the Almere model, representing social acceptance through existing attributes such as perceived ease of use and perceived usefulness, along with added ones like dependability and security in the user-agent interaction. The XR agent was tested through a controlled experiment in a real-world setting, with data collected on users' perceptions. Our findings, based on quantitative and qualitative measurements involving questionnaires, contribute to the understanding of user perceptions and acceptance of XR agent solutions within a specific social context, while also identifying areas for the improvement of XR systems.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2504.20403.pdf' target='_blank'>https://arxiv.org/pdf/2504.20403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Liu, Yifang Men, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20403">Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2504.04968.pdf' target='_blank'>https://arxiv.org/pdf/2504.04968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayang Huang, Lingjie Li, Kang Zhang, David Yip
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04968">The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the art project The Dream Within Huang Long Cave, an AI-driven interactive and immersive narrative experience. The project offers new insights into AI technology, artistic practice, and psychoanalysis. Inspired by actual geographical landscapes and familial archetypes, the work combines psychoanalytic theory and computational technology, providing an artistic response to the concept of the non-existence of the Big Other. The narrative is driven by a combination of a large language model (LLM) and a realistic digital character, forming a virtual agent named YELL. Through dialogue and exploration within a cave automatic virtual environment (CAVE), the audience is invited to unravel the language puzzles presented by YELL and help him overcome his life challenges. YELL is a fictional embodiment of the Big Other, modeled after the artist's real father. Through a cross-temporal interaction with this digital father, the project seeks to deconstruct complex familial relationships. By demonstrating the non-existence of the Big Other, we aim to underscore the authenticity of interpersonal emotions, positioning art as a bridge for emotional connection and understanding within family dynamics.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2503.16432.pdf' target='_blank'>https://arxiv.org/pdf/2503.16432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young-Ho Bae, Casey C. Bennett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16432">Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game "Dont Starve Together", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2410.12051.pdf' target='_blank'>https://arxiv.org/pdf/2410.12051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cindy Xu, Mengyu Chen, Pranav Deshpande, Elvir Azanli, Runqing Yang, Joseph Ligman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12051">Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel system designed to enhance customer service in the financial and retail sectors through a context-aware 3D virtual agent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our approach focuses on enabling data-driven and empathetic interactions that ensure customer satisfaction by introducing situational awareness of the physical location, personalized interactions based on customer profiles, and rigorous privacy and security standards. We discuss our design considerations critical for deployment in real-world customer service environments, addressing challenges in user data management and sensitive information handling. We also outline the system architecture and key features unique to banking and retail environments. Our work demonstrates the potential of integrating MR and VLMs in service industries, offering practical insights in customer service delivery while maintaining high standards of security and personalization.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2312.12634.pdf' target='_blank'>https://arxiv.org/pdf/2312.12634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Payam Jome Yazdian, Rachel Lagasse, Hamid Mohammadi, Eric Liu, Li Cheng, Angelica Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12634">MotionScript: Natural Language Descriptions for Expressive 3D Human Motions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MotionScript, a novel framework for generating highly detailed, natural language descriptions of 3D human motions. Unlike existing motion datasets that rely on broad action labels or generic captions, MotionScript provides fine-grained, structured descriptions that capture the full complexity of human movement including expressive actions (e.g., emotions, stylistic walking) and interactions beyond standard motion capture datasets. MotionScript serves as both a descriptive tool and a training resource for text-to-motion models, enabling the synthesis of highly realistic and diverse human motions from text. By augmenting motion datasets with MotionScript captions, we demonstrate significant improvements in out-of-distribution motion generation, allowing large language models (LLMs) to generate motions that extend beyond existing data. Additionally, MotionScript opens new applications in animation, virtual human simulation, and robotics, providing an interpretable bridge between intuitive descriptions and motion synthesis. To the best of our knowledge, this is the first attempt to systematically translate 3D motion into structured natural language without requiring training data.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2311.12707.pdf' target='_blank'>https://arxiv.org/pdf/2311.12707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hye Sun Yun, Mehdi Arjmand, Phillip Sherlock, Michael K. Paasche-Orlow, James W. Griffith, Timothy Bickmore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12707">Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standardized, validated questionnaires are vital tools in research and healthcare, offering dependable self-report data. Prior work has revealed that virtual agent-administered questionnaires are almost equivalent to self-administered ones in an electronic form. Despite being an engaging method, repeated use of virtual agent-administered questionnaires in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose using large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants interacted with our agent system and responded daily for two weeks to one of the following questionnaires: a standardized depression questionnaire, question variants generated by LLMs, or question variants accompanied by LLM-generated small talk. The responses were compared to a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found that the variants were significantly less repetitive than repeated administrations of the same standardized questionnaire. Our findings highlight the potential of LLM-generated variants to invigorate agent-administered questionnaires and foster engagement and interest, without compromising their validity.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2310.13482.pdf' target='_blank'>https://arxiv.org/pdf/2310.13482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyan Yu, Shihuan Wang, Dong zhang, Yingying Zhang, Chaoqun Cen, Zhixiang you, Xiaobing zou, Hongzhu Deng, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13482">HSVRS: A Virtual Reality System of the Hide-and-Seek Game to Enhance Gaze Fixation Ability for Autistic Children</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous children diagnosed with Autism Spectrum Disorder (ASD) exhibit abnormal eye gaze pattern in communication and social interaction. Due to the high cost of ASD interventions and a shortage of professional therapists, researchers have explored the use of virtual reality (VR) systems as a supplementary intervention for autistic children. This paper presents the design of a novel VR-based system called the Hide and Seek Virtual Reality System (HSVRS). The HSVRS allows children with ASD to enhance their ocular gaze abilities while engaging in a hide-and-seek game with a virtual avatar. By employing face and voice manipulation technology, the HSVRS provides the option to customize the appearance and voice of the avatar, making it resemble someone familiar to the child, such as their parents. We conducted a pilot study at the Third Affiliated Hospital of Sun Yat-sen University, China, to evaluate the feasibility of HSVRS as an auxiliary intervention for children with autism (N=24). Through the analysis of subjective questionnaires completed by the participants' parents and objective eye gaze data, we observed that children in the VR-assisted intervention group demonstrated better performance compared to those in the control group. Furthermore, our findings indicate that the utilization of face and voice manipulation techniques to personalize avatars in hide-and-seek games can enhance the efficiency and effectiveness of the system.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2308.10385.pdf' target='_blank'>https://arxiv.org/pdf/2308.10385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanseob Kim, Bin Han, Jieun Kim, Muhammad Firdaus Syawaludin, Gerard Jounghyun Kim, Jae-In Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10385">Engaged and Affective Virtual Agents: Their Impact on Social Presence, Trustworthiness, and Decision-Making in the Group Discussion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates how different virtual agent (VA) behaviors influence subjects' perceptions and group decision-making. Participants carried out experimental group discussions with a VA exhibiting varying levels of engagement and affective behavior. Engagement refers to the VA's focus on the group task, whereas affective behavior reflects the VA's emotional state. The findings revealed that VA's engagements effectively captured participants' attention even in the group setting and enhanced group synergy, thereby facilitating more in-depth discussion and producing better consensus. On the other hand, VA's affective behavior negatively affected the perceived social presence and trustworthiness. Consequently, in the context of group discussion, participants preferred the engaged and non-affective VA to the non-engaged and affective VA. The study provides valuable insights for improving the VA's behavioral design as a team member for collaborative tasks.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2306.10656.pdf' target='_blank'>https://arxiv.org/pdf/2306.10656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kenta Oono, Nontawat Charoenphakdee, Kotatsu Bito, Zhengyan Gao, Hideyoshi Igata, Masashi Yoshikawa, Yoshiaki Ota, Hiroki Okui, Kei Akita, Shoichiro Yamaguchi, Yohei Sugawara, Shin-ichi Maeda, Kunihiko Miyoshi, Yuki Saito, Koki Tsuda, Hiroshi Maruyama, Kohei Hayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10656">Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying the relationship between healthcare attributes, lifestyles, and personality is vital for understanding and improving physical and mental well-being. Machine learning approaches are promising for modeling their relationships and offering actionable suggestions. In this paper, we propose the Virtual Human Generative Model (VHGM), a novel deep generative model capable of estimating over 2,000 attributes across healthcare, lifestyle, and personality domains. VHGM leverages masked modeling to learn the joint distribution of attributes, enabling accurate predictions and robust conditional sampling. We deploy VHGM as a web service, showcasing its versatility in driving diverse healthcare applications aimed at improving user well-being. Through extensive quantitative evaluations, we demonstrate VHGM's superior performance in attribute imputation and high-quality sample generation compared to existing baselines. This work highlights VHGM as a powerful tool for personalized healthcare and lifestyle management, with broad implications for data-driven health solutions.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2301.09702.pdf' target='_blank'>https://arxiv.org/pdf/2301.09702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Guo, Amy R. Reibman, Edward J. Delp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09702">Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the proposed SMB outperforms other synthesis methods on several re-ID benchmarks.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2508.10586.pdf' target='_blank'>https://arxiv.org/pdf/2508.10586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Birgit Nierula, Mustafa Tevfik Lafci, Anna Melnik, Mert AkgÃ¼l, Farelle Toumaleu Siewe, Sebastian Bosse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10586">Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proxemics, the study of spatial behavior, is fundamental to social interaction and increasingly relevant for virtual reality (VR) applications. While previous research has established that users respond to personal space violations in VR similarly as in real-world settings, phase-specific physiological responses and the modulating effects of facial expressions remain understudied. We investigated physiological and subjective responses to personal space violations by virtual avatars, to understand how threatening facial expressions and interaction phases (approach vs. standing) influence these responses. Sixteen participants experienced a 2x2 factorial design manipulating Personal Space (intrusion vs. respect) and Facial Expression (neutral vs. angry) while we recorded skin conductance response (SCR), heart rate variability (HRV), and discomfort ratings. Personal space boundaries were individually calibrated using a stop-distance procedure. Results show that SCR responses are significantly higher during the standing phase compared to the approach phase when personal space was violated, indicating that prolonged proximity within personal space boundaries is more physiologically arousing than the approach itself. Angry facial expressions significantly reduced HRV, reflecting decreased parasympathetic activity, and increased discomfort ratings, but did not amplify SCR responses. These findings demonstrate that different physiological modalities capture distinct aspects of proxemic responses: SCR primarily reflects spatial boundary violations, while HRV responds to facial threat cues. Our results provide insights for developing comprehensive multi-modal assessments of social behavior in virtual environments and inform the design of more realistic avatar interactions.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2508.09973.pdf' target='_blank'>https://arxiv.org/pdf/2508.09973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geonhee Sim, Gyeongsik Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09973">PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two major approaches exist for creating animatable human avatars. The first, a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a single person, achieving personalization through a disentangled identity representation. However, modeling pose-driven deformations, such as non-rigid cloth deformations, requires numerous pose-rich videos, which are costly and impractical to capture in daily life. The second, a diffusion-based approach, learns pose-driven deformations from large-scale in-the-wild videos but struggles with identity preservation and pose-dependent identity entanglement. We present PERSONA, a framework that combines the strengths of both approaches to obtain a personalized 3D human avatar with pose-driven deformations from a single image. PERSONA leverages a diffusion-based approach to generate pose-rich videos from the input image and optimizes a 3D avatar based on them. To ensure high authenticity and sharp renderings across diverse poses, we introduce balanced sampling and geometry-weighted optimization. Balanced sampling oversamples the input image to mitigate identity shifts in diffusion-generated training videos. Geometry-weighted optimization prioritizes geometry constraints over image loss, preserving rendering quality in diverse poses.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2507.06060.pdf' target='_blank'>https://arxiv.org/pdf/2507.06060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Symeonidis-Herzig, Ãzge MercanoÄlu Sincan, Richard Bowden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06060">VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2503.19334.pdf' target='_blank'>https://arxiv.org/pdf/2503.19334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ghazanfar Ali, Hong-Quan Le, Junho Kim, Seoung-won Hwang, Jae-In Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19334">Design of Seamless Multi-modal Interaction Framework for Intelligent Virtual Agents in Wearable Mixed Reality Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the design of a multimodal interaction framework for intelligent virtual agents in wearable mixed reality environments, especially for interactive applications at museums, botanical gardens, and similar places. These places need engaging and no-repetitive digital content delivery to maximize user involvement. An intelligent virtual agent is a promising mode for both purposes. Premises of framework is wearable mixed reality provided by MR devices supporting spatial mapping. We envisioned a seamless interaction framework by integrating potential features of spatial mapping, virtual character animations, speech recognition, gazing, domain-specific chatbot and object recognition to enhance virtual experiences and communication between users and virtual agents. By applying a modular approach and deploying computationally intensive modules on cloud-platform, we achieved a seamless virtual experience in a device with limited resources. Human-like gaze and speech interaction with a virtual agent made it more interactive. Automated mapping of body animations with the content of a speech made it more engaging. In our tests, the virtual agents responded within 2-4 seconds after the user query. The strength of the framework is flexibility and adaptability. It can be adapted to any wearable MR device supporting spatial mapping.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2503.14943.pdf' target='_blank'>https://arxiv.org/pdf/2503.14943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wang, Ivan Molodetskikh, Ondrej Texler, Dimitar Dinev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14943">3D Engine-ready Photorealistic Avatars via Dynamic Textures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the digital and physical worlds become more intertwined, there has been a lot of interest in digital avatars that closely resemble their real-world counterparts. Current digitization methods used in 3D production pipelines require costly capture setups, making them impractical for mass usage among common consumers. Recent academic literature has found success in reconstructing humans from limited data using implicit representations (e.g., voxels used in NeRFs), which are able to produce impressive videos. However, these methods are incompatible with traditional rendering pipelines, making it difficult to use them in applications such as games. In this work, we propose an end-to-end pipeline that builds explicitly-represented photorealistic 3D avatars using standard 3D assets. Our key idea is the use of dynamically-generated textures to enhance the realism and visually mask deficiencies in the underlying mesh geometry. This allows for seamless integration with current graphics pipelines while achieving comparable visual quality to state-of-the-art 3D avatar generation methods.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2503.14408.pdf' target='_blank'>https://arxiv.org/pdf/2503.14408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parisa Ghanad Torshizi, Laura B. Hensel, Ari Shapiro, Stacy C. Marsella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14408">Large Language Models for Virtual Human Gesture Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-speech gestures convey a wide variety of meanings and play an important role in face-to-face human interactions. These gestures significantly influence the addressee's engagement, recall, comprehension, and attitudes toward the speaker. Similarly, they impact interactions between humans and embodied virtual agents. The process of selecting and animating meaningful gestures has thus become a key focus in the design of these agents. However, automating this gesture selection process poses a significant challenge. Prior gesture generation techniques have varied from fully automated, data-driven methods, which often struggle to produce contextually meaningful gestures, to more manual approaches that require crafting specific gesture expertise and are time-consuming and lack generalizability. In this paper, we leverage the semantic capabilities of Large Language Models to develop a gesture selection approach that suggests meaningful, appropriate co-speech gestures. We first describe how information on gestures is encoded into GPT-4. Then, we conduct a study to evaluate alternative prompting approaches for their ability to select meaningful, contextually relevant gestures and to align them appropriately with the co-speech utterance. Finally, we detail and demonstrate how this approach has been implemented within a virtual agent system, automating the selection and subsequent animation of the selected gestures for enhanced human-agent interactions.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2412.07912.pdf' target='_blank'>https://arxiv.org/pdf/2412.07912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bora Tarlan, Nisa Erdal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07912">How Can I Assist You Today?: A Comparative Analysis of a Humanoid Robot and a Virtual Human Avatar in Human Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores human perceptions of intelligent agents by comparing interactions with a humanoid robot and a virtual human avatar, both utilizing GPT-3 for response generation. The study aims to understand how physical and virtual embodiments influence perceptions of anthropomorphism, animacy, likeability, and perceived intelligence. The uncanny valley effect was also investigated in the scope of this study based on the two agents' human-likeness and affinity. Conducted with ten participants from Sabanci University, the experiment involved tasks that sought advice, followed by assessments using the Godspeed Questionnaire Series and structured interviews. Results revealed no significant difference in anthropomorphism between the humanoid robot and the virtual human avatar, but the humanoid robot was perceived as more likable and slightly more intelligent, highlighting the importance of physical presence and interactive gestures. These findings suggest that while virtual avatars can achieve high human-likeness, physical embodiment enhances likeability and perceived intelligence. However, the study's scope was insufficient to claim the existence of the uncanny valley effect in the participants' interactions. The study offers practical insights for designing future intelligent assistants, emphasizing the need for integrating physical elements and sophisticated communicative behaviors to improve user experience and acceptance.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2411.05653.pdf' target='_blank'>https://arxiv.org/pdf/2411.05653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leon O. H. Kroczek, Alexander May, Selina Hettenkofer, Andreas Ruider, Bernd Ludwig, Andreas MÃ¼hlberger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05653">The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities in conversational tasks. Embodying an LLM as a virtual human allows users to engage in face-to-face social interactions in Virtual Reality. However, the influence of person- and task-related factors in social interactions with LLM-controlled agents remains unclear. In this study, forty-six participants interacted with a virtual agent whose persona was manipulated as extravert or introvert in three different conversational tasks (small talk, knowledge test, convincing). Social-evaluation, emotional experience, and realism were assessed using ratings. Interactive engagement was measured by quantifying participants' words and conversational turns. Finally, we measured participants' willingness to ask the agent for help during the knowledge test. Our findings show that the extraverted agent was more positively evaluated, elicited a more pleasant experience and greater engagement, and was assessed as more realistic compared to the introverted agent. Whereas persona did not affect the tendency to ask for help, participants were generally more confident in the answer when they had help of the LLM. Variation of personality traits of LLM-controlled embodied virtual agents, therefore, affects social-emotional processing and behavior in virtual interactions. Embodied virtual agents allow the presentation of naturalistic social encounters in a virtual environment.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2409.18745.pdf' target='_blank'>https://arxiv.org/pdf/2409.18745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Christina Almada Campos, Bruno Vilhena Adorno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18745">A study on the effects of mixed explicit and implicit communications in human-virtual-agent interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication between humans and robots (or virtual agents) is essential for interaction and often inspired by human communication, which uses gestures, facial expressions, gaze direction, and other explicit and implicit means. This work presents an interaction experiment where humans and virtual agents interact through explicit (gestures, manual entries using mouse and keyboard, voice, sound, and information on screen) and implicit (gaze direction, location, facial expressions, and raise of eyebrows) communication to evaluate the effect of mixed explicit-implicit communication against purely explicit communication. Results obtained using Bayesian parameter estimation show that the number of errors and task execution time did not significantly change when mixed explicit and implicit communications were used, and neither the perceived efficiency of the interaction. In contrast, acceptance, sociability, and transparency of the virtual agent increased when using mixed communication modalities (88.3%, 92%, and 92.9% of the effect size posterior distribution of each variable, respectively, were above the upper limit of the region of practical equivalence). This suggests that task-related measures, such as time, number of errors, and perceived efficiency of the interaction, have not been influenced by the communication type in our particular experiment. However, the improvement of subjective measures related to the virtual agent, such as acceptance, sociability, and transparency, suggests that humans are more receptive to mixed explicit and implicit communications.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2406.10561.pdf' target='_blank'>https://arxiv.org/pdf/2406.10561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Palash Moon, Pushpak Bhattacharyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10561">We Care: Multimodal Depression Detection and Knowledge Infused Mental Health Therapeutic Response Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection of depression through non-verbal cues has gained significant attention. Previous research predominantly centred on identifying depression within the confines of controlled laboratory environments, often with the supervision of psychologists or counsellors. Unfortunately, datasets generated in such controlled settings may struggle to account for individual behaviours in real-life situations. In response to this limitation, we present the Extended D-vlog dataset, encompassing a collection of 1, 261 YouTube vlogs. Additionally, the emergence of large language models (LLMs) like GPT3.5, and GPT4 has sparked interest in their potential they can act like mental health professionals. Yet, the readiness of these LLM models to be used in real-life settings is still a concern as they can give wrong responses that can harm the users. We introduce a virtual agent serving as an initial contact for mental health patients, offering Cognitive Behavioral Therapy (CBT)-based responses. It comprises two core functions: 1. Identifying depression in individuals, and 2. Delivering CBT-based therapeutic responses. Our Mistral model achieved impressive scores of 70.1% and 30.9% for distortion assessment and classification, along with a Bert score of 88.7%. Moreover, utilizing the TVLT model on our Multimodal Extended D-vlog Dataset yielded outstanding results, with an impressive F1-score of 67.8%
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2405.07418.pdf' target='_blank'>https://arxiv.org/pdf/2405.07418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Guillermo FeijÃ³o-GarcÃ­a, Chase Wrenn, Alexandre Gomes de Siqueira, Rashi Ghosh, Jacob Stuart, Heng Yao, Benjamin Lok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07418">Exploring the Effects of User-Agent and User-Designer Similarity in Virtual Human Design to Promote Mental Health Intentions for College Students</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual humans (i.e., embodied conversational agents) have the potential to support college students' mental health, particularly in Science, Technology, Engineering, and Mathematics (STEM) fields where students are at a heightened risk of mental disorders such as anxiety and depression. A comprehensive understanding of students, considering their cultural characteristics, experiences, and expectations, is crucial for creating timely and effective virtual human interventions. To this end, we conducted a user study with 481 computer science students from a major university in North America, exploring how they co-designed virtual humans to support mental health conversations for students similar to them. Our findings suggest that computer science students who engage in co-design processes of virtual humans tend to create agents that closely resemble them demographically--agent-designer demographic similarity. Key factors influencing virtual human design included age, gender, ethnicity, and the matching between appearance and voice. We also observed that the demographic characteristics of virtual human designers, especially ethnicity and gender, tend to be associated with those of the virtual humans they designed. Finally, we provide insights concerning the impact of user-designer demographic similarity in virtual humans' effectiveness in promoting mental health conversations when designers' characteristics are shared explicitly or implicitly. Understanding how virtual humans' characteristics serve users' experiences in mental wellness conversations and the similarity-attraction effects between agents, users, and designers may help tailor virtual humans' design to enhance their acceptance and increase their counseling effectiveness.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2401.06957.pdf' target='_blank'>https://arxiv.org/pdf/2401.06957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Nadeem, Raza Imam, Rouqaiah Al-Refai, Meriem Chkir, Mohamad Hoda, Abdulmotaleb El Saddik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06957">EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has grown. Addressing this demand, we introduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation (EVOKE), a lightweight emotion recognition framework designed for the seamless integration of emotion recognition into 3D avatars within virtual environments. Our approach leverages knowledge distillation involving multi-label classification on the publicly available DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Remarkably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results, boasting an accuracy of 87% while demanding far less computational resources. This equilibrium between performance and deployability positions our framework as an ideal choice for virtual environment systems. Furthermore, the multi-label classification outcomes are utilized to map emotions onto custom-designed 3D avatars.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2312.02128.pdf' target='_blank'>https://arxiv.org/pdf/2312.02128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vitor Miguel Xavier Peres, Greice Pinho Dal Molin, Soraia Raupp Musse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02128">Can we truly transfer an actor's genuine happiness to avatars? An investigation into virtual, real, posed and spontaneous faces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A look is worth a thousand words is a popular phrase. And why is a simple look enough to portray our feelings about something or someone? Behind this question are the theoretical foundations of the field of psychology regarding social cognition and the studies of psychologist Paul Ekman. Facial expressions, as a form of non-verbal communication, are the primary way to transmit emotions between human beings. The set of movements and expressions of facial muscles that convey some emotional state of the individual to their observers are targets of studies in many areas. Our research aims to evaluate Ekman's action units in datasets of real human faces, posed and spontaneous, and virtual human faces resulting from transferring real faces into Computer Graphics faces. In addition, we also conducted a case study with specific movie characters, such as SheHulk and Genius. We intend to find differences and similarities in facial expressions between real and CG datasets, posed and spontaneous faces, and also to consider the actors' genders in the videos. This investigation can help several areas of knowledge, whether using real or virtual human beings, in education, health, entertainment, games, security, and even legal matters. Our results indicate that AU intensities are greater for posed than spontaneous datasets, regardless of gender. Furthermore, there is a smoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a real face is transformed into CG.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2311.15251.pdf' target='_blank'>https://arxiv.org/pdf/2311.15251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Misirlis, Yiannis Nikolaidis, Anna Sabidussi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15251">Should I use metaverse or not? An investigation of university students behavioral intention to use MetaEducation technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metaverse, a burgeoning technological trend that combines virtual and augmented reality, provides users with a fully digital environment where they can assume a virtual identity through a digital avatar and interact with others as they were in the real world. Its applications span diverse domains such as economy (with its entry into the cryptocurrency field), finance, social life, working environment, healthcare, real estate, and education. During the COVID-19 and post-COVID-19 era, universities have rapidly adopted e-learning technologies to provide students with online access to learning content and platforms, rendering previous considerations on integrating such technologies or preparing institutional infrastructures virtually obsolete. In light of this context, the present study proposes a framework for analyzing university students' acceptance and intention to use metaverse technologies in education, drawing upon the Technology Acceptance Model (TAM). The study aims to investigate the relationship between students' intention to use metaverse technologies in education, hereafter referred to as MetaEducation, and selected TAM constructs, including Attitude, Perceived Usefulness, Perceived Ease of Use, Self-efficacy of metaverse technologies in education, and Subjective Norm. Notably, Self-efficacy and Subjective Norm have a positive influence on Attitude and Perceived Usefulness, whereas Perceived Ease of Use does not exhibit a strong correlation with Attitude or Perceived Usefulness. The authors postulate that the weak associations between the study's constructs may be attributed to limited knowledge regarding MetaEducation and its potential benefits. Further investigation and analysis of the study's proposed model are warranted to comprehensively understand the complex dynamics involved in the acceptance and utilization of MetaEducation technologies in the realm of higher education
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2311.15088.pdf' target='_blank'>https://arxiv.org/pdf/2311.15088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Liu, Gizem Kayar, Ken Perlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15088">A GPU-based Hydrodynamic Simulator with Boid Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a hydrodynamic simulation system using the GPU compute shaders of DirectX for simulating virtual agent behaviors and navigation inside a smoothed particle hydrodynamical (SPH) fluid environment with real-time water mesh surface reconstruction. The current SPH literature includes interactions between SPH and heterogeneous meshes but seldom involves interactions between SPH and virtual boid agents. The contribution of the system lies in the combination of the parallel smoothed particle hydrodynamics model with the distributed boid model of virtual agents to enable agents to interact with fluids. The agents based on the boid algorithm influence the motion of SPH fluid particles, and the forces from the SPH algorithm affect the movement of the boids. To enable realistic fluid rendering and simulation in a particle-based system, it is essential to construct a mesh from the particle attributes. Our system also contributes to the surface reconstruction aspect of the pipeline, in which we performed a set of experiments with the parallel marching cubes algorithm per frame for constructing the mesh from the fluid particles in a real-time compute and memory-intensive application, producing a wide range of triangle configurations. We also demonstrate that our system is versatile enough for reinforced robotic agents instead of boid agents to interact with the fluid environment for underwater navigation and remote control engineering purposes.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2308.01492.pdf' target='_blank'>https://arxiv.org/pdf/2308.01492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Blooma John, Ramanathan Subramanian, Jayan Chirayath Kurian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01492">A Virtual Reality Game to Improve Physical and Cognitive Acuity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Virtual Human Benchmark (VHB) game to evaluate and improve physical and cognitive acuity. VHB simulates in 3D the BATAK lightboard game, which is designed to improve physical reaction and hand-eye coordination, on the \textit{Oculus Rift} and \textit{Quest} headsets. The game comprises the \textit{reaction}, \textit{accumulator} and \textit{sequence} modes; \bj{along} with the \textit{reaction} and \textit{accumulator} modes which mimic BATAK functionalities, the \textit{sequence} mode involves the user repeating a sequence of illuminated targets with increasing complexity to train visual memory and cognitive processing. A first version of the game (VHB v1) was evaluated against the real-world BATAK by 20 users, and their feedback was utilized to improve game design and obtain a second version (VHB v2). Another study to evaluate VHB v2 was conducted with 20 users, whose results confirmed that the deign improvements enhanced game usability and user experience in multiple respects. Also, logging and visualization of performance data such as \textit{reaction time}, \textit{speed between targets} and \textit{completed sequence patterns} provides useful data for coaches/therapists monitoring sports/rehabilitation regimens.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2307.01468.pdf' target='_blank'>https://arxiv.org/pdf/2307.01468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanyu Pan, Guowei Yang, Taijiang Mu, Yu-Kun Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01468">Generating Animatable 3D Cartoon Faces from Single Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the booming of virtual reality (VR) technology, there is a growing need for customized 3D avatars. However, traditional methods for 3D avatar modeling are either time-consuming or fail to retain similarity to the person being modeled. We present a novel framework to generate animatable 3D cartoon faces from a single portrait image. We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN. Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture, which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision. Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer. Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria. Furthermore, we demonstrate the capability of real-time facial animation of our 3D model.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2304.12152.pdf' target='_blank'>https://arxiv.org/pdf/2304.12152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Li Ding, Liang Chen, Kai Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12152">Efficient Halftoning via Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Halftoning aims to reproduce a continuous-tone image with pixels whose intensities are constrained to two discrete levels. This technique has been deployed on every printer, and the majority of them adopt fast methods (e.g., ordered dithering, error diffusion) that fail to render structural details, which determine halftone's quality. Other prior methods of pursuing visual pleasure by searching for the optimal halftone solution, on the contrary, suffer from their high computational cost. In this paper, we propose a fast and structure-aware halftoning method via a data-driven approach. Specifically, we formulate halftoning as a reinforcement learning problem, in which each binary pixel's value is regarded as an action chosen by a virtual agent with a shared fully convolutional neural network (CNN) policy. In the offline phase, an effective gradient estimator is utilized to train the agents in producing high-quality halftones in one action step. Then, halftones can be generated online by one fast CNN inference. Besides, we propose a novel anisotropy suppressing loss function, which brings the desirable blue-noise property. Finally, we find that optimizing SSIM could result in holes in flat areas, which can be avoided by weighting the metric with the contone's contrast map. Experiments show that our framework can effectively train a light-weight CNN, which is 15x faster than previous structure-aware methods, to generate blue-noise halftones with satisfactory visual quality. We also present a prototype of deep multitoning to demonstrate the extensibility of our method.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2304.06678.pdf' target='_blank'>https://arxiv.org/pdf/2304.06678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Viceconti, Maarten De Vos, Sabato Mellone, Liesbet Geris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06678">From the digital twins in healthcare to the Virtual Human Twin: a moon-shot project for digital health research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The idea of a systematic digital representation of the entire known human pathophysiology, which we could call the Virtual Human Twin, has been around for decades. To date, most research groups focused instead on developing highly specialised, highly focused patient-specific models able to predict specific quantities of clinical relevance. While it has facilitated harvesting the low-hanging fruits, this narrow focus is, in the long run, leaving some significant challenges that slow the adoption of digital twins in healthcare. This position paper lays the conceptual foundations for developing the Virtual Human Twin (VHT). The VHT is intended as a distributed and collaborative infrastructure, a collection of technologies and resources (data, models) that enable it, and a collection of Standard Operating Procedures (SOP) that regulate its use. The VHT infrastructure aims to facilitate academic researchers, public organisations, and the biomedical industry in developing and validating new digital twins in healthcare solutions with the possibility of integrating multiple resources if required by the specific context of use. Healthcare professionals and patients can also use the VHT infrastructure for clinical decision support or personalised health forecasting. As the European Commission launched the EDITH coordination and support action to develop a roadmap for the development of the Virtual Human Twin, this position paper is intended as a starting point for the consensus process and a call to arms for all stakeholders.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2303.14255.pdf' target='_blank'>https://arxiv.org/pdf/2303.14255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Mullen, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14255">PACE: Data-Driven Virtual Agent Interaction in Dense and Cluttered Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PACE, a novel method for modifying motion-captured virtual agents to interact with and move throughout dense, cluttered 3D scenes. Our approach changes a given motion sequence of a virtual agent as needed to adjust to the obstacles and objects in the environment. We first take the individual frames of the motion sequence most important for modeling interactions with the scene and pair them with the relevant scene geometry, obstacles, and semantics such that interactions in the agents motion match the affordances of the scene (e.g., standing on a floor or sitting in a chair). We then optimize the motion of the human by directly altering the high-DOF pose at each frame in the motion to better account for the unique geometric constraints of the scene. Our formulation uses novel loss functions that maintain a realistic flow and natural-looking motion. We compare our method with prior motion generating techniques and highlight the benefits of our method with a perceptual study and physical plausibility metrics. Human raters preferred our method over the prior approaches. Specifically, they preferred our method 57.1% of the time versus the state-of-the-art method using existing motions, and 81.0% of the time versus a state-of-the-art motion synthesis method. Additionally, our method performs significantly higher on established physical plausibility and interaction metrics. Specifically, we outperform competing methods by over 1.2% in terms of the non-collision metric and by over 18% in terms of the contact metric. We have integrated our interactive system with Microsoft HoloLens and demonstrate its benefits in real-world indoor scenes. Our project website is available at https://gamma.umd.edu/pace/.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2302.10186.pdf' target='_blank'>https://arxiv.org/pdf/2302.10186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karan Singla, Yeon-Jun Kim, Srinivas Bangalore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10186">E2E Spoken Entity Extraction for Virtual Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human-computer conversations, extracting entities such as names, street addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech ignoring the superfluous portions such as carrier phrases, or spell name entities. In the context of dialog from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step approach which first generates lexical transcriptions followed by text-based entity extraction for identifying spoken entities.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2301.02527.pdf' target='_blank'>https://arxiv.org/pdf/2301.02527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bianca Marques, Rui NÃ³brega, Carmen Morgado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02527">Avatar-centred AR Collaborative Mobile Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interaction with the physical environment and different users is essential to foster a collaborative experience. For this, we propose an interaction based on a central point represented by an Augmented Reality marker in which several users can capture the attention and interact with a virtual avatar. The interface provides different game modes, with various challenges, supporting a collaborative mobile interaction. The system fosters various group interactions with a virtual avatar and enables various tasks with playful and didactic components.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2209.01320.pdf' target='_blank'>https://arxiv.org/pdf/2209.01320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddarth Ravichandran, OndÅej Texler, Dimitar Dinev, Hyun Jae Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01320">Synthesizing Photorealistic Virtual Humans Through Cross-modal Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last few decades, many aspects of human life have been enhanced with virtual domains, from the advent of digital assistants such as Amazon's Alexa and Apple's Siri to the latest metaverse efforts of the rebranded Meta. These trends underscore the importance of generating photorealistic visual depictions of humans. This has led to the rapid growth of so-called deepfake and talking-head generation methods in recent years. Despite their impressive results and popularity, they usually lack certain qualitative aspects such as texture quality, lips synchronization, or resolution, and practical aspects such as the ability to run in real-time. To allow for virtual human avatars to be used in practical scenarios, we propose an end-to-end framework for synthesizing high-quality virtual human faces capable of speaking with accurate lip motion with a special emphasis on performance. We introduce a novel network utilizing visemes as an intermediate audio representation and a novel data augmentation strategy employing a hierarchical image synthesis approach that allows disentanglement of the different modalities used to control the global head motion. Our method runs in real-time, and is able to deliver superior results compared to the current state-of-the-art.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2203.09016.pdf' target='_blank'>https://arxiv.org/pdf/2203.09016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rachel Love, Edith Law, Philip R. Cohen, Dana KuliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.09016">Natural Language Communication with a Teachable Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conversational teachable agents offer a promising platform to support learning, both in the classroom and in remote settings. In this context, the agent takes the role of the novice, while the student takes on the role of teacher. This framing is significant for its ability to elicit the ProtÃ©gÃ© effect in the student-teacher, a pedagogical phenomenon known to increase engagement in the teaching task, and also improve cognitive outcomes. In prior work, teachable agents often take a passive role in the learning interaction, and there are few studies in which the agent and student engage in natural language dialogue during the teaching task. This work investigates the effect of teaching modality when interacting with a virtual agent, via the web-based teaching platform, the Curiosity Notebook. A method of teaching the agent by selecting sentences from source material is compared to a method paraphrasing the source material and typing text input to teach. A user study has been conducted to measure the effect teaching modality on the learning outcomes and engagement of the participants. The results indicate that teaching via paraphrasing and text input has a positive effect on learning outcomes for the material covered, and also on aspects of affective engagement. Furthermore, increased paraphrasing effort, as measured by the similarity between the source material and the material the teacher conveyed to the robot, improves learning outcomes for participants.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2103.10316.pdf' target='_blank'>https://arxiv.org/pdf/2103.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohith G, Madhu Vadali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.10316">A Quasi-centralized Collision-free Path Planning Approach for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel quasi-centralized approach for collision-free path planning of multi-robot systems (MRS) in obstacle-ridden environments. A new formation potential fields (FPF) concept is proposed around a virtual agent, located at the center of the formation which ensures self-organization and maintenance of the formation. The path of the virtual agent is centrally planned and the robots at the minima of the FPF are forced to move along with the virtual agent. In the neighborhood of obstacles, individual robots selfishly avoid collisions, thus marginally deviating from the formation. The proposed quasi-centralized approach introduces formation flexibility into the MRS, which enables MRS to effectively navigate in an obstacle-ridden workspace. Methodical analysis of the proposed approach and guidelines for selecting the FPF are presented. Results using a candidate FPF are shown that ensure a pentagonal formation effectively squeezes through a narrow passage avoiding any collisions with the walls.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2505.05475.pdf' target='_blank'>https://arxiv.org/pdf/2505.05475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonwoo Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05475">SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2411.18047.pdf' target='_blank'>https://arxiv.org/pdf/2411.18047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jennifer Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18047">The Trusted Caregiver: The Influence of Eye and Mouth Design Incorporating the Baby Schema Effect in Virtual Humanoid Agents on Older Adults Users' Perception of Trustworthiness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing proportion of the older adult population has made the smart home care industry one of the critical markets for virtual human-like agents. It is crucial to effectively promote a trustworthy human-computer partnership with older adults, enhancing service acceptance and effectiveness. However, few studies have focused on the facial features of the agents themselves, where the "baby schema" effect plays a vital role in enhancing trustworthiness. The eyes and mouth, in particular, attract most of the audience's attention and are especially significant. This study explores the impact of eye and mouth design on users' perception of trustworthiness. Specifically, a virtual humanoid agents model was developed, and based on this, 729 virtual facial images of children were designed. Participants (N=162) were asked to evaluate the impact of variations in the size and positioning of the eyes and mouth regions on the perceived credibility of these virtual agents. The results revealed that when the facial aspect ratio (width and height denoted as W and H, respectively) aligned with the "baby schema" effect (eye size at 0.25W, mouth size at 0.27W, eye height at 0.64H, eye distance at 0.43W, mouth height at 0.74H, and smile arc at 0.043H), the virtual agents achieved the highest facial credibility. This study proposes a design paradigm for the main facial features of virtual humanoid agents, which can increase the trust of older adults during interactions and significantly contribute to the research on the trustworthiness of virtual humanoid agents.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2408.10040.pdf' target='_blank'>https://arxiv.org/pdf/2408.10040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moshe BenBassat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10040">The Practimum-Optimum Algorithm for Manufacturing Scheduling: A Paradigm Shift Leading to Breakthroughs in Scale and Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Practimum-Optimum (P-O) algorithm represents a paradigm shift in developing automatic optimization products for complex real-life business problems such as large-scale manufacturing scheduling. It leverages deep business domain expertise to create a group of virtual human expert (VHE) agents with different "schools of thought" on how to create high-quality schedules. By computerizing them into algorithms, P-O generates many valid schedules at far higher speeds than human schedulers are capable of. Initially, these schedules can also be local optimum peaks far away from high-quality schedules. By submitting these schedules to a reinforced machine learning algorithm (RL), P-O learns the weaknesses and strengths of each VHE schedule, and accordingly derives reward and punishment changes in the Demand Set that will modify the relative priorities for time and resource allocation that jobs received in the prior iteration that led to the current state of the schedule. These cause the core logic of the VHE algorithms to explore, in the subsequent iteration, substantially different parts of the schedules universe and potentially find higher-quality schedules. Using the hill climbing analogy, this may be viewed as a big jump, shifting from a given local peak to a faraway promising start point equipped with knowledge embedded in the demand set for future iterations. This is a fundamental difference from most contemporary algorithms, which spend considerable time on local micro-steps restricted to the neighbourhoods of local peaks they visit. This difference enables a breakthrough in scale and performance for fully automatic manufacturing scheduling in complex organizations. The P-O algorithm is at the heart of Plataine Scheduler that, in one click, routinely schedules 30,000-50,000 tasks for real-life complex manufacturing operations.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2406.01056.pdf' target='_blank'>https://arxiv.org/pdf/2406.01056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Mandava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01056">Virtual avatar generation models as world navigators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SABR-CLIMB, a novel video model simulating human movement in rock climbing environments using a virtual avatar. Our diffusion transformer predicts the sample instead of noise in each diffusion step and ingests entire videos to output complete motion sequences. By leveraging a large proprietary dataset, NAV-22M, and substantial computational resources, we showcase a proof of concept for a system to train general-purpose virtual avatars for complex tasks in robotics, sports, and healthcare.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2304.01443.pdf' target='_blank'>https://arxiv.org/pdf/2304.01443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01443">Virtual Avatar Stream: a cost-down approach to the Metaverse experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Metaverse through VR headsets is a rapidly growing concept, but the high cost of entry currently limits access for many users. This project aims to provide an accessible entry point to the immersive Metaverse experience by leveraging web technologies. The platform developed allows users to engage with rendered avatars using only a web browser, microphone, and webcam. By employing the WebGL and MediaPipe face tracking AI model from Google, the application generates real-time 3D face meshes for users. It uses a client-to-client streaming cluster to establish a connection, and clients negotiate SRTP protocol through WebRTC for direct data streaming. Additionally, the project addresses backend challenges through an architecture that is serverless, distributive, auto-scaling, highly resilient, and secure. The platform offers a scalable, hardware-free solution for users to experience a near-immersive Metaverse, with the potential for future integration with game server clusters. This project provides an important step toward a more inclusive Metaverse accessible to a wider audience.
